---
title: "üì• Kansas Frontier Matrix ‚Äî Data Ingestion Pipeline & ETL Framework"
path: "docs/guides/data/ingestion-pipeline.md"
version: "v10.0.0"
last_updated: "2025-11-09"
review_cycle: "Quarterly / Autonomous"
commit_sha: "<latest-commit-hash>"
sbom_ref: "../../../releases/v10.0.0/sbom.spdx.json"
manifest_ref: "../../../releases/v10.0.0/manifest.zip"
data_contract_ref: "../../../docs/contracts/data-contract-v3.json"
telemetry_ref: "../../../releases/v10.0.0/focus-telemetry.json"
telemetry_schema: "../../../schemas/telemetry/data-ingestion-v1.json"
governance_ref: "../../../docs/standards/governance/ROOT-GOVERNANCE.md"
license: "CC-BY 4.0"
mcp_version: "MCP-DL v6.3"
---

<div align="center">

# üì• **Kansas Frontier Matrix ‚Äî Data Ingestion Pipeline & ETL Framework**
`docs/guides/data/ingestion-pipeline.md`

**Purpose:**  
Define standardized, FAIR+CARE-compliant **data ingestion and transformation workflows** for integrating raw, historical, and environmental data into the Kansas Frontier Matrix (KFM).  
This framework ensures **reproducibility, provenance, and ethical data handling** across tabular, raster, and vector datasets.

[![Docs ¬∑ MCP](https://img.shields.io/badge/Docs-MCP_v6.3-blue)](../../README.md)
[![License: CC-BY 4.0](https://img.shields.io/badge/License-CC--BY%204.0-green)](../../../LICENSE)
[![FAIR+CARE](https://img.shields.io/badge/FAIR%2BCARE-ETL_Compliance-orange)](../../../docs/standards/README.md)
[![Status](https://img.shields.io/badge/Status-Operational-brightgreen)](../../../releases/)
</div>

---

## üìò Overview

The KFM **ETL pipeline** (Extract‚ÄìTransform‚ÄìLoad) automates ingestion of diverse data sources‚Äîsuch as archives, maps, climate records, and census tables‚Äîinto harmonized formats.  
All stages include **schema validation**, **license tracking**, and **FAIR+CARE governance hooks** for provenance, energy usage, and consent metadata.

**Core Features**
- Multi-source ingestion (local, API, FTP, archives)
- Schema-aware transformations with geospatial support
- Automated FAIR+CARE validation and ledger integration
- Version-controlled datasets with checksum verification

---

## üóÇÔ∏è Directory Layout

```plaintext
src/pipelines/etl/
‚îú‚îÄ‚îÄ extract/                       # Source ingestion scripts
‚îÇ   ‚îú‚îÄ‚îÄ noaa_ingest.py             # NOAA climate data fetch
‚îÇ   ‚îú‚îÄ‚îÄ usgs_ingest.py             # USGS topo & hydrology data
‚îÇ   ‚îú‚îÄ‚îÄ dpla_ingest.py             # Historical document imports
‚îÇ   ‚îî‚îÄ‚îÄ census_ingest.py           # Demographic data import
‚îú‚îÄ‚îÄ transform/                     # Schema normalization
‚îÇ   ‚îú‚îÄ‚îÄ normalize_tabular.py       # Clean and unify column structures
‚îÇ   ‚îú‚îÄ‚îÄ reproject_vector.py        # CRS unification (EPSG:4326)
‚îÇ   ‚îî‚îÄ‚îÄ enrich_metadata.py         # Add lineage, license, and FAIR+CARE fields
‚îú‚îÄ‚îÄ load/                          # Storage and output steps
‚îÇ   ‚îú‚îÄ‚îÄ to_parquet.py              # Save tabular data
‚îÇ   ‚îú‚îÄ‚îÄ to_geoparquet.py           # Save vector data
‚îÇ   ‚îî‚îÄ‚îÄ to_cog.py                  # Convert rasters to COGs
‚îî‚îÄ‚îÄ governance/                    # Provenance + FAIR+CARE validation
    ‚îú‚îÄ‚îÄ validate_etl.py
    ‚îú‚îÄ‚îÄ ledger_writer.py
    ‚îî‚îÄ‚îÄ ethics_review.py
```

---

## üß© ETL Process Overview

```mermaid
flowchart TD
A["Extract (Raw Data)"] --> B["Transform (Normalize + Enrich)"]
B --> C["Validate (Schema + FAIR+CARE)"]
C --> D["Load (COG / Parquet / GeoParquet)"]
D --> E["Governance Ledger Record"]
E --> F["STAC/DCAT Catalog Publication"]
```

---

## ‚öôÔ∏è Step-by-Step Workflow

### 1Ô∏è‚É£ Extract Raw Data
```bash
python src/pipelines/etl/extract/noaa_ingest.py --year 2024 --state KS
python src/pipelines/etl/extract/usgs_ingest.py --layers "hydrology,terrain"
```

Output:  
`data/raw/noaa/2024/temperature.csv`  
`data/raw/usgs/hydrology/rivers.geojson`

### 2Ô∏è‚É£ Transform & Normalize
```bash
python src/pipelines/etl/transform/normalize_tabular.py \
  --input data/raw/noaa/2024/temperature.csv \
  --output data/work/tabular/noaa_temperature_clean.csv \
  --schema docs/contracts/data-contract-v3.json
```

Output:  
`data/work/tabular/noaa_temperature_clean.csv`

### 3Ô∏è‚É£ Load to Processed Workspace
```bash
python src/pipelines/etl/load/to_parquet.py \
  --input data/work/tabular/noaa_temperature_clean.csv \
  --output data/processed/tabular/noaa_temperature.parquet
```

Output:  
`data/processed/tabular/noaa_temperature.parquet`

### 4Ô∏è‚É£ Validate and Record Provenance
```bash
python src/pipelines/etl/governance/validate_etl.py \
  --input data/processed/tabular/noaa_temperature.parquet \
  --output reports/data/etl-validation.json
```

---

## üßæ Validation Metrics

| Category | Description | Target |
|-----------|--------------|--------|
| **Schema Conformance** | Columns, datatypes match contract | 100% |
| **License Compliance** | License field validated (SPDX/CC) | Pass |
| **Provenance Integrity** | SHA-256 recorded in ledger | Verified |
| **Ethical Review** | CARE review passed | Pass |
| **Energy Efficiency** | ETL job energy usage | < 1.5 kWh |

---

## üßÆ Example ETL Validation Record

```json
{
  "dataset_id": "kfm-noaa-temp-2024",
  "etl_stage": "load",
  "schema_compliance": "Pass",
  "provenance_sha256": "8f2cd0a3b1234...",
  "license": "CC-BY 4.0",
  "faircare_compliance": "Pass",
  "energy_metrics": { "joules": 4500, "carbon_gCO2e": 0.9 },
  "timestamp": "2025-11-09T12:00:00Z"
}
```

---

## ‚öñÔ∏è FAIR+CARE Integration

| Principle | Implementation | Evidence |
|------------|----------------|-----------|
| **Findable** | STAC/DCAT records for all outputs | `data/stac/*.json` |
| **Accessible** | Conditionally public datasets under open licenses | `manifest_ref` |
| **Interoperable** | ISO/OGC schemas for spatial + tabular data | `data-contract-v3.json` |
| **Reusable** | Metadata, provenance, and lineage preserved | `sbom_ref` |
| **Collective Benefit** | Datasets aid historical, ecological, and cultural research | FAIR+CARE Council |
| **Authority to Control** | CARE-compliant sensitive data masking | `data-generalization/README.md` |
| **Responsibility** | ETL telemetry logged in FAIR+CARE reports | `focus-telemetry.json` |
| **Ethics** | Publication gated by FAIR+CARE review | `faircare-validate.yml` |

---

## üß∞ CI/CD Integration

| Workflow | Function | Output |
|-----------|-----------|--------|
| `data-ingest.yml` | Automate extraction and staging | `reports/data/ingest-log.json` |
| `etl-validate.yml` | Validate transformations | `reports/data/etl-validation.json` |
| `faircare-validate.yml` | Audit ethical and FAIR+CARE compliance | `reports/faircare/data-audit.json` |
| `ledger-sync.yml` | Append provenance to governance ledger | `docs/standards/governance/LEDGER/data-ledger.json` |

---

## üßæ Governance Ledger Example

```json
{
  "ledger_id": "data-ledger-2025-11-09-001",
  "dataset": "noaa_temperature.parquet",
  "sha256": "3fe9c7a3a4b8...",
  "etl_pipeline": "src/pipelines/etl/transform/normalize_tabular.py",
  "faircare_status": "Pass",
  "auditors": ["FAIR+CARE Council"],
  "timestamp": "2025-11-09T12:00:00Z"
}
```

---

## üï∞Ô∏è Version History

| Version | Date | Author | Summary |
|----------|------|--------|----------|
| v10.0.0 | 2025-11-09 | Core Team | Comprehensive ETL guide with FAIR+CARE governance and telemetry integration |
| v9.7.0  | 2025-11-03 | A. Barta | Introduced initial multi-source ETL pipeline and validation schema |

---

<div align="center">

¬© 2025 Kansas Frontier Matrix Project  
Master Coder Protocol v6.3 ¬∑ FAIR+CARE Certified ¬∑ Diamond‚Åπ Œ© / Crown‚àûŒ© Ultimate Certified  

[Back to Data Guides](./README.md) ¬∑ [Governance Charter](../../../docs/standards/governance/ROOT-GOVERNANCE.md)

</div>

