---
title: "ğŸª Kansas Frontier Matrix â€” Cookiecutter Template for AI/ETL Pipelines (Diamondâ¹ Î© / CrownâˆÎ© Ultimate Certified)"
path: "docs/guides/pipelines/cookiecutter-kfm-ai-pipeline/README.md"
version: "v10.4.2"
last_updated: "2025-11-16"
review_cycle: "Quarterly Â· FAIR+CARE Council Oversight"
commit_sha: "<latest-commit-hash>"
sbom_ref: "../../../../releases/v10.4.2/sbom.spdx.json"
manifest_ref: "../../../../releases/v10.4.2/manifest.zip"
telemetry_ref: "../../../../releases/v10.4.2/pipeline-telemetry.json"
telemetry_schema: "../../../../schemas/telemetry/pipelines-cookiecutter-ai-v2.json"
governance_ref: "../../../standards/governance/ROOT-GOVERNANCE.md"
license: "MIT"
mcp_version: "MCP-DL v6.3"
markdown_protocol_version: "KFM-MDP v10.4.2"
status: "Active / Enforced"
doc_kind: "Template Guide"
intent: "cookiecutter-ai-etl"
fair_category: "F1-A1-I1-R1"
care_label: "C2-A2-R2-E1"
kfm_readme_template: "Platinum v7.1"
ci_enforced: true
---

<div align="center">

# ğŸª **Kansas Frontier Matrix â€” Cookiecutter Template for AI/ETL Pipelines**  
`docs/guides/pipelines/cookiecutter-kfm-ai-pipeline/README.md`

**Purpose**  
Provide the **official Cookiecutter scaffolding** for building new **AI**, **ETL**, **STAC**,  
**geospatial**, or **remote-sensing** pipelines inside the Kansas Frontier Matrix (KFM).  

All generated pipelines automatically conform to **FAIR+CARE v2**, **MCP-DL v6.3**,  
**STAC/DCAT**, **Neo4j**, **RDF/GeoSPARQL**, **Lineage v2**, **Telemetry v2**, and  
**Governance** requirements.

This template is the **only approved starting point** for creating new KFM pipelines.

</div>

---

# ğŸ“˜ Overview

The **Cookiecutter KFM AI Pipeline template** generates a pipeline that includes:

- âœ… Canonical directory structure:  
  `ingest â†’ preprocess â†’ analytics â†’ validate â†’ promote â†’ publish`
- ğŸ§¬ Lineage v2 stubs (PROV-O Â· GeoSPARQL Â· CIDOC Â· CARE v2)
- ğŸ§ª Great Expectations (GX) validation checkpoint stubs
- âš–ï¸ FAIR+CARE v2 masking, sovereignty checks, governance hooks
- ğŸ›° STAC/DCAT metadata creation helpers
- ğŸŒ Neo4j upsert + spatial indexing boilerplate
- ğŸ§  Optional AI module (agent-assisted transform / summarization)
- ğŸ“¡ Telemetry v2 emitters (energy, COâ‚‚, metrics, violations)
- ğŸ” SBOM + SLSA attestation placeholders
- ğŸ§± Idempotent Makefile tasks
- ğŸ›  CI workflow templates (lint, tests, schemas, governance)

The resulting project is **deployment-ready**, **testable**, **documented**, **governed**, and **reproducible**.

---

# ğŸ“ Directory Layout (Generated Pipeline Skeleton)

~~~text
{{ cookiecutter.project_slug }}/
â”œâ”€â”€ README.md                               # Pipeline-specific README
â”œâ”€â”€ pyproject.toml                          # Project metadata & dependencies
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ pipeline.config.yaml                # Domain/pipeline configuration
â”‚   â””â”€â”€ ai_prompt.txt                       # Optional AI prompt template
â”œâ”€â”€ ingest/
â”‚   â”œâ”€â”€ fetch.py                            # HTTP/S3/FS ingest logic
â”‚   â”œâ”€â”€ schema/
â”‚   â”‚   â””â”€â”€ ingest.schema.json              # Ingest manifest/data schema (JSON Schema)
â”‚   â””â”€â”€ utils.py                            # Shared ingest helpers
â”œâ”€â”€ preprocessing/
â”‚   â”œâ”€â”€ cloud_mask.py                       # Cloud/shadow/snow masking (optional)
â”‚   â”œâ”€â”€ reprojection.py                     # CRS transforms (GDAL/PROJ)
â”‚   â”œâ”€â”€ harmonize_gsd.py                    # Resolution harmonization
â”‚   â””â”€â”€ utils.py                            # Preprocessing helpers
â”œâ”€â”€ analytics/
â”‚   â”œâ”€â”€ ndvi.py                             # Example analytic (placeholder)
â”‚   â”œâ”€â”€ flood_extent.py                     # Example analytic (placeholder)
â”‚   â”œâ”€â”€ trend.py                            # Example analytic (placeholder)
â”‚   â””â”€â”€ utils.py                            # Analytics helpers
â”œâ”€â”€ validate/
â”‚   â”œâ”€â”€ great_expectations.yml              # GX config
â”‚   â”œâ”€â”€ checkpoints/
â”‚   â”‚   â””â”€â”€ pipeline_schema.yml             # Example checkpoint
â”‚   â””â”€â”€ expectations/
â”‚       â””â”€â”€ schema_{{ cookiecutter.project_slug }}.json  # Example expectations
â”œâ”€â”€ promote/
â”‚   â”œâ”€â”€ promote.py                          # Move staging â†’ processed (Validateâ†’Promote)
â”‚   â””â”€â”€ metadata.json                       # Promotion config/metadata stub
â”œâ”€â”€ publish/
â”‚   â”œâ”€â”€ stac_publish.py                     # STAC Items/Collections creation
â”‚   â”œâ”€â”€ neo4j_publish.py                    # Graph upsert logic (Neo4j)
â”‚   â””â”€â”€ rdf_export.py                       # RDF + GeoSPARQL export
â”œâ”€â”€ lineage/
â”‚   â”œâ”€â”€ build_lineage.py                    # Lineage v2 JSON-LD builder
â”‚   â”œâ”€â”€ lineage.context.jsonld              # JSON-LD context for KFM lineage
â”‚   â””â”€â”€ lineage.schema.json                 # Lineage schema for validation
â”œâ”€â”€ telemetry/
â”‚   â””â”€â”€ writer.py                           # Telemetry v2 writer (NDJSON)
â”œâ”€â”€ governance/
â”‚   â”œâ”€â”€ care_rules.json                     # CARE v2 rules config
â”‚   â”œâ”€â”€ sovereignty_masks.geojson           # Sovereignty AOI overlays (synthetic stub)
â”‚   â””â”€â”€ audit_hooks.py                      # Governance hooks for pipeline
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_ingest.py                      # Ingest smoke tests
â”‚   â”œâ”€â”€ test_preproc.py                     # Preprocessing tests
â”‚   â”œâ”€â”€ test_analytics.py                   # Analytics tests
â”‚   â”œâ”€â”€ test_validate.py                    # GX validation tests
â”‚   â”œâ”€â”€ test_publish.py                     # STAC/graph/RDF tests (stubs)
â”‚   â””â”€â”€ data/                               # Test fixtures
â””â”€â”€ Makefile                                # Idempotent entrypoints
~~~

---

# ğŸ§© Architecture Model (Conceptual Flow)

```text
Ingest â†’ Preprocess â†’ Analytics â†’ Validate (GX) â†’ Promote â†’ Publish â†’ Lineage â†’ Governance
````

* **Ingest** â€” Controlled data acquisition with ETag, checksums, manifests.
* **Preprocess** â€” Harmonize, clean, transform, enforce CARE early mask hints.
* **Analytics** â€” Domain-specific computation, hazard/climate/historical models.
* **Validate (GX)** â€” Schema + integrity + FAIR+CARE checks.
* **Promote** â€” Move from staging â†’ processed (Validateâ†’Promote pattern).
* **Publish** â€” STAC/DCAT/Neo4j/RDF exports (optional per pipeline).
* **Lineage** â€” Lineage v2 bundle (JSON-LD) linking everything.
* **Governance** â€” Append to governance ledger; integrate SBOM and SLSA.

---

# âš™ï¸ Required Cookiecutter Variables

The template prompts for:

| Variable              | Description                                                  |
| --------------------- | ------------------------------------------------------------ |
| `project_name`        | Human-readable project name                                  |
| `project_slug`        | Python package + directory name                              |
| `project_description` | Short description of the pipeline                            |
| `domain`              | `remote_sensing`, `hazards`, `hydrology`, `historical`, etc. |
| `care_label_default`  | Default CARE label: `public`, `sensitive`, or `restricted`   |
| `stac_collections`    | Comma-separated list of STAC collections to integrate with   |
| `ai_enabled`          | `y/n` â€” whether to include AI/agent modules                  |
| `use_github_actions`  | `y/n` â€” include CI workflow                                  |
| `license`             | Project license (default: MIT)                               |

---

# ğŸ§ª Validation & GX Integration

The template provides a baseline **Great Expectations** structure:

* `great_expectations.yml` â€” project config stub.
* `checkpoints/pipeline_schema.yml` â€” example checkpoint.
* `expectations/schema_{{ project_slug }}.json` â€” starter expectations.

### Standard expectation suites to extend:

* Schema expectations (required columns, types).
* Value range checks.
* Nullability rules.
* CARE-related expectations (flags, AOI intersection status).
* Temporal expectations (monotonic time for timeseries).

Pipelines instantiated from this template should then tighten these expectations based on domain requirements.

---

# âš–ï¸ Governance & CARE v2 Integration

The generated pipeline contains governance stubs in `governance/`:

* `care_rules.json` â€” defines CARE v2 policy profile for the pipeline.
* `sovereignty_masks.geojson` â€” synthetic AOI overlay example.
* `audit_hooks.py` â€” Python hooks to:

  * enrich metadata with CARE v2 fields
  * run sovereignty intersection checks
  * apply masking strategies (e.g., H3, centroid-only)
  * append governance entries to the ledger

Developers must:

* Configure `care_rules.json` per domain.
* Replace synthetic AOIs with real overlays in controlled repos.
* Integrate `audit_hooks.py` into preprocess / publish steps.

---

# ğŸ§¬ Lineage v2 Stubs

The `lineage/` folder contains:

* `build_lineage.py` â€” to generate lineage JSON-LD bundles.
* `lineage.context.jsonld` â€” KFM lineage context (PROV-O + GeoSPARQL + CARE v2).
* `lineage.schema.json` â€” schema for validating lineage bundles.

Every pipeline is expected to:

* Call `build_lineage.py` during Promote or Publish.
* Update the lineage bundle with:

  * input datasets
  * steps performed
  * transformations
  * CARE decisions
  * telemetry summary
  * published STAC/DCAT/Graph/RDF references

---

# ğŸ“¡ Telemetry v2

The `telemetry/writer.py` module:

* Writes Telemetry v2 entries into NDJSON:

```text
data/telemetry/<project_slug>.ndjson
```

* Fields include:

  * `pipeline`, `stage`, `run_id`
  * `status`
  * `duration_ms`
  * `rows_processed` / `pixels_processed`
  * `energy_wh`, `co2_g`
  * `care_violations`, `sovereigntyConflicts`
  * `errors[]`

These are aggregated into:

```text
releases/v10.4.2/pipeline-telemetry.json
```

The exact aggregator & schema must be wired in the main KFM repo once the pipeline is installed.

---

# ğŸ›° STAC / DCAT / Neo4j / RDF Exports

The template provides:

* `publish/stac_publish.py` â€” helper to write STAC Items/Collections.
* `publish/neo4j_publish.py` â€” helper to upsert Nodes and relationships.
* `publish/rdf_export.py` â€” helper to write RDF + GeoSPARQL TTL or JSON-LD.

These are **stubs**, deliberately minimal but:

* Use official KFM paths.
* Carry forward the `kfm:*` metadata.
* Accept published artifact and manifest as inputs.
* Should be extended with domain-specific mapping (e.g., Scenes, Places, Events).

---

# ğŸ§ª Tests & CI Templates

The template includes:

* Pytest tests in `tests/`:

  * `test_ingest.py`, `test_preproc.py`, `test_analytics.py`, `test_validate.py`, `test_publish.py`.
  * Schema and lineage tests.

* A CI workflow stub in `.github/workflows/ci.yml` that runs:

  * Python setup
  * `pip install -e .[dev]`
  * Lint (e.g., `ruff`)
  * Tests (`pytest`)
  * JSON Schema validation for `schemas/` and `lineage/`

Each new pipeline can adjust CI but **must keep governance checks** in place.

---

# ğŸš€ Usage

## 1. Generate a New Pipeline

From the repo root (or dedicated tools directory):

```bash
pip install cookiecutter
cookiecutter docs/guides/pipelines/cookiecutter-kfm-ai-pipeline
```

Answer prompts to define:

* `project_name`
* `project_slug`
* domain, CARE defaults, and publishing modes.

The scaffold will appear as:

```text
{{ project_slug }}/
```

You may then move or rename it under KFMâ€™s canonical pipeline space, e.g.:

```text
src/pipelines/{{ project_slug }}/
```

## 2. Initialize

```bash
cd {{ project_slug }}
python -m venv .venv
source .venv/bin/activate
pip install -e ".[dev]"
```

## 3. First Validation Run

```bash
make validate
make test
```

Fix any schema, CARE, or test failures before integrating into the main KFM repo.

---

# ğŸ§­ Relationship to `kfm-ai-pipeline-cookiecutter` Guide

This README documents the **template itself**.

For design rationale and higher-level architecture, see:

```text
docs/guides/pipelines/kfm-ai-pipeline-cookiecutter.md
```

That document explains:

* Why RunContext + ledger exist
* The conceptual Ingest â†’ Validate â†’ Transform (Agent) â†’ Publish lifecycle
* How to use the template within KFM pipelines and CI

---

# ğŸ•° Version History

| Version | Date       | Summary                                                                                           |
| ------: | ---------- | ------------------------------------------------------------------------------------------------- |
| v10.4.2 | 2025-11-16 | Upgraded to KFM-MDP v10.4.2; added Telemetry v2, CARE v2, lineage v2, updated paths & CI guidance |
| v10.3.1 | 2025-11-14 | Initial cookiecutter template documentation; aligned with FAIR+CARE, governance, telemetry, STAC  |

---

<div align="center">

**Kansas Frontier Matrix â€” Cookiecutter AI/ETL Pipeline Template (v10.4.2)**
Reproducible Pipelines Ã— FAIR+CARE v2 Ã— Provenance Ã— AI Safety Ã— STAC/DCAT/Graph/RDF
Â© 2025 Kansas Frontier Matrix â€” MIT License

</div>
