# ğŸ“š `docs/data/` â€” Data Documentation Hub (Runbooks + Contracts)

![Status](https://img.shields.io/badge/status-active-brightgreen)
![Provenance](https://img.shields.io/badge/provenance-first-blue)
![FAIR+CARE](https://img.shields.io/badge/ethics-FAIR%20%2B%20CARE-purple)
![Metadata](https://img.shields.io/badge/metadata-STAC%20%2B%20DCAT-orange)
![Lineage](https://img.shields.io/badge/lineage-W3C%20PROV-lightgrey)

Welcome to **KFMâ€™s data documentation layer** ğŸ§­  
This folder is the â€œrunbook shelfâ€ that explains **whatâ€™s in `data/`, where it came from, how it was processed, and how itâ€™s safe to use**.

> [!IMPORTANT]
> In KFM, *data is not â€œupload-and-forgetâ€*. Every dataset must be traceable and reviewed before it reaches databases, APIs, or UI.

---

## âœ¨ What belongs in `docs/data/`

This directory contains **human-readable** documentation for every data domain and dataset family:

- ğŸ“¦ **Domain runbooks**: `docs/data/<domain>/README.md`
- ğŸ§¾ **Source notes**: where raw files came from, licenses, citations, download steps
- ğŸ§ª **ETL notes**: how pipelines transform raw â†’ processed (including assumptions)
- ğŸ§± **Schema + contracts**: required fields, geometry rules, time semantics, CRS expectations
- âœ… **QA expectations**: validations, sanity checks, known issues, and edge cases
- ğŸ§° **Operational playbooks**: how to refresh data, backfill years, or rebuild derived layers

> [!TIP]
> If you add a new domain under `data/raw/<domain>/...`, you should add a matching runbook folder under `docs/data/<domain>/`.

---

## ğŸ§¬ The â€œTruth Pathâ€ (how data becomes usable)

KFM treats data like a **pipelineâ€“catalogâ€“databaseâ€“APIâ€“UI** system.  
This means raw files become trustworthy, explorable knowledge only after they pass through standardized stages.

```mermaid
flowchart LR
  A["ğŸ“¥ Raw sources<br/>data/raw/..."] --> B["ğŸ§ª ETL + normalization<br/>pipelines/..."]
  B --> C["âœ… Processed outputs<br/>data/processed/..."]
  C --> D["ğŸ§¾ Catalog metadata<br/>STAC/DCAT"]
  C --> E["ğŸ”— Provenance logs<br/>W3C PROV"]
  D --> F["ğŸ—„ï¸ Databases<br/>PostGIS / Neo4j"]
  E --> F
  F --> G["ğŸ§© API layer<br/>FastAPI / GraphQL"]
  G --> H["ğŸ—ºï¸ UI<br/>React + MapLibre (+ Cesium optional)"]
```

> [!WARNING]
> **No skipping steps.** Anything that bypasses metadata or provenance is treated as a broken contribution.

---

## ğŸ—‚ï¸ Where this fits in the repo

### ğŸ” Data lives in `data/` (repo root)
Youâ€™ll usually work across these folders:

```text
ğŸ“¦ Kansas-Frontier-Matrix/
â”œâ”€ ğŸ“ data/
â”‚  â”œâ”€ ğŸ“ raw/           # immutable source snapshots (exact downloads/scans)
â”‚  â”œâ”€ ğŸ“ work/          # optional intermediate artifacts (scratch / staging)
â”‚  â”œâ”€ ğŸ“ processed/     # cleaned + standardized outputs (ready for KFM)
â”‚  â”œâ”€ ğŸ“ catalog/       # dataset discovery metadata (STAC / DCAT)
â”‚  â””â”€ ğŸ“ provenance/    # lineage logs (W3C PROV-style)
â””â”€ ğŸ“ docs/
   â””â”€ ğŸ“ data/          # âœ… YOU ARE HERE: runbooks + contracts for each domain
```

> [!NOTE]
> Some modules may split catalog storage into dedicated subfolders (e.g., `data/stac/...`, `data/catalog/dcat/...`).  
> The key idea is the same: **catalog + provenance are mandatory â€œboundary artifacts.â€**

---

## ğŸ§© Data types KFM expects (and recommended formats)

KFM commonly ingests **geospatial + historical** sources. Expect a mix of:

### ğŸ—ºï¸ Vector data (points / lines / polygons)
- Preferred: **GeoJSON** for lightweight sharing & review diffs
- Better for bigger datasets: **GeoPackage (`.gpkg`)** or **FlatGeobuf (`.fgb`)**
- Always include: geometry type, CRS, and core identifiers

### ğŸ›°ï¸ Raster data (grids / imagery)
- Preferred: **Cloud-Optimized GeoTIFF (COG)** for scalable map rendering
- Alternative: GeoTIFF + sidecar metadata when needed

### â±ï¸ Time series + tabular
- Preferred: **Parquet** for analytical workflows
- Acceptable: CSV for smaller sources (especially if itâ€™s â€œas-downloadedâ€ raw)

### ğŸ§  Derived â€œevidence artifactsâ€
Outputs from OCR, models, simulations, or AI-assisted extraction are treated as **first-class datasets**:
- They must live in `data/processed/...`
- They must have catalog + provenance like everything else

---

## ğŸ§¾ Metadata requirements (catalog = discoverability)

Before a dataset is considered â€œpublished,â€ it needs **catalog records** describing:

- Title, summary, owner/maintainer
- Spatial extent (bbox/geometry)
- Temporal extent (date range + semantics)
- License + attribution (always)
- Links to raw sources and processing scripts
- Update cadence (if applicable)
- Sensitivity classification (if applicable)

Typical catalog patterns:
- **STAC** for geospatial discovery (items/collections)
- **DCAT** for dataset-level catalog publishing / portal alignment

> [!IMPORTANT]
> If a dataset canâ€™t be discovered via catalog metadata, it effectively â€œdoesnâ€™t existâ€ in KFM.

---

## ğŸ”— Provenance requirements (lineage = trust)

Every processed dataset should have an accompanying provenance document that answers:

- **What** raw sources were used? (filenames, checksums, URLs if public)
- **How** was it produced? (pipeline script + parameters + timestamps)
- **Who/what** produced it? (agents: pipeline + operator)
- **Which outputs** were generated? (paths in `data/processed/...`)
- **What changed** vs prior versions? (if an update)

### Minimal PROV mental model
- **Entities** = inputs/outputs (raw file(s), processed file(s))
- **Activity** = pipeline execution (run + timestamp)
- **Agent** = script + person (or CI job) that produced it

> [!WARNING]
> No provenance file = **red flag**. It means the dataset cannot be audited.

---

## ğŸ§° Contribution workflow (adding or updating data)

Hereâ€™s the standard sequence when adding a new dataset or updating an existing one:

1. ğŸ“¥ **Add raw source snapshot** under `data/raw/<domain>/...`
2. ğŸ§ª **Run or write a pipeline** under `pipelines/<domain>/...`
3. âœ… **Write processed outputs** to `data/processed/<domain>/...`
4. ğŸ§¾ **Generate/update catalog metadata** (STAC/DCAT)
5. ğŸ”— **Generate/update provenance logs** (PROV)
6. ğŸ§¹ **Validate + sanity check** (geometry validity, ranges, date parsing, etc.)
7. âœ… **Commit + PR** (CI should fail if metadata/provenance is missing)

> [!TIP]
> Use small PRs. Data PRs review best when diffs are readable and the runbook is complete.

---

## ğŸ§± Domain runbook template (`docs/data/<domain>/README.md`)

Create a runbook per domain using a consistent outline:

```markdown
# ğŸ§­ <Domain Name>

## Overview
- What this domain represents
- Why it exists in KFM

## Source Inventory
| Source | Type | License | Where stored (raw) | Notes |
|---|---:|---|---|---|

## Processing Pipeline
- Entry script(s)
- Key transformations (units, joins, CRS handling)
- Output dataset IDs

## Outputs (Processed)
| Dataset ID | Path | Format | Geometry | Time range |
|---|---|---:|---:|---:|

## Metadata & Provenance
- STAC/DCAT locations
- PROV location + required fields

## QA / Validation
- Checks performed
- Known failure modes

## Update Strategy
- How updates happen (append, backfill, rebuild)
- Versioning expectations

## Maintainers
- Who to contact
```

---

## â“ FAQ (common â€œgotchasâ€)

**Q: Can I load raw data directly into PostGIS/Neo4j?**  
A: No. Raw is immutable evidence. Only processed + documented datasets are eligible.

**Q: Where should intermediate files go?**  
A: Use `data/work/<domain>/...` for scratch artifacts you donâ€™t want treated as final datasets.

**Q: Do I really need both catalog and provenance?**  
A: Yes. Catalog = discovery; provenance = trust.

---

## ğŸ”— Suggested â€œnext docsâ€ to link from here

- `docs/architecture/system_overview.md` (end-to-end design)
- `docs/governance/` (licenses, sensitivity tiers, review rules)
- `pipelines/README.md` (how to run ETL)
- `data/README.md` (if present: canonical data layout at repo root)

---

## ğŸ“Œ House rules (short version)

- ğŸ§¾ **No license â†’ no merge**
- ğŸ”— **No provenance â†’ no trust**
- ğŸ§­ **No catalog â†’ no discovery**
- âœ… **No validation â†’ no confidence**
- ğŸ§± **No runbook â†’ no maintainability**

---

ğŸ§  If youâ€™re unsure where something belongs:  
**Put evidence in `data/raw/`, publish in `data/processed/`, document it in `docs/data/`, and let CI enforce the rest.**