---
title: "ðŸ§­ðŸ¤– AI Architecture â€” Focus Mode (KFM)"
path: "docs/architecture/ai/README.md"
version: "v0.1.0"
last_updated: "2026-01-26"
status: "draft"
doc_kind: "Architecture"
keywords:
  - Focus Mode
  - Provenance-First
  - RAG
  - Knowledge Graph
  - Ollama
  - OPA
  - FAIR
  - CARE
---

# ðŸ§­ðŸ¤– AI Architecture â€” Focus Mode

`ðŸ§¬ Provenance-First` `ðŸ” Hybrid RAG` `ðŸ—ºï¸ Geo+Time Context` `ðŸ›¡ï¸ Policy-as-Code (OPA)` `ðŸ¦™ Ollama Runtime` `ðŸ§ª Reproducible Analytics`

> [!IMPORTANT]
> **Focus Mode is advisory-only.** It never takes autonomous actions and it never â€œfreewheelsâ€ beyond KFMâ€™s data. If it canâ€™t cite sources, it must say so.

---

## ðŸ“Œ Purpose

This document describes how KFMâ€™s AI capabilities are designed and integrated, focusing on:

- **Focus Mode** (user-facing Q&A + analysis assistant)
- **Evidence packaging** (citations, provenance, explainability)
- **Governance** (prompt filtering + OPA output policy layer)
- **Ollama integration** (local-first LLM runtime + embedding workflow)
- **Related AI subsystems** (Watcherâ†’Plannerâ†’Executor automation + analytics modules)

---

## ðŸŽ¯ Scope

| âœ… In scope | ðŸš« Out of scope |
|---|---|
| Focus Mode pipeline & contracts | UI component styling specifics |
| Retrieval architecture (graph + DB + doc search + vectors) | Full ingestion pipeline internals (see data pipeline docs) |
| Governance layers (Prompt Gate + OPA) | Detailed Rego policy pack contents |
| Provenance & citation rules | Concrete dataset schemas (STAC/DCAT/PROV specifics live elsewhere) |
| Ollama runtime patterns (model config, embeddings, caching) | Picking â€œthe bestâ€ model family (decision is environment-dependent) |

---

## ðŸ‘¥ Audience

- ðŸ§‘â€ðŸ’» Backend engineers (FastAPI, retrieval, policy, logging)
- ðŸ§‘â€ðŸŽ¨ Frontend engineers (Focus Mode panel integration + citations UX)
- ðŸ§‘â€ðŸ”¬ Data/ML engineers (embeddings, evaluation, reproducible analytics)
- ðŸ§‘â€âš–ï¸ Governance/security reviewers (OPA rules, sensitive-data constraints)

---

## ðŸ§  Glossary

- **Focus Mode**: KFMâ€™s integrated AI assistant for context-aware Q&A with citations.
- **Evidence Pack**: The curated set of snippets/records/graph paths used to answer a question.
- **Hybrid RAG**: Retrieval across multiple stores (graph + spatial DB + full-text + embeddings).
- **Prompt Gate**: Input filtering stage to neutralize prompt-injection / policy violations.
- **OPA**: Open Policy Agent; evaluates AI output + context to allow/deny/redact.
- **PROV / Provenance record**: Machine-readable trace of what sources + processing created an output.
- **W-P-E**: Watcherâ†’Plannerâ†’Executor automation loop (maintenance + pipeline ops).

---

## ðŸ—ºï¸ Quick Mental Model

Focus Mode is a **thin UI client** plus a **server-side orchestration pipeline**:

1. UI sends **question + current context** (map extent, layers, timeline, story state)
2. Backend runs **Prompt Gate** â†’ **retrieval** â†’ **LLM synthesis** â†’ **OPA check**
3. Backend returns **structured answer + references + provenance ID**
4. UI renders answer with **footnotes/clickable citations** + (optional) **audit/explain panel**

---

## ðŸ§± Architecture Overview

### ðŸ§© C4-ish System Context

```mermaid
flowchart LR
  U((User)) --> UI[ðŸ–¥ï¸ Web UI\nFocus Mode Panel]
  UI --> API[ðŸ§° API Orchestrator\nFastAPI: /focus-mode/query]

  API --> PG[ðŸ§¼ Prompt Gate\nInput Sanitization]
  API --> RET[ðŸ”Ž Retrieval Layer\nHybrid Evidence Fetch]
  RET --> KG[(ðŸ•¸ï¸ Neo4j\nKnowledge Graph)]
  RET --> GIS[(ðŸ—ºï¸ PostGIS\nSpatial DB)]
  RET --> TXT[(ðŸ“š Full-text Index\nDocs & narratives)]
  RET --> VEC[(ðŸ§² Vector Store\nEmbeddings)]

  API --> LLM[ðŸ¦™ Ollama\nLLM Runtime]
  API --> OPA[ðŸ›¡ï¸ OPA\nPolicy Engine]
  API --> PROV[â›“ï¸ Provenance Service\nCitations + Audit Trail]
  API --> TEL[ðŸ“ˆ Focus Telemetry\nUsage + Latency + Sustainability]

  API --> UI
```

### ðŸ§  Why the split?

- **UI stays modular** âœ… (Focus Mode is additive, not entangled across the app)
- **Backend owns trust** âœ… (citations + policy enforcement happen server-side)
- **LLM remains replaceable** âœ… (Ollama is a pluggable runtime, not the architecture)

---

## ðŸ”„ Focus Mode Pipeline

### âœ… Step-by-step (contractual)

1. **Context Gathering** ðŸ—ºï¸  
   Collect viewport, selected feature(s), active layers, timeline range, current story node.

2. **Prompt Gate (input)** ðŸ§¼  
   Strip/neutralize prompt injection and block policy-violating content.

3. **Intent + Query Plan** ðŸ§­  
   Identify entity/time/space constraints and select retrieval routes (graph vs spatial vs docs vs vectors).

4. **Hybrid Retrieval** ðŸ”Ž  
   Pull:
   - graph paths (relationships + entities)
   - spatial features/aggregates
   - document snippets (full-text)
   - semantically similar passages (vector)

5. **Evidence Pack Assembly** ðŸ“¦  
   Normalize all retrieved items into a consistent structure (each with provenance pointers).

6. **Prompt Construction** ðŸ§   
   Compose system + task prompt + evidence pack, with strict citation rules.

7. **LLM Synthesis (Ollama)** ðŸ¦™  
   Generate a draft answer with citation markers (e.g., `[1] [2]`).

8. **Post-processing** ðŸ§¾  
   Validate citation coverage, map markers â†’ reference objects, format answer.

9. **OPA Policy Check (output)** ðŸ›¡ï¸  
   Allow/deny/redact + produce user-facing explanation if restricted.

10. **Provenance Record + Telemetry** â›“ï¸ðŸ“ˆ  
   Persist provenance and log metrics (including sustainability metrics if enabled).

### ðŸ” Sequence Diagram

```mermaid
sequenceDiagram
  participant UI as ðŸ–¥ï¸ UI (Focus Mode Panel)
  participant API as ðŸ§° FastAPI Orchestrator
  participant PG as ðŸ§¼ Prompt Gate
  participant RET as ðŸ”Ž Retrieval Layer
  participant LLM as ðŸ¦™ Ollama
  participant OPA as ðŸ›¡ï¸ OPA
  participant PROV as â›“ï¸ Provenance
  participant TEL as ðŸ“ˆ Telemetry

  UI->>API: POST /focus-mode/query (question + context)
  API->>PG: sanitize(question + context)
  PG-->>API: safe_question + safe_context

  API->>RET: retrieve(safe_question, safe_context)
  RET-->>API: evidence_pack + references[]

  API->>LLM: generate(prompt + evidence_pack)
  LLM-->>API: draft_answer (with [n] citations)

  API->>OPA: evaluate(answer + refs + role + sensitivity)
  OPA-->>API: allow / redact / deny (+ reason)

  API->>PROV: write(prov_record)
  PROV-->>API: provenance_id

  API->>TEL: emit(metrics)
  TEL-->>API: ok

  API-->>UI: answer + references + provenance_id (+ policy notes)
```

---

## ðŸ§© Core Components

## 1) ðŸ–¥ï¸ Frontend â€” Focus Mode Panel

**Design goals**
- â€œThin clientâ€ that only orchestrates UI state + API calls
- Provide the backend with *enough context* to narrow retrieval
- Render citations as **clickable footnotes** and support **audit/explain UI**

**Context you should send (minimum)**
- `viewport`: bounding box + zoom/center
- `timeline`: selected year or range
- `layers`: list of active layer IDs
- `selection`: selected feature IDs (county, point feature, story node)
- `story`: active story node ID (if any)
- `session`: conversation ID + (optional) short chat history

> [!TIP]
> Add â€œAsk Focus Modeâ€ entry points on dataset pages and story nodes (contextual Q&A). It keeps questions precise and reduces retrieval load.

---

## 2) ðŸ§° API Orchestrator â€” `/focus-mode/query`

The orchestrator is the â€œcontract ownerâ€ for Focus Mode:

- âœ… owns retrieval routing + evidence pack schema
- âœ… owns citation enforcement + refusal behavior
- âœ… owns OPA policy evaluation + redaction messaging
- âœ… owns provenance recording + telemetry events

**Design rule**
- UI never talks to the model directly.
- Model never talks to DBs directly.
- Retrieval + governance happen server-side only.

---

## 3) ðŸ”Ž Retrieval Layer â€” Hybrid RAG

Focus Mode uses **multiple truth stores**, each good at something:

### ðŸ•¸ï¸ Knowledge Graph (Neo4j)
Best for:
- entity relationships (people â†” places â†” events â†” organizations)
- multi-hop reasoning (via retrieval traversal, not â€œLLM guessingâ€)
- disambiguation (same name, different entity)

### ðŸ—ºï¸ Spatial DB (PostGIS)
Best for:
- spatial containment/intersection queries
- aggregations (counts, totals, histograms)
- feature discovery within map viewport/time filters

### ðŸ“š Full-text Index
Best for:
- matching exact phrases (quotes, names, dates)
- story/narrative documents, scanned-text sources (once indexed)
- quick â€œfind the paragraphâ€ evidence

### ðŸ§² Vector Store (Embeddings)
Best for:
- semantic similarity (â€œthis sounds like thatâ€)
- fuzzy matches across historical phrasing
- thematic retrieval when keywords donâ€™t match

---

## 4) ðŸ“¦ Evidence Pack Contract

Focus Modeâ€™s reliability depends on a strict evidence schema.

Every evidence item should contain:

- `ref_id`: stable reference ID (dataset/doc/node)
- `ref_type`: `dataset | document | graph_node | query_result`
- `title`: human-readable label
- `excerpt`: short snippet used in the answer
- `spatiotemporal`: optional bbox + time range
- `provenance`: source pointer(s), license, version/digest, checksum if available
- `sensitivity`: public / sensitive / restricted
- `retrieval_trace`: how we got it (query used, rank score, etc.)

> [!NOTE]
> Think of the evidence pack as a **mini bibliography** + **lab notebook** for the answer.

---

## 5) ðŸ¦™ LLM Runtime â€” Ollama

Ollama is treated like an **LLM runtime service**:
- local-first (offline-friendly)
- model switching via tags
- supports **Modelfiles** for templating + context settings
- can run embedding models for semantic search

### ðŸ§  Context size + caching
- Prefer models with larger context windows for â€œevidence-heavyâ€ answers.
- Configure `num_ctx` in Modelfile when appropriate.
- Cache at the **application layer**:
  - frequently asked question contexts
  - repeated evidence packs for similar map/time queries
  - short-term session memory packs

### ðŸ§¬ Model customization
- Use Modelfiles to define:
  - base model (`FROM`)
  - optional LoRA adapters (`ADAPTER`)
  - system prompt + formatting rules (`SYSTEM`, `TEMPLATE`)
  - context window (`PARAMETER num_ctx ...`)

### ðŸ§² Embeddings + vector store
A typical pattern:
1. Use an Ollama embedding model to embed documents/chunks
2. Store vectors in a dedicated vector store
3. Retrieve nearest neighbors and include them as evidence items

---

## 6) ðŸ›¡ï¸ Governance â€” Prompt Gate + OPA

## Prompt Gate (Input) ðŸ§¼
Goals:
- remove/neutralize prompt injection attempts
- block disallowed queries early (policy violations)
- normalize and de-risk user text before retrieval/LLM calls

## OPA (Output) ðŸ›¡ï¸
OPA evaluates the **draft answer + references + user role + sensitivity** to decide:

- âœ… allow
- âš ï¸ redact (with explanation)
- â›” deny/refuse (with explanation)

Typical rule families:
- **No citation â†’ no answer**
- **No precise locations for sensitive sites**
- **No personal identifying details**
- **Role-based access constraints**
- **â€œSay no when you donâ€™t knowâ€** (no hallucinated completions)

> [!IMPORTANT]
> The policy layer must be updateable **without changing AI code** (policy-as-code workflow).

---

## 7) â›“ï¸ Provenance & Citation Enforcement

**Hard rule:** If a claim cannot be traced to evidence, it does not get said.

### Citation UX conventions
- Answer contains numbered markers: `[1] [2] ...`
- Response includes `references[]` with full metadata
- UI renders citations as footnotes/clickable links/popovers

### Provenance record content (minimum)
- question + sanitized question
- user context snapshot (map/time/layers/story)
- evidence pack IDs + digests/versions if available
- model identifier (name/tag) + prompt template version
- policy decision output
- timestamps + request IDs (traceability)

---

## 8) ðŸ“ˆ Observability â€” Focus Telemetry

Focus Telemetry should capture:
- usage frequency
- latency breakdown (retrieval vs generation vs policy)
- cache hits/misses
- policy denies/redactions (counts + reasons)
- sustainability metrics (energy/carbon if enabled)
- quality signals (citation coverage %, refusal rate, user feedback)

> [!TIP]
> Use telemetry thresholds to trigger scaling (e.g., add caching or scale LLM instances).

---

## 9) ðŸ¤– Maintenance AI â€” Watcherâ†’Plannerâ†’Executor (W-P-E)

Focus Mode is **user-facing AI**. W-P-E is **ops-facing AI**.

- **Watcher** detects issues (new data available, schema drift, vulnerabilities, governance gaps)
- **Planner** proposes a safe plan (policy-constrained)
- **Executor** runs changes (pipelines / PRs) with final policy gate + human review

Key safeguards:
- **idempotency** (donâ€™t loop or spam)
- **global kill switch**
- **OPA-governed agent behavior**
- **human-in-the-loop merges**
- **supply-chain integrity** (SBOM, signatures, attestations)

---

## 10) ðŸ§ª AI Analytics Modules (beyond Q&A)

KFMâ€™s AI architecture is extensible to:
- remote sensing + ML pipelines (deterministic, reproducible)
- scenario/simulation â€œwhat-ifâ€ models (treated as annotated experiments)
- model governance workflows (approval, audit trail, reproducible environments)

---

## ðŸ”Œ API Contract (Recommended)

> [!NOTE]
> This is an **architecture-level contract**. Align field names with actual code when implementing.

### Request (example)

```json
{
  "question": "What happened here around this time?",
  "context": {
    "viewport_bbox": [-101.2, 37.0, -94.6, 40.1],
    "timeline": { "start": "1930-01-01", "end": "1939-12-31" },
    "active_layers": ["drought_index", "county_boundaries"],
    "selected_features": [
      { "type": "county", "id": "ks_finney" }
    ],
    "story_node_id": "dust-bowl-intro"
  },
  "session": {
    "conversation_id": "uuid-123",
    "history": [
      { "role": "user", "content": "Show me drought trends." }
    ]
  },
  "user": { "role": "public" },
  "model": { "name": "kfm-local-llm", "mode": "balanced" }
}
```

### Response (example)

```json
{
  "answer_markdown": "In the 1930s, this county experienced severe drought impacts... [1][2]",
  "references": [
    {
      "ref": 1,
      "ref_type": "dataset",
      "id": "stac:ks:drought:1930s:v3",
      "title": "Kansas Drought Index (1930â€“1939)",
      "excerpt": "1934â€“1936 values show sustained drought severity...",
      "provenance": {
        "prov_id": "prov:abc123",
        "license": "CC-BY-4.0",
        "digest": "sha256:..."
      },
      "spatiotemporal": {
        "bbox": [-100.9, 37.7, -100.0, 38.3],
        "time_range": { "start": "1930-01-01", "end": "1939-12-31" }
      }
    }
  ],
  "provenance_id": "prov:focusmode:run:xyz789",
  "policy": {
    "status": "allowed",
    "notes": []
  },
  "telemetry": {
    "latency_ms": 2840,
    "cache_hit": false
  }
}
```

---

## ðŸ—‚ï¸ Suggested Backend Layout (AI module)

```text
KFM-Backend/
â”œâ”€â”€ api/
â”‚   â””â”€â”€ routes/
â”‚       â””â”€â”€ focus_mode.py         # POST /focus-mode/query
â”œâ”€â”€ ai/
â”‚   â”œâ”€â”€ focus_pipeline.py         # Orchestration pipeline
â”‚   â”œâ”€â”€ ollama_client.py          # Ollama API wrapper
â”‚   â”œâ”€â”€ retrieval.py              # Graph/DB/search/vector retrieval
â”‚   â”œâ”€â”€ prompt_templates/
â”‚   â”‚   â””â”€â”€ focus_mode.txt        # System + task prompt templates
â”‚   â”œâ”€â”€ policy_checks.py          # OPA integration + local checks
â”‚   â””â”€â”€ provenance.py             # provenance records & reference formatting
â””â”€â”€ telemetry/
    â””â”€â”€ focus_telemetry.py        # metrics/events
```

---

## ðŸ” Security & Privacy Checklist

### âœ… Must-have controls
- Prompt Gate on every request
- OPA policy evaluation on every response
- AI runs sandboxed with **whitelisted tool access only**
- Secrets/credentials never enter prompts
- Rate limiting + abuse controls on the API
- Audit logging for governance review

### ðŸ•µï¸ Privacy & inference control (design posture)
- Prefer aggregation/generalization for sensitive spatial features
- Consider query auditing patterns to reduce inference attacks (especially on small groups)

---

## âœ… Testing & Evaluation Checklist

### ðŸ§ª Functional correctness
- retrieval unit tests (graph, spatial, text, vector)
- evidence pack schema validation tests
- response schema tests (answer + references always well-formed)

### ðŸ§¾ Trust & safety
- golden test suite: â€œno citation â†’ refusalâ€
- policy tests: OPA rules (allow/deny/redact) with fixtures
- prompt injection tests (Prompt Gate should neutralize)

### ðŸ“‰ Regression & drift
- benchmark latency per environment
- monitor refusal/redaction rates
- track citation coverage % and â€œunknownâ€ responses

---

## ðŸ§­ Roadmap Hooks

- ðŸŒ Federation across multiple â€œFrontier Matrixâ€ instances (cross-region Q&A)
- ðŸ§  Multi-model routing (fast vs deep modes)
- ðŸ§© Expanded analytics modules (remote sensing, what-if simulation)
- ðŸ§‘â€ðŸ¤â€ðŸ§‘ Community governance tooling (policy packs + review workflows)
- ðŸ—ºï¸ 4D/3D + AR experiences where AI citations remain first-class

---

## ðŸ“š Sources of Truth in This Repo

> [!TIP]
> Treat these as **design contracts**. If code disagrees, either fix the code or create an ADR explaining why.

### ðŸ§¾ Core KFM Architecture Docs
- `Kansas Frontier Matrix (KFM) â€“ AI System Overview ðŸ§­ðŸ¤–.pdf`
- `KFM AI Infrastructure â€“ Ollama Integration Overview.pdf`
- `ðŸ“š Kansas Frontier Matrix (KFM) â€“ Expanded Technical & Design Guide.pdf`
- `Kansas Frontier Matrix (KFM) â€“ Comprehensive Technical Documentation.pdf`
- `Kansas Frontier Matrix (KFM) â€“ Comprehensive Platform Overview and Roadmap.pdf`
- `Kansas Frontier Matrix (KFM) â€“ Comprehensive Architecture, Features, and Design.pdf`
- `Kansas Frontier Matrix (KFM) â€“ Comprehensive UI System Overview (Technical Architecture Guide).pdf`
- `Kansas-Frontier-Matrix_ Open-Source Geospatial Historical Mapping Hub Design.pdf`

### ðŸ§ª Research + Engineering Library (Project Files)
<details>
<summary>ðŸ“¦ Supplemental PDFs & protocols (click to expand)</summary>

- `Scientific Method _ Research _ Master Coder Protocol Documentation.pdf` (reproducibility + documentation rigor)
- `Data Mining Concepts and Applications.pdf` (privacy, inference control, query auditing patterns)
- `KFM- python-geospatial-analysis-cookbook.pdf` (geospatial workflows + visualization patterns)

**PDF Portfolios (multi-doc bundles â€” open with a PDF portfolio-capable viewer):**
- `AI Concepts & more.pdf`
- `Maps-GoogleMaps-VirtualWorlds-Archaeological-Computer Graphics-Geospatial-webgl.pdf`
- `Various programming langurages & resources 1.pdf`
- `Data Managment-Theories-Architures-Data Science-Baysian Methods-Some Programming Ideas.pdf`
- `Mapping-Modeling-Python-Git-HTTP-CSS-Docker-GraphQL-Data Compression-Linux-Security.pdf`
- `Geographic Information-Security-Git-R coding-SciPy-MATLAB-ArcGIS-Apache Spark-Type Script-Web Applications.pdf`

</details>

---

## âœ… Next Actions (Recommended)

- [ ] Create/confirm the **evidence pack schema** (JSON Schema) and enforce it in CI
- [ ] Add a minimal **OPA policy pack** that enforces:
  - citations required
  - sensitive locations redacted
  - no PII
- [ ] Implement a **golden test suite** for Focus Mode (citations + refusal behavior)
- [ ] Add **telemetry dashboards** for latency, redactions, refusal rate, and cache efficacy
- [ ] Add ADRs for:
  - Ollama selection rationale
  - vector store selection
  - evidence pack contract versioning

---