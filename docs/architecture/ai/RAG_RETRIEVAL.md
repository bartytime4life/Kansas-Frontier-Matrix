# ğŸ§  RAG Retrieval (Focus Mode) ğŸ”ğŸ“š

![Docs](https://img.shields.io/badge/docs-architecture%2Fai-blue)
![Pattern](https://img.shields.io/badge/pattern-RAG%20%2B%20Hybrid%20Retrieval-purple)
![LLM](https://img.shields.io/badge/LLM-Ollama-orange)
![Governance](https://img.shields.io/badge/governance-OPA%20%2B%20PROV-green)
![Principle](https://img.shields.io/badge/principle-evidence--first-brightgreen)

> [!NOTE]
> This document defines **how retrieval works** for KFMâ€™s **Focus Mode** (Retrieval-Augmented Generation). It complements:
> - ğŸ“„ `docs/architecture/ai/OLLAMA_INTEGRATION.md`
> - ğŸ“„ `docs/architecture/AI_SYSTEM_OVERVIEW.md`
> - ğŸ“„ `docs/architecture/system_overview.md`

---

## ğŸ¯ Goals

KFMâ€™s RAG retrieval is designed to:

- âœ… Produce **evidence-backed** answers (not â€œbest guessesâ€)
- âœ… Keep context **compact + high-signal** (no full-article dumps)
- âœ… Provide **traceable citations** and clickable provenance
- âœ… Enforce **governance** (RBAC, sensitivity, safety) at runtime
- âœ… Make retrieval + prompting **debuggable** (no black boxes)

KFMâ€™s Focus Mode pipeline explicitly treats retrieval as multi-channel (Neo4j + PostGIS + full-text + vector similarity).:contentReference[oaicite:0]{index=0}

---

## ğŸ—ï¸ Where Retrieval Lives in the Architecture

High-level responsibilities:

- ğŸ–¥ï¸ **UI**: sends question + current map context (does **not** talk to models directly)
- ğŸ§  **API Orchestrator (FastAPI)**: sanitizes â†’ retrieves â†’ assembles prompt â†’ calls LLM â†’ policy gate â†’ returns answer + citations
- ğŸ—ƒï¸ **Datastores** remain the sources of truth: Neo4j, PostGIS, search index (incl. vectors), etc.:contentReference[oaicite:1]{index=1}

Focus Mode retrieval is part of the orchestrated pipeline from user question â†’ retrieval â†’ LLM generation.:contentReference[oaicite:2]{index=2}

---

## ğŸ”„ End-to-End Flow

```mermaid
flowchart TD
  U["ğŸ§‘ User Question"] --> PG["ğŸ§¼ Prompt Gate â€” sanitize input"]
  PG --> PI["ğŸ§­ Parse intent â€” entities, dates, location"]
  PI --> G["ğŸ•¸ï¸ Neo4j â€” graph context"]
  PI --> S["ğŸ—ºï¸ PostGIS â€” spatial/stat context"]
  PI --> FT["ğŸ” Full-text search â€” keywords & phrases"]
  PI --> VS["ğŸ§² Vector search â€” embedding similarity"]
  G --> M["ğŸ§© Merge + rank + dedupe â€” evidence bundle"]
  S --> M
  FT --> M
  VS --> M
  M --> PA["ğŸ§¾ Context assembly â€” sources 1..N"]
  PA --> LLM["ğŸ¤– Ollama â€” POST /api/generate"]
  LLM --> OPA["ğŸ›¡ï¸ OPA policy gate â€” citations + safety + RBAC"]
  OPA -->|"allow"| PP["ğŸ§· Post-process â€” attach citations; write provenance"]
  OPA -->|"deny"| SAFE["ğŸš« Safe fallback â€” or ask clarification"]
  PP --> UI["ğŸ“¤ Return answer + citations"]
  PP --> LEDGER["ğŸ“’ Append-only ledger / PROV logs"]
```

The pipeline includes:
- **Prompt sanitization** before retrieval (Prompt Gate):contentReference[oaicite:3]{index=3}
- **Multi-channel retrieval** for evidence:contentReference[oaicite:4]{index=4}
- **Prompt assembly with numbered sources** and explicit citation instructions:contentReference[oaicite:5]{index=5}
- **OPA policy gate** after generation:contentReference[oaicite:6]{index=6}

---

## ğŸ§¼ Prompt Gate (Input Sanitization)

Before any retrieval or prompting, KFM sanitizes the raw user question:

- ğŸš« Neutralizes prompt injection attempts
- ğŸš« Removes/encodes disallowed instructions
- ğŸš« Filters profanity (and other disallowed content classes)

This prevents the LLM from seeing malicious content or hidden instructions.:contentReference[oaicite:7]{index=7}

> [!IMPORTANT]
> Retrieval is only as safe as its *inputs*. Prompt Gate is the front door.

---

## ğŸ—‚ï¸ Retrieval Channels (Evidence-First)

KFM uses **four retrieval channels** and bundles the results into a single â€œevidence packetâ€:

1) ğŸ•¸ï¸ **Neo4j (Graph context)**  
   - Related entities, events, relationships, story nodes

2) ğŸ—ºï¸ **PostGIS (Spatial/statistical context)**  
   - Map-bounded statistics, counts, time-sliced aggregations (e.g., â€œcount of wells in county in 1930â€)

3) ğŸ” **Full-text search (Keyword context)**  
   - Documents/stories containing explicit terms

4) ğŸ§² **Vector similarity search (Semantic context)**  
   - Embedding-based retrieval of semantically relevant chunks/snippets

These channels are explicitly named in the system documentation and are combined into a single bundle of evidence with source IDs and titles.:contentReference[oaicite:8]{index=8}

### ğŸ›ï¸ Output of Retrieval: â€œEvidence Bundleâ€
Each retrieved item should carry, at minimum:

- `source_id` âœ…
- `title` âœ…
- `channel` (neo4j | postgis | fulltext | vector) âœ…
- `snippet/fact` âœ…
- relevant metadata (time range, bbox, entity IDs) âœ…

KFM compiles evidence into a **numbered list of sources** that will be fed to the LLM and deliberately keeps context compact and high-signal.:contentReference[oaicite:9]{index=9}

---

## ğŸ§± Indexing + Embeddings (Vector Retrieval)

### ğŸ§  Embeddings generation
KFM generates vector embeddings for unstructured text (reports, newspapers, archival texts) and potentially knowledge graph entries. The docs explicitly note using **Ollamaâ€™s embedding API** (`/api/embed`) and embedding models like **`all-minilm`** or **`mxbai-embed-large`**, potentially down to **paragraph-level** chunks.:contentReference[oaicite:10]{index=10}

### ğŸ—„ï¸ Vector store
The system describes maintaining an embedding store using:
- an in-memory vector DB like **Chroma**, or
- a service like **Qdrant**  
with embeddings stored alongside document IDs for similarity search.:contentReference[oaicite:11]{index=11}

### ğŸ” Ingestion-time indexing
During ingestion, embeddings can be precomputed and the vector index updated for new entries so runtime semantic retrieval is fast.:contentReference[oaicite:12]{index=12}

> [!TIP]
> Treat vector indexing as part of your **ETL** pipeline, not a runtime side-effect.

---

## ğŸ”€ Hybrid Retrieval (Structured + Unstructured)

Focus Mode retrieval is explicitly described as **hybrid**: structured querying (Neo4j/PostGIS) and unstructured querying (document index / vectors) run in tandem.:contentReference[oaicite:13]{index=13}

### âœ… Why hybrid matters
- Graph + spatial queries answer **â€œwhat/where/when/how manyâ€**
- Full-text + vectors answer **â€œshow me relevant text evidenceâ€**
- Together they produce an answer with citations that is *both* correct *and* explainable

---

## ğŸ§¾ Context Assembly & Prompting

After retrieval, the orchestrator:

- Builds a prompt that contains:
  - **system instruction** emphasizing citation rules
  - **sanitized user question**
  - **current map context** (selected county/year/layers, etc.)
  - a list of **SOURCES** labeled `[1]`, `[2]`, â€¦  
- Explicitly instructs the model to use **only provided sources** and cite them for every factual claim:contentReference[oaicite:14]{index=14}

### âœï¸ Prompt Skeleton (Illustrative)
```text
SYSTEM:
  You are KFM's assistant. Use ONLY the SOURCES. Cite every factual claim as [n].

USER QUESTION:
  <sanitized question>

MAP CONTEXT:
  county=<...>
  year=<...>
  active_layers=<...>

SOURCES:
  [1] <title> â€” <snippet>
  [2] <title> â€” <snippet>
  ...
```

> [!IMPORTANT]
> The system documentation explicitly warns against dumping full articles into prompts; keep excerpts compact and high-signal.:contentReference[oaicite:15]{index=15}

---

## ğŸ¤– LLM Call (Ollama)

The orchestrator sends the prompt to **Ollamaâ€™s** generation endpoint (`/api/generate`).:contentReference[oaicite:16]{index=16}

The model is treated as a **least-privilege** component: it cannot access the internet or other data beyond provided snippets, and it has no tools except text generation (and embeddings, as configured).:contentReference[oaicite:17]{index=17}

---

## ğŸ›¡ï¸ Policy Gate + Citation Enforcement (OPA)

After the model returns a draft answer:

- KFM runs an **OPA policy check** (Rego) to verify:
  - Required citation brackets are present
  - Content does not violate safety rules/sensitivity labels
  - The user role permits seeing the information
- If the answer fails policy (missing citations, sensitive disclosure, etc.), KFM returns a safe fallback or clarification instead of the draft output.:contentReference[oaicite:18]{index=18}

### ğŸ”’ Citation-as-Policy
The system documentation includes an example Rego policy for citation enforcement:

```rego
package kfm.ai

default allow_answer = false

# Allow answer only if it contains at least one citation like "[number]"
allow_answer {
  re_match("\\[\\d+\\]", input.answer)
}
```

:contentReference[oaicite:19]{index=19}

It also states the architecture expects automated checks/tests such that changes that would remove citations get caught (e.g., by CI).:contentReference[oaicite:20]{index=20}

---

## ğŸ§· Post-Processing, Clickable Citations, and Provenance

The blueprint describes that after LLM generation:

- The system attaches source references using KFMâ€™s **clickable citation format** (e.g., `ã€â€ ã€‘` linking to Story nodes or data sources)
- The answer is run through the **policy engine** to block disallowed content
- The UI displays the answer with citations that users can click to inspect originals:contentReference[oaicite:21]{index=21}

### ğŸ“’ Provenance ledger
KFM keeps mandatory provenance/audit trails, including an immutable ledger of AI queries/outputs that records:

- the question
- sources used
- model version
- policy decision

This ledger is described as append-only and intended for auditability and reproducibility.:contentReference[oaicite:22]{index=22}

The documentation also suggests storing â€œAI answerâ€ nodes (with model identity) linked to their sources in Neo4j.:contentReference[oaicite:23]{index=23}

---

## ğŸ“¡ API Surface (How Clients Use It)

The public API exposes Focus Mode AI endpoints under `/api/v1/ai/`, including:

- `POST /api/v1/ai/query` (question + optional context like place/time) â†’ answer with citations
- `POST /api/v1/ai/stream` (experimental streaming tokens)
- `GET /api/v1/ai/suggestions` (optional/future):contentReference[oaicite:24]{index=24}

### ğŸ§ª Example Payload (Dev Testing)
The blueprint suggests testing with a JSON payload like:

```json
{"question": "What happened in Kansas in 1850?"}
```

:contentReference[oaicite:25]{index=25}

---

## ğŸ§© Reference Implementation Layout (Recommended Repo Structure)

The Ollama integration doc proposes a clean modular backend layout:

```text
ğŸ“¦ KFM-Backend/
  ğŸ“ api/
    ğŸ“ routes/
      ğŸ“„ focus_mode.py          # API endpoint for AI queries
  ğŸ“ ai/
    ğŸ§  focus_pipeline.py       # Orchestrates parse -> retrieve -> prompt -> postprocess
    ğŸ”Œ ollama_client.py        # Wrapper for Ollama API calls (generate, embed)
    ğŸ” retrieval.py            # Query Neo4j, PostGIS, search index
    ğŸ›¡ï¸ policy_checks.py        # Invoke OPA / apply rules to answers
    ğŸ“ prompt_templates/
      ğŸ“„ focus_mode.txt
```

:contentReference[oaicite:26]{index=26}

---

## ğŸ”§ Configuration Knobs (Env-Driven)

The system documentation explicitly calls out environment configuration for Ollama usage, including:

- `FOCUS_MODE_MODEL` (model name)
- `OLLAMA_API_URL` (Ollama service URL, e.g., `http://ollama:11434` in compose setups):contentReference[oaicite:27]{index=27}

> [!NOTE]
> Retrieval-specific knobs (top-k per channel, max source tokens, chunk sizes, etc.) should be kept env-configurable even if not yet standardized.

---

## ğŸ“ˆ Observability & Debugging

KFM emphasizes that the AI pipeline is not a black box:

- Every step of Focus Mode (retrieval + prompt construction + output) is logged
- Developers can inspect what was retrieved and what was sent to the model
- Queries can be replayed offline by reusing logged context and rerunning prompts against the same local model:contentReference[oaicite:28]{index=28}

---

## ğŸ§ª Testing Expectations (CI-Friendly)

Recommended automated checks:

- âœ… Citation presence checks (OPA + tests)
- âœ… â€œNo evidence â†’ no confident answerâ€ behavior
- âœ… Regression tests for retrieval (stable inputs â†’ stable evidence bundle)
- âœ… Policy-deny test cases (sensitive data, missing citations, prompt injection attempts)

The documentation explicitly mentions the idea of regression tests ensuring answers include citations and that Ollama can run locally (even in CI using smaller models / CPU mode).:contentReference[oaicite:29]{index=29}

---

## ğŸ§¯ Failure Modes & Fallbacks

Common failures and expected system behavior:

- â“ **No evidence found** â†’ ask for clarification (time/place) or return â€œinsufficient evidenceâ€
- ğŸš« **Policy deny** (missing citations / sensitive content) â†’ safe fallback / refuse response:contentReference[oaicite:30]{index=30}
- ğŸ§² **Vector index unavailable** â†’ degrade to full-text + structured retrieval (still answer with citations)
- ğŸ•¸ï¸ **Graph/DB partial outage** â†’ return partial evidence bundle + explicit limitation note

---

## ğŸ›£ï¸ Roadmap Ideas (Optional Enhancements)

> These are suggestions consistent with the architecture (not necessarily implemented yet).

- ğŸ” **Reranking**: cross-encoder or heuristic rerank of merged evidence bundle
- ğŸ§­ **Query router**: detect if query is mostly spatial/stat vs narrative vs entity-relationship and prioritize channel weights
- ğŸ§Š **Caching**: cache query embeddings + retrieval bundle for repeated queries in same map context
- ğŸ§¼ **Chunk governance**: propagate dataset/story sensitivity labels into chunk metadata so restricted chunks never enter prompts
- ğŸ“ **Token budgeting**: enforce strict per-source token limits so prompts stay compact

---

## ğŸ“š Source Anchors (Project Docs Used)

- Multi-step Focus Mode RAG pipeline (Prompt Gate â†’ retrieval â†’ prompt â†’ generation):contentReference[oaicite:31]{index=31}
- Retrieval uses four channels + compact high-signal context:contentReference[oaicite:32]{index=32}
- Prompt assembly with map context + numbered sources + citation rule:contentReference[oaicite:33]{index=33}
- OPA policy gate after generation:contentReference[oaicite:34]{index=34}
- Citation enforcement policy example (Rego):contentReference[oaicite:35]{index=35}
- Embeddings + vector DB (Chroma/Qdrant) + Ollama `/api/embed`:contentReference[oaicite:36]{index=36}
- API endpoints (`/api/v1/ai/query`, streaming, suggestions):contentReference[oaicite:37]{index=37}
- Post-processing attaches clickable citations `ã€â€ ã€‘` + policy engine scan:contentReference[oaicite:38]{index=38}
- Provenance ledger + auditability for AI answers:contentReference[oaicite:39]{index=39}

---

