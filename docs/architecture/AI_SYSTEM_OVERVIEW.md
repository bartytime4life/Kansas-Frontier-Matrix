# ğŸ§­ğŸ¤– AI System Overview
> **Kansas Frontier Matrix (KFM)** â€” **Focus Mode AI** is a provenance-first, evidence-backed, *advisory-only* assistant that helps users interpret maps, timelines, stories, and datasets without turning KFM into a black box.

---

## ğŸ§· At a glance

| ğŸ·ï¸ Area | âœ… What KFMâ€™s AI *does* | ğŸš« What it *doesnâ€™t do* |
|---|---|---|
| ğŸ—£ï¸ User experience | Answers natural-language questions *in context* (map viewport, selected layers, timeline) | Doesnâ€™t â€œdriveâ€ the UI or change layers/time for the user |
| ğŸ” Knowledge access | Uses hybrid retrieval across **PostGIS + Neo4j + Search Index + Vector Store** | Doesnâ€™t browse the internet or pull from external sources by default |
| ğŸ§¾ Trust model | â€œ**No citation, no answer**â€ + provenance panels + audit trails | Doesnâ€™t present uncited facts as truth |
| ğŸ›¡ï¸ Safety | Prompt Gate (input) + OPA (output) + allow/deny tool model | Doesnâ€™t run arbitrary code, call HTTP tools, or access filesystem |
| ğŸ¦™ LLM runtime | Local-first LLM runtime via **Ollama** (pluggable) | Doesnâ€™t require cloud AI to function in dev |

---

## ğŸ¯ Purpose

Focus Mode AI exists to make KFM feel like a **knowledgeable research librarian + GIS analyst** living inside the map:

- Explain what a user is seeing (â€œWhat does this layer show in 1935 here?â€)
- Summarize connected evidence (â€œWhat happened here around this time?â€)
- Point to next steps (â€œOpen this dataset / story node / provenance recordâ€)
- Maintain accountability with citations, provenance metadata, and auditability

---

## ğŸ§  Core principles

### 1) ğŸ”— Provenance-first (chain-of-custody)
Every insight is tethered to:
- a dataset/story/document **ID**
- metadata (STAC/DCAT)
- lineage (PROV)
- governance checks (policy decisions)

### 2) ğŸ§­ Advisory-only (human stays in control)
Focus Mode provides **guidance** and **interpretation**. It does *not* take actions on behalf of users.

### 3) ğŸ§± Layered architecture (clean boundaries)
The UI never calls an LLM directly. The AI is orchestrated **server-side** behind the governed API.

### 4) ğŸ›¡ï¸ â€œLeast privilegeâ€ AI
The model is treated like untrusted code:
- sandboxed
- tool access is explicit (allowlist)
- input/output validation enforced

### 5) ğŸ§ª Reproducible + reviewable
AI behavior is versioned and auditable:
- prompts are versioned
- policies are versioned
- model versions are tracked
- responses can be logged with sources and policy results

---

## ğŸ—ï¸ Where AI fits in the KFM system

KFMâ€™s AI is **not** a separate â€œsmart layerâ€ floating above the platform. It is an **interface** into KFMâ€™s governed data ecosystem.

### ğŸ§© Primary building blocks

- ğŸ–¥ï¸ **UI (React/TypeScript)**  
  Focus Mode panel + chat UX + citation rendering + â€œaudit/explainâ€ UI affordances.
- ğŸšª **Governed API (FastAPI + REST + GraphQL)**  
  The only gateway for data access, retrieval orchestration, and AI execution.
- ğŸ—ƒï¸ **Datastores**
  - ğŸŒ **PostGIS** â€” geospatial + temporal queries, layers, tiles
  - ğŸ•¸ï¸ **Neo4j** â€” entities/relationships, story graph, semantic traversal
  - ğŸ§¾ **Search Index** â€” full-text (docs/story content)
  - ğŸ§  **Vector Store** â€” semantic similarity (embeddings)
- ğŸ¦™ **LLM Runtime (Ollama)**  
  Local model serving + embeddings API + pluggable models.
- ğŸ›¡ï¸ **Policy Layer (OPA + prompt guards)**  
  Enforces input/output rules, redactions, refusals, and governance constraints.
- ğŸ§¾ **Provenance + Governance Ledger**  
  Append-only event record of AI interactions and decisions.

---

## ğŸ”„ End-to-end Focus Mode flow

```mermaid
flowchart LR
  U[ğŸ‘¤ User] --> UI[ğŸ—ºï¸ UI: Focus Mode Panel]
  UI -->|POST /focus-mode/query + map context| API[ğŸšª Governed API (FastAPI)]

  API --> PG[ğŸ§¼ Prompt Gate\n(input sanitize + injection defense)]
  PG --> INT[ğŸ§­ Intent + Context Parser\n(entities, time, place, layers)]
  INT --> RET[ğŸ” Retrieval Layer\nNeo4j + PostGIS + Search + Vector]
  RET --> PB[ğŸ§© Prompt Builder\n(prompt template + cited context)]
  PB --> LLM[ğŸ¦™ Ollama\n(generate)]
  LLM --> OPA[ğŸ›¡ï¸ OPA Output Policy\n(redact / block / require citations)]
  OPA --> CITE[ğŸ”— Citation + Provenance Enforcer\n(no citation => refuse)]
  CITE --> LED[ğŸ§¾ Immutable AI Ledger\n(log Q/A, sources, policy hashes)]
  CITE --> UI2[ğŸ—ºï¸ UI Render\n(answer + footnotes + audit panel)]
```

---

## ğŸ—ºï¸ Context awareness (the â€œmap state is part of the questionâ€)

Focus Mode is designed to interpret user questions relative to the UI state, such as:

- ğŸ“ geographic focus (county/feature/viewport bounds)
- ğŸ•°ï¸ timeline slider (year/range)
- ğŸ§± active layers
- ğŸ“š open story node / narrative context
- ğŸ›ï¸ filters (if any)

This allows â€œhere/now/this layerâ€ questions to be resolved correctly (e.g., â€œWhat happened here around this time?â€).

---

## ğŸ” Retrieval & RAG architecture

### ğŸ§¬ Hybrid retrieval
Focus Mode uses multiple retrieval channels and merges them into a single â€œevidence packâ€:

1) ğŸ•¸ï¸ **Graph retrieval (Neo4j)**  
   Traverses relationships to find connected entities/events/places/sources.

2) ğŸŒ **Geospatial retrieval (PostGIS)**  
   Spatial + temporal queries: intersections, proximity, bounding boxes, time-filtered aggregates.

3) ğŸ§¾ **Text retrieval (Search Index)**  
   Full-text search over documents, narrative content, dataset descriptions.

4) ğŸ§  **Semantic retrieval (Vector Store)**  
   Embedding similarity search for â€œmeaning-matchâ€ and paraphrases.

> âœ… The LLM is a **composer**, not a source of truth. Retrieval provides the facts; the model writes the explanation.

### ğŸ§± Evidence packaging rules
To keep provenance tight, retrieval results should be normalized into a common schema:

```json
{
  "evidence": [
    {
      "id": "kg:event:1856_black_jack",
      "type": "knowledge_graph",
      "title": "Battle of Black Jack (1856)",
      "snippet": "Event summaryâ€¦",
      "provenance": {
        "source_ids": ["doc:news:1856_05_31", "dataset:places_kansas_v1"],
        "stac": "catalog/stac/â€¦",
        "prov": "provenance/prov/â€¦"
      }
    }
  ]
}
```

### ğŸ§¾ Citation mapping strategy
To prevent â€œcitation theaterâ€ (random sources pasted at the end):

- Each retrieved chunk is assigned a stable `evidence_id`
- Prompt template instructs the model to cite with `[evidence_id]` markers
- Post-processing maps markers â†’ structured citations list
- OPA enforces minimum citation rules (and optional stricter policies)

---

## ğŸ¦™ Model runtime & infrastructure (Ollama-first)

### âœ… Why Ollama
Ollama provides a **local** LLM runtime compatible with:
- offline development
- containerized deployment
- fast iteration without API keys
- model version switching

### ğŸ§© Model packaging
KFM can package a custom model with:
- a base model (`FROM llamaâ€¦`)
- optional LoRA adapter (`ADAPTER â€¦`)
- a template ensuring system instructions + citation behavior

<details>
<summary>ğŸ§± Example Modelfile (illustrative)</summary>

```txt
FROM llama2:latest
ADAPTER ./kansas_finetune_lora.safetensors

TEMPLATE """{{ if .System }}<|system|>{{ .System }}<|end|>{{ end }}
{{ if .Prompt }}<|user|>{{ .Prompt }}<|end|>{{ end }}<|assistant|>"""

SYSTEM """You are KFM's assistant. Use ONLY provided context. Provide answers with sources."""
```
</details>

### ğŸ§  Embeddings + vector store
For semantic search, KFM generates embeddings using an embedding model (e.g., `all-minilm` / `mxbai-embed-large`) and stores vectors in a vector DB (e.g., **Chroma** or **Qdrant**).

---

## ğŸ§© Suggested backend module layout

The AI integration is designed to be modular so the LLM runtime can be swapped with minimal impact.

```text
ğŸ“¦ KFM-Backend/
â”œâ”€â”€ ğŸ“‚ api/
â”‚   â””â”€â”€ ğŸ“‚ routes/
â”‚       â””â”€â”€ ğŸ§  focus_mode.py          # /focus-mode/query
â”œâ”€â”€ ğŸ“‚ ai/
â”‚   â”œâ”€â”€ ğŸ§  focus_pipeline.py          # parse â†’ retrieve â†’ prompt â†’ generate â†’ postprocess
â”‚   â”œâ”€â”€ ğŸ” retrieval.py               # Neo4j + PostGIS + Search + Vector queries
â”‚   â”œâ”€â”€ ğŸ¦™ ollama_client.py           # generate() + embed()
â”‚   â”œâ”€â”€ ğŸ›¡ policy_checks.py           # OPA integration + rule helpers
â”‚   â””â”€â”€ ğŸ“‚ prompt_templates/
â”‚       â””â”€â”€ ğŸ“ focus_mode.txt
â””â”€â”€ ...
```

---

## ğŸ›¡ï¸ Governance & safety model

### ğŸ§¼ Prompt Gate (input security)
Incoming questions are sanitized before reaching the model:

- prompt-injection stripping / normalization
- profanity / hate / disallowed-content filtering
- PII/sensitive request detection (deny or safe-rewrite)

### ğŸ§° Tooling sandbox (deny-by-default)
Focus Mode is intentionally constrained:

- ğŸš« no arbitrary code execution  
- ğŸš« no direct filesystem or secret access  
- ğŸš« no internet calls  
- âœ… only retrieval through governed KFM interfaces  
- âœ… any future tools must be explicitly allowlisted

### ğŸ§¾ OPA Output Policy (runtime enforcement)
OPA evaluates outputs **after generation**:

- block disallowed content
- redact sensitive details
- enforce â€œno citation, no answerâ€
- require hedging language for uncertainty (optional)
- require entity existence (optional anti-hallucination policy)

> ğŸ§  Policies are *code*, not â€œguidelines.â€ Theyâ€™re versioned, testable, and auditable.

---

## ğŸ”— Provenance, auditability, and explainability

### ğŸ§¾ Citations are mandatory
- Answers are rendered with numbered footnotes and clickable sources
- If sufficient evidence cannot be retrieved, the assistant should refuse or express uncertainty (never fabricate)

### â›“ï¸ PROV lineage for AI outputs
Each AI response can be recorded as a provenance **Activity** that:
- consumed evidence Entities
- used model version X
- produced an answer Entity
- occurred at time T

### ğŸ§¾ Immutable governance ledger
Every Focus Mode interaction can be logged as an append-only record including:
- question (or privacy-preserving summary)
- answer
- source IDs used
- policy checks triggered
- policy version hashes
- model version hashes

### ğŸ” â€œAI Audit Panelâ€
A user-facing explanation layer can surface:

- which sources were most influential
- which graph paths were traversed (simplified)
- any computations performed (with inputs shown)
- governance flags (e.g., â€œdetails omitted due to policyâ€)

---

## âœ… Quality assurance (QA) & human-in-the-loop

### ğŸ¤– Automated QA gates (â€œwatchersâ€)
Examples:
- AI output scanned for citations (and format)
- ingestion output checked for required metadata triplet
- sensitive terms flagged for review
- policy violations alerting

### ğŸ§‘â€âš–ï¸ Human review & community governance
- PR-based review on data/pipeline/policy/prompt changes
- domain expert review for sensitive datasets
- governance committee/council can update policy packs

### ğŸ‘ User feedback loop
- â€œflag answerâ€ flows into issue tracking
- feedback can improve retrieval, prompts, and policy

---

## ğŸ§° API contract: recommended shapes

### `POST /focus-mode/query`
**Inputs (recommended)**
- `question`: string
- `map_context`: viewport bounds, selected feature/county ID
- `time_context`: year/range
- `active_layers`: IDs
- `story_context`: open story node ID (optional)
- `user_context`: role/permissions (derived server-side)

**Outputs (recommended)**
- `answer_text`: string (with `[1]` footnotes)
- `citations`: list (IDs + titles + types)
- `audit`: optional (explainability bundle)
- `provenance`: optional (PROV activity ID, model version)

---

## ğŸ§¾ Data provenance requirements that support AI

Focus Mode inherits KFMâ€™s â€œevidence-firstâ€ data pipeline:

- Immutable raw inputs
- Repeatable ETL/processing
- Catalog metadata (STAC/DCAT)
- Lineage records (PROV)
- Indexing into PostGIS/Neo4j/Search/Vector

> ğŸ§± AI reliability rises directly with data provenance quality.

---

## ğŸ§­ Roadmap hooks (AI-related)

These are common extension points that fit the architecture without breaking governance:

- ğŸ§‘â€ğŸ’¼ **AI Data Steward**: metadata drafting + suggested entity linking on ingest
- ğŸ““ **Notebook workflows**: reproducible analyses that read from KFM â†’ compute â†’ write back with provenance
- ğŸ§  **Domain models**: statistical/ML modules (e.g., regression, Bayesian models) publishing outputs as governed datasets
- ğŸ§© **Federation**: multi-region â€œFrontier Matricesâ€ with cross-instance search (GraphQL federation)

---

## ğŸ“š Project file library used to inform this overview

### ğŸ›ï¸ KFM system design docs
- `Kansas Frontier Matrix (KFM) â€“ AI System Overview ğŸ§­ğŸ¤–.pdf`
- `KFM AI Infrastructure â€“ Ollama Integration Overview.pdf`
- `ğŸ“š Kansas Frontier Matrix (KFM) â€“ Expanded Technical & Design Guide.pdf`
- `Kansas Frontier Matrix (KFM) â€“ Comprehensive Technical Documentation.pdf`
- `Kansas Frontier Matrix (KFM) â€“ Comprehensive Architecture, Features, and Design.pdf`
- `Kansas Frontier Matrix (KFM) â€“ Comprehensive UI System Overview (Technical Architecture Guide).pdf`
- `Kansas Frontier Matrix (KFM) â€“ Comprehensive Platform Overview and Roadmap.pdf`

### ğŸ§ª R&D + documentation protocol resources
- `Scientific Method _ Research _ Master Coder Protocol Documentation.pdf`
- `Data Mining Concepts & applictions.pdf`

### ğŸ§° Curated learning/reference portfolios (open in Adobe Reader)
- `AI Concepts & more.pdf`
- `Data Managment-Theories-Architures-Data Science-Baysian Methods-Some Programming Ideas.pdf`
- `Mapping-Modeling-Python-Git-HTTP-CSS-Docker-GraphQL-Data Compression-Linux-Security.pdf`
- `Geographic Information-Security-Git-R coding-SciPy-MATLAB-ArcGIS-Apache Spark-Type Script-Web Applications.pdf`
- `Maps-GoogleMaps-VirtualWorlds-Archaeological-Computer Graphics-Geospatial-webgl.pdf`
- `Various programming langurages & resources 1.pdf`

---

## ğŸ§¾ Glossary

- **Focus Mode** â€” KFMâ€™s AI assistant mode (context-aware Q&A with citations)
- **RAG** â€” Retrieval-Augmented Generation (retrieve evidence â†’ LLM composes answer)
- **OPA** â€” Open Policy Agent (policy engine for runtime enforcement)
- **PROV** â€” W3C provenance model (activities/entities/agents + lineage)
- **STAC** â€” SpatioTemporal Asset Catalog (spatiotemporal dataset catalog metadata)
- **DCAT** â€” Data Catalog Vocabulary (dataset catalog metadata)
- **Neo4j** â€” graph DB for semantic relationships
- **PostGIS** â€” geospatial extension for PostgreSQL
- **Vector store** â€” embedding similarity DB (semantic retrieval)
- **Prompt Gate** â€” input sanitization + injection defense layer

---

## âœ… Implementation checklist (starter)

- [ ] Add `POST /focus-mode/query` route
- [ ] Implement `parse_intent(question, ui_context)`  
- [ ] Implement retrieval connectors: `neo4j()`, `postgis()`, `search()`, `vector()`
- [ ] Create `focus_mode.txt` prompt template with strict citation format
- [ ] Add Ollama client wrapper (`generate`, `embed`)
- [ ] Add output post-processor to normalize citations + structure response
- [ ] Integrate OPA policies (deny-by-default + citation enforcement)
- [ ] Add immutable ledger entry creation for each request (hash chaining)
- [ ] Add regression tests: â€œno citation â†’ refuseâ€ + â€œpolicy violation â†’ redact/blockâ€
- [ ] Add UI citation renderer + optional Audit Panel view

---