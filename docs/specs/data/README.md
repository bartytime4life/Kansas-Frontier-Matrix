<div align="center">

# ğŸ“¦ Kansas Frontier Matrix â€” Data Specs

**Authoritative conventions for how data is stored, processed, validated, cataloged, and published**  
within the Kansas-Matrix-System (KFM) ğŸ—ºï¸ğŸ§¬

![Status](https://img.shields.io/badge/status-draft-yellow)
![Spec](https://img.shields.io/badge/spec-data-blue)
![Metadata](https://img.shields.io/badge/metadata-STAC%20%7C%20DCAT%20%7C%20PROV-purple)
![Reproducibility](https://img.shields.io/badge/reproducible-deterministic%20ETL-brightgreen)
![Governance](https://img.shields.io/badge/governance-FAIR%2BCARE-orange)

</div>

> âœ… **Prime directive:** If it can affect a map, model, decision, or storyâ€¦ it must be **discoverable** (STAC/DCAT), **auditable** (PROV), and **tamper-evident** (checksums/signing).  
> ğŸš« If it isnâ€™t cataloged, it isnâ€™t published.

---

## ğŸ§­ What this README is

This is the **data subsystem spec index** for KFM. It defines:

- ğŸ“ **Canonical folder structure** for `data/`
- ğŸ§± **Dataset lifecycle** (raw â†’ work â†’ processed)
- ğŸ§¾ **Boundary artifacts**: STAC, DCAT, PROV (required for publication)
- ğŸ§¬ **Provenance & reproducibility** expectations
- ğŸ›¡ï¸ **FAIR+CARE / sovereignty** handling for sensitive materials
- âœ… **Quality gates** (schema, policy, geo-validity, and drift checks)

---

## ğŸ§© Canonical pipeline ordering (nonâ€‘negotiable)

KFM treats **data â†’ catalogs â†’ graph â†’ product surfaces** as a strict dependency chain:

```text
ETL â†’ STAC/DCAT/PROV catalogs â†’ Graph â†’ APIs â†’ UI â†’ Story Nodes â†’ Focus Mode
```

```mermaid
flowchart LR
  A[ğŸ“¥ Raw Sources] --> B[ğŸ§ª ETL / Ingest]
  B --> C[ğŸ§¾ STAC Catalog]
  B --> D[ğŸ—‚ï¸ DCAT Catalog]
  B --> E[ğŸ§¬ PROV Ledger]
  C --> F[ğŸ•¸ï¸ Graph Exports]
  D --> F
  E --> F
  F --> G[ğŸ”Œ APIs]
  G --> H[ğŸ—ºï¸ UI / Maps]
  H --> I[ğŸ“– Story Nodes]
  I --> J[ğŸ›ï¸ Focus Mode]
```

**Implication:** we never â€œhand craftâ€ story/UI artifacts without traceable upstream catalogs and provenance. ğŸ“

---

## ğŸ—‚ï¸ Canonical data homes

### âœ… The rule
Every file in `data/` must belong to **exactly one** of these categories:

| Category | Purpose | Stability |
|---|---|---|
| `raw/` | Unmodified source captures (as received) | Immutable (append-only) |
| `work/` | Intermediate transforms / scratch products | Mutable |
| `processed/` | Governed outputs (analysis-ready) | Immutable per version |
| `mappings/` | Schemas, crosswalks, rules, join keys | Governed |
| `stac/` | STAC boundary artifacts (Collections + Items) | Governed |
| `catalog/dcat/` | DCAT boundary artifacts | Governed |
| `prov/` | PROV boundary artifacts / run logs | Governed |
| `graph/` | Export for Neo4j import (CSV/Cypher) | Governed |

> ğŸ§  **One source of truth:** Donâ€™t duplicate the same logical dataset in multiple places.  
> Prefer **pointers** (STAC/DCAT/PROV links) over copies.

---

## ğŸ§± Expected `data/` directory layout

> This spec describes the *canonical* layout. If you find legacy variants (e.g., `data/raw/<domain>` vs `data/<domain>/raw`), migrate toward **one canonical layout** and document the decision in `data/README.md`.

```plaintext
ğŸ“ data/
â”œâ”€â”€ ğŸ“„ README.md                              # Human overview + whatâ€™s in/out of repo
â”‚
â”œâ”€â”€ ğŸ“ stac/                                  # âœ… STAC boundary artifacts
â”‚   â”œâ”€â”€ ğŸ“„ catalog.json
â”‚   â”œâ”€â”€ ğŸ“ collections/
â”‚   â”‚   â””â”€â”€ ğŸ“„ <collection-id>.json
â”‚   â””â”€â”€ ğŸ“ items/
â”‚       â””â”€â”€ ğŸ“ <collection-id>/
â”‚           â””â”€â”€ ğŸ“„ <item-id>.json
â”‚
â”œâ”€â”€ ğŸ“ catalog/
â”‚   â””â”€â”€ ğŸ“ dcat/                              # âœ… DCAT boundary artifacts
â”‚       â”œâ”€â”€ ğŸ“„ catalog.jsonld
â”‚       â””â”€â”€ ğŸ“„ dataset__<dataset-id>.jsonld
â”‚
â”œâ”€â”€ ğŸ“ prov/                                  # âœ… PROV boundary artifacts
â”‚   â”œâ”€â”€ ğŸ“ runs/
â”‚   â”‚   â””â”€â”€ ğŸ“„ <run-id>.prov.jsonld
â”‚   â””â”€â”€ ğŸ“ bundles/
â”‚       â””â”€â”€ ğŸ“„ <dataset-id>__<version>.prov.jsonld
â”‚
â”œâ”€â”€ ğŸ“ graph/                                 # âœ… graph exports for Neo4j
â”‚   â”œâ”€â”€ ğŸ“ csv/
â”‚   â””â”€â”€ ğŸ“ cypher/
â”‚
â”œâ”€â”€ ğŸ“ <domain>/                              # e.g., hydrology, soils, archaeology, treaties, hazards...
â”‚   â”œâ”€â”€ ğŸ“„ README.md
â”‚   â”œâ”€â”€ ğŸ“ raw/
â”‚   â”œâ”€â”€ ğŸ“ work/
â”‚   â”œâ”€â”€ ğŸ“ processed/
â”‚   â””â”€â”€ ğŸ“ mappings/
â”‚
â””â”€â”€ ğŸ“ staging/                               # (optional) automated ingest/watch outputs (PR-bound)
    â””â”€â”€ ğŸ“ processed/
        â”œâ”€â”€ ğŸ“ stac/
        â””â”€â”€ ğŸ“„ manifest.json
```

### ğŸ§ª `data/staging/` (optional but recommended for automation)
Use staging when a watcher/cron job creates deterministic artifacts and opens a PR (merge == publish).  
âœ… Works well with idempotency keys and â€œpromotionâ€ workflows.

---

## ğŸ·ï¸ Dataset identity, naming & partitioning

### Dataset ID
Use a stable identifier across **STAC + DCAT + PROV + Graph**:

- **Preferred:** `kfm.<domain>.<dataset>`
  - Example: `kfm.hydrology.nhd_flowlines`
  - Example: `kfm.soils.ssurgo_components`
- **Avoid:** spaces, camelCase, and location-specific prefixes unless truly needed.

### Versions
- **Dataset version:** semantic (e.g., `v1.2.0`) or data-driven (e.g., `2025-11-12`)  
- **Processing version:** bumps when transforms/mappings change  
- **Catalog version:** bumps when metadata schema or linkage changes

> ğŸ” Rebuilding the same version must be deterministic: identical inputs + same code + same env â‡’ identical outputs (byte-level when feasible).

### Partitioning
Use predictable partitions that scale:

- ğŸ—ºï¸ Spatial: `tile_id` (z/x/y), H3 cell, admin region, watershed unit
- ğŸ•°ï¸ Temporal: day/month/year or interval
- ğŸ§¾ Thematic: variable or product name

---

## ğŸ“¦ Formats & packaging rules

### Raster (grids, surfaces)
âœ… Preferred:
- **COG GeoTIFF** (`image/tiff; application=geotiff; profile=cloud-optimized`)
- Optional: **Zarr** / **NetCDF** for model-native workflows (catalog still required)

Rules:
- Include overviews/pyramids for web use
- Include `proj:*` and (when relevant) `raster:*` metadata in STAC

### Vector (features)
âœ… Preferred:
- **GeoParquet** (analysis + scalable)
- **GeoJSON** (UI / interop)
- Optional: FlatGeobuf for streaming

Rules:
- Shapefile is *raw-only* (ingest, then convert)
- Ensure geometry validity + CRS consistency

### Tabular / time series
âœ… Preferred:
- Parquet (partitioned)
- CSV (only when schema is fixed & small)

### Tiles for UI
âœ… Preferred:
- **PMTiles** for offline + CDN-friendly tile delivery
- Optional: MBTiles (tooling compatibility)

### Documents, scans, imagery
âœ… Preferred:
- PDF for originals (raw)
- PNG/JPEG/WebP for previews (work/processed)
- Text extraction (JSONL) in processed when permitted

### 3D + time-dynamic
âœ… Preferred:
- glTF/GLB for scenes
- CZML for time-dynamic Cesium streams

> ğŸ§¾ **Everything above is â€œdataâ€** in the KFM sense if it drives the UI/storyâ€”so it still needs metadata + checksums.

---

## ğŸ§¾ Boundary artifacts (STAC + DCAT + PROV)

KFM requires **three linked boundary artifacts** for anything publishable:

### 1) ğŸ§¾ STAC (discovery + assets)
STAC is the canonical **machine-discoverable registry**.

Minimum expectations:
- `stac_version`
- `id`
- `collection`
- `geometry` / `bbox` (or `geometry: null` for non-geospatial catalogs)
- `properties.datetime` or `properties.start_datetime`/`end_datetime`
- `assets.*` with:
  - `href`, `type`, `roles`, `title`
  - checksum fields (e.g., `checksum:sha256` or multihash)

Commonly used extensions (by domain):
- `proj` (projection)
- `raster` (raster bands / nodata / stats)
- `checksum` (integrity)
- `version` (dataset/item versioning)

### 2) ğŸ—‚ï¸ DCAT (publishing + interoperability)
DCAT is the canonical **catalog/distribution** view for broader data ecosystems and â€œdata spacesâ€ sharing.

Minimum expectations:
- Dataset: title, description, publisher/provider, license, theme(s)
- Distribution(s): access URL, media type, checksum, relation to provenance

### 3) ğŸ§¬ PROV (lineage + audit)
PROV is the canonical **lineage record** for:
- What was used
- What was generated
- Who/what performed the transformation
- Which plan/workflow rules were applied

Minimum expectations:
- `prov:Entity` for each published artifact
- `prov:Activity` for each transformation run
- `prov:Agent` for toolchain + steward roles
- links: `prov:used`, `prov:wasGeneratedBy`, `prov:wasDerivedFrom`, `prov:wasAssociatedWith`

---

## ğŸ”— Linkage rules between STAC/DCAT/PROV

A published dataset must be navigable like a triangle:

- **STAC Item â†’ PROV bundle** (run lineage for this item)
- **STAC Item â†’ DCAT dataset/distribution** (publishing metadata)
- **DCAT distribution â†’ checksum + provenance relation**
- **PROV entity â†’ file digest + location** (for verification)

> ğŸ§· Bonus (recommended): attach SBOM/signatures/attestations as alternates or sibling assets for supply-chain integrity.

---

## ğŸ§¬ Provenance & reproducibility (how we prevent â€œghost dataâ€)

### Determinism expectations
- Normalize timestamps (UTC, no microseconds unless required)
- Sort outputs consistently (stable ordering)
- Pin dependencies & capture environment identifiers
- Record `commit_sha` (and ideally `sbom_ref`, `manifest_ref`, `attestation_ref`) for governed releases

### Idempotent ingestion (automation-friendly)
When running a watcher/ingest:
- Use a deterministic key (e.g., hash of namespace + source URL + logical window)
- Write `.done` markers under `.state/` or similar
- Support a **kill switch** for safe dry-runs

---

## ğŸ›¡ï¸ FAIR+CARE, sovereignty & sensitivity

### Classification (required per dataset)
- `open` / `public`
- `public_with_constraints`
- `restricted`
- `sensitive` (requires review + masking)

### CARE rules (examples)
- Archaeological / cultural features may require **spatial generalization** (e.g., km-scale) before publication
- Never use AI transforms to â€œinfer sensitive locationsâ€ from restricted datasets

### What to do with sensitive data
- Keep raw sources protected (access controls, unlisted storage)
- Publish generalized derivatives + full provenance explaining the transformation
- Ensure STAC/DCAT reflect the sensitivity classification and constraints

---

## âœ… Validation & policy gates

KFM treats validation as a **required merge gate** (not optional best practice).

### Schema validation
- STAC validation (STAC schema + KFM profile)
- DCAT JSON-LD validation (shape/schema if available)
- PROV JSON-LD validation (shape/schema if available)
- Geo validation: geometry validity, CRS checks, bounds sanity

### Policy validation (OPA/Conftest style)
Block merge if:
- Missing `license`
- Missing `providers/publisher`
- Missing provenance linkage
- Missing CARE/sensitivity declarations when required

### Data quality validation
- Drift checks (time-series, distributions, missingness)
- Statistical sanity (regression residuals, experimental design checks where applicable)
- Snapshot / preview comparison for map assets when relevant

---

## ğŸš€ Publishing & promotion workflow

### Publication definition
A dataset is â€œpublishedâ€ when:
- It lives in `data/<domain>/processed/`
- It is referenced by **STAC + DCAT + PROV**
- It passes CI validation & policy gates
- (If applicable) it is exported to `data/graph/` for Neo4j import

### Promotion lifecycle (recommended)
1. Ingest writes deterministic artifacts under `data/staging/â€¦`
2. Validation + policy gates run in CI
3. A PR is opened (e.g., `ingest/<idempotency-key>`)
4. Merge == publish
5. Optional: promotion step moves staging outputs into canonical homes

---

## ğŸ”Œ Integration surfaces (why the structure matters)

- ğŸ•¸ï¸ **Graph (Neo4j)**: nodes/edges derived from STAC/DCAT/PROV + domain mappings
- ğŸ˜ **PostgreSQL/PostGIS**: query-ready tables for analysis and API backing
- ğŸ—ºï¸ **UI**: MapLibre + WebGL assets (PMTiles/GeoJSON/COG previews)
- ğŸ“– **Story Nodes**: narrative references must point to cataloged assets (IDs, not ad-hoc files)
- ğŸ›ï¸ **Focus Mode**: consumes validated + provenance-linked artifacts only

---

## ğŸ§ª Definition of Done (per dataset)

Use this checklist when adding or updating a dataset:

- [ ] Dataset has a stable `dataset_id`
- [ ] Raw capture stored under `data/<domain>/raw/` (or protected external store + pointer)
- [ ] Transform outputs stored under `data/<domain>/processed/` with versioning
- [ ] STAC Collection exists and validates
- [ ] STAC Items exist and validate
- [ ] DCAT dataset/distribution exists and validates
- [ ] PROV bundle exists and validates
- [ ] Checksums recorded (sha256 or multihash)
- [ ] Policy gates pass (license/providers/provenance/CARE)
- [ ] Graph exports updated (if this dataset participates in graph)
- [ ] README updated for the domain (what changed, why, and where to find it)

---

## ğŸ“š Project reference library (the â€œwhyâ€ behind these rules)

> These files influence the design decisions in this spec. Treat them as a curated grounding set for implementation choices, QA methods, and governance posture.

### ğŸ—ºï¸ Geospatial & cartography
- `Kansas-Frontier-Matrix_ Open-Source Geospatial Historical Mapping Hub Design.pdf` â€” hub architecture patterns (data + catalogs + UI)
- `python-geospatial-analysis-cookbook.pdf` â€” practical geospatial ETL + analysis patterns
- `making-maps-a-visual-guide-to-map-design-for-gis.pdf` â€” map readability + cartographic design constraints
- `Mobile Mapping_ Space, Cartography and the Digital - 9789048535217.pdf` â€” mobile/offline delivery implications (tiles, packaging)
- `Cloud-Based Remote Sensing with Google Earth Engine-Fundamentals and Applications.pdf` â€” cloud ingest/export patterns for remote sensing

### ğŸ§¾ Data catalogs, storage & interoperability
- `Data Spaces.pdf` â€” interoperability mindset (catalog + sharing + governance)
- `Scalable Data Management for Future Hardware.pdf` â€” performance, partitioning, and modern storage strategies
- `PostgreSQL Notes for Professionals - PostgreSQLNotesForProfessionals.pdf` â€” Postgres/PostGIS operational patterns

### ğŸ“Š Statistics, experimental design & modeling (QA + drift)
- `Understanding Statistics & Experimental Design.pdf`
- `graphical-data-analysis-with-r.pdf`
- `regression-analysis-with-python.pdf`
- `Regression analysis using Python - slides-linear-regression.pdf`
- `think-bayes-bayesian-statistics-in-python.pdf`

### ğŸ§  Modeling, simulation, graph theory
- `Scientific Modeling and Simulation_ A Comprehensive NASA-Grade Guide.pdf` â€” reproducibility, verification/validation mindset
- `Generalized Topology Optimization for Structural Design.pdf` â€” complex modeling outputs as first-class data artifacts
- `Spectral Geometry of Graphs.pdf` â€” graph analytics foundations (relevant for Neo4j/knowledge graph reasoning)

### ğŸ§‘â€âš–ï¸ Ethics, humanism, and governance posture
- `Introduction to Digital Humanism.pdf` â€” human-centered constraints on systems
- `Principles of Biological Autonomy - book_9780262381833.pdf` â€” autonomy/agency framing for governance
- `On the path to AI Lawâ€™s prophecies and the conceptual foundations of the machine learning age.pdf` â€” legal/AI governance framing

### ğŸ›¡ï¸ Security & integrity (do not ship â€œghost artifactsâ€)
- `ethical-hacking-and-countermeasures-secure-network-infrastructures.pdf` â€” security posture & threat models (high-level)
- `Gray Hat Python - Python Programming for Hackers and Reverse Engineers (2009).pdf` â€” security awareness (not operational guidance here)
- `compressed-image-file-formats-jpeg-png-gif-xbm-bmp.pdf` â€” image format tradeoffs (previews, scans, compression)

### ğŸ–¥ï¸ Web delivery & rendering
- `responsive-web-design-with-html5-and-css3.pdf` â€” UI delivery constraints affect data packaging (PMTiles, previews)
- `webgl-programming-guide-interactive-3d-graphics-programming-with-webgl.pdf` â€” GPU constraints drive tiling/LOD choices

### ğŸ§° Language/library packs (implementation reference)
- `A programming Books.pdf`
- `B-C programming Books.pdf`
- `D-E programming Books.pdf`
- `F-H programming Books.pdf`
- `I-L programming Books.pdf`
- `M-N programming Books.pdf`
- `O-R programming Books.pdf`
- `S-T programming Books.pdf`
- `U-X programming Books.pdf`

### ğŸ§¾ KFM internal system docs (source of truth)
- `Kansas Frontier Matrix (KFM) â€“ Comprehensive Engineering Design.docx`
- `MARKDOWN_GUIDE_v13.md.gdoc` â€” canonical doc + directory expectations
- `Comprehensive Markdown Guide_ Syntax, Extensions, and Best Practices.docx`
- `Latest Ideas.docx`
- `Other Ideas.docx`
- `Deep Learning for Coders with fastai and PyTorch - Deep.Learning.for.Coders.with.fastai.and.PyTorchpdf` *(reference; availability may vary)*

---

## ğŸ§° Templates (copy/paste starters)

<details>
<summary>ğŸ§¾ STAC Item â€” minimal skeleton</summary>

```json
{
  "stac_version": "1.0.0",
  "type": "Feature",
  "id": "<item-id>",
  "collection": "<collection-id>",
  "geometry": null,
  "bbox": [],
  "properties": {
    "datetime": "2026-01-08T00:00:00Z",
    "kfm:dataset_id": "kfm.<domain>.<dataset>",
    "kfm:version": "v0.1.0",
    "kfm:sensitivity": "public"
  },
  "assets": {
    "data": {
      "href": "data/<domain>/processed/<path-to-artifact>",
      "type": "application/octet-stream",
      "roles": ["data"],
      "title": "Primary artifact",
      "checksum:sha256": "<sha256>"
    },
    "prov": {
      "href": "data/prov/bundles/<dataset-id>__v0.1.0.prov.jsonld",
      "type": "application/ld+json",
      "roles": ["provenance"],
      "title": "PROV bundle"
    },
    "dcat": {
      "href": "data/catalog/dcat/dataset__<dataset-id>.jsonld",
      "type": "application/ld+json",
      "roles": ["metadata"],
      "title": "DCAT dataset metadata"
    }
  }
}
```

</details>

<details>
<summary>ğŸ—‚ï¸ DCAT Dataset â€” minimal skeleton</summary>

```json
{
  "@context": "https://www.w3.org/ns/dcat3.jsonld",
  "@type": "dcat:Dataset",
  "dct:title": "â€¦",
  "dct:description": "â€¦",
  "dct:license": "â€¦",
  "dcat:distribution": [{
    "@type": "dcat:Distribution",
    "dcat:accessURL": "data/<domain>/processed/<artifact>",
    "dcat:mediaType": "â€¦",
    "spdx:checksum": {
      "@type": "spdx:Checksum",
      "spdx:algorithm": "spdx:checksumAlgorithm_sha256",
      "spdx:checksumValue": "<sha256>"
    }
  }]
}
```

</details>

<details>
<summary>ğŸ§¬ PROV Bundle â€” minimal skeleton</summary>

```json
{
  "@context": "https://www.w3.org/ns/prov.jsonld",
  "entity": {
    "kfm:artifact": {
      "prov:label": "<artifact-name>",
      "prov:value": "data/<domain>/processed/<artifact>",
      "kfm:sha256": "<sha256>"
    }
  },
  "activity": {
    "kfm:run": {
      "prov:label": "ETL run <run-id>",
      "prov:startedAtTime": "2026-01-08T00:00:00Z",
      "prov:endedAtTime": "2026-01-08T00:10:00Z"
    }
  },
  "wasGeneratedBy": {
    "kfm:artifact": { "prov:activity": "kfm:run" }
  }
}
```

</details>

---

## ğŸ§­ Next docs in this folder (recommended)

Create these as this spec matures:

- `docs/specs/data/formats.md` â€” deep dive on accepted formats, media types, and conversion rules
- `docs/specs/data/stac.md` â€” KFM-STAC specifics (extensions, required fields, IDs)
- `docs/specs/data/dcat.md` â€” KFM-DCAT specifics (themes, distributions, mappings)
- `docs/specs/data/prov.md` â€” KFM-PROV specifics (activities, agents, plans, run IDs)
- `docs/specs/data/validation.md` â€” CI policy gates and local validation commands
- `docs/specs/data/sensitivity.md` â€” FAIR+CARE rules + sovereignty playbook

---

<div align="center">

ğŸ§© *Data is a promise.*  
ğŸ§¾ *Catalogs are the receipts.*  
ğŸ§¬ *Provenance is the audit trail.*

</div>

