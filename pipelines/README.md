<!--
ğŸ“Œ This README defines the *canonical pipeline boundary* for KFM (Kansas Frontier Matrix) / Kansasâ€‘Matrixâ€‘System.
ğŸ—“ï¸ Last updated: 2026-01-19
ğŸ” Review cycle: 90 days (or anytime pipeline order / catalogs / policy changes)
-->

<div align="center">

# ğŸ§¬ KFM Pipelines  
`pipelines/README.md`

**Deterministic ETL â†’ source manifests â†’ governed catalogs â†’ graph ingest â†’ APIs â†’ UI â†’ Story Nodes â†’ Focus Mode**  
The operational spine of **Kansas Frontier Matrix (KFM)**. ğŸ§ ğŸ—ºï¸ğŸ§¾

![Status](https://img.shields.io/badge/status-active-brightgreen)
![Master Guide](https://img.shields.io/badge/Master%20Guide-v13-1f6feb)
![Contract-first](https://img.shields.io/badge/contracts-contract--first-0aa3a3)
![Policy Pack](https://img.shields.io/badge/policy-OPA%20%7C%20Conftest-7c3aed)
![Evidence-first](https://img.shields.io/badge/evidence-catalog--before--graph-8957e5)
![Determinism](https://img.shields.io/badge/determinism-idempotent%20ETL-success)
![KFM Profiles](https://img.shields.io/badge/profiles-STAC%20%7C%20DCAT%20%7C%20PROV-7b42f6)
![Graph](https://img.shields.io/badge/graph-Neo4j-00c853)
![Spatial DB](https://img.shields.io/badge/spatial-PostGIS-336791)
![API Boundary](https://img.shields.io/badge/UI%20access-API%20only%20(no%20graph%20direct)-ff6b6b)
![UI](https://img.shields.io/badge/ui-React%20%7C%20MapLibre%20%7C%20Cesium(optional)-0ea5e9)
![Security](https://img.shields.io/badge/security-hostile--inputs%20%2B%20deny--by--default-red)
![Supply Chain](https://img.shields.io/badge/supply%20chain-SBOM%20%7C%20SLSA%20%7C%20signing-111827)
![Governance](https://img.shields.io/badge/governance-FAIR%20%2B%20CARE%20%2B%20Sovereignty-2ea043)

</div>

> **TL;DR:** `pipelines/` is the **portal + contract** for how KFM builds evidence.  
> The **executable pipeline code** lives in `src/pipelines/`.  
> The **publishable artifacts** live in `data/processed/**` and are not â€œrealâ€ until theyâ€™re **cataloged (STAC/DCAT)** and **traceable (PROV)**.

> [!IMPORTANT]
> **Prime directive:** **No catalog â†’ no graph â†’ no API â†’ no UI.**  
> Catalogs are the interface. Provenance is the receipt. ğŸ§¾âœ…

> [!IMPORTANT]
> **Second directive:** **No policy pass â†’ no merge â†’ no publish.**  
> Governance is enforced (automated + human review), not â€œbest-effort.â€ âš–ï¸ğŸ”’

---

## ğŸ”— Quick links (start here) ğŸ§­
- ğŸ  Repo overview: `../README.md`
- ğŸ§© Executable boundary: `../src/README.md` *(if present)*
- ğŸšª API boundary (governed trust edge): `../api/README.md` *(if present)*
- âš–ï¸ Policy Pack (OPA/Rego): `../api/scripts/policy/README.md` *(if present)*
- ğŸ“¦ Data + metadata boundary: `../data/README.md` *(required reading)*
- ğŸ§ª Tests & QA gates: `../tests/README.md`
- ğŸ§° Governed toolchain surface: `../tools/README.md`
- ğŸ§° Automation wrappers: `../scripts/README.md`
- ğŸ““ MCP (runs, experiments, receipts): `../mcp/README.md` *(or `../mcp/MCP-README.md` if thatâ€™s the canonical name)*
- ğŸ“˜ Master Guide (canonical intent + paths): `../docs/MASTER_GUIDE_v13.md` *(if present)*
- ğŸ§± Architecture & ADRs: `../docs/architecture/`
- ğŸ§¾ Governance: `../docs/governance/`
- ğŸ“ Schemas + profiles: `../schemas/` **and** `../docs/standards/`
- ğŸ§© Templates: `../docs/templates/`
- ğŸ“š Story Nodes (narrative content): `../docs/reports/story_nodes/` *(draft/published workflow)*
- ğŸŒ Web UI boundary: `../web/` *(React Â· MapLibre Â· optional Cesium)*

---

<details>
<summary><b>ğŸ§­ Table of contents</b></summary>

- [ğŸ§¾ Doc metadata](#-doc-metadata)
- [ğŸš¦ Nonâ€‘negotiables](#-non-negotiables)
- [ğŸ§  What a â€œpipelineâ€ means in KFM](#-what-a-pipeline-means-in-kfm)
- [ğŸ§­ Canonical paths & aliases](#-canonical-paths--aliases)
- [ğŸ§± The canonical ordering](#-the-canonical-ordering)
- [ğŸ§  Pipelines as â€œcompilersâ€](#-pipelines-as-compilers)
- [ğŸ§© Pipeline taxonomy](#-pipeline-taxonomy)
- [ğŸ“¦ Data & metadata lifecycle](#-data--metadata-lifecycle)
- [ğŸš€ Promotion workflow](#-promotion-workflow)
- [ğŸ§° GitOps publish boundary](#-gitops-publish-boundary)
- [âš–ï¸ Policy-as-code](#ï¸-policy-as-code-opa--conftest)
- [ğŸ§‘â€ğŸ¤â€ğŸ§‘ Wâ€‘Pâ€‘E automation](#-wpe-automation-watcher--planner--executor)
- [ğŸ“ Where things live](#-where-things-live)
- [ğŸ§¾ Standard artifacts](#-standard-artifacts)
- [ğŸ“œ KFM Pipeline Definition Contract](#-kfm-pipeline-definition-contract)
- [âš™ï¸ Running pipelines](#ï¸-running-pipelines)
- [âœ… Quality gates](#-quality-gates)
- [ğŸ§¾ Receipts, telemetry, and replay](#-receipts-telemetry-and-replay)
- [ğŸ” Governance & sovereignty](#-governance--sovereignty)
- [ğŸ›¡ï¸ Security & hostile inputs](#ï¸-security--hostile-inputs)
- [ğŸ”­ Performance & scaling](#-performance--scaling)
- [ğŸŒ¾ Example pipeline archetypes](#-example-pipeline-archetypes)
- [ğŸ§© Adding a new pipeline](#-adding-a-new-pipeline)
- [ğŸ“š Project reference library influence map](#-project-reference-library-influence-map)
- [ğŸ§¾ Metadata](#-metadata)
- [ğŸ•°ï¸ Version history](#ï¸-version-history)
- [ğŸ“ Evidence anchors](#-evidence-anchors)

</details>

---

## ğŸ§¾ Doc metadata

| Field | Value |
|---|---|
| Doc | `pipelines/README.md` |
| Status | Active âœ… |
| Last updated | **2026-01-19** |
| Review cycle | 90 days ğŸ” |
| Audience | Contributors implementing ETL jobs, validators, catalog writers, graph exports/ingest bridges |
| Prime directive | **No catalog â†’ no graph â†’ no API â†’ no UI.** Catalogs are the interface. |
| Second directive | **No policy pass â†’ no merge â†’ no publish.** |
| System mission fit | Make Kansas spatial truth **searchable, mappable, auditable, modelable** (provenance-first; AI is advisory) ğŸ§ ğŸ§¾ |

---

## ğŸš¦ Nonâ€‘negotiables

1) **Deterministic, idempotent ETL** ğŸ§ª  
   Same inputs + same config + same code â‡’ same outputs (stable IDs/hashes) and reruns do not corrupt or duplicate.

2) **Contract-first** ğŸ“œ  
   Pipelines are driven by declared contracts (schemas, profiles, OpenAPI) and contract changes trigger compatibility checks.

3) **Catalogs are not optional** ğŸ—‚ï¸  
   Data is not â€œrealâ€ in KFM until it has:
   - **STAC** (assets + spatial/temporal metadata)
   - **DCAT** (dataset discovery & distributions)
   - **PROV** (lineage + run identity)

4) **Evidence-first narrative** ğŸ“š  
   Story Nodes / Focus Mode must cite **cataloged evidence**. No unsourced narrative content.  
   If AI helps generate text: label it, attach provenance, and include confidence/uncertainty where applicable.

5) **API boundary rule** ğŸ›¡ï¸  
   The UI must **never** query Neo4j/DB directly; all access goes through governed APIs (contracts + redaction).

6) **Governed ordering is sacred** ğŸ§±  
   **ETL â†’ STAC/DCAT/PROV â†’ Graph â†’ API â†’ UI â†’ Story Nodes â†’ Focus Mode**

7) **Stable identifiers (no semantic IDs)** ğŸ§·  
   IDs must be **information-free** and invariant over time (donâ€™t encode meaning that will drift). Prefer UUID/ULID + metadata.  
   *If it â€œneeds renaming,â€ it wasnâ€™t a stable ID.* ğŸ§ 

8) **Policy-as-code gating (fail closed)** âš–ï¸ğŸ”’  
   Governance rules are enforced automatically (OPA/Rego + Conftest is the default posture). If a policy canâ€™t be evaluated, default is **deny**.

9) **GitOps & auditable publishing** ğŸ§¾ğŸ”  
   â€œPublishedâ€ generally means: validated artifacts + catalogs + provenance + policy report + review trail (PR/approvals).  
   *If it didnâ€™t go through review, itâ€™s not production evidence.*

> [!TIP]
> If your pipeline canâ€™t produce a clean paper trail (inputs â†’ transforms â†’ outputs â†’ catalogs â†’ lineage), itâ€™s not ready to merge. âœ…ğŸ§¾

---

## ğŸ§  What a â€œpipelineâ€ means in KFM

A KFM pipeline is a **replayable builder** that produces (at minimum):

- ğŸ“ **Source manifests** â†’ `data/sources/**` *(where the data came from, rights, sensitivity; pointer-over-payload)*
- ğŸ“¦ **Evidence artifacts** â†’ `data/processed/**` *(COG, GeoParquet, CSV, tiles, thumbnails, reports, model artifacts, etc.)*
- ğŸ—‚ï¸ **Catalog artifacts** â†’ `data/stac/**` + `data/catalog/dcat/**`
- ğŸ§¬ **Lineage artifacts** â†’ `data/prov/**` *(W3C PROV JSONâ€‘LD recommended)*
- ğŸ§· **Integrity artifacts** â†’ manifests, checksums, inventories
- ğŸ§ª **Gate artifacts** â†’ schema reports, policy reports, link-check reports *(deterministic + storable)*
- ğŸ“ˆ **Telemetry artifacts** â†’ run summaries, gate outcomes, timings *(location is configurable; keep it deterministic and linkable)*

> [!IMPORTANT]
> Pipelines do **not** â€œsecretly update the graph.â€  
> The graph ingests **from catalogs** (and/or explicit graph export artifacts) via controlled paths.

---

## ğŸ§­ Canonical paths & aliases

KFM has a few â€œnames youâ€™ll see in old notes.â€ Hereâ€™s the **current canonical set**:

| Concept | Canonical path âœ… | Common aliases you may see âš ï¸ |
|---|---|---|
| Source manifests | `data/sources/**` | *(varies)* |
| Raw drops | `data/raw/**` | *(same)* |
| Work / intermediate | `data/work/**` | *(same)* |
| Processed evidence | `data/processed/**` | *(same)* |
| STAC catalogs | `data/stac/**` | `data/catalog/**` *(older drafts)* |
| DCAT catalogs | `data/catalog/dcat/**` | `data/catalogs/**`, `data/catalog/**` |
| Provenance | `data/prov/**` | `data/provenance/**` |

> [!NOTE]
> When in doubt: follow **Master Guide v13** paths. Older path spellings should be treated as legacy aliases.

---

## ğŸ§± The canonical ordering

> [!IMPORTANT]
> This is a governance boundary, not a preference.

```mermaid
flowchart LR
  A["ğŸ§ª ETL + Normalization"] --> B["ğŸ—‚ï¸ STAC/DCAT/PROV Catalogs"]
  B --> C["ğŸ•¸ï¸ Graph (references catalogs)"]
  C --> D["ğŸ›¡ï¸ APIs (contracts + redaction)"]
  D --> E["ğŸ—ºï¸ Web UI (React Â· MapLibre Â· optional Cesium)"]
  E --> F["ğŸ“š Story Nodes (draft/published)"]
  F --> G["ğŸ¯ Focus Mode (context + evidence bundle)"]
```

---

## ğŸ§  Pipelines as â€œcompilersâ€

A helpful mental model: **pipelines behave like compilers** â€” inputs go through phases, and each phase has gates.  
This keeps the system honest: â€œbuild stepsâ€ are explicit, testable, and replayable. ğŸ§±

| Compiler concept ğŸ§© | Pipeline analogue ğŸ§¬ | What we enforce âœ… |
|---|---|---|
| Lexing/parsing | ingest + schema parse | reject malformed inputs early |
| Type checking | semantic validation | CRS, geometry validity, ranges, licensing |
| IR transforms | normalization | canonical encodings + stable sort order |
| Linking | catalog linkage | STAC â†” DCAT â†” PROV cross-refs present |
| Linting | policy checks | OPA/Rego denies block publish |
| Codegen | artifacts + catalogs | COG/Parquet + STAC/DCAT + PROV receipts |
| Optimization | scaling tactics | tiling, partitioning, caching, indexing |
| Error reporting | receipts & logs | actionable failures + correlation IDs |

> [!NOTE]
> A pipeline that â€œkind of worksâ€ but canâ€™t explain itself is a governance bug, not a feature. ğŸ§¾

---

## ğŸ§© Pipeline taxonomy

Not all pipelines look the same. KFM supports a few **governed shapes**:

| Type | When to use | Key rule ğŸ”‘ |
|---|---|---|
| ğŸ§± **Build (batch)** | One-time or periodic creation of a dataset | Must be deterministic + cataloged + provâ€™d before use |
| ğŸ” **Refresh (scheduled)** | Regular updates (daily/weekly/monthly) | Must be idempotent; versioned outputs; diffs are inspectable |
| ğŸ‘€ **Watcher (nearâ€‘realâ€‘time)** | Polling/streaming feeds (e.g., GTFSâ€‘RT) | Each window produces catalogable â€œunitsâ€ + receipts; no mystery updates |
| ğŸ”Œ **Adapter (import bridge)** | Bring in external exports (partner datasets, agency drops) | Must validate schema/license/classification before promotion |
| ğŸ§ª **Analysis/Model** | Derived indicators, Bayesian inference, simulation runs | Record params/seeds; output uncertainty + diagnostics as artifacts |
| ğŸ§® **Optimization** | Multi-constraint optimization runs (optional) | Record objective/constraints; deterministic run IDs; replay rules |
| ğŸ§± **Graph build/export** | Build bounded graph exports from catalogs | Graph edges must reference catalog IDs + provenance |
| ğŸ“„ **Document ingest** | PDFs/scans â†’ extracted text/entities | Store raw + derived; provenance + redaction rules required |
| ğŸ§Š **3D/volumetric** | 3D meshes, point clouds, volumes, 3D tiles | Coordinate conventions + LOD/tiling + validation gates |
| ğŸ§³ **Offline pack builder** | Field/classroom bundles (tiles + data slices) | Packs must embed manifests + catalog pointers + license bundle |

> [!NOTE]
> Watchers are still bound by ordering: **they produce cataloged outputs first**, then (optionally) graph/API consumption follows.

---

## ğŸ“¦ Data & metadata lifecycle

KFM uses a required staging lifecycle so everyone can tell â€œwhat stage is this file in?â€ at a glance:

### ğŸ“ Source manifests (pointer-over-payload)
- `data/sources/<domain>/<dataset>/source.json` â†’ where it came from, license, sensitivity, checksums/URLs *(best-effort)*  
  Think: **intent + rights + risk**.

### ğŸ“¥ Data stages
- `data/raw/<domain>/...` â†’ raw source drops *(read-only mindset)*
- `data/work/<domain>/...` â†’ intermediate transforms *(ok to delete/regenerate)*
- `data/processed/<domain>/...` â†’ final evidence artifacts *(publishable)*

### ğŸ—‚ï¸ Catalog + provenance stages (required before downstream use)
- `data/stac/` â†’ STAC collections/items (assets + metadata)
- `data/catalog/dcat/` â†’ DCAT datasets/distributions (discovery)
- `data/prov/` â†’ PROV bundles (run + dataset lineage)

### ğŸ•¸ï¸ Graph exchange stages (recommended when graph updates are needed)
- `data/graph/csv/` â†’ bounded import/export CSVs (bulk ingest friendly)
- `data/graph/cypher/` *(optional)* â†’ bounded Cypher scripts for controlled ingest

### ğŸ§³ Offline pack stages (optional, but governed)
- `data/packs/<pack_id>/` â†’ a self-contained â€œevidence bundleâ€ *(tiles + indexes + manifests + README)*

---

## ğŸš€ Promotion workflow

A pipeline output is either **not yet trustworthy**, or **published as governed evidence**.

### âœ… Promotion states (recommended)

| State | Location | Who can use it? | Required artifacts |
|---|---|---|---|
| ğŸŸ¡ `candidate` | `data/work/**` | pipeline devs only | none (but logs helpful) |
| ğŸŸ  `staged` | `data/processed/**` | reviewers + QA | checksums + basic gates |
| ğŸŸ¢ `published` | `data/processed/**` + catalogs | everyone downstream | **STAC + DCAT + PROV + manifest + policy pass** |

### ğŸ” Promotion rules (fail closed ğŸ”’)
- **No publish without license + classification.**
- **No publish without PROV lineage** (inputs + run config + output IDs).
- **No publish without STAC/DCAT** for anything user-visible.
- **No publish if classification would downgrade** (unless audited redaction step exists).
- **No publish if policy checks cannot run** (missing policies/inputs = deny).

```mermaid
flowchart TB
  C["ğŸŸ¡ candidate\n(data/work)"] -->|gates pass| S["ğŸŸ  staged\n(data/processed)"]
  S -->|catalog+prov+policy emitted| P["ğŸŸ¢ published\n(STAC/DCAT/PROV + policy pass)"]
  S -->|gates fail| F["ğŸ›‘ fail closed\n(receipt + fixes)"]
```

> [!TIP]
> Think â€œ**atomic publish**â€: write new outputs to a run-scoped directory â†’ validate â†’ promote/swap pointer â†’ emit catalogs â†’ declare published. âœ…

---

## ğŸ§° GitOps publish boundary

KFM treats the repository + CI as part of the pipeline boundary:

- PRs are the default â€œchange envelopeâ€ for **datasets, catalogs, policies, and pipeline code**
- CI runs **data QA + schema validation + policy pack** (deny-by-default)
- Merge (or signed release) is what *turns a candidate into published evidence*

```mermaid
flowchart LR
  A["PR opened ğŸ§¾\n(dataset/code/catalog)"] --> B["CI: Detect â†’ Validate âš™ï¸"]
  B -->|policy deny| X["âŒ Block\n(fail closed)"]
  B -->|all pass| C["Review âœ…\n(human + council as needed)"]
  C --> D["Merge/Release ğŸŸ¢\n(Publish)"]
  D --> E["Deploy/Sync ğŸ”\n(services + catalogs)"]
```

> [!NOTE]
> This is why â€œNo policy pass â†’ no merge â†’ no publishâ€ is a **pipeline rule**, not an ops detail.

---

## âš–ï¸ Policy-as-code (OPA + Conftest)

KFM governance rules should be executable:

- **OPA/Rego** encodes rules (license required, classification propagation, citation coverage for AI outputs, no direct-graph UI access, etc.)
- **Conftest** runs those rules in CI and produces actionable failures (rule IDs + messages)
- Policy checks are just another ring in the quality gates (and must be replayable)

**Recommended homes (common patterns):**
- `api/scripts/policy/` *(docs + CI hooks)*
- `tools/validation/policy/*.rego` *(policy source)*

> [!TIP]
> Treat policy failures like compiler errors: fix the input until it compiles. ğŸ§©âš–ï¸

---

## ğŸ§‘â€ğŸ¤â€ğŸ§‘ Wâ€‘Pâ€‘E automation (Watcher Â· Planner Â· Executor)

Some KFM maintenance can be automated â€” but only with guardrails:

- ğŸ‘€ **Watcher** detects events (new upstream data drop, broken link, schema drift, policy warning)
- ğŸ§  **Planner** drafts a plan **under policy constraints**
- ğŸ› ï¸ **Executor** performs the work by opening a PR (and never bypasses CI/policy)

**Rule of thumb:** automation must still produce **the same artifacts humans do**  
(manifests â†’ processed â†’ catalogs â†’ provenance â†’ policy pass â†’ PR trail). ğŸ§¾âœ…

---

## ğŸ“ Where things live

### ğŸ§­ Repo context (target shape)
```text
ğŸ“ pipelines/                 # ğŸ“ this folder (portal + conventions; not executable code)
ğŸ“ src/                       # ğŸ§© executable source code
â”‚  â”œâ”€â”€ ğŸ“ pipelines/          # ğŸ§ª ETL jobs + catalog writers + validators
â”‚  â”œâ”€â”€ ğŸ“ graph/              # ğŸ•¸ï¸ graph export/ingest tooling (from catalogs)
â”‚  â”œâ”€â”€ ğŸ“ server/             # ğŸ›¡ï¸ APIs (contracts + redaction enforcement)
â”‚  â””â”€â”€ ğŸ“ ai/                 # ğŸ¤– AI services (Focus Mode; advisory-only; citation gates)
ğŸ“ api/                       # ğŸšª API boundary docs/contracts (if separated)
â”‚  â””â”€â”€ ğŸ“ scripts/policy/     # âš–ï¸ policy pack docs + hooks (if separated)
ğŸ“ data/                      # ğŸ“¦ sources â†’ raw â†’ work â†’ processed + STAC/DCAT/PROV + graph exports
â”‚  â”œâ”€â”€ ğŸ“ sources/            # ğŸ“ source manifests (rights + sensitivity + pointers)
â”‚  â”œâ”€â”€ ğŸ“ raw/                # ğŸ“¥ raw drops
â”‚  â”œâ”€â”€ ğŸ“ work/               # ğŸ§ª intermediates
â”‚  â”œâ”€â”€ ğŸ“ processed/          # ğŸ“¦ publishable evidence artifacts
â”‚  â”œâ”€â”€ ğŸ“ stac/               # ğŸ—‚ï¸ STAC catalogs
â”‚  â”œâ”€â”€ ğŸ“ catalog/dcat/       # ğŸ—‚ï¸ DCAT catalogs
â”‚  â”œâ”€â”€ ğŸ“ prov/               # ğŸ§¬ provenance
â”‚  â”œâ”€â”€ ğŸ“ graph/csv/          # ğŸ•¸ï¸ bounded CSV exports/imports
â”‚  â””â”€â”€ ğŸ“ packs/              # ğŸ§³ offline packs (optional)
ğŸ“ schemas/                   # ğŸ“ JSON Schemas (contracts)
ğŸ“ docs/                      # ğŸ“˜ governed documentation (templates, standards, governance)
ğŸ“ tools/                     # ğŸ§° validators, QA tools, deterministic entrypoints
ğŸ“ scripts/                   # ğŸ§° orchestration wrappers (call tools/src)
ğŸ“ tests/                     # âœ… automated tests (unit/integration/e2e)
ğŸ“ web/                       # ğŸŒ UI (React + MapLibre + optional Cesium)
ğŸ“ releases/                  # ğŸ“¦ packaged releases (manifest + SBOM + attestations)
ğŸ“ .github/                   # ğŸ¤ CI/CD, policies, automation
```

### ğŸ§ª Pipeline code (canonical)
```text
ğŸ“ src/pipelines/
â””â”€â”€ ğŸ“ <domain>/
    â””â”€â”€ ğŸ“ <pipeline_name>/
        â”œâ”€â”€ run.py                     # entrypoint (CLI)
        â”œâ”€â”€ pipeline.yml               # ğŸ“œ pipeline contract (recommended)
        â”œâ”€â”€ README.md                  # short notes + examples (dev-facing)
        â”œâ”€â”€ config/                    # env configs (dev/stage/prod)
        â”œâ”€â”€ schemas/                   # domain schemas (if needed)
        â”œâ”€â”€ validators/                # QA gates (schema, bounds, link checks, etc.)
        â”œâ”€â”€ tests/                     # mini-run tests + fixtures
        â””â”€â”€ _shared/                   # optional submodules (prefer src/pipelines/_shared)
```

### ğŸ“˜ Domain module docs (recommended)
```text
ğŸ“ docs/data/
â””â”€â”€ ğŸ“ <domain>/
    â”œâ”€â”€ README.md                      # domain overview + pipeline list + access notes
    â””â”€â”€ ğŸ“ pipelines/
        â””â”€â”€ ğŸ“ <pipeline_name>/
            â””â”€â”€ README.md              # runbook: IO, cadence, gates, failure modes, replay rules
```

### ğŸ“š Story Node content (governed narrative)
```text
ğŸ“ docs/reports/story_nodes/
â”œâ”€â”€ ğŸ“ draft/
â””â”€â”€ ğŸ“ published/
```

> [!TIP]
> If you add a new domain pipeline, add a domain module README under `docs/data/<domain>/README.md` so reviewers can find the â€œwhat/why/howâ€ quickly. ğŸ§­âœ…

---

## ğŸ§¾ Standard artifacts

KFM evidence is **pointer-over-payload** whenever possible: catalogs + IDs + signed URLs/paths  
(instead of dumping giant blobs into API/UI). ğŸ”—ğŸ—‚ï¸

### âœ… Minimum publishable artifact set (KFM standard)
For any dataset intended for search/map/story/focus:

1) **Source manifests** in `data/sources/**`
   - origin, license, sensitivity/classification, access pointers
   - (optional) checksums for remote sources

2) **Evidence artifacts** in `data/processed/**`
   - preferred geo formats: **COG**, **GeoParquet**, **PMTiles** *(as needed)*
   - optional: thumbnails/quicklooks (small, cacheable)
   - optional: 3D Tiles / glTF assets (for 3D domains)

3) **Catalog artifacts**
   - **STAC** items/collections that reference the evidence assets
   - **DCAT** dataset + distributions for discovery

4) **Lineage artifacts**
   - **PROV** bundle: inputs â†’ activities â†’ outputs
   - includes run identity, config hash, git SHA *(where available)*

5) **Integrity + gate artifacts**
   - checksums manifest (sha256 preferred)
   - policy report (OPA/Conftest output, machine-readable)
   - optional: inventory (file sizes + media types)

6) **Graph exchange artifacts (only when needed)**
   - bounded CSV export/import in `data/graph/csv/**`
   - edges reference catalog IDs + provenance IDs

7) **Safety artifacts (when shipping containers/releases)**
   - SBOM (software bill of materials)
   - signed images/artifacts + attestations *(SLSA-like posture)*

> [!NOTE]
> Quicklooks are UX helpers. The authoritative truth is the evidence artifact + catalog metadata. ğŸ—‚ï¸âœ…

---

## ğŸ“œ KFM Pipeline Definition Contract

KFM pipelines are contract-first. A pipeline should have a machine-readable contract file (recommended: `pipeline.yml`)
that explains **what it reads, what it writes, and what it guarantees**.

### âœ… Minimum contract fields (recommended)
- `id` (stable, versioned; do not encode secrets)
- `name`, `description`, `owner`
- `sources` (source manifest refs; rights + sensitivity posture)
- `inputs` (sources, paths, checksums when feasible)
- `outputs` (paths + formats + dataset IDs)
- `catalogs` (STAC/DCAT targets; collections/items/datasets)
- `provenance` (how run_id/config hash is captured)
- `gates` (schema/bounds/link/license/classification/policy checks)
- `determinism` (stable sorting, seed strategy, idempotency key)
- `classification` + `license` rules (deny-by-default on unknowns)
- `policy` (which policy bundles/rulesets must pass)
- `network` posture (deny-by-default; allowlist & logging if enabled)
- `resources` (optional: memory/CPU hints; chunking strategy)
- `retention` (optional: how long intermediate artifacts persist)
- `privacy` posture (optional: PII checks; redaction/generalization rules)
- `graph_exports` *(optional)* (CSV/Cypher export paths and constraints)
- `offline_packs` *(optional)* (pack output + manifest rules)

### ğŸ§© Example `pipeline.yml` (starter template)
```yaml
id: "kfm.hydrology.watersheds.v1"
name: "Hydrology Watersheds Builder"
owner: "@kfm-engineering"
description: "Derives watershed boundaries + flow products from DEM inputs and publishes map-ready layers."

envs: ["dev", "stage", "prod"]

sources:
  - ref: "data/sources/elevation/3dep/source.json"
    notes: "Rights + sensitivity are enforced from source.json into catalogs and APIs."

inputs:
  - id: "kfm.elevation.dem.3dep.v1"
    stage: "raw"
    paths:
      - "data/raw/elevation/3dep/**"
    license: "public-domain-or-provider-license"
    classification: "public"

outputs:
  stage: "processed"
  datasets:
    - id: "kfm.hydrology.watersheds.v1"
      paths:
        - "data/processed/hydrology/watersheds/**"
      formats: ["COG", "GeoParquet", "PMTiles"]
      classification: "public"
      license: "CC-BY-4.0"

catalogs:
  stac_root: "data/stac"
  dcat_root: "data/catalog/dcat"
  collections:
    - "kfm.hydrology"
  items:
    strategy: "one item per logical unit (tile/county/basin)"
  dcat:
    dataset_id: "kfm.hydrology.watersheds.v1"

provenance:
  prov_root: "data/prov"
  run_id_env: "KFM_RUN_ID"
  config_hash: "sha256(pipeline.yml + config/<env>.yml)"
  record_git_sha: true
  record_seeds: true

gates:
  - "schema_required"
  - "crs_required"
  - "geometry_valid"
  - "bounds_sane"
  - "license_required"
  - "classification_no_downgrade"
  - "stac_schema"
  - "dcat_schema"
  - "prov_bundle_present"
  - "policy_pack_pass"
  - "link_check"

determinism:
  stable_sorting: true
  seeded: true
  seed_source: "KFM_SEED or derived from run_id"
  idempotency_key: "(dataset_id, input_checksums, config_hash)"

policy:
  required_rulesets:
    - "tools/validation/policy"
  conftest_profile: "kfm-v13"

network:
  default: "deny"
  allow_with_flag: "--allow-network"
  ssrf_protection: true
  log_urls_and_checksums: true

graph_exports:
  enabled: false
  csv_root: "data/graph/csv"
  rule: "edges must reference catalog ids + prov activity ids"

offline_packs:
  enabled: false
  packs_root: "data/packs"

retention:
  work_dir_ttl_days: 14
  keep_failed_runs: true
```

> [!IMPORTANT]
> The contract does not replace docs; it makes the docs **enforceable**.  
> CI can validate `pipeline.yml` shape and cross-check it against produced artifacts.

---

## âš™ï¸ Running pipelines

> [!NOTE]
> Prefer the repoâ€™s **make/CI entrypoints** when available.  
> If your repo doesnâ€™t have these targets yet, treat this section as intended ergonomics.

### âœ… Recommended: `make` entrypoints (examples)
```bash
# list pipelines (example)
make pipelines-list

# run a pipeline (example)
make pipeline RUN=hydrology/watersheds ENV=dev

# validate catalogs + policy (example)
make catalog-qa
make policy-qa

# graph ingest/export (example)
make graph-export
make graph-ingest
```

### ğŸ Direct execution (module style)
```bash
python -m src.pipelines.hydrology.watersheds.run --env dev --config config/dev.yml --run-id "RUN-2026-01-19-demo"
python -m src.pipelines.hazards.refresh.run --env dev --since "2026-01-01T00:00:00Z" --run-id "RUN-2026-01-19-hazards"
```

### ğŸ§± Expected flags (strongly recommended)
- `--help` (must include â‰¥2 runnable examples)
- `--env {dev|stage|prod}`
- `--config <path>`
- `--run-id <id>` (or `KFM_RUN_ID`)
- `--dry-run` default OR â€œno writes unless `--apply`â€
- `--apply` for state mutation
- `--allow-network` for any remote fetching (deny-by-default)
- `--telemetry-root <path>` *(optional but recommended)*
- `--log-level {DEBUG|INFO|WARNING|ERROR}` *(optional)*

### ğŸ§± Typical environment variables
| Variable | Purpose |
|---|---|
| `KFM_ENV` | `dev|stage|prod` |
| `KFM_RUN_ID` | provenance correlation across logs/catalogs/PROV |
| `KFM_DATA_ROOT` | data root (if not repo-relative) |
| `KFM_SOURCES_ROOT` | source manifests root |
| `KFM_STAC_ROOT` | STAC output root |
| `KFM_DCAT_ROOT` | DCAT output root |
| `KFM_PROV_ROOT` | PROV output root |
| `KFM_GRAPH_EXPORT_ROOT` | graph export root (CSV/Cypher/JSON) |
| `KFM_TELEMETRY_ROOT` | telemetry output root |
| `KFM_POLICY_ROOT` | policy pack root (OPA/Rego) |
| `KFM_SEED` | RNG seed for stochastic pipelines |
| `KFM_NEO4J_URI` | graph endpoint *(only for controlled ingest steps)* |

> [!TIP]
> For heavy geo deps (GDAL/PROJ), **Docker is your friend** ğŸ³  
> Containerize pipeline environments to reduce â€œworks on my machineâ€ drift.

---

## âœ… Quality gates

A pipeline is â€œdoneâ€ only when these pass (prefer â€œfail closedâ€ ğŸ”’):

### Ring 0 â€” Structure ğŸ§±
- JSON/YAML parses
- schema validation for outputs + catalogs + manifests
- required files exist (`pipeline.yml`, configs, outputs present)

### Ring 1 â€” Integrity ğŸ§·
- checksums/manifests recorded
- deterministic IDs stable when inputs unchanged
- atomic publish (no half-written processed outputs)

### Ring 2 â€” Semantics ğŸ§ 
- CRS correctness + axis order
- geometry validity (and any repair policy is explicit + logged)
- raster sanity (nodata, resolution, overviews for COG)
- bounds/time sanity (Kansas extent, plausible ranges, monotonic windows where required)

### Ring 3 â€” Governance & safety ğŸ”ğŸ›¡ï¸
- license required before publish
- classification/sensitivity propagation (no downgrade)
- redaction/generalization audited (when required)
- hostile input guards (archives, rasters, PDFs, GeoJSON, etc.)
- secrets/sensitive patterns not leaked to logs

### Ring 4 â€” Policy pack (OPA/Rego) âš–ï¸
- policy checks run and produce a deterministic report artifact
- deny rules block merge/publish (missing policies = deny)
- AI narrative outputs must include citations (cite-or-refuse)

### Ring 5 â€” Modeling credibility (when doing inference/simulation) ğŸ§ªğŸ“Š
If a pipeline produces analytical/model outputs, it must emit *diagnostics artifacts*:
- EDA/QC summaries (missingness, distribution checks)
- regression diagnostics (residual checks, assumptions, baselines)
- Bayesian outputs (priors, posterior summaries, credible intervals)
- simulation V&V posture (verification/validation notes, sensitivity metadata)
- uncertainty is first-class (intervals, confidence/credible bounds, caveats)

### ğŸ§° Catalog QA tooling (standard)
KFM uses a **Catalog QA gate** in CI/pre-release:
- expected home: `tools/validation/catalog_qa/`

> [!TIP]
> Make it easy for reviewers: `make catalog-qa` should be boring. ğŸ˜Œâœ…

---

## ğŸ§¾ Receipts, telemetry, and replay

KFM is evidence-first: pipelines should emit â€œreceiptsâ€ that let someone reproduce the run.

### âœ… Minimum receipt set (recommended for any publish)
- ğŸ§¾ `data/prov/<RUN-ID>.jsonld` (or a bundle directory)
- ğŸ§· checksums manifest for produced outputs
- ğŸ—‚ï¸ STAC + DCAT references for all published artifacts
- âš–ï¸ policy report artifact (Conftest output; machine-readable)
- ğŸªµ structured logs (human + optional JSONL)
- ğŸ”— PR/review reference *(recommended)*: publish trail is part of provenance

### â­ Recommended: MCP run receipt (when used for decisions or publish)
- `mcp/runs/<RUN-ID>/MANIFEST.md` (human narrative of â€œwhat happenedâ€)
- links to the relevant catalogs + outputs + gates
- any redactions/generalizations applied + rationale

> [!NOTE]
> Telemetry should help answer: **what ran, what changed, what gates passed, what was withheld/redacted, and why**.  
> In KFM, governance/AI can also produce an **append-only ledger** of significant outputs (especially AI answers).

---

## ğŸ” Governance & sovereignty

KFM is FAIR + CARE + sovereignty-aware by design ğŸª¶

### ğŸªª Classification propagation (deny-by-default)
- Outputs cannot be **less restricted** than inputs unless an explicit redaction/generalization step exists and is reviewed.
- If classification cannot be determined, default to **restricted**.

### ğŸ§· Stable IDs (information-free)
- Donâ€™t embed meaning (names, years, sequence, geography) into identifiers.
- Treat IDs as stable pointers; store meaning in metadata where it can evolve safely.

### ğŸª¶ Sovereignty & cultural protocols (first-class)
Some datasets require extra governance and/or special handling:
- culturally sensitive locations (coordinate fuzzing / aggregation)
- restricted heritage knowledge (access controls + disclosure UX)
- council/community approvals recorded as part of provenance

> [!TIP]
> If you need a custom field like `care_label` or `kfm:sensitivity`, treat it like a *contracted interface* and validate it in CI.

### âœ‚ï¸ Redaction/generalization is multi-layer
If redaction is required, it must be applied consistently:
- `data/processed/**` (redacted evidence artifact)
- STAC/DCAT metadata (flags + documentation)
- API layer (access control + redaction enforcement)
- UI layer (additional disclosure/UX checks)

### ğŸ§¾ Audit trails
- Pipelines should emit telemetry and provenance notes when redaction/generalization occurs.
- Governance reviews are required for classification/sensitivity changes.

### ğŸŒ Federation-ready posture (optional, but planned)
KFM is designed to scale into a multi-region â€œFrontier Matrixâ€ federation:
- prefer global/URN-like dataset identifiers
- allow cross-instance catalog references (donâ€™t duplicate what can be cited)
- permit policy pack tuning per jurisdiction (while sharing a baseline)

---

## ğŸ›¡ï¸ Security & hostile inputs

Pipelines ingest â€œfiles from the world.â€ Assume inputs are hostile by default. ğŸ§¯

### âœ… Required defensive posture
- validate file types & magic bytes (donâ€™t trust extensions)
- prevent path traversal (archives/extractors)
- defend against decompression bombs (archives/images)
- sanitize subprocess args when calling GDAL/other tooling
- parameterize SQL (never string-concat untrusted values)
- **never log secrets**; never print sensitive raw content

### ğŸš Shell scripting standards (when using Bash wrappers)
- default to strict mode: `set -euo pipefail`
- quote variables *always*
- never `eval` user-controlled inputs
- prefer explicit allowlists for arguments and file patterns

### ğŸŒ Network posture
- default: **no network**
- if a pipeline fetches remote inputs:
  - require `--allow-network`
  - block private IP ranges by default (SSRF defense)
  - log URLs + checksums of downloaded artifacts

> [!CAUTION]
> If someone malicious controls this input, whatâ€™s the maximum harm?  
> If the answer includes â€œrun code / exfiltrate / crash,â€ add guards **before** merging. ğŸš«ğŸ§¨âœ…

---

## ğŸ”­ Performance & scaling

KFM scales by staying **metadata-driven** and **chunk-friendly**:

- ğŸ“¦ partition work (tiles, counties, watersheds, time windows)
- ğŸ§± pipeline breakers at materialization boundaries (COG/Parquet outputs)
- ğŸ” replay safety (idempotency keys + deterministic ordering)
- â™»ï¸ avoid reprocessing unchanged inputs (checksums + manifests)
- ğŸ—„ï¸ push heavy spatial ops into PostGIS when safe (joins, intersects, buffers)
- âš–ï¸ acknowledge workload mix (real-time vs batch; read-heavy vs write-heavy) and isolate where needed
- ğŸ›°ï¸ compute-to-data for imagery-heavy domains
- ğŸ§³ offline packs (PMTiles + compact indexes) for field/classroom modes

> [!TIP]
> Prefer â€œboring performance winsâ€: stable chunking + caching + deterministic manifests.  
> Speed is good â€” **but correctness and provenance come first**. ğŸ§¾âœ…

---

## ğŸŒ¾ Example pipeline archetypes

Match an archetype before inventing a new one ğŸ§©

### 1) ğŸŒŠ Time-series & sensor ingestion (batch/refresh)
**Use when:** climate records, stream gauges, socio-economic time series  
**Outputs:** Parquet + temporal coverage metadata + catalog entries  
**Key gates:** schema, time window sanity, missingness checks, license, provenance.

### 2) ğŸ›°ï¸ Remote sensing compute-to-data ingest (batch/refresh)
**Use when:** imagery too large for local processing  
**Pattern:** compute externally â†’ ingest derived product â†’ publish COG + STAC + DCAT + PROV  
**Key gates:** range sanity, export params captured, uncertainty metadata.

### 3) ğŸ’§ Hydrology terrain processing (batch)
**Use when:** DEM-derived flow direction/accumulation, watersheds, streams  
**Outputs:** COG rasters + vectors + STAC Items per logical unit  
**Key gates:** CRS, nodata, alignment, geometry validity, Kansas bounds.

### 4) ğŸŒªï¸ Hazards refresh (scheduled refresh)
**Use when:** multi-source hazard chronicles (tornado, flood, drought, fire)  
**Pattern:** scheduled ETL â†’ normalized event records â†’ cataloged evidence + summaries  
**Downstream:** events become graph nodes linked to provenance + sources.

### 5) ğŸš GTFSâ€‘RT watcher (nearâ€‘realâ€‘time)
**Use when:** live transit telemetry (vehicle positions, trip updates)  
**Pattern:** watcher polls/streams â†’ writes time-windowed artifacts â†’ emits STAC Items per window/day â†’ DCAT distributions â†’ PROV per run/window  
**Key gates:** strict timestamp handling, dedupe, retention policy, governance classification.

### 6) ğŸ“„ Bulk document ingest (evidence-first)
**Use when:** PDFs/scans (reports, notices, historical docs) must become searchable evidence  
**Pattern:** store raw doc â†’ extract text (and optional entities) â†’ catalog as evidence with provenance + redaction rules  
**Key gates:** hostile PDF handling, PII policy checks, attribution/license capture.

### 7) ğŸ§® Simulation + optimization runs (job-style)
**Use when:** scenario runs matter for decision support  
**Pattern:** parameterized run â†’ outputs + uncertainty + diagnostics â†’ STAC/DCAT + PROV run bundle  
**Key gates:** V&V posture, sensitivity metadata, deterministic seeds, reproducible configs.

### 8) ğŸ§Š 3D GIS / volumetric artifacts (optional advanced)
**Use when:** 3D trenches, volumes, meshes, point clouds, LOD needs  
**Pattern:** ingest â†’ validate CRS/scale â†’ generate LOD/tiles â†’ catalog assets + provenance  
**Key gates:** coordinate sanity, metadata completeness, LOD budgets.

### 9) ğŸ§³ Offline pack builder (field/classroom)
**Use when:** no-network mode, demos, outreach, field research  
**Pattern:** compile PMTiles + small GeoParquet slices + indexes + README + license bundle  
**Key gates:** pack manifest present, license bundle present, size budgets, reproducible build.

> [!TIP]
> â€œValue-addedâ€ derived layers (summaries, clustering, indices) are still **evidence artifacts**: store in `data/processed/**` + STAC/DCAT + PROV. âœ…ğŸ—‚ï¸ğŸ§¬

---

## ğŸ§© Adding a new pipeline

### âœ… Checklist (minimum bar)
- [ ] Choose a domain: `src/pipelines/<domain>/`
- [ ] Define inputs/outputs **before** coding (contract-first)
- [ ] Add/confirm `data/sources/**/source.json` (rights + sensitivity + pointers)
- [ ] Implement deterministic ETL (config-driven; stable IDs)
- [ ] Write to `data/raw â†’ data/work â†’ data/processed` *(stage appropriately)*
- [ ] Emit STAC + DCAT + PROV (before downstream use)
- [ ] Produce a policy report (OPA/Conftest) and ensure it passes
- [ ] Add validators (schema, bounds, links, license, classification propagation)
- [ ] Add tests (unit + at least one mini end-to-end run)
- [ ] Add docs: `docs/data/<domain>/pipelines/<pipeline_name>/README.md`
- [ ] Ensure graph ingest/export is driven from catalogs (no ad-hoc inserts)

### ğŸ§¾ Pipeline runbook contract (what every pipeline doc must include)
Under `docs/data/<domain>/pipelines/<pipeline_name>/README.md`:

- ğŸ¯ Purpose + scope + SLA cadence
- ğŸ§º Inputs (sources, access requirements, licenses)
- ğŸ“ Source manifest notes (rights + sensitivity + access pointers)
- âœ… Validation gates (what fails fast; what warns)
- âš–ï¸ Policy gates (which denies could block publish)
- ğŸ§· Integrity model (hashing, manifests, idempotency)
- ğŸ—‚ï¸ STAC/DCAT mapping (collections/items/datasets)
- ğŸ§¬ PROV mapping (entities/activities/agents)
- ğŸ’¥ Failure modes + replay rules + kill switch
- ğŸª¶ Governance notes (classification, redaction/generalization, restrictions)

### ğŸ§ª Run receipts (MCP alignment)
If this run is used to justify decisions or publish evidence:
- add a run receipt: `mcp/runs/RUN-YYYY-MM-DD-.../`
- link evidence outputs (paths + catalog IDs)
- include gate outcomes + any redactions applied

---

## ğŸ“š Project reference library influence map

These project files shape pipeline design + review standards: determinism, validation, scaling, governance, security posture, map readiness, and human-centered constraints. ğŸ§ ğŸ§¾

<details>
<summary><strong>ğŸ“¦ Expand: Project files â†’ what they influence in pipelines</strong></summary>

### ğŸ§­ Core KFM design docs (direct pipeline influence)
| Project file | Primary lens | Pipeline-level impact |
|---|---|---|
| `Kansas Frontier Matrix (KFM) â€“ Comprehensive Technical Documentation.pdf` | ğŸ§­ System blueprint | Defines evidence-first posture, advisory-only AI, audit logging, and how UI/API/graph relate. |
| `Kansas Frontier Matrix (KFM) â€“ Comprehensive Architecture, Features, and Design.pdf` | ğŸ§± Architecture | Clean architecture boundaries, hybrid data stores, automation patterns (incl. Wâ€‘Pâ€‘E), observability posture. |
| `Kansas Frontier Matrix (KFM) â€“ AI System Overview ğŸ§­ğŸ¤–.pdf` | ğŸ¤– AI governance | Focus Mode is cite-or-refuse; XAI audit panel; immutable governance ledger; policy pack + CI pattern. |
| `Kansas Frontier Matrix â€“ Comprehensive UI System Overview.pdf` | ğŸ—ºï¸ UI constraints | â€œMap behind the mapâ€ provenance UX, offline pack needs, story + focus UX constraints that pipelines must serve. |
| `ğŸ“š Kansas Frontier Matrix (KFM) Data Intake â€“ Technical & Design Guide.pdf` | ğŸ“¥ Intake mechanics | Source manifests, catalog triplet linkage, CSV bulk graph ingest posture, federation considerations, policy pack role. |
| `Innovative Concepts to Evolve the Kansas Frontier Matrix (KFM).pdf` | ğŸš€ Next features | 4D digital twin posture, indigenous data sovereignty handling, AI co-pilot boundaries, community workflows. |
| `ğŸŒŸ Kansas Frontier Matrix â€“ Latest Ideas & Future Proposals.docx.pdf` | ğŸ§ª Roadmap | Performance/CI improvements, simulation expansion, UX + pipeline ergonomics targets. |

### ğŸ“š Reference library bundles (PDF portfolios)
These are **shelf bundles** containing many embedded PDFs used as implementation references.

| Bundle file | Primary lens | Pipeline-level impact |
|---|---|---|
| `AI Concepts & more.pdf` | ğŸ¤– ML/AI foundations | Model credibility gates (diagnostics, uncertainty), human-centered AI constraints, advisory-only posture. |
| `Data Managment-Theories-Architures-Data Science-Baysian Methods-Some Programming Ideas.pdf` | ğŸ—„ï¸ Data architecture | Catalogs as interfaces, scalability, CI/CD discipline, Bayesian methods for uncertainty-first outputs. |
| `Maps-GoogleMaps-VirtualWorlds-Archaeological-Computer Graphics-Geospatial-webgl.pdf` | ğŸ§Š GIS/3D/WebGL | Map honesty, projection hygiene, 3D evidence conventions, GPU-friendly formats + tiling/LOD discipline. |
| `Various programming langurages & resources 1.pdf` | ğŸ§° Implementation shelf | Polyglot tooling, compilers/phase thinking, security mindset, scripting discipline for wrappers and CI. |

</details>

---

## ğŸ§¾ Metadata

```yaml
title: "KFM Pipelines â€” canonical pipeline boundary"
path: "pipelines/README.md"
version: "v1.6.0"
last_updated: "2026-01-19"
review_cycle: "90 days"
prime_directive: "No catalog â†’ no graph â†’ no API â†’ no UI"
second_directive: "No policy pass â†’ no merge â†’ no publish"
pipeline_order: "ETL â†’ STAC/DCAT/PROV â†’ Graph â†’ APIs â†’ UI â†’ Story Nodes â†’ Focus Mode"
principles:
  - "contract-first"
  - "evidence-first"
  - "determinism-by-default"
  - "stable-identifiers (information-free)"
  - "deny-by-default security"
  - "policy-as-code (OPA/Rego) fail-closed"
  - "GitOps publish trail (PR/review as provenance)"
  - "FAIR+CARE + sovereignty-aware"
  - "modeling credibility (V&V + uncertainty artifacts)"
  - "federation-ready (cross-instance references)"
```

---

## ğŸ•°ï¸ Version history

| Version | Date | Summary | Author |
|---:|---|---|---|
| v1.6.0 | 2026-01-19 | Added manifest-first intake (`data/sources/**`); clarified canonical path aliases (catalog/prov); added GitOps publish boundary; formalized policy-as-code ring (OPA/Rego + Conftest); added Wâ€‘Pâ€‘E automation section; expanded standard artifacts (policy report, graph exchange, offline packs); refreshed influence map to the current project docs + portfolio bundles. | KFM Engineering |
| v1.5.0 | 2026-01-13 | Tightened â€œpipelines as compilersâ€ phase model; formalized promotion workflow (candidateâ†’stagedâ†’published); added standard artifact set incl. integrity + supply-chain notes; expanded credibility gates for inference/simulation; updated influence map. | KFM Engineering |
| v1.4.0 | 2026-01-11 | Aligned pipeline README with Master Guide v13 invariants (API boundary, evidence-first narrative); added pipeline taxonomy + PDC contract template; expanded receipts/telemetry; clarified docs paths. | KFM Engineering |
| v1.3.0 | 2026-01-09 | Strengthened pipeline contract essentials (declared IO, PROV, schema/bounds, atomic publish); expanded governance, security, scaling, and archetype guidance. | KFM Engineering |

---

## ğŸ“ Evidence anchors

> These are the project files directly used while updating this README.

### ğŸ§­ Core KFM docs
- `Kansas Frontier Matrix (KFM) â€“ Comprehensive Technical Documentation.pdf`
- `Kansas Frontier Matrix (KFM) â€“ Comprehensive Architecture, Features, and Design.pdf`
- `Kansas Frontier Matrix (KFM) â€“ AI System Overview ğŸ§­ğŸ¤–.pdf`
- `Kansas Frontier Matrix â€“ Comprehensive UI System Overview.pdf`
- `ğŸ“š Kansas Frontier Matrix (KFM) Data Intake â€“ Technical & Design Guide.pdf`
- `Innovative Concepts to Evolve the Kansas Frontier Matrix (KFM).pdf`
- `ğŸŒŸ Kansas Frontier Matrix â€“ Latest Ideas & Future Proposals.docx.pdf`

### ğŸ“š Reference bundles (PDF portfolios)
- `AI Concepts & more.pdf`
- `Data Managment-Theories-Architures-Data Science-Baysian Methods-Some Programming Ideas.pdf`
- `Maps-GoogleMaps-VirtualWorlds-Archaeological-Computer Graphics-Geospatial-webgl.pdf`
- `Various programming langurages & resources 1.pdf`

---

<div align="center">

**Â© 2026 Kansas Frontier Matrix** Â· CCâ€‘BY 4.0 (project docs)  
ğŸ§¬ FAIR+CARE Â· ğŸª¶ Sovereignty-aware Â· ğŸ›¡ï¸ Policy-gated builds Â· ğŸ§¾ Evidence-first

</div>