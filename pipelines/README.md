# ğŸš° Pipelines

![Pipelines](https://img.shields.io/badge/Pipelines-ETL%20%7C%20GIS%20%7C%20AI-blue)
![Standards](https://img.shields.io/badge/Standards-STAC%20%7C%20DCAT%20%7C%20PROV-green)
![Storage](https://img.shields.io/badge/Storage-PostGIS%20%7C%20Object%20Store-orange)
![Governance](https://img.shields.io/badge/Governance-OPA%20Policies%20%7C%20Fail--Closed-critical)
![Interoperability](https://img.shields.io/badge/Interoperability-OGC%20Tiles%20%7C%20GeoJSON%20%7C%20COG-informational)

---

## ğŸ¯ Purpose

This directory is the **runbook + conventions hub** for KFMâ€™s data pipelines.

Pipelines are responsible for:
- ğŸ“¥ **Ingesting** raw data (vector, raster, tabular, historical artifacts)
- ğŸ§ª **Transforming** into interoperable, queryable formats
- ğŸ“š **Publishing metadata** (DCAT + STAC) and ğŸ§¾ **provenance** (W3C PROV)
- ğŸ—„ï¸ **Loading / serving** via PostGIS + tile services
- ğŸ§© **Exposing** through the API (datasets, search, tiles, safe query)
- ğŸ–¥ï¸ **Powering** UI + AI experiences with consistent, governed data

> ğŸ§  **Design principle:** â€œfail-closedâ€ governance â€” if required metadata/policy checks are missing, pipelines **do not publish**.

---

## ğŸ§­ Quick Links

- ğŸ—ï¸ System architecture: `../docs/architecture/system_overview.md`
- ğŸ“¦ Repo standards (STAC/DCAT/PROV + structure): `../docs/standards/`
- ğŸ§© API contract (datasets, tiles, pipeline triggers): `../src/server/api/README.md`
- ğŸ§° Tools (validators / helpers): `../tools/`
- ğŸ§ª Tests: `../tests/`
- ğŸ“š Data catalog outputs:
  - STAC: `../data/stac/`
  - DCAT: `../data/catalog/dcat/`
  - PROV bundles: `../data/prov/`

---

## ğŸ—ºï¸ Canonical Pipeline Flow

```mermaid
flowchart LR
  Raw["ğŸ§± Raw\n(data/<domain>/raw)"] --> Processed["âœ… Processed\n(data/<domain>/processed)"]
  Processed --> Catalog["ğŸ“š Catalog\n(STAC + DCAT)"]
  Catalog --> Prov["ğŸ§¾ Provenance\n(W3C PROV)"]
  Prov --> DB["ğŸ—„ï¸ Database\n(PostGIS + indexes)"]
  DB --> API["ğŸ§© API\n(/api/v1/* + /tiles/*)"]
  API --> UI["ğŸ–¥ï¸ UI + ğŸ¤– AI\n(web/ + tools)"]

  Gate["ğŸ›¡ï¸ Quality + Policy Gates\n(OPA + CI + validators)"] -.enforces.-> Raw
  Gate -.enforces.-> Processed
  Gate -.enforces.-> Catalog
  Gate -.enforces.-> API
```

âœ… **Rule of thumb:** if a dataset isnâ€™t **cataloged** (STAC/DCAT/PROV), it isnâ€™t â€œrealâ€ in KFM.

---

## ğŸ§± Where Data & Metadata Live

KFM splits **data** from **pipeline code**:

### ğŸ“ Data domains (inputs + outputs)
Each domain has the same pattern:

```
data/<domain>/
  ğŸ§± raw/         # read-only source drops (immutable snapshots)
  ğŸ§ª work/        # intermediate artifacts (cache, scratch, temp)
  âœ… processed/   # final outputs (publishable)
  ğŸ—ºï¸ mappings/    # optional: dataset â†’ STAC/DCAT/PROV mapping notes
  ğŸ“„ README.md    # domain runbook
```

### ğŸ“ Catalog / provenance (system-wide)
```
data/
  ğŸ“ stac/                # STAC Collections + Items
  ğŸ“ catalog/dcat/        # DCAT outputs (JSON-LD, etc.)
  ğŸ“ prov/                # provenance bundles per run/dataset
```

### ğŸ§  Pipeline implementation code
Depending on the repo version, pipeline code may live in:
- âœ… `src/pipelines/` (canonical layout)
- or this directory acts as the orchestration layer pointing into `src/pipelines/`

---

## ğŸš€ Running Pipelines

### Option A â€” Local dev (recommended) ğŸ³

1) Start the stack (DB/API/services):
```bash
docker compose up -d
```

2) Run a pipeline (examples â€” map to your actual runner):
```bash
# Example runner patterns (pick the one your repo implements)
python -m kfm.pipelines run <pipeline_name>
python -m src.pipelines run <pipeline_name>

# If you use Makefile targets:
make pipeline PIPELINE=<pipeline_name>
```

3) Validate outputs:
```bash
# Validators should confirm: schemas, metadata, provenance, policy
make validate
python -m tools.validate all
```

---

### Option B â€” Trigger via API (maintainers) ğŸ”

Pipelines can be triggered remotely (useful for automation / CI):

```bash
curl -X POST "http://localhost:5000/api/v1/ingest/runPipeline" \
  -H "Authorization: Bearer $KFM_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"pipeline":"<pipeline_name>","reason":"new raw drop"}'
```

Then monitor status via the pipeline run endpoints (run logs + PROV bundle).

> ğŸ” **CI/CD hook idea:** GitHub Actions (or another CI) can call this endpoint after data changes land.

---

### Option C â€” Scheduled runs â±ï¸

Typical schedules:
- ğŸŒ™ Nightly: â€œpoll sourcesâ€ + refresh federated datasets
- ğŸ“† Weekly: heavier recompute / derived analytics refresh
- ğŸ›°ï¸ Event-driven: STAC feed triggers remote sensing ingest

---

## ğŸ›°ï¸ Pipeline Types

### 1) Remote sensing ingest (STAC-fed)
**Trigger:** new scene notification (e.g., Landsat/Sentinel/LiDAR)  
**Compute:** clip/mosaic, reproject, tile pyramid, COG conversion  
**Publish:** store raster in object store + emit STAC Item + PROV run bundle

Outputs often include:
- ğŸ—ºï¸ COGs (Cloud-Optimized GeoTIFF)
- ğŸ§± Tile sets (PMTiles / XYZ tiles)
- ğŸ§¾ Provenance + processing lineage

Optional: run AI steps (cloud masks, land cover classification, change detection) to produce new â€œgeo-intelligenceâ€ layers ğŸ¤–ğŸŒ¾

---

### 2) Vector + tabular ingest (federated + harvested)
**Inputs:** boundary layers, infrastructure networks, historical features, CSVs  
**Storage:** PostGIS (spatial indexing, joins, point-in-polygon, etc.)  
**Serving:** vector tiles (MVT), GeoJSON streaming, approved SQL views

Federated approach options:
- ğŸ”— Live-link via external services (ArcGIS/OGC)
- ğŸ“¦ Periodic harvest into `data/<domain>/raw/` snapshots for reproducibility

---

### 3) AI-derived layers (governed outputs)
If a pipeline produces AI-derived content (classifications, extractions, summaries):
- must attach ğŸ“Œ model/run context
- must include ğŸ§¾ provenance + citations (where applicable)
- must pass ğŸš« disallowed-content / policy checks  
Failing checks means: **no publish**.

---

## ğŸ“š Metadata + Provenance (Nonâ€‘negotiable)

Every published dataset must be discoverable & auditable:

### âœ… DCAT (dataset catalog)
- Title, description, license, publisher/owner
- Distributions / assets (files, API links, tile URLs)
- Tags, temporal coverage, spatial extent

### âœ… STAC (spatiotemporal assets)
For imagery & tiled datasets:
- Collection + Items (footprints, timestamps, resolution, cloud cover, etc.)
- Links to physical assets (COGs, thumbnails, tile endpoints)

### âœ… W3C PROV (lineage)
Per run / dataset, record:
- inputs (source manifest, raw artifact hash)
- processing steps + parameters
- software versions / container digests
- outputs + checksums

---

## ğŸ›¡ï¸ Quality Gates (Failâ€‘Closed)

These gates are intentionally strict. Pipelines **must** stop if requirements arenâ€™t met:

### ğŸ§¾ Source manifest required (before processing)
A pipeline **cannot begin** unless a source manifest exists with at least:
- publisher / origin
- license / usage rights
- sensitivity classification

### ğŸ“š Catalog publish requires metadata completeness
A dataset **cannot be listed** unless:
- STAC/DCAT/PROV artifacts exist
- mapping/links are valid
- required fields pass schema validation

### ğŸ¤– AI outputs require extra checks
AI-derived layers must include:
- citations / justification (where applicable)
- disallowed content checks
- governance policy compliance

### ğŸ§ª CI checks fail the merge
CI should block merges if:
- validations fail
- formatting/linting fails
- license header / attribution checks fail
- policy-as-code checks fail (OPA)

> ğŸ’¥ â€œFail-closedâ€ means **default deny** â€” no silent success, no partial publish.

---

## ğŸ§° Authoring a New Pipeline

### 1) Pick a dataset ID ğŸ·ï¸
Use a stable, readable ID:
- `ks_hydrology_1880`
- `ks_trails_historic`
- `ks_landsat_ndvi_monthly`

### 2) Create a source manifest ğŸ§¾
Put it next to the raw drop or pipeline definition.

Example (YAML):
```yaml
id: ks_hydrology_1880
title: Kansas Hydrology (1880)
publisher: "<agency_or_archive>"
source_url: "<where_it_came_from>"
license: "<license_name_or_url>"
sensitivity: public   # public | internal | restricted
update_frequency: "one-time" # or daily/weekly/monthly/event-driven
spatial_extent: "Kansas"
notes: "Add any acquisition constraints or caveats here"
```

### 3) Implement the ETL stages ğŸ§±â†’âœ…
Recommended breakdown:
- `extract` â†’ fetch/read/normalize raw
- `transform` â†’ clean/standardize/projection fixes
- `validate` â†’ schema + geometry + constraints
- `load` â†’ PostGIS / object store
- `publish` â†’ metadata + provenance + tiles

### 4) Emit STAC/DCAT/PROV ğŸ“šğŸ§¾
- DCAT dataset summary + distributions
- STAC items/collections (if spatiotemporal assets)
- PROV run bundle with hashes + parameters

### 5) Add tests âœ…
Minimum:
- schema validation
- sample query checks (can the API serve it?)
- metadata completeness checks
- policy checks (OPA)

---

## ğŸ§ª Recommended File Layout (Template)

> Adjust to match your repo layout â€” the goal is consistency.

```
src/pipelines/
  ğŸ“ <domain>/
    ğŸ“ <dataset_id>/
      ğŸ“„ pipeline.yaml
      ğŸ“„ source_manifest.yaml
      ğŸ extract.py
      ğŸ transform.py
      ğŸ load.py
      ğŸ publish.py
      ğŸ“ tests/
        ğŸ§ª test_contracts.py
        ğŸ§ª test_metadata.py
      ğŸ“„ README.md   # dataset-specific runbook (optional)

tools/
  ğŸ§° validate.py
  ğŸ§° stac_build.py
  ğŸ§° dcat_build.py
  ğŸ§° prov_build.py

data/
  ğŸ“ <domain>/
    ğŸ§± raw/
    ğŸ§ª work/
    âœ… processed/
```

---

## ğŸ§© How Pipelines Surface Through the API

Once processed + cataloged, datasets should be available via:
- `GET /api/v1/datasets/{id}` â†’ DCAT summary + asset links (STAC, files, etc.)
- `GET /api/v1/catalog/search` â†’ discoverability by keyword/bbox/time
- `GET /api/v1/datasets/{id}/data?format=geojson&bbox=...` â†’ data streaming
- `GET /api/v1/query?...` â†’ safe, logged SQL-like access to approved views
- `GET /tiles/{layer}/{z}/{x}/{y}.pbf|.png` â†’ map tile serving

---

## ğŸ”’ Security & Sensitivity

KFM pipelines are expected to:
- ğŸ§· tag datasets with sensitivity (public/internal/restricted)
- ğŸ§¹ sanitize outputs where required (columns, geometry precision, redactions)
- ğŸ•µï¸ log access + pipeline runs for auditability
- ğŸ›¡ï¸ enforce policy checks at every boundary (ingest, publish, API)

---

## ğŸ§¯ Troubleshooting

- **Pipeline wonâ€™t start** â†’ check for missing `source_manifest.yaml`
- **Dataset not appearing in UI/search** â†’ DCAT/STAC not published or schema invalid
- **Tiles 404** â†’ layer not registered / tile cache not built / permissions issue
- **API returns sanitized output** â†’ sensitivity classification or policy enforcement
- **CI blocks merge** â†’ treat as a feature; fix gates rather than bypass them ğŸ˜‰

---

## ğŸ§  Glossary

- **STAC**: SpatioTemporal Asset Catalog (imagery + spatiotemporal asset metadata)
- **DCAT**: Data Catalog Vocabulary (dataset discovery + distributions)
- **PROV**: W3C Provenance model (lineage + reproducibility)
- **COG**: Cloud-Optimized GeoTIFF (efficient raster access)
- **MVT**: Mapbox Vector Tiles (`.pbf`) for fast vector map rendering
- **OPA**: Open Policy Agent (policy-as-code enforcement)

---

### ğŸŒ¾ Build responsibly. Publish confidently. Keep provenance forever. ğŸ§¾âœ¨