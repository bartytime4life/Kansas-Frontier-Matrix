<div align="center">

<picture>
  <source media="(prefers-reduced-motion: reduce)" srcset="../docs/assets/kfm-seal-320.png">
  <img src="../docs/assets/branding/kfm-seal-animated-320.gif" width="180" alt="Kansas Frontier Matrix (KFM) â€” seal (animated)" />
</picture>

# ğŸš° KFM Pipelines

**Deterministic ingestion + GIS processing + evidence/AI-derived artifacts â€” published only when metadata, provenance, and policy gates pass.**  
<sub><em>â€œETL â†’ STAC/DCAT/PROV â†’ Graph â†’ API â†’ UI â†’ Story Nodes â†’ Focus Modeâ€ â€” the canonical ordering is nonâ€‘negotiable.</em></sub>

<br/>

<!-- Project posture -->
<img alt="Status: Under Construction" src="https://img.shields.io/badge/status-under_construction-yellow?style=for-the-badge">
<img alt="Governance: Fail-Closed" src="https://img.shields.io/badge/governance-fail--closed-critical?style=for-the-badge">
<img alt="Evidence: Provenance-First" src="https://img.shields.io/badge/evidence-provenance--first-8a2be2?style=for-the-badge">
<img alt="Boundary Artifacts: STAC/DCAT/PROV" src="https://img.shields.io/badge/boundary_artifacts-STAC%20%7C%20DCAT%20%7C%20PROV-1f6feb?style=for-the-badge">
<br/>
<img alt="Runtime Stores" src="https://img.shields.io/badge/stores-PostGIS%20%7C%20Neo4j%20%7C%20Search/Vector-0ea5e9?style=for-the-badge">
<img alt="API Boundary" src="https://img.shields.io/badge/api-trust_membrane-111827?style=for-the-badge">

<br/>

<a href="#-purpose">ğŸ¯ Purpose</a> â€¢
<a href="#-canonical-truth-path">ğŸ§­ Truth Path</a> â€¢
<a href="#-where-pipeline-code-lives-v13">ğŸ“ Code Location</a> â€¢
<a href="#-pipeline-types">ğŸ›°ï¸ Types</a> â€¢
<a href="#-contracts--required-artifacts">ğŸ“œ Contracts</a> â€¢
<a href="#-running-pipelines-local-to-ci">ğŸš€ Run</a> â€¢
<a href="#-gates-fail-closed">ğŸ›¡ï¸ Gates</a> â€¢
<a href="#-auditability--observability">ğŸ§¾ Audit</a> â€¢
<a href="#-author-a-new-pipeline">ğŸ§° New Pipeline</a>

</div>

---

> [!WARNING]
> ğŸš§ **Under Construction (Active Development)**  
> This hub captures the **current intent + guardrails** for pipelines in KFM.  
> If this doc conflicts with implementation, **implementation wins** â€” then update this doc to restore alignment.

---

## ğŸ¯ Purpose

Pipelines are where KFM turns **raw evidence** into **publishable, governed artifacts**.

A KFM-grade pipeline is responsible for:

- ğŸ“¥ **Ingesting raw snapshots** (immutable evidence)
- ğŸ§ª Producing **work artifacts** (auditables, QA outputs, intermediate transforms)
- âœ… Emitting **processed outputs** (serve-ready, analysis-ready, reproducible)
- ğŸ—‚ï¸ Publishing **boundary artifacts** for discovery + reuse:
  - **STAC** (spatiotemporal assets)
  - **DCAT** (dataset discovery + distributions)
  - **PROV** (lineage: inputs â†’ activities â†’ outputs)
- ğŸ—„ï¸ Loading **runtime stores** for performance (PostGIS / Neo4j / search/vector)
- ğŸ”’ Enforcing **fail-closed governance** at publish boundaries (license/attribution/sensitivity required)

> [!IMPORTANT]
> **Databases are performance caches â€” not truth.**  
> Truth is the versioned artifacts in `data/` + their catalogs/provenance + the pipeline code that can rebuild them.

---

## ğŸ§­ Canonical Truth Path

KFM enforces a strict ordering so every dataset (and every derived layer) is **rebuildable, auditable, and governable**:

```text
ETL â†’ STAC/DCAT/PROV catalogs â†’ Neo4j graph â†’ APIs â†’ React/Map UI â†’ Story Nodes â†’ Focus Mode
```

No stage may leapfrog or bypass prior contracts/outputs. âœ…

### Data-lifecycle view (what pipelines actually do)

```mermaid
flowchart LR
  R["ğŸ§¾ Raw snapshots<br/>data/raw/&lt;domain&gt;/"] --> W["ğŸ§ª Work artifacts<br/>data/work/&lt;domain&gt;/"]
  W --> P["âœ… Processed outputs<br/>data/processed/&lt;domain&gt;/"]

  P --> STAC["ğŸ—‚ï¸ STAC<br/>data/stac/collections + items"]
  P --> DCAT["ğŸ—ƒï¸ DCAT<br/>data/catalog/dcat/"]
  P --> PROV["ğŸ”— PROV<br/>data/prov/"]

  STAC --> PG[(PostGIS)]
  STAC --> OBJ[(Object store/CDN<br/>COGs â€¢ PMTiles â€¢ 3D Tiles)]
  DCAT --> S[(Search/Vector)]
  PROV --> PG
  PROV --> S
  PG --> API["ğŸ§© API boundary<br/>src/server/ (policy + contracts)"]
  S --> API
  API --> UI["ğŸ–¥ï¸ UI / Maps"]
  API --> SN["ğŸ“š Story Nodes"]
  API --> FM["ğŸ¯ Focus Mode"]
```

> [!CAUTION]
> If a â€œshortcutâ€ bypasses catalogs/provenance/policy, itâ€™s treated as a **defect** â€” not a feature.

---

## ğŸ“ Where pipeline code lives (v13)

In the v13 layout, pipeline implementation is **canonical** under:

- `src/pipelines/` â€” ETL jobs and domain pipelines  
- `data/` â€” raw/work/processed outputs + catalog/prov boundary artifacts  
- `src/server/` â€” API boundary (serves only governed outputs)  
- `web/` â€” UI (never direct DB access)

> [!TIP]
> If you create a new pipeline, place it in `src/pipelines/` â€” donâ€™t scatter data processing code elsewhere.

---

## ğŸ§­ Quick links (repo-local)

- ğŸ—ï¸ System architecture: `../docs/architecture/system_overview.md`
- ğŸ¤– AI / Focus Mode: `../docs/architecture/AI_SYSTEM_OVERVIEW.md`
- ğŸ§¾ Standards & profiles: `../docs/standards/`
- ğŸ“¦ Data vault rules: `../data/README.md`
  - STAC: `../data/stac/`
  - DCAT: `../data/catalog/dcat/`
  - PROV: `../data/prov/`
- ğŸŒ API boundary: `../src/server/`
- âœ… Tests: `../tests/`
- ğŸ§° Tools & validators: `../tools/`

---

## ğŸ›°ï¸ Pipeline types

| Type | Examples | Primary outputs | Boundary artifacts |
|---|---|---|---|
| ğŸ—ºï¸ Vector + tabular ingest | boundaries, railroads, trails, census tables | GeoParquet/Parquet (GeoJSON only when small) | DCAT + PROV always; STAC when assetized/spatiotemporal |
| ğŸ›°ï¸ Raster / remote sensing | Landsat/Sentinel, LiDAR products, NOAA rasters | COGs, PMTiles, thumbnails, derived rasters | STAC item/collection + PROV; DCAT record for dataset discovery |
| ğŸ•¸ï¸ Graph enrichment | entity extraction, relationship building, story-to-data linking | Neo4j import CSV + mapping artifacts | DCAT + PROV (graph references catalogs; no bulky duplication) |
| ğŸ¤– Evidence/AI-derived artifacts | OCR corpora, masks, change layers, confidence layers | publishable layers + run documentation | Treated as **first-class datasets**: STAC/DCAT/PROV required + extra governance |
| ğŸ§ª QA / validation | schema checks, geometry QA, profiling | reports, metrics, QA artifacts | Publish only if referenced downstream; otherwise keep as work artifacts |

---

## ğŸ“œ Contracts & required artifacts

### âœ… Dataset contract: minimum publishable bundle

A dataset is â€œservableâ€ in KFM only when these exist and cross-link correctly:

| Artifact | Canonical location |
|---|---|
| Raw snapshot(s) | `data/raw/<domain>/...` |
| Work artifacts (optional but recommended) | `data/work/<domain>/...` |
| Processed output(s) | `data/processed/<domain>/...` |
| STAC Items/Collections (when applicable) | `data/stac/collections/*.json` + `data/stac/items/*.json` |
| DCAT dataset record | `data/catalog/dcat/*.jsonld` |
| PROV lineage bundle | `data/prov/*.(prov.)json` |

> [!IMPORTANT]
> Missing license / attribution / sensitivity metadata is a **publish blocker**.  
> CI and policy should treat incomplete bundles as **fail-closed**.

### ğŸ”— Cross-link expectations (what â€œtraceableâ€ means)

If something is shown on a map, used in a story, or retrieved for Focus Mode, reviewers must be able to walk:

```text
UI/Story/Focus â†’ DCAT â†’ STAC â†’ PROV â†’ Raw Evidence
```

---

## ğŸš€ Running pipelines (local to CI)

> [!NOTE]
> Runner commands vary by repo iteration. Prefer calling thin wrappers in `tools/` or `Makefile` targets when available.

### Pattern A â€” Local dev stack (Docker Compose)
```bash
docker compose up -d
```

### Pattern B â€” Run a pipeline via a CLI runner (examples)
```bash
# Example patterns (adjust to match the repoâ€™s actual runner)
python -m kfm.pipelines run <dataset_id>
python -m src.pipelines.<domain>.<pipeline_module> --help
make pipeline PIPELINE=<dataset_id>
```

### Pattern C â€” Validate outputs â€œlike CIâ€
```bash
# Examples (adjust to match repo tooling)
conftest test .            # policy gates (OPA/Conftest)
python -m tools.validate   # schema/catalog/prov validators
pytest                     # pipeline/unit tests
```

---

## ğŸ›¡ï¸ Gates (fail-closed)

> [!IMPORTANT]
> **Default deny.** If policy canâ€™t decide: deny.  
> If required metadata/provenance/license is missing: deny.

### Gate 0 â€” Source & rights (before processing)
A pipeline should not begin unless the source can be described in a reviewable way:
- publisher/origin and acquisition method
- license/rights statement
- sensitivity classification/tagging (and any handling constraints)

### Gate 1 â€” Output validity (before â€œprocessedâ€ is accepted)
Before considering outputs publishable:
- schema validity (domain-specific + common invariants)
- geometry validity + bbox sanity (if spatial)
- CRS explicitly declared (donâ€™t guess)
- null conventions documented
- deterministic behavior (same inputs/config â†’ same outputs, or differences explainable)

### Gate 2 â€” Boundary artifacts (before discovery/search/UI)
A dataset cannot appear in search/UI unless:
- DCAT record exists and validates
- STAC exists when required (imagery/tiled/spatiotemporal assets)
- PROV exists and links inputs â†’ steps â†’ outputs
- external assets include checksums + retrieval method (if not committed)

### Gate 3 â€” Evidence/AI-derived outputs (extra governance)
If a pipeline emits AI/analysis outputs:
- record model/tool version + parameters in PROV (and/or a run card)
- enforce sensitivity propagation (outputs cannot be less restricted than inputs)
- ensure downstream retrieval/citation behavior can point back to the bundle

---

## ğŸ§¾ Auditability & observability

Pipelines should produce artifacts humans can review **and** machines can validate.

### Minimum per run
- `run_id` (timestamp + dataset id + git sha or build id)
- structured logs (JSON preferred)
- checksums for publishable outputs
- PROV bundle describing:
  - inputs (raw snapshots)
  - activities (transforms)
  - agents (pipeline/tooling)
  - outputs (processed assets + boundary artifacts)

### Strongly recommended
- a short â€œRun Cardâ€ markdown summarizing:
  - what changed
  - counts/bbox/time coverage deltas
  - QA warnings
  - policy outcomes / obligations

---

## ğŸ”’ Security & sensitivity

Pipelines must:
- propagate classification from inputs â†’ outputs (no â€œless restrictedâ€ outputs)
- redact/sanitize outputs when required (attributes, geometry precision, derived aggregates)
- never embed secrets in configs/manifests
- log sensitive runs appropriately (audit posture depends on policy tier)

---

## ğŸ§° Author a new pipeline

This sequence aligns with the projectâ€™s â€œadd a datasetâ€ flow.

1) **Add raw snapshot**  
   Place source snapshots under: `data/raw/<domain>/...`  
   (If assets are too large, commit a pointer strategy + checksums rather than silently omitting identity.)

2) **Write pipeline code (rebuildable from scratch)**  
   Create a script/module under: `src/pipelines/<domain>/...`  
   It should read raw inputs, write intermediates to `data/work/<domain>/...`, and publish outputs to `data/processed/<domain>/...`.

3) **Generate boundary artifacts**  
   For processed outputs, create:
   - STAC (item/collection) under `data/stac/`
   - DCAT under `data/catalog/dcat/`
   - PROV under `data/prov/`

4) **Update indexes/caches (only if needed for runtime)**  
   Load into PostGIS/Neo4j, and/or update search/vector indexes where appropriate.

5) **Expose via the API boundary (if new surface needed)**  
   If existing dataset endpoints suffice, prefer reuse.  
   If new behavior is needed, extend FastAPI routes under `src/server/` and keep UI/API separation strict.

6) **Confirm governance completeness**  
   Ensure license, attribution, and sensitivity tagging are present â€” CI should flag missing requirements.

### âœ… Definition of Done (pipeline PR)
- [ ] Raw snapshot (or pointer + checksum strategy) is present and documented
- [ ] Pipeline is deterministic and rerunnable
- [ ] Outputs land in `data/work/<domain>/` and `data/processed/<domain>/`
- [ ] STAC/DCAT/PROV exist, validate, and cross-link correctly
- [ ] Any required cache loads (PostGIS/Neo4j/indexes) are reproducible
- [ ] Tests + validators pass locally and in CI
- [ ] Docs/runbook updated (domain notes, known caveats)

---

## ğŸ§¯ Troubleshooting

<details>
<summary><b>Pipeline wonâ€™t start</b> ğŸš«</summary>

Common causes:
- missing or ambiguous rights/license info
- sensitivity not declared
- runner canâ€™t locate the pipeline module/config

</details>

<details>
<summary><b>Dataset doesnâ€™t show up in search/UI</b> ğŸ”</summary>

Common causes:
- DCAT record missing/invalid
- STAC missing (for assets that require it)
- PROV missing or not linked
- policy denies due to missing metadata or sensitivity tags

</details>

<details>
<summary><b>Tiles 404</b> ğŸ§±</summary>

Common causes:
- layer not registered in API
- tiles not built or built to non-canonical paths
- policy denies tile access/caching due to classification

</details>

---

## ğŸ§  Glossary

- **STAC** ğŸ·ï¸: SpatioTemporal Asset Catalog (asset metadata: where/when/what)
- **DCAT** ğŸ—‚ï¸: Data Catalog Vocabulary (dataset discovery + distributions)
- **W3C PROV** ğŸ”—: provenance model (inputs â†’ process â†’ outputs)
- **COG** ğŸ›°ï¸: Cloudâ€‘Optimized GeoTIFF (partial reads, web-friendly)
- **PMTiles / MVT** ğŸ§©: tile packaging and vector tile formats for web maps
- **OPA** ğŸ§¾: Open Policy Agent (policy-as-code enforcement)
- **Failâ€‘Closed** ğŸ”’: default deny; publish only on complete compliance

---

<div align="center">

### ğŸŒ¾ Build responsibly. Publish confidently. Keep provenance forever. ğŸ§¾âœ¨
<sub>When in doubt: make it reproducible, traceable, and policy-compliant.</sub>

</div>

<details>
<summary><b>ğŸ“ Source material used to shape this Pipelines hub</b> (audit trail)</summary>

- **KFM v13 canonical ordering + directory layout:** `docs/MASTER_GUIDE_v13.md` (see project master guide)
- **Dataset addition workflow (pipeline â†’ STAC/DCAT/PROV â†’ stores â†’ API):** KFM System Documentation
- **API boundary (â€œtrust membraneâ€) posture and â€œDB as cacheâ€ framing:** KFM Developer Guide
- **FAIR/CARE and sovereignty posture:** Indigenous Statistics / KFM blueprint governance sections

</details>