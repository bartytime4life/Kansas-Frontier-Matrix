# ğŸ“„ MCP Datasheets â€” `mcp/datasheets/`  
> **Human-friendly dataset documentation** (with the same â€œevidence-firstâ€ mindset as the rest of KFM) ğŸ§­

![Provenance-first](https://img.shields.io/badge/provenance-first-2ea44f)
![FAIR+CARE](https://img.shields.io/badge/FAIR%20%2B%20CARE-governance-blue)
![STAC%20%2B%20DCAT%20%2B%20PROV](https://img.shields.io/badge/STAC%20%2B%20DCAT%20%2B%20PROV-catalog%20triplet-purple)
![MCP](https://img.shields.io/badge/MCP-documentation--first-orange)

Welcome to **Datasheets** âœ…  
This folder contains **datasheets for datasets** used by the Kansas Frontier Matrix (KFM): concise, standardized Markdown docs describing *what the dataset is*, *how it was built*, *whatâ€™s inside*, *how to use it safely*, and *what to watch out for*.

Datasheets complement (but **do not replace**) KFMâ€™s machine-readable metadata (STAC/DCAT/PROV). Theyâ€™re the â€œreaderâ€™s guideâ€ that makes the catalog legible to humans â€” historians, GIS folks, devs, and AI reviewers alike. ğŸ§ ğŸ—ºï¸

---

## ğŸ§­ Why datasheets exist (the â€œso what?â€)

Datasheets help us:
- âœ… **Onboard fast**: understand a dataset without spelunking ETL code.
- âœ… **Stay reproducible**: capture assumptions, transformations, versions, and pitfalls.
- âœ… **Enforce governance**: license clarity, sensitive-data handling, FAIR/CARE notes.
- âœ… **Support UI + Focus Mode**: give the product and AI assistant accurate dataset context (without inventing facts).
- âœ… **Prevent â€œmystery dataâ€**: every dataset should have an identity, story, and limits.

---

## ğŸ§± How datasheets fit the KFM evidence chain

KFM generally treats dataset publication as a set of **boundary artifacts** that downstream systems can trust:

- **STAC** â†’ spatial/temporal footprint + assets + discovery  
- **DCAT** â†’ catalog/portal discovery + distribution links  
- **PROV** â†’ lineage: sources â†’ processes â†’ outputs  
- **Datasheet (this folder)** â†’ human narrative: purpose, schema, quality, ethics, usage

If STAC/DCAT/PROV is the â€œmachine citation,â€ the datasheet is the â€œhuman explanation.â€ ğŸ§¾âœ¨

---

## ğŸ“¦ Directory layout (recommended)

> We keep this folder organized so contributors can find things quickly.

```text
mcp/
â””â”€ datasheets/
   â”œâ”€ README.md                 ğŸ‘ˆ you are here
   â”œâ”€ _templates/               ğŸ§© copy/paste templates
   â”‚  â”œâ”€ dataset-datasheet.md
   â”‚  â””â”€ external-dataset-note.md
   â”œâ”€ domains/                  ğŸ—‚ï¸ grouped by theme/domain
   â”‚  â”œâ”€ environment/
   â”‚  â”œâ”€ history/
   â”‚  â”œâ”€ infrastructure/
   â”‚  â””â”€ demographics/
   â””â”€ _index.yml                ğŸ§­ optional: index of dataset_id â†’ path
```

> If your repo structure differs, keep the *spirit*: predictable naming + easy navigation.

---

## ğŸ·ï¸ Naming rules (important)

**One datasheet per dataset_id.**

Recommended filename:
- `mcp/datasheets/domains/<domain>/<dataset_id>.md`

Where:
- `dataset_id` is a **stable slug** (also used by API/catalog IDs)
- `domain` matches KFMâ€™s dataset grouping (Environment / Historical / etc.)

Examples:
- `domains/environment/ks_drought_index_1895_2020.md`
- `domains/history/ks_land_treaties_1803_1905.md`

---

## ğŸ”— Required cross-links (keep humans + machines aligned)

Every datasheet should link to (or reference IDs for):
- ğŸ§Š **STAC** item/collection location
- ğŸ—ƒï¸ **DCAT** dataset record location
- ğŸ§¬ **PROV** lineage bundle location
- ğŸ“ Produced assets (e.g., GeoJSON / COG / PMTiles / GeoParquet)
- ğŸ§ª ETL pipeline entry point (script/notebook/config) and run instructions

> The goal: if someone reads the datasheet, they can *recreate the dataset* or *audit its lineage*.

---

## ğŸ§  Safety & governance rules (donâ€™t skip)

### ğŸ›¡ï¸ Sensitive data
If a dataset includes sensitive locations, personal data, or culturally sensitive info:
- **Do not** publish precise coordinates or personal identifiers in the datasheet.
- Describe redaction/generalization at a high level.
- Add a clear **access policy note** (who can see what, and why).
- Prefer *policy references* over *sensitive content*.

### âš–ï¸ FAIR + CARE callouts
Include short sections for:
- **FAIR**: discovery, access, formats, interoperability, reuse constraints
- **CARE**: community benefit, authority to control, responsibility, ethics

> Treat these as first-class metadata, not footnotes.

---

## âœï¸ Datasheet template (copy/paste)

> Use this template for any dataset compiled or curated by KFM.

<details>
<summary><strong>ğŸ“„ Click to expand: Dataset Datasheet Template</strong></summary>

```markdown
---
dataset_id: ks_example_dataset_YYYY
title: "Kansas Example Dataset (YYYY)"
domain: environment | history | infrastructure | demographics | ...
version: "v1.0.0"
status: draft | review | published | deprecated
license: "SPDX-ID or text"
created: "YYYY-MM-DD"
updated: "YYYY-MM-DD"
owners:
  - name: "Team/Person"
    role: "Maintainer"
contact: "email-or-issue-link"
stac:
  collection: "path/or/id"
  items: "path/or/pattern"
dcat:
  record: "path/or/id"
prov:
  bundle: "path/or/id"
assets:
  - type: "GeoJSON | COG | PMTiles | GeoParquet | CSV | ..."
    path: "data/processed/..."
---

# 1) Overview ğŸ§­
## What is this?
A 2â€“4 sentence description of the dataset.

## Why does it exist?
Problem/use-case it supports (UI layer, story nodes, analysis, etc.).

## What questions should it answer?
- Q1
- Q2
- Q3

# 2) Scope ğŸ—ºï¸â³
## Spatial coverage
- CRS:
- Bounding region:
- Resolution/scale:

## Temporal coverage
- Start:
- End:
- Granularity (daily/monthly/yearly/event-based):

# 3) Lineage & Methods ğŸ§¬
## Sources (raw inputs)
- Source A (what, where, retrieval method)
- Source B ...

## Processing summary (ETL)
- Step 1: â€¦
- Step 2: â€¦
- Step 3: â€¦
> Link to pipeline scripts/configs; note deterministic expectations.

## Versioning policy
- How updates happen
- How old versions are preserved

# 4) Data dictionary ğŸ“š
> List every field users will see.

| Field | Type | Units | Nullable | Description | Notes |
|------|------|-------|----------|-------------|------|
| id   | str  | â€”     | no       | Stable record id | |
| ...  |      |       |          |             | |

# 5) Quality & validation âœ…
## Validation checks performed
- Schema checks:
- Geometry checks:
- Range checks:
- Deduplication rules:

## Known issues / limitations âš ï¸
- Limitation 1
- Limitation 2

# 6) Ethics, privacy, and CARE ğŸ§‘ğŸ½â€ğŸ¤â€ğŸ§‘ğŸ¾
## Sensitive data handling
- Redaction/generalization approach:
- Access policy notes:

## CARE notes
- Collective benefit:
- Authority to control:
- Responsibility:
- Ethics:

# 7) Access & usage ğŸ”Œ
## How to access
- API endpoints / GraphQL queries:
- Download links:
- UI layers/story nodes that use this dataset:

## Recommended uses
- Use case A
- Use case B

## Non-recommended uses ğŸš«
- Misuse A
- Misuse B

# 8) Changelog ğŸ§¾
- v1.0.0 â€” Initial published version
```

</details>

---

## ğŸ§¾ External dataset note (when you *didnâ€™t* compile it)

If KFM uses a third-party dataset â€œas-isâ€, add a short note instead of pretending we produced it.

<details>
<summary><strong>ğŸŒ Click to expand: External Dataset Note Template</strong></summary>

```markdown
---
dataset_id: external_example_dataset
title: "External Dataset Name"
status: published
license: "As stated by publisher"
source_url: "..."
stac/dcat/prov: "links/ids if mirrored into KFM catalog"
---

# External Dataset Note ğŸŒ
## What it is
Short summary of what the publisher provides.

## What KFM changed (if anything)
- None (mirrored as-is)
- Or: reprojected / normalized schema / clipped to Kansas, etc.

## Caveats
- Known publisher limitations
- Any KFM-specific handling (rate limits, caching, etc.)
```
</details>

---

## ğŸ§‘â€ğŸ’» Contribution workflow (PR checklist)

When adding or updating a datasheet, your PR should include:

- [ ] Datasheet added/updated under `mcp/datasheets/...`
- [ ] Dataset **version** bumped (if content changed materially)
- [ ] Links/IDs for **STAC/DCAT/PROV** are present and correct
- [ ] Data dictionary updated to match actual output schema
- [ ] Quality checks + known limitations documented
- [ ] License clearly stated
- [ ] FAIR + CARE notes included (especially for people/community/cultural content)
- [ ] Sensitive content not leaked (no precise protected coordinates, no personal info)
- [ ] If dataset behavior changed: changelog updated

---

## ğŸ§© How this connects to the rest of KFM

- **UI** expects governed, provenance-linked data (datasheets help humans understand what the UI is showing).  
- **Story Nodes** pair Markdown narrative with JSON â€œmap/timeline stateâ€; they should reference dataset_ids and reuse datasheet language for accuracy.  
- **Focus Mode AI** should rely on dataset metadata + provenance; datasheets provide the â€œhuman semanticsâ€ that prevent misinterpretation.

---

## ğŸ“š Project reference library (optional but handy)

These PDFs were added to the project as a shared research bookshelf. Some are **PDF portfolios** that may require Adobe Reader/Acrobat to browse their embedded documents.

- `AI Concepts & more.pdf` ğŸ¤–  
- `Data Managment-Theories-Architures-Data Science-Baysian Methods-Some Programming Ideas.pdf` ğŸ—ƒï¸  
- `Maps-GoogleMaps-VirtualWorlds-Archaeological-Computer Graphics-Geospatial-webgl.pdf` ğŸŒ  
- `Mapping-Modeling-Python-Git-HTTP-CSS-Docker-GraphQL-Data Compression-Linux-Security.pdf` ğŸ§°  
- `Geographic Information-Security-Git-R coding-SciPy-MATLAB-ArcGIS-Apache Spark-Type Script-Web Applications.pdf` ğŸ§ª  
- `Various programming langurages & resources 1.pdf` ğŸ§‘â€ğŸ’»  

---

## ğŸ§­ Quick FAQ

**Q: Do we need a datasheet for every dataset?**  
A: If KFM compiles/curates it, yes. If itâ€™s external â€œas-is,â€ prefer a short external note.

**Q: Where should I describe ETL details â€” datasheet or code?**  
A: Both. The datasheet should summarize *what* and *why*, and link to the pipeline for *how*.

**Q: Can I add new facts to a datasheet?**  
A: Only if those facts are supported by the datasetâ€™s sources/metadata. If itâ€™s a claim, it should trace back to provenance artifacts.

---

## âœ… TL;DR
Datasheets are the **human layer** of KFMâ€™s provenance chain. Keep them **accurate**, **auditable**, and **useful** â€” and always tie them back to STAC/DCAT/PROV + pipeline reality. ğŸ§ ğŸ”—ğŸ—ºï¸