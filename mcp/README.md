---
title: "ğŸ§ª MCP â€” Master Coder Protocol (Methods, Controls & Processes) ğŸ§¾âš™ï¸"
path: "mcp/README.md"
version: "v1.5.0"
last_updated: "2026-01-19"
status: "active"
doc_kind: "Directory README"
license: "CC-BY-4.0"

# Repo doctrine (aligns with KFM redesign blueprint v13+)
doctrine:
  contract_first: true
  provenance_first: true
  evidence_first: true
  deny_by_default: true
  focus_mode_advisory_only: true
  atomic_promotion: true

# Governance posture (FAIR+CARE + sovereignty-aware)
governance:
  fair_category: "FAIR+CARE+Sovereignty"
  care_label_default: "public"
  sensitivity_default: "public"
  classification_default: "open"
  ai_human_in_loop_default: "required"
  policy_packs_expected: true

owners:
  primary: "KFM Engineering"
  reviewers: ["Maintainers", "Data Steward(s)", "Governance Council (when applicable)"]
---

# ğŸ§ª MCP â€” Master Coder Protocol  
### *Methods, Controls & Processes* ğŸ§¾âš™ï¸

![README](https://img.shields.io/badge/README-v1.5.0-8957e5)
![Docs-first](https://img.shields.io/badge/docs-documentation--first-blue)
![Contract-first](https://img.shields.io/badge/contracts-schema--first-1f6feb)
![Provenance-first](https://img.shields.io/badge/provenance-first-7b42f6)
![Reproducible](https://img.shields.io/badge/reproducible-audit--ready-success)
![Evidence](https://img.shields.io/badge/evidence-catalog--linked-informational)
![PROV](https://img.shields.io/badge/lineage-W3C%20PROV-7b42f6)
![Governance](https://img.shields.io/badge/governance-FAIR%2BCARE%20%2B%20Sovereignty-2ea043)
![Policy](https://img.shields.io/badge/policy-OPA%20%2B%20Conftest-important)
![Security](https://img.shields.io/badge/security-hostile--inputs%20%2B%20deny--by--default-red)
![KFM](https://img.shields.io/badge/KFM-Kansas%20Frontier%20Matrix-orange)

> **TL;DR:** `mcp/` is KFMâ€™s **methods + receipts** layer ğŸ““ğŸ§   
> It holds **protocols**, **run receipts**, **SOPs**, **model cards**, **gate reports**, and **review artifacts**â€”so every result can be **re-run, reviewed, and trusted** âœ…  
>
> **Design alignment (v13+):** KFM is **APIâ€‘centric**, **catalogâ€‘driven**, and **governanceâ€‘enforced**. MCP is the paper trail that keeps those promises honest. ğŸ§¾ğŸ›¡ï¸

> [!IMPORTANT]
> In this repo, **MCP = Master Coder Protocol** âœ…  
> **MCP â‰  Model Context Protocol** ğŸš« *(not what we mean here)*  
> Keep this distinction consistent in docs, PRs, issues, and commit messages.

---

## ğŸ”— Quick links
- ğŸ§­ Repo overview: **[`../README.md`](../README.md)**
- ğŸ“š Docs boundary: **[`../docs/README.md`](../docs/README.md)** *(if present)*
- ğŸ§¬ Pipelines boundary (contract portal): **[`../pipelines/README.md`](../pipelines/README.md)** *(if present)*
- ğŸ§° Scripts boundary (automation): **[`../scripts/README.md`](../scripts/README.md)** *(if present)*
- ğŸ§© Executable source boundary: **[`../src/README.md`](../src/README.md)**
- ğŸ“¦ Data + metadata boundary: **[`../data/README.md`](../data/README.md)**
- ğŸ§ª Notebooks boundary (lab bench): **[`../notebooks/README.md`](../notebooks/README.md)** *(if present)*
- ğŸ§° Validators & tooling: **[`../tools/README.md`](../tools/README.md)** *(if present)*
- âœ… Tests & CI gates: **[`../tests/README.md`](../tests/README.md)** *(if present)*
- ğŸ¤ CI/CD & policies: **[`../.github/`](../.github/)** *(workflows, security policy, automation)*

---

## âš¡ Quick Nav
- [ğŸ§¾ Doc metadata](#-doc-metadata)
- [ğŸ§­ What MCP is](#-what-mcp-is)
- [ğŸ§· Repo invariants](#-repo-invariants)
- [ğŸ§± MCP artifacts](#-mcp-artifacts-types-ids-and-immutability)
- [ğŸš¦ Nonâ€‘negotiables](#-non-negotiables)
- [ğŸ Quick Start](#-quick-start)
- [ğŸ—‚ï¸ Directory layout](#-directory-layout)
- [ğŸ” The MCP workflow loop](#-the-mcp-workflow-loop)
- [ğŸš¥ Detect â†’ Validate â†’ Promote](#-detect--validate--promote)
- [ğŸ” Policy packs & gate reports](#-policy-packs--gate-reports)
- [ğŸ¤– Watcher â†’ Planner â†’ Executor receipts](#-watcher--planner--executor-receipts)
- [ğŸ§¬ DevOps provenance](#-devops-provenance)
- [ğŸ§¾ Front matter + schemas](#-front-matter--schemas-machine-readable-mcp)
- [âœ… Definition of done](#-definition-of-done)
- [ğŸ“¦ Required artifacts](#-required-artifacts)
- [ğŸ§ª Experiment reports](#-experiment-reports)
- [ğŸƒ Run receipts](#-run-receipts)
- [ğŸ“ˆ Performance & scalability experiments](#-performance--scalability-experiments)
- [ğŸ§° SOPs](#-sops)
- [ğŸ§  Model cards](#-model-cards)
- [ğŸ““ Notebooks](#-notebooks)
- [ğŸ§­ Traceability matrix](#-traceability-matrix)
- [ğŸ§¯ Bad evidence protocol](#-bad-evidence-protocol)
- [ğŸ”— MCP â†” KFM evidence pipeline](#-mcp--kfm-evidence-pipeline)
- [ğŸ§© Domain checklists](#-domain-checklists)
- [ğŸ” Safety, privacy, licensing](#-safety-privacy-licensing)
- [ğŸ›¡ï¸ Threat model & hostile inputs](#-threat-model--hostile-inputs)
- [âœ… QA, audits, and CI hooks](#-qa-audits-and-ci-hooks)
- [ğŸ¤ PR / review checklist](#-pr--review-checklist)
- [ğŸ“š Project reference library influence map](#-project-reference-library-influence-map)
- [ğŸ§¾ Reference library index](#-reference-library-index)
- [ğŸ•°ï¸ Version history](#-version-history)

---

## ğŸ§¾ Doc metadata

| Field | Value |
|---|---|
| Doc | `mcp/README.md` |
| Status | Active âœ… |
| Last updated | **2026-01-19** |
| Audience | Contributors writing experiments, running jobs, training models, shipping evidence artifacts, authoring Story Nodes |
| Prime directive | If it changes â€œspatial truth,â€ it must be **traceable + reproducible + reviewable** |
| KFM canon | **ETL/Streaming â†’ Catalogs (STAC/DCAT/PROV) â†’ Graph â†’ APIs â†’ UI/Story â†’ Focus Mode** |
| Special doctrine | **Policy packs gate publishing** + **atomic promotion** + **Focus Mode must cite** |

---

## ğŸ§­ What MCP is

### âœ… MCP isâ€¦
A **governed method layer** that turns â€œwe tried somethingâ€ into **auditable science**:

- ğŸ§ª **Protocols** (what we intended to do + why)
- ğŸƒ **Receipts** (what we actually ran + how)
- ğŸ§° **SOPs** (repeatable procedures for risky/repeated work)
- ğŸ§  **Model cards** (responsible AI/ML usage)
- ğŸ§¾ **Gate reports** (policy/contract checks that prove something was safe to publish)
- ğŸ‘€ **Review artifacts** (what was checked, by whom, and what failed)
- ğŸ§­ **Traceability** (decision â†” evidence â†” catalogs â†” provenance â†” PRs)

### ğŸš« MCP is notâ€¦
- âŒ A data lake (thatâ€™s `data/`)
- âŒ A code dump (thatâ€™s `src/` and `web/`)
- âŒ A place for large outputs (store in `data/processed/**` + catalogs)
- âŒ A place for â€œunsourced narrativeâ€ (Story Nodes must cite evidence artifacts)
- âŒ A runtime governance ledger (that belongs to system telemetry), but MCP **references** ledger IDs when relevant

> [!TIP]
> MCP exists to make â€œresultsâ€ **explainable and replayable**â€”not merely impressive. ğŸ§¾âœ…

---

## ğŸ§· Repo invariants

These are the â€œyou can build anything, but not *like that*â€ constraints.  
They align MCP with KFMâ€™s v13+ architecture: catalogâ€‘driven, APIâ€‘centric, governanceâ€‘enforced, and transparent AI.

### ğŸ§± Invariant 1 â€” Pipeline ordering is absolute
**ETL/Streaming â†’ Catalogs (STAC/DCAT/PROV) â†’ Graph â†’ APIs â†’ UI/Story â†’ Focus Mode**  
No skipping steps. No â€œtemporary UI demoâ€ without catalogs + lineage.

### ğŸ§¬ Invariant 2 â€” Provenance-first publishing (including streaming)
Anything â€œpublishedâ€ (referenced by graph, APIs, UI, Story Nodes, dashboards, or Focus Mode) must have:
- a **catalog record** (STAC/DCAT) ğŸ—‚ï¸  
- a **PROV lineage record** ğŸ§¬  
â€¦**before** it is used downstream.  
Streaming data isnâ€™t an exceptionâ€”each snapshot/aggregation must still be traceable.

### ğŸ§¾ Invariant 3 â€” Contracts are first-class
Schemas, profiles, and data contracts are not â€œnice-to-have.â€ They are the interface.  
If it canâ€™t validate, it canâ€™t ship.

### ğŸ” Invariant 4 â€” ETL (and automation) must be deterministic + idempotent
If inputs + config are unchanged, reruns must be stable:
- same IDs, same hashes, same catalogs
- safe to rerun without double-writing or corrupting state
- automation must use **idempotency keys** (and a kill switch)

### ğŸš€ Invariant 5 â€” Promotion is atomic (transaction mindset)
**Promote = data/processed + STAC/DCAT + PROV + (optional) graph ingest** as one consistent publish step.  
If any gate fails, the publish must roll back or remain stagedâ€”no halfâ€‘published artifacts.

### ğŸ” Invariant 6 â€” Policy packs gate publishing
Publishing must pass automated policy checks (OPA/Conftest style), including:
- contract/schema validation
- provenance completeness
- no sensitivity downgrade
- license/attribution propagation
- â€œFocus Mode citations requiredâ€ checks for AI outputs and narratives

### ğŸ¤– Invariant 7 â€” Focus Mode is advisory-only and evidence-bound
Focus Mode can **suggest** and **summarize**. It cannot â€œdeclare truthâ€ unless it:
- cites evidence artifacts (catalog IDs / graph entities)  
- passes governance checks (content + sensitivity rules)  
- shows uncertainty/refuses when evidence is insufficient

### ğŸ§¾ Invariant 8 â€” Everything decision-worthy gets a receipt
Every decision-worthy run captures:
- input/output hashes
- config + environment snapshot
- commit hash and entrypoint
- policy/gate report(s)
- references to catalog IDs and (when applicable) governance ledger IDs

---

## ğŸ§± MCP artifacts (types, IDs, and immutability)

MCP stays reliable because artifacts are **typed**, **named**, and **treated correctly**.

| Artifact | Prefix / ID pattern | Where | Immutable? | Purpose |
|---|---|---|---:|---|
| ğŸ§ª Experiment protocol | `EXP-YYYY-MM-DD-<slug>.md` | `mcp/experiments/` | âš ï¸ *Mutable while `draft`* | Pre-register intent, assumptions, validation plan |
| ğŸƒ Run receipt | `RUN-YYYY-MM-DD-<slug>/` | `mcp/runs/` | âœ… **Yes** | What executed + how to reproduce |
| ğŸ” Gate report | `GATE-YYYY-MM-DD-<slug>.md` *(or `.json`)* | `mcp/gates/` *(recommended)* | âœ… **Yes** | Proof of policy/contract checks (OPA/Conftest + schema + link integrity) |
| ğŸš€ Promotion receipt | `PROMOTE-YYYY-MM-DD-<slug>/` *(optional)* | `mcp/promotions/` *(optional)* | âœ… **Yes** | Atomic publish record: what was promoted, what changed in catalogs |
| ğŸ§¬ DevOps provenance bundle | `DEVPROV-YYYY-MM-DD-<slug>.jsonld` *(optional)* | `mcp/dev_prov/` *(optional)* | âœ… **Yes** | PROV for PRs/builds (who changed what, linked to datasets/runs) |
| ğŸ§° SOP | `SOP-<topic>-v<semver>.md` | `mcp/sops/` | âœ… *Versioned* | Repeatable procedures (risky or frequent tasks) |
| ğŸ§  Model card | `MODEL-<name>-v<semver>.md` | `mcp/model_cards/` | âœ… *Versioned* | Intended use, limits, datasets, governance for ML/AI |
| ğŸ‘€ Review note | `REV-YYYY-MM-DD-<slug>.md` | `mcp/reviews/` *(recommended)* | âœ… **Yes** | Independent reproduction and governance sign-off |
| ğŸ§­ Traceability | `TRACEABILITY.md` *(or `TRACE-*.md`)* | `mcp/traceability/` | âœ… *Append-only mindset* | Decision/feature â†’ EXP/RUN â†’ evidence + catalogs + PRs |
| ğŸ§¯ Incident / anomaly | `INC-YYYY-MM-DD-<slug>.md` *(optional)* | `mcp/incidents/` *(optional)* | âœ… **Yes** | When something shipped wrong: what happened + fix + new gates |

> [!IMPORTANT]
> **Run receipts and gate reports are immutable.**  
> If anything changes (inputs, params, code, environment, or policy pack), create a **new** artifact and link it. âœ…

---

## ğŸš¦ Nonâ€‘negotiables

These rules keep KFM **scientific**, **auditable**, and **governed**:

1) **Evidence lives in `data/` â€” not in `mcp/`.** ğŸ“¦  
   - `mcp/` = methods, decisions, receipts, checklists  
   - `data/processed/...` = outputs (**and they must be cataloged**)

2) **Contract-first by default.** ğŸ§¾ğŸ§±  
   If you publish an artifact, you must also publish the contract it conforms to (schema/profile/version).

3) **Provenance-first publishing (always).** ğŸ§¬  
   If itâ€™s referenced by graph/APIs/UI/Story/Focus, it must have STAC/DCAT + PROV before use.

4) **Policy packs gate shipping.** ğŸ”  
   If a policy pack says â€œno,â€ the artifact does not shipâ€”no exceptions, no â€œtemporaryâ€ bypass.

5) **Protocol before run (when it matters).** ğŸ§¾â¡ï¸ğŸƒ  
   If results could influence product decisions, public narratives, pipelines, or models: write an **EXP** first.

6) **No â€œmagic results.â€** ğŸª„ğŸš«  
   If you canâ€™t reproduce it using:
   - a commit hash
   - an environment snapshot
   - a config
   - linked inputs/outputs (**catalog IDs**)
   - input/output hashes  
   â€¦then itâ€™s not â€œdone.â€

7) **Immutable receipts.** ğŸ§¾  
   Donâ€™t edit a run receipt to â€œfix history.â€  
   Make a **new** run folder and link it.

8) **Label AI involvement + bind to evidence.** ğŸ¤–  
   AI-assisted outputs must be labeled and evidence-linked.  
   Focus Mode remains **advisory-only** and must be evidence-backed.

9) **No privacy / sensitivity downgrade.** ğŸ”’  
   Outputs cannot be less restricted than inputs without an explicit, reviewed redaction step.

10) **Licensing isnâ€™t optional.** ğŸ§¾âš–ï¸  
   Every dataset / artifact must carry license + attribution requirements through catalogs, narratives, and exports.

11) **Stable IDs are a correctness feature.** ğŸ·ï¸  
   IDs should be invariant across reruns when inputs havenâ€™t changedâ€”treat IDs like API contracts.

> [!TIP]
> Motto: **â€œIf I canâ€™t reproduce it in 30 minutes, itâ€™s not complete.â€** â±ï¸âœ…

---

## ğŸ Quick Start

### 1) Create an experiment report ğŸ§ª
Add:
- `mcp/experiments/EXP-YYYY-MM-DD-<slug>.md`

### 2) Create a run receipt ğŸƒ
Add:
- `mcp/runs/RUN-YYYY-MM-DD-<slug>/`
  - config, env snapshot, logs, metrics, and `MANIFEST.md`

### 3) Run policy + contract gates ğŸ”âœ…
Add (recommended):
- `mcp/gates/GATE-YYYY-MM-DD-<slug>.md` *(or write into `mcp/runs/.../gates/`)*

### 4) Store evidence outputs in the governed data layer ğŸ“¦
Put evidence artifacts in:
- `data/processed/...` âœ…  
Then publish metadata + lineage:
- `data/stac/**` + `data/catalog/dcat/**` + `data/prov/**` ğŸ—‚ï¸ğŸ§¬

### 5) Link it to decisions ğŸ§­
Update traceability (recommended):
- `mcp/traceability/TRACEABILITY.md`

> [!IMPORTANT]
> `mcp/` should stay **lightweight** and human-readable.  
> Large artifacts go to `data/processed/` (or object storage) and get catalog records + lineage.

---

## ğŸ—‚ï¸ Directory layout

```text
ğŸ“ mcp/
â”œâ”€â”€ ğŸ“„ README.md                 # you are here ğŸ‘‹
â”œâ”€â”€ ğŸ“ experiments/              # human-readable experiment protocols ğŸ§ªğŸ§¾
â”œâ”€â”€ ğŸ“ runs/                     # immutable run receipts (configs, env, logs, metrics) ğŸƒğŸ§¾
â”œâ”€â”€ ğŸ“ gates/                    # immutable gate reports (policy packs, schema checks) ğŸ”âœ… (recommended)
â”œâ”€â”€ ğŸ“ promotions/               # atomic publish receipts ğŸš€ (optional)
â”œâ”€â”€ ğŸ“ dev_prov/                 # PR/build provenance bundles (PROV JSON-LD) ğŸ§¬ (optional)
â”œâ”€â”€ ğŸ“ sops/                     # Standard Operating Procedures (repeatable tasks) ğŸ§°
â”œâ”€â”€ ğŸ“ model_cards/              # model cards for any ML/AI used or trained ğŸ§ 
â”œâ”€â”€ ğŸ“ notebooks/                # tidy, reproducible notebooks ğŸ““
â”œâ”€â”€ ğŸ“ traceability/             # decision â†” evidence mapping ğŸ§­ (recommended)
â”œâ”€â”€ ğŸ“ reviews/                  # peer reproduction notes / governance sign-offs ğŸ‘€ (recommended)
â”œâ”€â”€ ğŸ“ incidents/                # post-mortems / anomaly reports ğŸ§¯ (optional)
â””â”€â”€ ğŸ“ templates/                # local templates ğŸ§© (or use docs/templates/)
```

> [!NOTE]
> Repo implementations vary. If `gates/`, `traceability/`, or `reviews/` donâ€™t exist yet, consider adding themâ€”  
> policy packs + traceability + reproduction notes are core promises in KFMâ€™s v13+ redesign.

---

## ğŸ” The MCP workflow loop

KFM work is **question â†’ protocol â†’ run â†’ evidence â†’ gates â†’ publish â†’ review â†’ iterate**:

```mermaid
flowchart LR
  Q["â“ Question"] --> P["ğŸ§¾ Protocol (EXP)"]
  P --> R["ğŸƒ Run (RUN receipt)"]
  R --> G["ğŸ” Gates (policy + contracts)"]
  G --> E["ğŸ“¦ Evidence (data/processed + catalogs)"]
  E --> S["ğŸ§ª Summary report (interpretation + limits)"]
  S --> V["ğŸ‘€ Review (repro + governance)"]
  V --> Q
```

---

## ğŸš¥ Detect â†’ Validate â†’ Promote

KFM risk comes from â€œhalf-publishedâ€ artifacts. MCP encourages a controlled rhythm:

1) **Detect** ğŸ•µï¸  
   Identify new inputs / changes (sources updated, schema drift, new tiles, new sensors, new docs).

2) **Validate** âœ…  
   Run fast gates:
   - schema & bounds checks (**contracts**)
   - catalog validity (STAC/DCAT)
   - PROV lineage presence & link integrity
   - governance propagation (no downgrade)
   - security scans (secrets / sensitive patterns)
   - **policy packs** (OPA/Conftest) ğŸ”

3) **Promote** ğŸš€  
   Only after validation:
   - write evidence to `data/processed/**`
   - write STAC/DCAT/PROV (provenance-first)
   - (optionally) export graph ingest bundles
   - add MCP RUN receipt + gate report + traceability update

> [!TIP]
> Treat â€œpromotionâ€ like a release: **atomic publish or nothing.** ğŸ§¾âœ…

---

## ğŸ” Policy packs & gate reports

KFMâ€™s architecture explicitly treats policy checks as a **first-class publish gate** (not a reviewer â€œniceâ€‘toâ€‘haveâ€).  
In practice, we expect:
- policy logic in versioned **OPA/Rego** policy packs
- execution via **Conftest** (or equivalent) in CI/local
- a **gate report** artifact that proves what ran and what passed

### âœ… What belongs in a gate report
- policy pack version (and commit hash)
- checks executed (schemas, link integrity, sensitivity downgrade, license propagation)
- inputs evaluated (catalog IDs / files)
- results (pass/fail + reasons)
- who ran it (human, CI, Wâ€‘Pâ€‘E Executor)
- pointers to logs (redacted if needed)

### ğŸ“„ Minimal `GATE-...` template (copy/paste)

```md
---
gate_id: GATE-YYYY-MM-DD-<slug>
date: YYYY-MM-DD
owner: "@github-handle"
scope: ["data", "catalogs", "prov", "api", "ui", "ai"]
policy_pack:
  name: "kfm-policy-pack"
  version: "vX.Y.Z"
  commit: "<sha>"
runner:
  type: local | ci | wpe_executor
  run_id: "RUN-YYYY-MM-DD-<slug>"   # optional link
inputs:
  - "data/stac/..."
  - "data/catalog/dcat/..."
  - "data/prov/..."
results:
  status: pass | fail
  checks:
    - name: "stac_validate"
      status: pass
    - name: "prov_present"
      status: pass
    - name: "no_sensitivity_downgrade"
      status: pass
    - name: "license_propagation"
      status: pass
notes: ""
---

# Summary ğŸ”âœ…
- What was evaluated and what changed?

# Findings ğŸ§¾
- Failures/warnings (if any) + remediation links

# Evidence ğŸ”—
- Pointers to the relevant run receipt and promotion receipt
```

> [!IMPORTANT]
> If the gate report fails, **the publish is invalid**.  
> Fix â†’ re-run â†’ new gate report. âœ…

---

## ğŸ¤– Watcher â†’ Planner â†’ Executor receipts

KFMâ€™s v13+ roadmap introduces a **Watcherâ€“Plannerâ€“Executor (Wâ€‘Pâ€‘E)** automation pattern for safe, auditable maintenance:
- Watcher observes and emits immutable events
- Planner proposes deterministic plans (often as PR-ready diffs)
- Executor opens PRs / triggers pipeline execution **under the same governance gates as humans**
- A **kill switch** must be available to disable agent actions

### âœ… How MCP records Wâ€‘Pâ€‘E activity
Wâ€‘Pâ€‘E is â€œjust another run,â€ so it gets receipts:
- Watch events â†’ `mcp/runs/.../events/` *(or `mcp/incidents/` for anomalies)*
- Planner output â†’ `mcp/runs/.../plans/` *(diffs, patch proposals)*
- Executor action â†’ `mcp/runs/.../` (logs + PR link) + **gate report**
- Traceability entry links the event â†’ plan â†’ PR â†’ merge â†’ resulting artifacts

> [!TIP]
> Treat Wâ€‘Pâ€‘E like a â€œrobot teammateâ€: everything it does must be reproducible, reviewable, and policy-gated. ğŸ¤ğŸ¤–

---

## ğŸ§¬ DevOps provenance

KFMâ€™s future-proof traceability includes **DevOps â†’ PROV**: mapping PR/build events to PROV JSONâ€‘LD so you can answer:
- *Which code version produced this dataset?*
- *Who reviewed and approved the change?*
- *Which policy pack version gated the publish?*

### âœ… Where to store DevOps PROV
Preferred (if your repo supports it):
- `data/prov/dev/DEVPROV-...jsonld`

Acceptable (MCP-layer pointer):
- `mcp/dev_prov/DEVPROV-...jsonld`

### Suggested fields
- PR number / URL
- commit SHA(s)
- CI run ID
- artifacts produced (RUN IDs, dataset IDs)
- approvals (reviewers)
- policy gates executed (GATE IDs)

---

## ğŸ§¾ Front matter + schemas (machine-readable MCP)

MCP documents are **human-first**, but they should also be **machine-readable** for dashboards, validation, and CI gates.

### âœ… Front matter conventions (recommended)
All MCP artifacts should start with YAML front matter, including:

- stable ID (`EXP-...`, `RUN-...`, `GATE-...`, `SOP-...`, `MODEL-...`)
- date
- owner
- status
- tags
- AI involvement (if applicable)
- links to evidence (catalog IDs or paths)
- contracts + lineage pointers (STAC/DCAT/PROV)
- policy pack version (for shippable work)
- dev provenance pointer (optional)

### â­ Recommended â€œgoverned docâ€ fields
Additive fields that improve audits and automation:
- `doc_uuid` (stable UUID)
- `commit_sha` (when doc corresponds to a release)
- `doc_integrity_checksum` (hash of doc body, optional)
- `care_label` / `classification`
- `policy_pack_version`
- `governance_ledger_ref` *(pointer only; ledger lives outside MCP)*

### ğŸ” Minimal example (recommended baseline)
```yaml
---
id: EXP-2026-01-19-example
doc_uuid: "b0b6d0a5-4f4e-4e5b-9f25-1a2b1b9b0a7e"
title: "Example MCP artifact"
date: 2026-01-19
owner: "@github-handle"
status: draft
repro_level: L1
risk_level: medium
care_label: public
ai_used: false

policy_pack_version: "vX.Y.Z"
gate_reports: ["GATE-2026-01-19-example"]

contracts:
  - "schemas/mcp/exp.schema.json"
catalog_pointers:
  stac: ["stac://collection-id", "stac://item-id"]
  dcat: ["data/catalog/dcat/dataset.jsonld"]
  prov: ["data/prov/PROV-2026-01-19-example.jsonld"]
dev_prov: ["data/prov/dev/DEVPROV-2026-01-19-pr-123.jsonld"]
---
```

---

## âœ… Definition of done

### âœ… MCP â€œdoneâ€ means: reproducible + governed
For any EXP/RUN that influences production pipelines, APIs, UI layers, Story Nodes, dashboards, or Focus Mode:

- [ ] Front-matter complete + consistent (IDs, dates, owner, status)
- [ ] Claims link to evidence inputs/outputs (**catalog pointers**)
- [ ] Contracts + lineage pointers present (STAC/DCAT/PROV)
- [ ] Policy pack gates executed and recorded (**gate report present**)
- [ ] Validation steps are listed and repeatable
- [ ] Governance + FAIR/CARE + sovereignty considerations stated (when applicable)
- [ ] Another contributor can reproduce results without tribal knowledge
- [ ] If AI was involved, human sign-off is recorded

### ğŸ§± Reproducibility levels (recommended)
- **L0** ğŸŸ¡: exploratory note (not decision-worthy)
- **L1** ğŸŸ : reproducible by author (config + env captured)
- **L2** ğŸŸ¢: reproducible by reviewer (independent re-run)
- **L3** ğŸ†: CI-backed / automated rerun (pipeline job + validators + policy packs)

---

## ğŸ“¦ Required artifacts

### âœ… â€œReal workâ€ minimum bar
If an experiment influences decisions, pipelines, published results, Story Nodes, or Focus Mode, it must include:

- ğŸ§ª **Experiment report** â†’ `mcp/experiments/...`
- ğŸƒ **Run receipt** â†’ `mcp/runs/...`
- ğŸ” **Gate report** â†’ `mcp/gates/...` *(or inside the run folder)*
- ğŸ”— **Code pointer** â†’ commit hash + entrypoint
- ğŸ§± **Environment snapshot** â†’ Docker digest **or** lockfile/requirements
- ğŸ² **Seeds / determinism flags** (where applicable)
- ğŸ”¢ **Input/output hashes** (recommended for decision-relevant work)
- ğŸ“¦ **Outputs stored as evidence** â†’ `data/processed/...`
- ğŸ—‚ï¸ **Catalog records** â†’ STAC/DCAT
- ğŸ§¬ **Lineage** â†’ PROV pointers (inputs + outputs)
- ğŸ‘€ **Review notes** â†’ reproduction sign-off for L2/L3 work (recommended)
- ğŸ§­ **Traceability entry** â†’ decision/feature â†” EXP/RUN/GATE â†” evidence
- ğŸ§¬ **(Optional) DevOps PROV** â†’ PR/build provenance bundle

> [!WARNING]
> Avoid committing large binaries to `mcp/`.  
> If itâ€™s an â€œoutput,â€ it belongs in `data/processed/` with catalogs + lineage.

---

## ğŸ§ª Experiment reports

### ğŸ“› Naming convention
- `EXP-YYYY-MM-DD-<short-slug>.md`  
  Example: `EXP-2026-01-02-ocr-ner-baseline.md`

### ğŸ·ï¸ Status values
- `draft` ğŸ“ â€” in progress
- `complete` âœ… â€” reproducible; linked receipts + evidence + gate report
- `superseded` ğŸ§¯ â€” replaced by a newer experiment

### ğŸ§¾ Experiment template (copy/paste)

```md
---
id: EXP-YYYY-MM-DD-<slug>
title: "<short, explicit title>"
date: YYYY-MM-DD
owner: "@github-handle"
status: draft | complete | superseded
repro_level: L0 | L1 | L2 | L3
risk_level: low | medium | high
ai_used: true | false
care_label: public | restricted | confidential
tags: [gis, ocr, nlp, stac, dcat, prov, graph, sim, stats, web, security]

policy_pack_version: "vX.Y.Z"        # recommended
gate_reports: []                    # e.g. ["GATE-YYYY-MM-DD-..."]

contracts:
  - "schemas/mcp/exp.schema.json"
---

# Objective / Question â“
- What are we trying to learn or improve?

# Background / Prior Art ğŸ“š
- Links to prior experiments, issues, story claims, or system docs.

# Hypothesis âœ…/âŒ
- What do we expect and why?

# Variables & Controls ğŸ›ï¸
- Key variables youâ€™re changing.
- Controls / baselines.
- What stays fixed.

# Inputs (Evidence In) ğŸ—ƒï¸
- Dataset IDs + STAC/DCAT pointers.
- Sampling rules, inclusion/exclusion, time range, bbox.
- Licensing + sensitivity notes (if applicable).

# Method / Protocol ğŸ§¾
- Step-by-step procedure.
- Parameters + configs (link to run receipt config).
- Tools + versions (OS/GPU/driver notes if relevant).

# Validation Plan âœ…
- What fails fast?
- What warns?
- What â€œsanity checksâ€ must pass?
- What policy checks must pass?

# Run Receipt ğŸƒ
- Code commit: `abcdef1`
- Entrypoint: `src/...` or notebook path
- Run folder: `mcp/runs/RUN-YYYY-MM-DD-.../`
- Gate report: `mcp/gates/GATE-YYYY-MM-DD-...`
- Seeds: `...`

# Outputs (Evidence Out) ğŸ“¦
- Where outputs live (paths under `data/processed/...`)
- Catalog pointers:
  - STAC item(s): `...`
  - DCAT dataset: `...`
  - PROV bundle: `...`

# Results ğŸ“ˆ
- Metrics, charts, qualitative examples (keep small).
- Add 1â€“3 â€œsanity checkâ€ examples.

# Uncertainty, Bias, and Validation ğŸ”
- What could be wrong?
- Checks performed (spot checks, CV, error bounds, leakage checks).
- Bias risks / perspective gaps (especially for historical corpora).

# Interpretation ğŸ§ 
- What do results mean for KFM decisions?

# Decision / Next Steps ğŸ§­
- Adopt / iterate / abandon (and why).

# Reproducibility Checklist âœ…
- [ ] Parameters & configs documented
- [ ] Code committed + hash recorded
- [ ] Environment captured (Docker/lockfile)
- [ ] Inputs/outputs linked via STAC/DCAT/PROV
- [ ] Gate report recorded (policy packs + contracts)
- [ ] Reviewer can reproduce (for L2/L3)
```

---

## ğŸƒ Run receipts

Runs are the **receipt** for an experiment: what you ran, how you ran it, where outputs went, and what changed.

### ğŸ“› Naming convention
- `RUN-YYYY-MM-DD-<slug>/`

### ğŸ“¦ Suggested run folder contents
- `config/` ğŸ§¾ â€” YAML/JSON config used for the run
- `env/` ğŸ§± â€” `pip freeze`, lockfiles, Docker digest, OS info
- `logs/` ğŸªµ â€” structured logs (**redacted if needed**)
- `metrics/` ğŸ“ˆ â€” CSV/JSON metrics, evaluations
- `artifacts/` ğŸ§© â€” *small* artifacts (thumbnails, sample outputs)
- `gates/` ğŸ” â€” policy pack outputs / schema validation logs (recommended)
- `events/` ğŸ‘€ â€” watcher events (if Wâ€‘Pâ€‘E)
- `plans/` ğŸ—ºï¸ â€” planner diffs/patches (if Wâ€‘Pâ€‘E)
- `MANIFEST.md` ğŸ§¾ â€” reproduction instructions + evidence links + checks performed
- `hashes/` ğŸ”¢ *(recommended)* â€” input/output checksums for audit-grade reruns

### ğŸ§¾ Minimal `MANIFEST.md` template (copy/paste)

```md
---
run_id: RUN-YYYY-MM-DD-<slug>
run_type: standard | benchmark | streaming | backfill | hotfix
related_experiment: EXP-YYYY-MM-DD-<slug>
date: YYYY-MM-DD
owner: "@github-handle"
env: dev | staging | prod
care_label: public | restricted | confidential

code:
  commit: abcdef1
  entrypoint: "src/pipelines/..."
  args: ["--config", "config/run.yml"]
  dirty_worktree: false

policy:
  policy_pack_version: "vX.Y.Z"
  gate_report: "mcp/gates/GATE-YYYY-MM-DD-<slug>.md"

environment:
  docker_image: "ghcr.io/org/project:tag@sha256:..."     # preferred
  # or:
  requirements: "env/requirements.lock.txt"
  os: "..."
  cpu: "..."
  gpu: "..."  # optional

randomness:
  seeds: [123, 456]
  deterministic_flags: ["..."]

contracts:
  - "schemas/<domain>/...schema.json"
  - "schemas/mcp/run.schema.json"

inputs:
  - catalog_ref: "stac://<collection_or_item_id>"
    sha256: "..."          # recommended
  - dcat: "data/catalog/dcat/<dataset>.jsonld"
  - prov: "data/prov/<prior-run>.jsonld"

outputs:
  - path: "data/processed/<domain>/<dataset>/..."
    sha256: "..."          # recommended
    stac_item: "data/stac/items/..."
    dcat: "data/catalog/dcat/..."
    prov: "data/prov/<run-id>.jsonld"

governance:
  ledger_ref: ""           # pointer only (optional)

validation:
  - "contract validation: pass/fail"
  - "schema validation (STAC/DCAT/PROV): pass/fail"
  - "link checks: pass/fail"
  - "classification propagation: pass/fail"
  - "spot checks: ..."

policy_checks:
  - "opa/conftest: pass/fail"
  - "prompt/ai citation coverage: pass/fail"  # when AI outputs exist

notes: ""
---

# Summary ğŸ§¾
- What did this run do?

# Evidence outputs ğŸ“¦
- Where outputs are stored (`data/processed/...`) + catalog IDs

# How to reproduce ğŸ”
1. Checkout commit: `abcdef1`
2. Restore environment: ...
3. Run: ...
4. Validate (contracts + catalogs + lineage + policy packs): ...
```

> [!TIP]
> Treat run folders as **immutable receipts**.  
> New parameters â†’ new run folder âœ…

---

## ğŸ“ˆ Performance & scalability experiments

Performance claims can change architecture decisions, so they still need receipts + gates.

### âœ… When to treat a run as a benchmark
- tile generation tuning (COG params, overviews, compression) ğŸ§Š
- PostGIS index/migration affecting query latency ğŸ˜
- graph ingest optimizations / new analytics jobs ğŸ•¸ï¸
- Focus Mode latency or citation coverage regression ğŸ¤–
- offline pack generation (bundle size, time-to-download, cache correctness) ğŸ“¦

### ğŸ§¾ Benchmark add-ons (recommended in `MANIFEST.md`)
Add a `performance:` section:

```yaml
performance:
  workload:
    description: "API tile requests @ zoom 6â€“12 for 3 AOIs"
    concurrency: 25
    warm_cache: true
    duration_s: 600
  dataset:
    size_bytes: 123456789
    feature_count: 987654
  system:
    cpu_cores: 16
    ram_gb: 64
    storage: "nvme"
  results:
    p50_ms: 120
    p95_ms: 410
    p99_ms: 900
    throughput_rps: 85
  artifacts:
    - "metrics/latency_histogram.csv"
    - "logs/explain_analyze.txt"
```

---

## ğŸ§° SOPs

SOPs turn â€œtribal knowledgeâ€ into a repeatable, reviewable procedure.  
Write an SOP whenever a task is repeated or risky: data intake, catalog publishing, redaction, OCR, tile generation, secure parsing, policy packs, Wâ€‘Pâ€‘E operations, offline packs, AR alignment, etc.

### SOP template (copy/paste)

```md
---
id: SOP-<topic>-v1.0.0
title: "<clear title>"
owner: "@github-handle"
last_updated: YYYY-MM-DD
risk_level: low | medium | high
care_label: public | restricted | confidential
---

# Purpose ğŸ¯
What this SOP accomplishes and when to use it.

# Scope âœ…
Whatâ€™s included / excluded.

# Prerequisites ğŸ§±
Accounts, tools, access, permissions.

# Tools & Versions ğŸ§°
Software + versions.

# Procedure ğŸ§­
1. Step...
2. Step...
3. Step...

# Verification âœ…
How to confirm it worked (checks + expected outputs).

# Troubleshooting ğŸ§¯
Common failure modes + fixes.

# Audit Trail ğŸ§¾
Example PRs / runs / experiments that used this SOP.
```

### â­ High-value SOPs to add (starter set)
- `sops/data_intake_connectors.md` ğŸ§² (source manifests, checksums, quarantine, streaming snapshots)
- `sops/publish_stac_dcat_prov.md` ğŸ—‚ï¸ğŸ§¬ (profiles, validation, link checks)
- `sops/policy_packs_and_gate_reports.md` ğŸ” (OPA/Conftest usage, failure triage)
- `sops/wpe_agents.md` ğŸ¤– (Watcher/Planner/Executor rules, idempotency keys, kill switch)
- `sops/offline_data_packs.md` ğŸ“¦ (bundle creation, size budgets, validation)
- `sops/ar_overlay_alignment.md` ğŸ•¶ï¸ (coordinate alignment, uncertainty, safety constraints)
- `sops/devops_prov_capture.md` ğŸ§¬ (PR/build â†’ PROV JSONâ€‘LD, linking to datasets)
- `sops/redaction_and_sensitive_locations.md` ğŸ” (coordinate coarsening, approvals, provenance of redaction)

---

## ğŸ§  Model cards

Any ML/AI model used in KFM (trained or adopted) needs a model card:

- what it is
- what it was trained on / sourced from
- what it should be used for âœ…
- what it must **not** be used for ğŸš«
- known limitations, bias risks, failure modes âš ï¸
- provenance + licensing + governance labels ğŸ§¾ğŸ”’
- **citation coverage expectations** (Focus Mode must cite)
- drift/bias monitoring plan

### Model card template (copy/paste)

```md
---
model_id: MODEL-<name>-v<semver>
owner: "@github-handle"
date: YYYY-MM-DD
ai_used: true
source:
  type: trained | third_party
  license: "..."
  reference: "paper/link/registry id"
datasets:
  - "stac://<collection_or_item_id>"
  - "dcat://<dataset_id>"
governance:
  care_label: public | restricted | confidential
  human_in_the_loop: required | recommended | optional
  redaction_required: yes | no
evaluation:
  citation_coverage_required: true
  refusal_policy: "refuse-if-no-evidence"
monitoring:
  drift_checks: "monthly"
  bias_checks: "release-gated"
---

# Model overview ğŸ§ 
- What problem does it solve?

# Intended use âœ…
- Supported use-cases.

# Out-of-scope / prohibited use ğŸš«
- What it must not be used for.

# Training data ğŸ—ƒï¸
- Datasets used (STAC/DCAT pointers), sampling, labeling notes.
- Known gaps / perspective bias notes.

# Evaluation ğŸ“ˆ
- Metrics, test sets, qualitative examples.
- Citation coverage checks and refusal behavior checks.

# Limitations & biases âš ï¸
- Known failure modes, bias risks, uncertainty notes.

# Governance & safety ğŸ”
- Any redaction rules or sensitivity constraints.
- How outputs are labeled in UI / Focus Mode.

# Reproducibility ğŸ§ª
- Training code commit hash
- Environment / hardware notes
- Hyperparameters / config
- Seeds
- Artifact pointers stored under `data/processed/...` with catalogs + lineage
```

---

## ğŸ““ Notebooks

Notebooks are welcomeâ€”**but must be readable and reproducible**:

- Start with a markdown cell: **purpose + inputs + outputs**
- Keep outputs small *(no huge embedded blobs)*
- Prefer parameterized notebooks or export to scripts when it becomes â€œrealâ€
- If a notebook produces evidence artifacts:
  - store outputs in `data/processed/...`
  - link them from an experiment report + run receipt
  - gate them with policy packs before publication

> [!NOTE]
> KFMâ€™s roadmap includes oneâ€‘click notebooks (JupyterHub/Binder style) for reproducible research.  
> If you support that workflow, your run receipts should include a â€œlaunch recipeâ€ (environment + dataset pointers + seeds).

---

## ğŸ§­ Traceability matrix

Traceability is how MCP connects â€œworkâ€ to â€œwhy it mattersâ€:

- â“ Question / requirement  
- ğŸ§ª EXP protocol  
- ğŸƒ RUN receipt  
- ğŸ” Gate report  
- ğŸ“¦ Evidence artifacts (+ STAC/DCAT/PROV)  
- ğŸ•¸ï¸ Graph IDs (if applicable)  
- ğŸ›¡ï¸ API endpoints (if applicable)  
- ğŸ“š Story Node(s) / Focus Mode behavior (if applicable)  
- ğŸ§¬ PR/build provenance (optional but recommended)

### âœ… Recommended traceability table (copy/paste)

```md
| Decision / Feature | EXP | RUN | GATE | Evidence outputs (data/processed) | Catalog pointers (STAC/DCAT/PROV) | PR/Build PROV | Reviewer repro | Notes |
|---|---|---|---|---|---|---|---|---|
| "Offline pack for County X (1930s)" | EXP-... | RUN-... | GATE-... | data/processed/packs/county-x/1930s/... | STAC: ... / DCAT: ... / PROV: ... | DEVPROV: ... | @reviewer âœ… | bundle size budget |
```

---

## ğŸ§¯ Bad evidence protocol

KFM must be resilient to â€œbad evidenceâ€ (messy scans, biased corpora, incomplete sensors, uncertain geocoding).

When evidence is questionable, MCP requires **restraint**:

1) **Quarantine lane** ğŸš§  
   Keep known-bad or uncertain inputs in a quarantined/staged area until validated.

2) **Inferential restraint** ğŸ§ â¬‡ï¸  
   Reduce strength/scope of conclusions; report uncertainty explicitly.

3) **Executional restraint** ğŸ›‘  
   Limit downstream actions: donâ€™t ship to UI/Story/Focus until reviewed, or serve only aggregated/redacted views.

4) **Community verification (optional)** ğŸ¤  
   For some historical claims, consider structured, moderated community verification (microtasks + consensus) â€” but treat it as evidence with its own receipts.

### âœ… Minimum â€œbad evidenceâ€ documentation
- Whatâ€™s wrong?
- How do we know?
- What we changed (or refused to change)
- What remains uncertain
- Who reviewed the restraint decision
- Whether quarantine/policy gates were updated

---

## ğŸ”— MCP â†” KFM evidence pipeline

KFM uses a strict evidence pipeline:

**ETL/Streaming â†’ Catalogs (STAC/DCAT/PROV) â†’ Graph â†’ APIs â†’ UI/Story â†’ Focus Mode**

So for MCP work:

- âœ… Protocols live here: `mcp/experiments/...`
- âœ… Receipts live here: `mcp/runs/...`
- âœ… Gate proofs live here: `mcp/gates/...`
- âœ… Evidence artifacts live here: `data/processed/...`
- âœ… Evidence artifacts must be:
  - cataloged (STAC/DCAT) ğŸ—‚ï¸
  - lineage-linked (PROV) ğŸ§¬
  - integrated through governed contracts (API boundary) ğŸ”’

```mermaid
flowchart LR
  EXP["ğŸ§ª EXP report"] --> RUN["ğŸƒ RUN receipt"]
  RUN --> GATE["ğŸ” GATE report"]
  GATE --> OUT["ğŸ“¦ data/processed outputs"]
  OUT --> CAT["ğŸ—‚ï¸ STAC/DCAT/PROV"]
  CAT --> GR["ğŸ•¸ï¸ Graph"]
  GR --> API["ğŸ›¡ï¸ APIs"]
  API --> UI["ğŸ—ºï¸ UI / Story / Dashboards"]
  UI --> FM["ğŸ¤– Focus Mode (evidence-only)"]
```

---

## ğŸ§© Domain checklists

Use the checklist that matches your work:

### ğŸ—ºï¸ GIS / Remote Sensing
- [ ] CRS documented (EPSG + axis order)
- [ ] Georeferencing method + control points documented
- [ ] Fit error/RMS recorded (if applicable)
- [ ] Raster outputs are COGs / tiled (with parameters)
- [ ] Vector outputs validate (geometry validity, topology as needed)
- [ ] Derived indices treated as evidence artifacts (parameters documented)
- [ ] Symbology/aggregation choices documented if they change interpretation ğŸ¨
- [ ] Catalog pointers included (STAC/DCAT) + lineage (PROV) ğŸ—‚ï¸ğŸ§¬
- [ ] Gate report recorded (policy packs) ğŸ”âœ…

### ğŸ” OCR / NLP
- [ ] Input corpus + sampling documented
- [ ] Labeling rules / evaluation rubric included
- [ ] Precision/recall (or spot-check protocol) documented
- [ ] Failure classes logged (scan quality, fonts, ambiguity)
- [ ] Geoparsing uncertainty documented (ambiguous place names, gazetteer limits)
- [ ] AI outputs (if any) include citations to source docs/entities

### ğŸ•¸ï¸ Graph analytics
- [ ] Graph schema/ontology version noted
- [ ] Metrics treated as **signals**, not facts (avoid over-interpretation)
- [ ] Provenance links from derived relations to source evidence
- [ ] No orphan IDs / referential integrity checks pass âœ…
- [ ] Derived relations are explainable (store â€œwhyâ€ edges or provenance pointers)

### ğŸ“¡ Streaming / Live data & dashboards
- [ ] Snapshot/aggregation cadence documented (windowing + retention)
- [ ] Each snapshot/aggregate has provenance (PROV activity + inputs)
- [ ] â€œNowâ€ vs â€œhistoricalâ€ semantics explicit (avoid mixing without labeling)
- [ ] Quarantine behavior defined for anomalies/outliers
- [ ] Publish is still policy-gated (no special exemptions)

### ğŸ“š Story Nodes & narratives
- [ ] Every claim cites evidence artifacts (STAC/DCAT/PROV pointers)
- [ ] Story config references valid layer IDs and time ranges
- [ ] Accessibility checks (alt text, contrast, keyboard nav expectations)
- [ ] Moderation workflow recorded (review note or PR approvals)
- [ ] Export includes citations/attribution automatically

### ğŸ¤– Focus Mode / AI assistance
- [ ] Answers are evidence-bound (citations required)
- [ ] Refusal/uncertainty behavior tested
- [ ] Governance checks applied (sensitivity, policy rules)
- [ ] Drift/bias monitoring plan exists for deployed models
- [ ] Ledger references recorded when relevant (pointer only)

### ğŸ•¶ï¸ AR / Immersive / 4D â€œdigital twinâ€ experiments
- [ ] Coordinate alignment assumptions explicit (units, axis order, altitude datum)
- [ ] Uncertainty bounds disclosed (no â€œpreciseâ€ overlays without error bars)
- [ ] Sensitive locations protected (coarsening/offset/omit)
- [ ] Offline pack constraints documented if used in field mode
- [ ] Narrative overlays remain evidence-cited (no â€œmagic AR factsâ€)

---

## ğŸ” Safety, privacy, licensing

- ğŸš« Donâ€™t store secrets, tokens, keys, or sensitive PII in `mcp/`
- ğŸ§½ Redact logs before committing if they contain identifiers, endpoints, or sensitive paths
- ğŸ§Š Prefer immutable receipts: new run folder > editing old run folder
- ğŸ§­ If superseded, mark as `superseded` and link the replacement
- ğŸ—ºï¸ Sensitive locations / culturally sensitive knowledge:
  - coarsen/offset/omit coordinates as required
  - require explicit permission & review before publishing
  - propagate sensitivity and sovereignty tags through catalogs, APIs, and UI
  - consider â€œtraditional knowledge labelsâ€ or community-defined access constraints
- âš–ï¸ Licensing must travel with evidence (and derived artifacts must honor upstream constraints)

> [!IMPORTANT]
> Licensing and sovereignty constraints must travel with evidence. If you combine layers, the resulting artifact must still honor attribution, access expectations, and community protocols. âš–ï¸ğŸ§¾

---

## ğŸ›¡ï¸ Threat model & hostile inputs

KFM handles *hostile-by-default* artifacts: PDFs, images, archives, and scraped web content.  
Treat every input as potentially maliciousâ€”even when it â€œlooks like a map.â€ ğŸ§¨

### âœ… Minimum defensive posture
- ğŸ§· **Deny-by-default parsing:** allow only expected MIME types and structures
- ğŸ§ª **Sandbox execution:** risky parsers in constrained env (no network, least privilege)
- ğŸ§Š **Decompression bomb awareness:** limits for image decode, ZIP expansion, PDF object counts
- ğŸ” **Metadata stripping:** prevent thumbnails/exports from leaking EXIF/paths
- ğŸ§¾ **Log redaction:** never commit secrets/URLs/tokens in logs
- âœ… **Dependency hygiene:** SBOMs, pinned versions, security scanning
- ğŸ” **Prompt injection awareness:** treat prompts as hostile input; governance checks for AI outputs

---

## âœ… QA, audits, and CI hooks

### CI intent (minimum bar)
- ğŸ§¹ lint + formatting (docs + code)
- âœ… unit tests (where applicable)
- ğŸ§¾ contract validation (schemas/profiles)
- ğŸ—‚ï¸ schema validation (STAC/DCAT/PROV)
- ğŸ”— link checks (assets exist; IDs resolve)
- ğŸ” policy pack evaluation (OPA/Conftest)
- ğŸ§· governance checks (classification propagation; redaction regressions)
- ğŸ¤– AI checks (citation coverage + refusal policy) when AI outputs exist

### Periodic audits (recommended)
- Quarterly: sample 3 completed EXP/RUN pairs â†’ verify re-run works end-to-end
- Before release: audit high-impact artifacts (models, major datasets, story bundles)
- After incidents: add an SOP + regression checks

> [!TIP]
> A good system becomes safer over time: every incident should produce a new gate. ğŸ§¯âœ…

---

## ğŸ¤ PR / review checklist

When your PR includes experiments, runs, or evidence:

- [ ] EXP report added/updated (`mcp/experiments/...`) *(if decision-relevant)*
- [ ] RUN receipt folder added (`mcp/runs/...`) with `MANIFEST.md`
- [ ] Gate report recorded (`mcp/gates/...` or run-local gate outputs)
- [ ] Evidence outputs stored under `data/processed/...` (or linked object storage)
- [ ] STAC/DCAT/PROV pointers added (IDs or paths)
- [ ] Policy packs pass (OPA/Conftest or equivalent)
- [ ] AI involvement labeled (if applicable)
- [ ] Reproduction steps included (copy/paste runnable)
- [ ] No secrets / no sensitive leaks in logs or outputs
- [ ] Reviewer can reproduce (required for L2/L3 work)
- [ ] Story Nodes (if included) cite evidence artifacts and pass moderation checks

> [!TIP]
> A great review comment is: **â€œI reproduced this and got the same outputs.â€** âœ…

---

## ğŸ“š Project reference library influence map

> These project files inform *how we design and review* MCP artifacts: reproducibility, governance, policy enforcement, AI transparency, UI/story discipline, streaming data receipts, and long-term roadmap alignment.

<details>
<summary><strong>ğŸ“¦ Expand: Reference library â†’ what it influences in <code>mcp/</code></strong></summary>

| Project file | Primary lens | How it upgrades MCP |
|---|---|---|
| `ğŸ“š Kansas Frontier Matrix (KFM) Data Intake â€“ Technical & Design Guide.pdf` | ğŸ§² Intake discipline | Defines intake stages, streaming semantics, standards integration (STAC/DCAT/PROV), quarantine/rollback expectations; motivates â€œpromotion is atomicâ€ and â€œstreaming still needs provenance.â€ |
| `Kansas Frontier Matrix (KFM) â€“ Comprehensive Architecture, Features, and Design.pdf` | ğŸ§­ Blueprint | Establishes API-centric, containerizable architecture; governance & observability layer; formalizes Wâ€‘Pâ€‘E agents as auditable automation under policy gates; supports adding gate reports + promotion receipts. |
| `Kansas Frontier Matrix (KFM) â€“ AI System Overview ğŸ§­ğŸ¤–.pdf` | ğŸ¤– Evidence-bound AI | Requires citations on every answer, refusal under uncertainty, governance checks, audit panel/XAI, drift/bias monitoring; motivates model cards with citation coverage + refusal policy and AI receipts. |
| `Kansas Frontier Matrix â€“ Comprehensive UI System Overview.pdf` | ğŸ—ºï¸ UI + Story discipline | Clarifies Story Nodes, offline packs, AR integration, contributor workflows/moderation; pushes MCP to include story evidence bundles, moderation receipts, and offline pack benchmark receipts. |
| `Kansas Frontier Matrix (KFM) â€“ Comprehensive Technical Documentation.pdf` | ğŸ§± System doctrine | Expands end-to-end vision: data domains, UI/3D demos, reproducible research (notebooks/DOIs), and education workflows; strengthens MCP guidance on notebooks + release artifacts. |
| `ğŸŒŸ Kansas Frontier Matrix â€“ Latest Ideas & Future Proposals.docx.pdf` | ğŸ§ª Roadmap & QA | Adds forward-looking QA: PROV-first CI agents, idempotency keys, kill switch; proposes DevOpsâ†’PROV mapping for PR traceability; motivates `dev_prov/` receipts and stronger CI hooks. |
| `Innovative Concepts to Evolve the Kansas Frontier Matrix (KFM).pdf` | ğŸš€ Future UX & community | Introduces 4D digital twin, immersive/AR storytelling, natural language query, GeoXAI, crowdsourced verification; motivates new MCP checklists for AR/simulation/community evidence and cultural protocols. |
| `AI Concepts & more.pdf` *(PDF portfolio)* | ğŸ§  AI reference shelf | Provides deeper background on AI architecture, reliability, and safety; supports stronger model-card discipline, prompt security posture, and evaluation harness design. |
| `Data Managment-Theories-Architures-Data Science-Baysian Methods-Some Programming Ideas.pdf` *(PDF portfolio)* | âš™ï¸ Systems & methods shelf | Background on data architectures, microservices, Bayesian methods, and data-intensive design; supports MCPâ€™s deterministic ETL + performance receipts + uncertainty reporting. |
| `Maps-GoogleMaps-VirtualWorlds-Archaeological-Computer Graphics-Geospatial-webgl.pdf` *(PDF portfolio)* | ğŸŒ Mapping & 3D shelf | Background on WebGL, virtual worlds, and 3D GIS; supports MCP guidance for 3D/AR coordinate conventions, performance budgets, and visual interpretation warnings. |
| `Various programming langurages & resources 1.pdf` *(PDF portfolio)* | ğŸ§° Engineering shelf | Polyglot engineering references (shell, Git, APIs, languages); supports reproducible build/run discipline and safe automation patterns in run receipts. |

</details>

---

## ğŸ§¾ Reference library index

This is the **local** reference library currently shaping MCP.  
Use it as â€œhow we thinkâ€ scaffoldingâ€”**not** as a substitute for evidence artifacts.

<details>
<summary><strong>ğŸ“š Expand: Full index of consulted project files</strong></summary>

### ğŸ§­ Core KFM system docs (v13+)
- `ğŸ“š Kansas Frontier Matrix (KFM) Data Intake â€“ Technical & Design Guide.pdf`
- `Kansas Frontier Matrix (KFM) â€“ Comprehensive Architecture, Features, and Design.pdf`
- `Kansas Frontier Matrix (KFM) â€“ Comprehensive Technical Documentation.pdf`
- `Kansas Frontier Matrix â€“ Comprehensive UI System Overview.pdf`
- `Kansas Frontier Matrix (KFM) â€“ AI System Overview ğŸ§­ğŸ¤–.pdf`
- `ğŸŒŸ Kansas Frontier Matrix â€“ Latest Ideas & Future Proposals.docx.pdf`
- `Innovative Concepts to Evolve the Kansas Frontier Matrix (KFM).pdf`

### ğŸ“¦ PDF portfolio â€œreference shelvesâ€ (treat as background, not canon)
These are **PDF portfolios** (containers of many embedded references). They are useful for deep dives, but they are **not** a substitute for repo contracts, catalogs, or receipts.

- `AI Concepts & more.pdf` *(portfolio; example embedded refs include: AI architecture guides, AI safety/reliability, enterprise AI, AI + cybersecurity, etc.)*
- `Data Managment-Theories-Architures-Data Science-Baysian Methods-Some Programming Ideas.pdf` *(portfolio; example embedded refs include: database surveys, architecture patterns, Bayesian methods, data-intensive design, microservices, etc.)*
- `Maps-GoogleMaps-VirtualWorlds-Archaeological-Computer Graphics-Geospatial-webgl.pdf` *(portfolio; example embedded refs include: 3D GIS/archaeology, WebGL, virtual worlds, geospatial visualization, etc.)*
- `Various programming langurages & resources 1.pdf` *(portfolio; polyglot engineering references and notes)*

</details>

---

## ğŸ•°ï¸ Version history

| Version | Date | Summary | Author |
|---:|---|---|---|
| v1.5.0 | 2026-01-19 | Aligned MCP with KFM v13+ architecture: added **policy packs + gate reports**, clarified **atomic promotion** and **streaming provenance**, added **Watcherâ€“Plannerâ€“Executor receipts** and optional **DevOpsâ†’PROV** bundles, expanded Story/Focus/Offline/AR guidance, and updated reference influence map to include all current KFM project docs + portfolio shelves. | KFM Engineering |
| v1.4.0 | 2026-01-13 | Brought MCP in line with contract-first + provenance-first doctrine: added repo invariants, strengthened front matter (doc UUID + care_label), expanded run receipts to include hashes + contract validation, added benchmark/performance receipt guidance, and added threat-model/hostile-input section. | KFM Engineering |
| v1.3.0 | 2026-01-11 | Tightened MCP into a typed, machine-readable â€œmethods + receiptsâ€ layer: added artifact/ID table, Detectâ†’Validateâ†’Promote guidance, front matter + schema notes, expanded run manifest to include policy checks/attestation hooks, added incident/review artifact guidance. | KFM Engineering |
| v1.2.0 | 2026-01-09 | Upgraded MCP to align with evidence-first/contract-first doctrine: added definition-of-done, reproducibility levels, traceability matrix, bad-evidence restraint protocol, expanded governance/licensing/sensitive-location guidance. | KFM Engineering |
| v1.1.0 | 2026-01-06 | Clarified nonâ€‘negotiables + pipeline linkage; added run receipt template, PR checklist, and workflow diagram. | KFM Engineering |
| v1.0.0 | 2025-12-31 | Initial MCP README: experiments, runs, SOPs, model cards, notebooks, safety rules. | KFM Engineering |

---

ğŸ§­ **Goal:** Make every output auditable and every method teachable.  
ğŸ§¾ **Promise:** If itâ€™s in production, it has a paper trail. âœ…