<!--
ğŸ“Œ MCP is KFMâ€™s canonical â€œmethods + receiptsâ€ boundary (not a data store, not runtime code).
ğŸ—“ï¸ Last updated: 2026-01-11
-->

# ğŸ§ª MCP â€” Master Coder Protocol  
### *Methods, Controls & Processes* ğŸ§¾âš™ï¸

![README](https://img.shields.io/badge/README-v1.3.0-8957e5)
![Docs-first](https://img.shields.io/badge/docs-documentation--first-blue)
![Reproducible](https://img.shields.io/badge/reproducible-audit--ready-success)
![Evidence](https://img.shields.io/badge/evidence-catalog--linked-informational)
![PROV](https://img.shields.io/badge/provenance-W3C%20PROV-7b42f6)
![Governance](https://img.shields.io/badge/governance-FAIR%2BCARE%20%2B%20Sovereignty-2ea043)
![Security](https://img.shields.io/badge/security-hostile--inputs%20%2B%20deny--by--default-red)
![KFM](https://img.shields.io/badge/KFM-Kansas%20Frontier%20Matrix-orange)

> **TL;DR:** `mcp/` is KFMâ€™s **methods + receipts** layer ğŸ““ğŸ§   
> It holds **protocols**, **run receipts**, **SOPs**, **model cards**, and **review artifacts**â€”so every result can be **re-run, reviewed, and trusted** âœ…  
>
> **Naming note:** Some internal docs expand MCP as **Methods, Controls & Processes**.  
> In *this repo*, we keep the canonical expansion **Master Coder Protocol** âœ… (same intent, same discipline).

> [!IMPORTANT]
> In this repo, **MCP = Master Coder Protocol** âœ…  
> **MCP â‰  Model Context Protocol** ğŸš« *(not what we mean here)*  
> Keep this distinction consistent in docs, PRs, issues, and commit messages.

---

## ğŸ”— Quick links
- ğŸ§­ Repo overview: **[`../README.md`](../README.md)**
- ğŸ§¬ Pipelines boundary (contract portal): **[`../pipelines/README.md`](../pipelines/README.md)** *(if present)*
- ğŸ§° Scripts boundary (automation): **[`../scripts/README.md`](../scripts/README.md)** *(if present)*
- ğŸ§© Executable source boundary: **[`../src/README.md`](../src/README.md)**
- ğŸ“¦ Data + metadata boundary: **[`../data/README.md`](../data/README.md)**
- ğŸ§ª Notebooks boundary (lab bench): **[`../notebooks/README.md`](../notebooks/README.md)** *(if present)*
- ğŸ§° Validators & tooling: **[`../tools/README.md`](../tools/README.md)** *(if present)*
- âœ… Tests & CI gates: **[`../tests/README.md`](../tests/README.md)** *(if present)*
- ğŸ¤ CI/CD & policies: **[`../.github/`](../.github/)** *(workflows, security policy, automation)*

---

## âš¡ Quick Nav
- [ğŸ§¾ Doc metadata](#-doc-metadata)
- [ğŸ§­ What MCP is](#-what-mcp-is)
- [ğŸ§± MCP artifacts (types, IDs, and immutability)](#-mcp-artifacts-types-ids-and-immutability)
- [ğŸš¦ Nonâ€‘negotiables](#-non-negotiables)
- [ğŸ Quick Start](#-quick-start)
- [ğŸ—‚ï¸ Directory layout](#ï¸-directory-layout)
- [ğŸ” The MCP workflow loop](#-the-mcp-workflow-loop)
- [ğŸš¥ Detect â†’ Validate â†’ Promote](#-detect--validate--promote)
- [ğŸ§¾ Front matter + schemas (machine-readable MCP)](#-front-matter--schemas-machine-readable-mcp)
- [âœ… Definition of done](#-definition-of-done)
- [ğŸ“¦ Required artifacts](#-required-artifacts)
- [ğŸ§ª Experiment reports](#-experiment-reports)
- [ğŸƒ Run receipts](#-run-receipts)
- [ğŸ§° SOPs](#-sops)
- [ğŸ§  Model cards](#-model-cards)
- [ğŸ““ Notebooks](#-notebooks)
- [ğŸ§­ Traceability matrix](#-traceability-matrix)
- [ğŸ§¯ Bad evidence protocol](#-bad-evidence-protocol)
- [ğŸ”— MCP â†” KFM evidence pipeline](#-mcp--kfm-evidence-pipeline)
- [ğŸ§© Domain checklists](#-domain-checklists)
- [ğŸ” Safety, privacy, licensing](#-safety-privacy-licensing)
- [âœ… QA, audits, and CI hooks](#-qa-audits-and-ci-hooks)
- [ğŸ¤ PR / review checklist](#-pr--review-checklist)
- [ğŸ“š Project reference library influence map](#-project-reference-library-influence-map)
- [ğŸ•°ï¸ Version history](#ï¸-version-history)

---

## ğŸ§¾ Doc metadata

| Field | Value |
|---|---|
| Doc | `mcp/README.md` |
| Status | Active âœ… |
| Last updated | **2026-01-11** |
| Audience | Contributors writing experiments, running jobs, training models, shipping evidence artifacts |
| Prime directive | If it changes â€œspatial truth,â€ it must be **traceable + reproducible + reviewable** |
| KFM canon | **ETL â†’ STAC/DCAT/PROV â†’ Graph â†’ APIs â†’ UI â†’ Story Nodes â†’ Focus Mode** |

---

## ğŸ§­ What MCP is

### âœ… MCP isâ€¦
A **governed method layer** that turns â€œwe tried somethingâ€ into **auditable science**:

- ğŸ§ª **Protocols** (what we intended to do + why)
- ğŸƒ **Receipts** (what we actually ran + how)
- ğŸ§° **SOPs** (repeatable procedures for risky/repeated work)
- ğŸ§  **Model cards** (responsible AI/ML usage)
- ğŸ‘€ **Review artifacts** (what was checked, by whom, and what failed)
- ğŸ§­ **Traceability** (decision â†” evidence â†” catalogs â†” provenance)

### ğŸš« MCP is notâ€¦
- âŒ A data lake (thatâ€™s `data/`)
- âŒ A code dump (thatâ€™s `src/` and `web/`)
- âŒ A place for large outputs (store in `data/processed/**` + catalogs)
- âŒ A place for â€œunsourced narrativeâ€ (that belongs in Story Nodes with explicit evidence links)

> [!TIP]
> MCP exists to make â€œresultsâ€ **explainable and replayable**â€”not merely impressive. ğŸ§¾âœ…

---

## ğŸ§± MCP artifacts (types, IDs, and immutability)

MCP stays reliable because artifacts are **typed**, **named**, and **treated correctly**.

| Artifact | Prefix / ID pattern | Where | Immutable? | Purpose |
|---|---|---|---:|---|
| ğŸ§ª Experiment protocol | `EXP-YYYY-MM-DD-<slug>` | `mcp/experiments/` | âš ï¸ *Mutable while `draft`* | Pre-register intent, assumptions, and validation plan |
| ğŸƒ Run receipt | `RUN-YYYY-MM-DD-<slug>/` | `mcp/runs/` | âœ… **Yes** | What was actually executed + how to reproduce |
| ğŸ§° SOP | `SOP-<topic>-v<semver>` | `mcp/sops/` | âœ… *Versioned* | Repeatable procedures (risky or frequent tasks) |
| ğŸ§  Model card | `MODEL-<name>-v<semver>` | `mcp/model_cards/` | âœ… *Versioned* | Intended use, limits, datasets, governance for ML/AI |
| ğŸ‘€ Review note | `REV-YYYY-MM-DD-<slug>.md` | `mcp/reviews/` *(recommended)* | âœ… **Yes** | Independent reproduction and governance sign-off |
| ğŸ§­ Traceability | `TRACEABILITY.md` *(or `TRACE-*.md`)* | `mcp/traceability/` | âœ… *Append-only mindset* | Decision/feature â†’ EXP/RUN â†’ evidence + catalogs |
| ğŸ§¯ Incident / anomaly | `INC-YYYY-MM-DD-<slug>.md` *(optional)* | `mcp/incidents/` *(optional)* | âœ… **Yes** | When something shipped wrong: what happened + fix + new gates |

> [!IMPORTANT]
> **Run receipts are immutable.**  
> If anything changes (inputs, params, code, environment), create a **new** run folder. âœ…

---

## ğŸš¦ Nonâ€‘negotiables

These rules keep KFM **scientific**, **auditable**, and **governed**:

1) **Evidence lives in `data/` â€” not in `mcp/`.** ğŸ“¦  
   - `mcp/` = methods, decisions, receipts, checklists  
   - `data/processed/...` = outputs (**and they must be cataloged**)

2) **Protocol before run (when it matters).** ğŸ§¾â¡ï¸ğŸƒ  
   If results could influence product decisions, public narratives, pipelines, or models: write an **EXP** first.

3) **No â€œmagic results.â€** ğŸª„ğŸš«  
   If you canâ€™t reproduce it using:
   - a commit hash
   - an environment snapshot
   - a config
   - linked inputs/outputs (**catalog IDs**)
   â€¦then itâ€™s not â€œdone.â€

4) **Immutable receipts.** ğŸ§¾  
   Donâ€™t edit a run receipt to â€œfix history.â€  
   Make a **new** run folder and link it.

5) **Label AI involvement.** ğŸ¤–  
   Any AI-assisted outputs must be labeled and provenance-linked.  
   Focus Mode remains **advisory-only** and must be evidence-backed.

6) **KFM pipeline order is sacred.** ğŸ§±  
   **ETL â†’ Catalogs (STAC/DCAT/PROV) â†’ Graph â†’ APIs â†’ UI â†’ Story Nodes â†’ Focus Mode**

7) **No privacy / sensitivity downgrade.** ğŸ”’  
   Outputs cannot be less restricted than inputs without an explicit, reviewed redaction step.

8) **Licensing isnâ€™t optional.** ğŸ§¾âš–ï¸  
   Every dataset / artifact must carry license + attribution requirements through catalogs and narratives.

9) **Stable IDs are a correctness feature.** ğŸ·ï¸  
   IDs should be *invariant* across reruns when inputs havenâ€™t changedâ€”treat IDs like API contracts.

> [!TIP]
> Motto: **â€œIf I canâ€™t reproduce it in 30 minutes, itâ€™s not complete.â€** â±ï¸âœ…

---

## ğŸ Quick Start

### 1) Create an experiment report ğŸ§ª
Add:
- `mcp/experiments/EXP-YYYY-MM-DD-<slug>.md`

### 2) Create a run receipt ğŸƒ
Add:
- `mcp/runs/RUN-YYYY-MM-DD-<slug>/`
  - config, env snapshot, logs, metrics, and `MANIFEST.md`

### 3) Store evidence outputs in the governed data layer ğŸ“¦
Put evidence artifacts in:
- `data/processed/...` âœ…  
Then publish metadata + lineage:
- `data/stac/**` + `data/catalog/dcat/**` + `data/prov/**` ğŸ—‚ï¸ğŸ§¬

### 4) Link it to decisions ğŸ§­
Update traceability (recommended):
- `mcp/traceability/TRACEABILITY.md`

> [!IMPORTANT]
> `mcp/` should stay **lightweight** and human-readable.  
> Large artifacts go to `data/processed/` (or object storage) and get catalog records.

---

## ğŸ—‚ï¸ Directory layout

```text
ğŸ“ mcp/
â”œâ”€â”€ ğŸ“„ README.md                 # you are here ğŸ‘‹
â”œâ”€â”€ ğŸ“ experiments/              # human-readable experiment protocols ğŸ§ªğŸ§¾
â”œâ”€â”€ ğŸ“ runs/                     # immutable run receipts (configs, env, logs, metrics) ğŸƒğŸ§¾
â”œâ”€â”€ ğŸ“ sops/                     # Standard Operating Procedures (repeatable tasks) ğŸ§°
â”œâ”€â”€ ğŸ“ model_cards/              # model cards for any ML/AI used or trained ğŸ§ 
â”œâ”€â”€ ğŸ“ notebooks/                # tidy, reproducible notebooks ğŸ““
â”œâ”€â”€ ğŸ“ traceability/             # decision â†” evidence mapping ğŸ§­ (recommended)
â”œâ”€â”€ ğŸ“ reviews/                  # peer reproduction notes / governance sign-offs ğŸ‘€ (recommended)
â”œâ”€â”€ ğŸ“ incidents/                # post-mortems / anomaly reports ğŸ§¯ (optional)
â””â”€â”€ ğŸ“ templates/                # local templates ğŸ§© (or use docs/templates/)
```

> [!NOTE]
> Repo implementations vary. If `traceability/` or `reviews/` doesnâ€™t exist yet, consider adding themâ€”  
> design docs call out traceability + modular documentation as a core MCP promise. âœ…

---

## ğŸ” The MCP workflow loop

KFM work is **question â†’ protocol â†’ run â†’ evidence â†’ report â†’ review â†’ iterate**:

```mermaid
flowchart LR
  Q["â“ Question"] --> P["ğŸ§¾ Protocol (EXP)"]
  P --> R["ğŸƒ Run (RUN receipt)"]
  R --> E["ğŸ“¦ Evidence (data/processed + catalogs)"]
  E --> S["ğŸ§ª Summary report (interpretation + limits)"]
  S --> V["ğŸ‘€ Review (repro + governance)"]
  V --> Q
```

### ğŸ”¬ Scientific method alignment (what we document)
MCP is a practical â€œscientific method adapterâ€ for software + data work:

- **Observation / question** â†’ Why are we doing this?
- **Hypothesis** â†’ What do we expect to see?
- **Method** â†’ Exact procedure + configuration
- **Experiment** â†’ The run receipt (what happened)
- **Analysis** â†’ Metrics, plots, error checks
- **Conclusion** â†’ What we learned (with limits)
- **Iteration** â†’ Next experiment / pipeline change

---

## ğŸš¥ Detect â†’ Validate â†’ Promote

A lot of KFM risk comes from â€œhalf-publishedâ€ artifacts. MCP encourages a controlled publishing rhythm:

1) **Detect** ğŸ•µï¸  
   Identify new inputs / changes (sources updated, new scans, schema drift, new tiles).

2) **Validate** âœ…  
   Run fast gates:
   - schema & bounds checks
   - catalog validity
   - link integrity
   - governance propagation (no downgrade)
   - security scans (secrets / sensitive patterns)

3) **Promote** ğŸš€  
   Only after validation:
   - write evidence to `data/processed/**`
   - write STAC/DCAT/PROV
   - (optionally) export graph ingest bundles
   - add MCP RUN receipt + traceability update

> [!TIP]
> Treat â€œpromotionâ€ like a release: **atomic publish or nothing.** ğŸ§¾âœ…

---

## ğŸ§¾ Front matter + schemas (machine-readable MCP)

MCP documents are **human-first**, but they should also be **machine-readable** for dashboards, validation, and CI gates.

### âœ… Front matter conventions (recommended)
All MCP artifacts should start with YAML front matter (or equivalent), including:

- stable ID (`EXP-...`, `RUN-...`, `SOP-...`, `MODEL-...`)
- date
- owner
- status
- tags
- AI involvement (if applicable)
- links to evidence (catalog IDs or paths)

> [!NOTE]
> If your repo includes MCP schemas under `schemas/mcp/`, keep MCP front matter compliant and validate in CI. âœ…

---

## âœ… Definition of done

### âœ… MCP â€œdoneâ€ means: reproducible + governed
For any EXP/RUN that influences production pipelines, APIs, UI layers, Story Nodes, or Focus Mode:

- [ ] Front-matter complete + consistent (IDs, dates, owner, status)
- [ ] Claims link to evidence inputs/outputs (**catalog pointers**)
- [ ] Validation steps are listed and repeatable
- [ ] Governance + FAIR/CARE + sovereignty considerations stated (when applicable)
- [ ] Another contributor can reproduce results without tribal knowledge

### ğŸ§± Reproducibility levels (recommended)
- **L0** ğŸŸ¡: exploratory note (not decision-worthy)
- **L1** ğŸŸ : reproducible by author (config + env captured)
- **L2** ğŸŸ¢: reproducible by reviewer (independent re-run)
- **L3** ğŸ†: CI-backed / automated rerun (pipeline job + validators)

---

## ğŸ“¦ Required artifacts

### âœ… â€œReal workâ€ minimum bar
If an experiment influences decisions, pipelines, or published results, it must include:

- ğŸ§ª **Experiment report** â†’ `mcp/experiments/...`
- ğŸƒ **Run receipt** â†’ `mcp/runs/...`
- ğŸ”— **Code pointer** â†’ commit hash + entrypoint
- ğŸ§± **Environment snapshot** â†’ Docker image digest **or** lockfile/requirements
- ğŸ² **Seeds / determinism flags** (where applicable)
- ğŸ“¦ **Outputs stored as evidence** â†’ `data/processed/...`
- ğŸ—‚ï¸ **Catalog records** â†’ STAC/DCAT
- ğŸ§¬ **Lineage** â†’ PROV pointers (inputs + outputs)
- ğŸ‘€ **Review notes** â†’ reproduction sign-off for L2/L3 work (recommended)
- ğŸ” **Policy checks evidence** â†’ list which gates ran (recommended)
- ğŸ§¾ **(Optional) CI attestation** â†’ if your repo signs artifacts / builds

> [!WARNING]
> Avoid committing large binaries to `mcp/`.  
> If itâ€™s an â€œoutput,â€ it probably belongs in `data/processed/` with catalogs + lineage.

---

## ğŸ§ª Experiment reports

### ğŸ“› Naming convention
Use one pattern consistently:

- `EXP-YYYY-MM-DD-<short-slug>.md`  
  Example: `EXP-2026-01-02-ocr-ner-baseline.md`

### ğŸ·ï¸ Status values
- `draft` ğŸ“ â€” in progress
- `complete` âœ… â€” reproducible; linked receipts + evidence
- `superseded` ğŸ§¯ â€” replaced by a newer experiment

### ğŸ§¾ Experiment template (copy/paste)

```md
---
id: EXP-YYYY-MM-DD-<slug>
title: "<short, explicit title>"
date: YYYY-MM-DD
owner: "@github-handle"
status: draft | complete | superseded
repro_level: L0 | L1 | L2 | L3
risk_level: low | medium | high
ai_used: true | false
supersedes: []          # optional: [EXP-...]
superseded_by: []       # optional: [EXP-...]
tags: [gis, ocr, nlp, stac, dcat, prov, graph, sim, stats, web, security]
---

# Objective / Question â“
- What are we trying to learn or improve?

# Background / Prior Art ğŸ“š
- Links to prior experiments, issues, papers, notes, or domain docs.

# Hypothesis âœ…/âŒ
- What do we expect and why?

# Variables & Controls ğŸ›ï¸
- Key variables youâ€™re changing.
- Controls / baselines.
- What stays fixed.

# Inputs (Evidence In) ğŸ—ƒï¸
- Dataset IDs + STAC/DCAT pointers.
- Sampling rules, inclusion/exclusion, time range, bbox.
- Licensing + sensitivity notes (if applicable).

# Method / Protocol ğŸ§¾
- Step-by-step procedure.
- Parameters + configs (link to run receipt config).
- Tools + versions (OS/GPU/driver notes if relevant).

# Validation Plan âœ…
- What fails fast?
- What warns?
- What â€œsanity checksâ€ must pass?

# Run Receipt ğŸƒ
- Code commit: `abcdef1`
- Entrypoint: `src/...` or notebook path
- Run folder: `mcp/runs/RUN-YYYY-MM-DD-.../`
- Seeds: `...`
- Determinism flags: `...`

# Outputs (Evidence Out) ğŸ“¦
- Where outputs live (paths under `data/processed/...`)
- Catalog pointers:
  - STAC item(s): `...`
  - DCAT dataset: `...`
  - PROV bundle: `...`

# Results ğŸ“ˆ
- Metrics, charts, qualitative examples (keep small).
- Add 1â€“3 â€œsanity checkâ€ examples.

# Uncertainty, Bias, and Validation ğŸ”
- What could be wrong?
- Checks performed (spot checks, CV, error bounds, leakage checks).
- Bias risks / perspective gaps (especially for historical corpora).

# Interpretation ğŸ§ 
- What do results mean for KFM decisions?

# Decision / Next Steps ğŸ§­
- Adopt / iterate / abandon (and why).

# Reproducibility Checklist âœ…
- [ ] Parameters & configs documented
- [ ] Code committed + hash recorded
- [ ] Environment captured (Docker/lockfile)
- [ ] Seeds recorded (if applicable)
- [ ] Inputs/outputs linked via STAC/DCAT/PROV
- [ ] Reviewer can reproduce (for L2/L3)
```

> [!TIP]
> If you canâ€™t write the â€œUncertainty, Bias, and Validationâ€ section honestly, the experiment isnâ€™t finished. ğŸ”âœ…

---

## ğŸƒ Run receipts

Runs are the **receipt** for an experiment: what you ran, how you ran it, where outputs went, and what changed.

### ğŸ“› Naming convention
- `RUN-YYYY-MM-DD-<slug>/`

### ğŸ“¦ Suggested run folder contents
- `config/` ğŸ§¾ â€” YAML/JSON config used for the run
- `env/` ğŸ§± â€” `pip freeze`, lockfiles, Docker digest, OS info
- `logs/` ğŸªµ â€” structured logs (**redacted if needed**)
- `metrics/` ğŸ“ˆ â€” CSV/JSON metrics, evaluations
- `artifacts/` ğŸ§© â€” *small* artifacts (thumbnails, sample outputs)
- `MANIFEST.md` ğŸ§¾ â€” reproduction instructions + evidence links + checks performed

### ğŸ§¾ Minimal `MANIFEST.md` template (copy/paste)

```md
---
run_id: RUN-YYYY-MM-DD-<slug>
related_experiment: EXP-YYYY-MM-DD-<slug>
date: YYYY-MM-DD
owner: "@github-handle"
env: dev | staging | prod

code:
  commit: abcdef1
  entrypoint: "src/pipelines/..."
  args: ["--config", "config/run.yml"]
  dirty_worktree: false  # recommended

environment:
  docker_image: "ghcr.io/org/project:tag@sha256:..."     # preferred
  # or:
  requirements: "env/requirements.lock.txt"
  os: "..."
  cpu: "..."
  gpu: "..."  # optional

randomness:
  seeds: [123, 456]
  deterministic_flags: ["..."]

inputs:
  - catalog_ref: "stac://<collection_or_item_id>"
  - dcat: "data/catalog/dcat/<dataset>.jsonld"
  - prov: "data/prov/<prior-run>.jsonld"

outputs:
  - path: "data/processed/<domain>/<dataset>/..."
    stac_item: "data/stac/items/..."
    dcat: "data/catalog/dcat/..."
    prov: "data/prov/<run-id>.jsonld"

validation:
  - "schema validation: pass/fail"
  - "link checks: pass/fail"
  - "classification propagation: pass/fail"
  - "spot checks: ..."

policy_checks:               # optional, recommended
  - "secrets scan: pass/fail"
  - "sensitive patterns scan: pass/fail"
  - "sbom generated: yes/no"
  - "attestation present: yes/no"

notes: ""
---

# Summary ğŸ§¾
- What did this run do?

# Evidence outputs ğŸ“¦
- Where outputs are stored (`data/processed/...`) + catalog IDs

# How to reproduce ğŸ”
1. Checkout commit: `abcdef1`
2. Restore environment: ...
3. Run: ...
4. Validate: ...
```

> [!TIP]
> Treat run folders as **immutable receipts**.  
> New parameters â†’ new run folder âœ…

---

## ğŸ§° SOPs

SOPs turn â€œtribal knowledgeâ€ into a repeatable, reviewable procedure.  
Write an SOP whenever a task is repeated or risky: georeferencing, catalog publishing, redaction, OCR, tile generation, etc. ğŸ§¯

### SOP template (copy/paste)

```md
---
id: SOP-<topic>-v1.0.0
title: "<clear title>"
owner: "@github-handle"
last_updated: YYYY-MM-DD
risk_level: low | medium | high
---

# Purpose ğŸ¯
What this SOP accomplishes and when to use it.

# Scope âœ…
Whatâ€™s included / excluded.

# Prerequisites ğŸ§±
Accounts, tools, access, permissions.

# Tools & Versions ğŸ§°
Software + versions.

# Procedure ğŸ§­
1. Step...
2. Step...
3. Step...

# Verification âœ…
How to confirm it worked (checks + expected outputs).

# Troubleshooting ğŸ§¯
Common failure modes + fixes.

# Audit Trail ğŸ§¾
Example PRs / runs / experiments that used this SOP.
```

### â­ High-value SOPs to add (starter set)
- `sops/georeference_map.md` ğŸ—ºï¸ (control points, RMS error, CRS discipline)
- `sops/build_cog_and_tiles.md` ğŸ§Š (COG params, overviews, tile scheme)
- `sops/ocr_pipeline.md` ğŸ” (scan QA, language assumptions, error classes)
- `sops/publish_stac_dcat_prov.md` ğŸ—‚ï¸ğŸ§¬ (profiles, validation, link checks)
- `sops/catalog_qa_gate.md` âœ… (how to run CI-like catalog QA locally)
- `sops/postgis_import_index.md` ğŸ˜ (schemas, indexes, vacuum/analyze)
- `sops/redaction_and_sensitive_locations.md` ğŸ” (coarsen/offset rules, approvals)
- `sops/detect_validate_promote.md` ğŸš¥ (release discipline for evidence publishing)
- `sops/rollback_and_prov_repair.md` ğŸ§¯ (how to revert + repair provenance safely)
- `sops/ci_attestation_and_signing.md` ğŸ” (if your repo signs builds / generates SBOMs)
- `sops/story_node_evidence_bundle.md` ğŸ“š (evidence pack for narratives + Focus Mode)

---

## ğŸ§  Model cards

Any ML/AI model used in KFM (trained or adopted) needs a model card:

- what it is
- what it was trained on / sourced from
- what it should be used for âœ…
- what it must **not** be used for ğŸš«
- known limitations, bias risks, failure modes âš ï¸
- provenance + licensing + governance labels ğŸ§¾ğŸ”’

### Model card template (copy/paste)

```md
---
model_id: MODEL-<name>-v<semver>
owner: "@github-handle"
date: YYYY-MM-DD
ai_used: true
source:
  type: trained | third_party
  license: "..."
  reference: "paper/link/registry id"
datasets:
  - "stac://<collection_or_item_id>"
  - "dcat://<dataset_id>"
governance:
  sensitivity: public | restricted | confidential
  human_in_the_loop: required | recommended | optional
  redaction_required: yes | no
---

# Model overview ğŸ§ 
- What problem does it solve?

# Intended use âœ…
- Supported use-cases.

# Out-of-scope / prohibited use ğŸš«
- What it must not be used for.

# Training data ğŸ—ƒï¸
- Datasets used (STAC/DCAT pointers), sampling, labeling notes.
- Known gaps / perspective bias notes.

# Evaluation ğŸ“ˆ
- Metrics, test sets, qualitative examples.
- Calibration / uncertainty notes when applicable.

# Limitations & biases âš ï¸
- Known failure modes, bias risks, uncertainty notes.

# Governance & safety ğŸ”
- Any redaction rules or sensitivity constraints.
- How outputs are labeled in UI / Focus Mode.

# Reproducibility ğŸ§ª
- Training code commit hash
- Environment / hardware notes
- Hyperparameters / config
- Seeds
- Artifact pointers (weights, charts) stored under `data/processed/...` with catalogs
```

---

## ğŸ““ Notebooks

Notebooks are welcomeâ€”**but must be readable and reproducible**:

- Start with a markdown cell: **purpose + inputs + outputs**
- Keep outputs small *(no huge embedded blobs)*
- Prefer parameterized notebooks or export to scripts when it becomes â€œrealâ€
- If a notebook produces evidence artifacts:
  - store outputs in `data/processed/...`
  - link them from an experiment report + run receipt

> [!CAUTION]
> Notebooks that silently write files without catalogs + provenance are **not shippable**.

---

## ğŸ§­ Traceability matrix

Traceability is how MCP connects â€œworkâ€ to â€œwhy it mattersâ€:

- â“ Question / requirement  
- ğŸ§ª EXP protocol  
- ğŸƒ RUN receipt  
- ğŸ“¦ Evidence artifacts (+ STAC/DCAT/PROV)  
- ğŸ•¸ï¸ Graph IDs (if applicable)  
- ğŸ›¡ï¸ API endpoints (if applicable)  
- ğŸ“š Story Node(s) / Focus Mode (if applicable)

### âœ… Recommended traceability table (copy/paste)

```md
| Decision / Feature | EXP | RUN | Evidence outputs (data/processed) | Catalog pointers (STAC/DCAT/PROV) | Reviewer repro | Notes |
|---|---|---|---|---|---|---|
| "OCR treaties baseline for Land Treaties domain" | EXP-2026-01-02-... | RUN-2026-01-02-... | data/processed/historical/land-treaties/ocr/... | STAC: ... / DCAT: ... / PROV: ... | @reviewer âœ… | error classes logged |
```

> [!TIP]
> If a Story Node makes a claim, traceability must point to the evidence artifacts that support it. ğŸ“šğŸ§¾

---

## ğŸ§¯ Bad evidence protocol

KFM must be resilient to â€œbad evidenceâ€ (messy scans, biased corpora, incomplete sensor data, uncertain geocoding).

When evidence is questionable, MCP requires **restraint**:

1) **Data pruning** âœ‚ï¸  
   Exclude known-bad inputs (or flag them as â€œquarantinedâ€ until fixed).

2) **Inferential restraint** ğŸ§ â¬‡ï¸  
   Reduce the strength/scope of conclusions; report uncertainty explicitly.

3) **Executional restraint** ğŸ›‘  
   Limit downstream actions: donâ€™t ship to UI/Story/Focus until reviewed, or serve only aggregated/redacted views.

### âœ… Minimum â€œbad evidenceâ€ documentation
- Whatâ€™s wrong?
- How do we know?
- What we changed (or refused to change)
- What remains uncertain
- Who reviewed the restraint decision

---

## ğŸ”— MCP â†” KFM evidence pipeline

KFM uses a strict evidence pipeline:

**ETL â†’ Catalogs (STAC/DCAT/PROV) â†’ Graph â†’ APIs â†’ UI â†’ Story Nodes â†’ Focus Mode**

So for MCP work:

- âœ… Protocols live here: `mcp/experiments/...`
- âœ… Receipts live here: `mcp/runs/...`
- âœ… Evidence artifacts live here: `data/processed/...`
- âœ… Evidence artifacts must be:
  - cataloged (STAC/DCAT) ğŸ—‚ï¸
  - lineage-linked (PROV) ğŸ§¬
  - integrated through governed contracts (API boundary) ğŸ”’

```mermaid
flowchart LR
  EXP["ğŸ§ª EXP report"] --> RUN["ğŸƒ RUN receipt"]
  RUN --> OUT["ğŸ“¦ data/processed outputs"]
  OUT --> CAT["ğŸ—‚ï¸ STAC/DCAT/PROV"]
  CAT --> GR["ğŸ•¸ï¸ Graph"]
  GR --> API["ğŸ›¡ï¸ APIs"]
  API --> UI["ğŸ—ºï¸ UI / Story / Focus"]
```

---

## ğŸ§© Domain checklists

Use the checklist that matches your work:

### ğŸ—ºï¸ GIS / Remote Sensing
- [ ] CRS documented (EPSG + axis order)
- [ ] Georeferencing method + control points documented
- [ ] Fit error/RMS recorded (if applicable)
- [ ] Raster outputs are COGs / tiled (with parameters)
- [ ] Vector outputs validate (geometry validity, topology as needed)
- [ ] Symbology/aggregation choices documented if they change interpretation ğŸ¨
- [ ] Catalog pointers included (STAC/DCAT) + lineage (PROV) ğŸ—‚ï¸ğŸ§¬

### ğŸ” OCR / NLP
- [ ] Input corpus + sampling documented
- [ ] Labeling rules / evaluation rubric included
- [ ] Precision/recall (or spot-check protocol) documented
- [ ] Failure classes logged (scan quality, fonts, ambiguity)
- [ ] Geoparsing uncertainty documented (ambiguous place names, gazetteer limits)

### ğŸ•¸ï¸ Graph analytics
- [ ] Graph schema/ontology version noted
- [ ] Metrics treated as **signals**, not facts (avoid over-interpretation)
- [ ] Provenance links from derived relations to source evidence
- [ ] No orphan IDs / referential integrity checks pass âœ…

### ğŸ“Š Statistics / Inference
- [ ] Outcomes + units defined
- [ ] Assumptions checked (independence, distribution, etc.)
- [ ] Effect sizes reported (not just p-values)
- [ ] Multiple comparisons handled (or explicitly scoped)
- [ ] Guardrails against optional stopping / publication bias documented ğŸ§¯

### ğŸ›°ï¸ Modeling & Simulation
- [ ] Assumptions enumerated explicitly
- [ ] Verification & validation approach documented (V&V mindset)
- [ ] Sensitivity analysis for key parameters
- [ ] Uncertainty quantified or bounded
- [ ] Results reported with uncertainty context (not single â€œtruthâ€ numbers)

### ğŸŒ Web UI / Visualization (when experiments affect front-end behavior)
- [ ] Payload budgets considered (tiles, vector sizes, images)
- [ ] Offline/low-bandwidth considerations documented (if relevant) ğŸ“±
- [ ] Accessibility and audit logging expectations noted â™¿ï¸ğŸ§¾
- [ ] Focus/Story evidence bundle is explicit (no unsourced claims) ğŸ“šğŸ§¾

---

## ğŸ” Safety, privacy, licensing

- ğŸš« Donâ€™t store secrets, tokens, keys, or sensitive PII in `mcp/`
- ğŸ§½ Redact logs before committing if they contain identifiers, endpoints, or sensitive paths
- ğŸ§Š Prefer immutable receipts: new run folder > editing old run folder
- ğŸ§­ If superseded, mark as `superseded` and link the replacement
- ğŸ—ºï¸ Sensitive locations: if a dataset could expose culturally sensitive or personal location data:
  - coarsen/offset/omit coordinates
  - require explicit permission & review before publishing
  - propagate sensitivity tags through catalogs and UI

> [!IMPORTANT]
> Licensing must travel with evidence. If you combine layers, the resulting artifact must still honor attribution and license constraints. âš–ï¸ğŸ§¾

---

## âœ… QA, audits, and CI hooks

### CI intent (minimum bar)
- ğŸ§¹ lint + formatting
- âœ… unit tests (where applicable)
- ğŸ§¾ schema validation (STAC/DCAT/PROV)
- ğŸ”— link checks (assets exist; IDs resolve)
- ğŸ” security scans (secrets; sensitive patterns; dependency risk)
- ğŸ§· governance checks (classification propagation; redaction regressions)

### Periodic audits (recommended)
- Quarterly: sample 3 completed EXP/RUN pairs â†’ verify re-run works end-to-end
- Before release: audit â€œhigh-impactâ€ artifacts (models, major new datasets, story bundles)
- After incidents: add an SOP + regression checks

> [!TIP]
> A good system becomes safer over time: every incident should produce a new gate. ğŸ§¯âœ…

---

## ğŸ¤ PR / review checklist

When your PR includes experiments, runs, or evidence:

- [ ] EXP report added/updated (`mcp/experiments/...`) *(if decision-relevant)*
- [ ] RUN receipt folder added (`mcp/runs/...`) with `MANIFEST.md`
- [ ] Evidence outputs stored under `data/processed/...` (or linked object storage)
- [ ] STAC/DCAT/PROV pointers added (IDs or paths)
- [ ] AI involvement labeled (if applicable)
- [ ] Reproduction steps included (1â€“4 steps; copy/paste runnable)
- [ ] No secrets / no sensitive leaks in logs or outputs
- [ ] Reviewer can reproduce (required for L2/L3 work)

> [!TIP]
> A great review comment is: **â€œI reproduced this and got the same outputs.â€** âœ…

---

## ğŸ“š Project reference library influence map

> These project files inform *how we design and review* MCP artifacts: reproducibility, governance, security, modeling rigor, statistical discipline, scaling, and visualization constraints.

<details>
<summary><strong>ğŸ“¦ Expand: Reference library â†’ what it influences in <code>mcp/</code></strong></summary>

| Project file | Primary lens | How it upgrades MCP |
|---|---|---|
| `Scientific Method _ Research _ Master Coder Protocol Documentation.pdf` | ğŸ”¬ Scientific method + documentation system | Reinforces protocol-first workflow, documentation standards, transparency, peer review, and modular domain protocols. |
| `Kansas Frontier Matrix (KFM) â€“ Comprehensive Technical Documentation.docx` | ğŸ§­ System blueprint | Aligns MCP with KFMâ€™s governed pipeline order, onboarding resources (glossary/templates), and evidence-backed Focus discipline. |
| `ğŸŒŸ Kansas Frontier Matrix â€“ Latest Ideas & Future Proposals.docx` | ğŸš€ Forward-looking operations | Encourages Detectâ†’Validateâ†’Promote, policy-as-code gates, provenance everywhere (PROV JSON-LD), and safer automation patterns (idempotency + kill switch). |
| `Kansas-Frontier-Matrix_ Open-Source Geospatial Historical Mapping Hub Design.pdf` | ğŸ—ï¸ Platform design | Clarifies end-to-end architecture (ingest â†’ catalogs â†’ analysis â†’ UI) and why experiment tracking/model cards are first-class. |
| `Kansas-Frontier-Matrix Design Audit â€“ Gaps and Enhancement Opportunities.pdf` | ğŸ§¯ Reality check | Highlights where MCP must be operational (actual SOPs, glossary, checklists, review notes)â€”not just â€œpaper MCP.â€ |
| `MARKDOWN_GUIDE_v13.md.gdoc` | ğŸ“˜ Repo-level invariants | Defines evidence-first + contract-first doctrine, Story Node/Focus constraints, and definition-of-done patterns for governed docs. |
| `Scientific Modeling and Simulation_ A Comprehensive NASA-Grade Guide.pdf` | ğŸ§ª V&V discipline | Shapes simulation experiment logging, V&V framing, uncertainty, and sensitivity analysis discipline. |
| `Understanding Statistics & Experimental Design.pdf` | ğŸ“Š Rigor + bias | Encourages guarding against optional stopping/publication bias and documenting assumptions + effect sizes. |
| `regression-analysis-with-python.pdf` + `Regression analysis using Python - slides-linear-regression.pdf` | ğŸ“ˆ Baselines + diagnostics | Improves reproducible modeling baselines and diagnostic reporting in EXP/RUN artifacts. |
| `think-bayes-bayesian-statistics-in-python.pdf` | ğŸ² Uncertainty | Encourages explicit priors, posterior uncertainty reporting, and calibrated decisions under uncertainty. |
| `graphical-data-analysis-with-r.pdf` | ğŸ“‰ EDA instincts | Reinforces visual sanity checks and anomaly detection before publishing evidence. |
| `Cloud-Based Remote Sensing with Google Earth Engine-Fundamentals and Applications.pdf` | ğŸ›°ï¸ EO workflows | Informs remote sensing SOPs (export patterns, time-series handling) and treating derived indices as evidence artifacts. |
| `python-geospatial-analysis-cookbook.pdf` | ğŸ—ºï¸ GIS engineering | Guides CRS hygiene, vector/raster IO, PostGIS integration, and safe geoprocessing SOPs. |
| `making-maps-a-visual-guide-to-map-design-for-gis.pdf` | ğŸ¨ Cartography ethics | Reminds that visualization choices shape meaning; demands documentation of map design decisions. |
| `Mobile Mapping_ Space, Cartography and the Digital - 9789048535217.pdf` | ğŸ“± Mobile/offline constraints | Encourages tiling, caching, and offline-aware documentation for downstream UX and performance. |
| `responsive-web-design-with-html5-and-css3.pdf` | ğŸŒ Real-device constraints | Pushes MCP to capture payload/latency constraints and test on realistic device assumptions. |
| `webgl-programming-guide-interactive-3d-graphics-programming-with-webgl.pdf` | ğŸ§Š GPU/3D | Motivates explicit coordinate conventions, LOD/tiling decisions, and 3D evidence display constraints. |
| `compressed-image-file-formats-jpeg-png-gif-xbm-bmp.pdf` | ğŸ–¼ï¸ Image pipelines | Shapes SOPs for thumbnails, compression, and safe handling of complex formats. |
| `PostgreSQL Notes for Professionals - PostgreSQLNotesForProfessionals.pdf` | ğŸ˜ Data store discipline | Informs SOPs around schemas, indexing, migrations, and reproducible data loading. |
| `Scalable Data Management for Future Hardware.pdf` | âš™ï¸ Performance + concurrency | Encourages documenting performance experiments, resource assumptions, and concurrency constraints in run receipts. |
| `Data Spaces.pdf` | ğŸ”— Interop & federation | Supports catalog-as-interface thinking and future federated evidence workflows. |
| `Spectral Geometry of Graphs.pdf` | ğŸ•¸ï¸ Graph theory | Encourages careful interpretation of graph metrics and provenance for derived relations. |
| `Generalized Topology Optimization for Structural Design.pdf` | ğŸ§® Optimization workflows | Suggests structuring optimization experiments as reproducible jobs with clear objectives/constraints. |
| `Principles of Biological Autonomy - book_9780262381833.pdf` | ğŸ§  Systems thinking | Promotes feedback-loop awareness and stability thinking when documenting pipelines and governance. |
| `Introduction to Digital Humanism.pdf` | â¤ï¸ Human-centered ethics | Reinforces transparency, accountability, and dignity in governance + AI documentation. |
| Security & concurrency references (e.g., `ethical-hacking-and-countermeasures...pdf`, `Gray Hat Python...pdf`, `concurrent-real-time-and-distributed-programming-in-java...pdf`) | ğŸ›¡ï¸ Adversarial mindset | Guides hostile-input posture, threat modeling, and defensive review of parsers/pipelines/services. |
| Programming bundles (`A...pdf`, `B-C...pdf`, `D-E...pdf`, `F-H...pdf`, `I-L...pdf`, `M-N...pdf`, `O-R...pdf`, `S-T...pdf`, `U-X...pdf`) | ğŸ§° Polyglot reference | Supports language/tooling best practices while keeping KFMâ€™s boundary invariants intact. |

</details>

---

## ğŸ•°ï¸ Version history

| Version | Date | Summary | Author |
|---:|---|---|---|
| v1.3.0 | 2026-01-11 | Tightened MCP into a typed, machine-readable â€œmethods + receiptsâ€ layer: added artifact/ID table, Detectâ†’Validateâ†’Promote guidance, front matter + schema notes, expanded run manifest to include policy checks/attestation hooks, added incident/review artifact guidance, and aligned reference map with current project docs. | KFM Engineering |
| v1.2.0 | 2026-01-09 | Upgraded MCP to align with v13 evidence-first/contract-first doctrine: added definition-of-done, reproducibility levels, traceability matrix, bad-evidence restraint protocol, expanded governance/licensing/sensitive-location guidance, and an updated reference-library influence map. | KFM Engineering |
| v1.1.0 | 2026-01-06 | Clarified nonâ€‘negotiables + pipeline linkage; added run receipt template, PR checklist, and workflow diagram. | KFM Engineering |
| v1.0.0 | 2025-12-31 | Initial MCP README: experiments, runs, SOPs, model cards, notebooks, safety rules. | KFM Engineering |

---

ğŸ§­ **Goal:** Make every output auditable and every method teachable.  
ğŸ§¾ **Promise:** If itâ€™s in production, it has a paper trail. âœ…

<!--
Evidence anchors (project docs consulted for this update):
- Evidence-first + artifact definitions + Focus/Story constraints:   
- MCP scientific-method posture + documentation standards + modular protocols:   
- Onboarding resources (glossary/templates) and MCP terminology in KFM doc:  
- Detectâ†’Validateâ†’Promote + policy-as-code + provenance/attestation ideas: :contentReference[oaicite:8]{index=8} :contentReference[oaicite:9]{index=9} 
- Design audit â€œmake MCP realâ€ (SOPs/glossary/checklists): 
- Open-source hub design: experiment tracking + model cards are first-class: :contentReference[oaicite:12]{index=12} :contentReference[oaicite:13]{index=13}
-->
