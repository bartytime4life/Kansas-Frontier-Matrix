# ğŸ§ª MCP â€” Master Coder Protocol  
### *Methods & Computational Experiments* ğŸ§¾âš™ï¸

![Docs-first](https://img.shields.io/badge/docs-documentation--first-blue)
![Reproducible](https://img.shields.io/badge/reproducible-audit--ready-success)
![Evidence](https://img.shields.io/badge/evidence-traceable-informational)
![Governance](https://img.shields.io/badge/governance-FAIR%2BCARE%20%2B%20Sovereignty-2ea043)
![KFM](https://img.shields.io/badge/KFM-Kansas%20Frontier%20Matrix-orange)

> **TL;DR:** `mcp/` is KFMâ€™s **lab notebook + playbook** ğŸ““ğŸ§   
> It holds experiment reports, run receipts, SOPs, notebooks, and model cardsâ€”so every result can be **re-run, reviewed, and trusted** âœ…

> [!IMPORTANT]
> In this repo, **MCP = Master Coder Protocol** (âœ… correct)  
> **MCP â‰  Model Context Protocol** (ğŸš« not what we mean here)  
> Keep this distinction consistent in docs, PRs, and issues.

---

## âš¡ Quick Nav
- [ğŸš¦ Nonâ€‘negotiables](#-non-negotiables)
- [ğŸ Quick Start](#-quick-start)
- [ğŸ—‚ï¸ Directory layout](#ï¸-directory-layout)
- [ğŸ” The MCP workflow loop](#-the-mcp-workflow-loop)
- [ğŸ“¦ Required artifacts](#-required-artifacts)
- [ğŸ§ª Experiment reports](#-experiment-reports)
- [ğŸƒ Run receipts](#-run-receipts)
- [ğŸ§° SOPs](#-sops)
- [ğŸ§  Model cards](#-model-cards)
- [ğŸ““ Notebooks](#-notebooks)
- [ğŸ”— MCP â†” KFM evidence pipeline](#-mcp--kfm-evidence-pipeline)
- [ğŸ§© Domain checklists](#-domain-checklists)
- [ğŸ” Safety & hygiene](#-safety--hygiene)
- [ğŸ¤ PR / review checklist](#-pr--review-checklist)
- [ğŸ•°ï¸ Version history](#ï¸-version-history)

---

## ğŸš¦ Nonâ€‘negotiables

These rules keep KFM **scientific**, **auditable**, and **governed**:

1. **Evidence lives in `data/` â€” not in `mcp/`.** ğŸ“¦  
   - `mcp/` = methods, decisions, receipts  
   - `data/processed/...` = outputs (and they must be cataloged)

2. **No â€œmagic results.â€** ğŸª„ğŸš«  
   If you canâ€™t reproduce it using:
   - a commit hash
   - an environment snapshot
   - a config
   - and linked inputs/outputs  
   â€¦then itâ€™s not â€œdone.â€

3. **Immutable receipts.** ğŸ§¾  
   Donâ€™t edit a run receipt to â€œfix history.â€  
   Make a new run folder if anything changes.

4. **Label AI involvement.** ğŸ¤–  
   Any AI-assisted outputs must be labeled and provenance-linked.

5. **KFM pipeline order is sacred.** ğŸ§±  
   **ETL â†’ Catalogs (STAC/DCAT/PROV) â†’ Graph â†’ APIs â†’ UI â†’ Story Nodes â†’ Focus Mode**

> [!TIP]
> Motto: **â€œIf I canâ€™t reproduce it in 30 minutes, itâ€™s not complete.â€** â±ï¸âœ…

---

## ğŸ Quick Start

### 1) Create an experiment report ğŸ§ª
Add:
- `mcp/experiments/EXP-YYYY-MM-DD-<slug>.md`

### 2) Create a run receipt ğŸƒ
Add:
- `mcp/runs/RUN-YYYY-MM-DD-<slug>/`
  - with config, env snapshot, logs, metrics, and a `MANIFEST.md`

### 3) Store evidence outputs in the governed data layer ğŸ“¦
Put artifacts in:
- `data/processed/...` âœ…  
Then catalog them:
- **STAC/DCAT/PROV** ğŸ§¾ğŸ§¬

> [!IMPORTANT]
> `mcp/` should stay **lightweight** and human-readable.  
> Large artifacts go to `data/processed/` (or object storage) and get catalog records.

---

## ğŸ—‚ï¸ Directory layout

```text
ğŸ“ mcp/
â”œâ”€â”€ ğŸ“„ README.md                 # you are here ğŸ‘‹
â”œâ”€â”€ ğŸ“ experiments/              # human-readable experiment reports ğŸ§ªğŸ§¾
â”œâ”€â”€ ğŸ“ runs/                     # run receipts (configs, env, logs, metrics) ğŸƒğŸ§¾
â”œâ”€â”€ ğŸ“ sops/                     # Standard Operating Procedures (repeatable tasks) ğŸ§°
â”œâ”€â”€ ğŸ“ model_cards/              # model cards for any ML/AI used or trained ğŸ§ 
â”œâ”€â”€ ğŸ“ notebooks/                # tidy, reproducible notebooks ğŸ““
â””â”€â”€ ğŸ“ templates/                # optional: local templates ğŸ§© (or use docs/templates/)
```

### ğŸ§­ Where to look next
- ğŸ“˜ Repo principles + structure: `../README.md`
- ğŸ§± Source architecture: `../src/README.md`
- ğŸ“¦ Data governance & metadata: `../data/README.md`
- ğŸ§© Contribution workflow: `../.github/README.md` *(if present)*

---

## ğŸ” The MCP workflow loop

KFM work is **question â†’ protocol â†’ run â†’ evidence â†’ report â†’ review**:

```mermaid
flowchart LR
  Q["â“ Question"] --> P["ğŸ§¾ Protocol (EXP)"]
  P --> R["ğŸƒ Run (RUN receipt)"]
  R --> E["ğŸ“¦ Evidence (data/processed + catalogs)"]
  E --> S["ğŸ§ª Summary report (interpretation + limits)"]
  S --> V["ğŸ‘€ Review (repro check)"]
  V --> Q
```

---

## ğŸ“¦ Required artifacts

### âœ… â€œReal workâ€ minimum bar
If an experiment influences decisions, pipelines, or published results, it must include:

- ğŸ§ª **Experiment report** â†’ `mcp/experiments/...`
- ğŸƒ **Run receipt** â†’ `mcp/runs/...`
- ğŸ”— **Code pointer** â†’ commit hash + entrypoint
- ğŸ§± **Environment snapshot** â†’ Docker image digest **or** lockfile/requirements
- ğŸ² **Seeds / determinism flags** (where applicable)
- ğŸ“¦ **Outputs stored as evidence** â†’ `data/processed/...`
- ğŸ§¬ **Provenance links** â†’ STAC/DCAT/PROV IDs (inputs + outputs)

> [!WARNING]
> Avoid committing large binaries to `mcp/`.  
> If itâ€™s an â€œoutput,â€ it probably belongs in `data/processed/` with catalogs + lineage.

---

## ğŸ§ª Experiment reports

### ğŸ“› Naming convention
Use one pattern consistently:

- `EXP-YYYY-MM-DD-<short-slug>.md`  
  Example: `EXP-2026-01-02-ocr-ner-baseline.md`

*(Optional alternative: `EXP-001-<slug>.md` â€” only if you maintain an index.)*

### ğŸ·ï¸ Status values
- `draft` ğŸ“ â€” in progress
- `complete` âœ… â€” reproducible; linked receipts + evidence
- `superseded` ğŸ§¯ â€” replaced by a newer experiment

### ğŸ§¾ Experiment template (copy/paste)

```md
---
id: EXP-YYYY-MM-DD-<slug>
title: "<short, explicit title>"
date: YYYY-MM-DD
owner: "@github-handle"
status: draft | complete | superseded
supersedes: []          # optional: [EXP-...]
superseded_by: []       # optional: [EXP-...]
tags: [gis, ocr, nlp, stac, dcat, prov, web, graph, sim, stats]
---

# Objective / Question â“
- What are we trying to learn or improve?

# Background / Prior Art ğŸ“š
- Links to prior experiments, issues, papers, or notes.

# Hypothesis âœ…/âŒ
- What do we expect and why?

# Inputs (Evidence In) ğŸ—ƒï¸
- Dataset IDs + STAC/DCAT references (paths/IDs).
- Sampling rules, inclusion/exclusion, time range, bbox.

# Method / Protocol ğŸ§¾
- Step-by-step procedure.
- Parameters and configs (link to run receipt config).
- Tools + versions (OS/GPU/driver notes if relevant).

# Run Receipt ğŸƒ
- Code commit: `abcdef1`
- Entrypoint: `src/...` or notebook path
- Run folder: `mcp/runs/RUN-YYYY-MM-DD-.../`
- Seeds: `...`
- Duration: `...`

# Outputs (Evidence Out) ğŸ“¦
- Where outputs live (paths under `data/processed/...`)
- Catalog pointers:
  - STAC item(s): `...`
  - DCAT dataset: `...`
  - PROV bundle: `...`

# Results ğŸ“ˆ
- Metrics, charts, qualitative examples (keep small).
- Add 1â€“3 â€œsanity checkâ€ examples.

# Uncertainty & Validation ğŸ”
- What could be wrong?
- Checks performed (spot checks, CV, error bounds, leakage checks).

# Interpretation ğŸ§ 
- What do results mean for KFM decisions?

# Decision / Next Steps ğŸ§­
- Adopt / iterate / abandon (and why).

# Reproducibility Checklist âœ…
- [ ] Parameters & configs documented
- [ ] Code committed + hash recorded
- [ ] Environment captured (Docker/lockfile)
- [ ] Seeds recorded (if applicable)
- [ ] Inputs/outputs linked via STAC/DCAT/PROV
- [ ] Another person can re-run using this doc
```

---

## ğŸƒ Run receipts

Runs are the **receipt** for an experiment: what you ran, how you ran it, and where outputs went.

### ğŸ“› Naming convention
- `RUN-YYYY-MM-DD-<slug>/`

### ğŸ“¦ Suggested run folder contents
- `config/` ğŸ§¾ â€” YAML/JSON config used for the run
- `env/` ğŸ§± â€” `pip freeze`, `conda env export`, Docker digest, OS info
- `logs/` ğŸªµ â€” structured logs (redacted if needed)
- `metrics/` ğŸ“ˆ â€” CSV/JSON metrics, evaluations
- `artifacts/` ğŸ§© â€” *small* artifacts (thumbnails, sample outputs)
- `MANIFEST.md` ğŸ§¾ â€” summary + reproduction instructions + links to evidence outputs

### ğŸ§¾ Minimal `MANIFEST.md` template (copy/paste)

```md
---
run_id: RUN-YYYY-MM-DD-<slug>
related_experiment: EXP-YYYY-MM-DD-<slug>
date: YYYY-MM-DD
owner: "@github-handle"

code:
  commit: abcdef1
  entrypoint: "src/pipelines/..."
  args: ["--config", "config/run.yml"]

environment:
  docker_image: "ghcr.io/org/project:tag@sha256:..."
  # or:
  requirements: "env/requirements.lock.txt"

randomness:
  seeds: [123, 456]
  deterministic_flags: ["torch.use_deterministic_algorithms=True"]

inputs:
  - stac: "data/stac/items/..."
  - dcat: "data/catalog/dcat/..."
  - prov: "data/prov/..."

outputs:
  - path: "data/processed/<domain>/<dataset>/..."
    stac_item: "data/stac/items/..."
    prov: "data/prov/<run-id>.jsonld"

notes: ""
---

# Summary ğŸ§¾
- What did this run do?

# Evidence outputs ğŸ“¦
- Where outputs are stored (`data/processed/...`) + catalog IDs

# How to reproduce ğŸ”
1. Checkout commit: `abcdef1`
2. Restore environment: ...
3. Run: ...
4. Validate: ...
```

> [!TIP]
> Treat run folders as **immutable receipts**.  
> New parameters â†’ new run folder âœ…

---

## ğŸ§° SOPs

SOPs turn â€œtribal knowledgeâ€ into a repeatable, reviewable procedure.  
Write an SOP whenever a task is repeated or risky (georeferencing, publishing catalogs, redaction, etc.).

### SOP template (copy/paste)

```md
---
id: SOP-<topic>-v1
title: "<clear title>"
owner: "@github-handle"
last_updated: YYYY-MM-DD
---

# Purpose ğŸ¯
What this SOP accomplishes and when to use it.

# Scope âœ…
Whatâ€™s included / excluded.

# Prerequisites ğŸ§±
Accounts, tools, access, permissions.

# Tools & Versions ğŸ§°
Software + versions.

# Procedure ğŸ§­
1. Step...
2. Step...
3. Step...

# Verification âœ…
How to confirm it worked (checks + expected outputs).

# Troubleshooting ğŸ§¯
Common failure modes + fixes.

# Audit Trail ğŸ§¾
Example PRs / runs / experiments that used this SOP.
```

### â­ High-value SOPs to add (starter set)
- `sops/georeference_map.md` ğŸ—ºï¸
- `sops/build_cog_tiles.md` ğŸ§Š
- `sops/ocr_pipeline.md` ğŸ”
- `sops/publish_stac_dcat_prov.md` ğŸ§¾ğŸ§¬
- `sops/train_or_update_model.md` ğŸ§ 

---

## ğŸ§  Model cards

Any ML/AI model used in KFM (trained or adopted) needs a model card:
- what it is
- what it was trained on / sourced from
- what it should be used for âœ…
- what it must **not** be used for ğŸš«
- known limitations, bias risks, failure modes âš ï¸

### Model card template (copy/paste)

```md
---
model_id: MODEL-<name>-v<semver>
owner: "@github-handle"
date: YYYY-MM-DD
source:
  type: trained | third_party
  license: "..."
  reference: "paper/link/registry id"
---

# Model overview ğŸ§ 
- What problem does it solve?

# Intended use âœ…
- Supported use-cases.

# Out-of-scope / prohibited use ğŸš«
- What it must not be used for.

# Training data ğŸ—ƒï¸
- Datasets used (STAC/DCAT refs), sampling, labeling notes.

# Evaluation ğŸ“ˆ
- Metrics, test sets, qualitative examples.

# Limitations & biases âš ï¸
- Known failure modes, bias risks, uncertainty notes.

# Reproducibility ğŸ§ª
- Training code commit hash
- Environment / hardware notes
- Hyperparameters / config
- Seeds
```

---

## ğŸ““ Notebooks

Notebooks are welcomeâ€”**but must be readable and reproducible**:

- Start with a markdown cell: **purpose + inputs + outputs**
- Keep outputs small *(no huge embedded blobs)*
- Prefer parameterized notebooks or export to scripts when it becomes â€œrealâ€
- If a notebook produces evidence artifacts:
  - store outputs in `data/processed/...`
  - link them from an experiment report + run receipt

> [!CAUTION]
> Notebooks that silently write files without catalogs + provenance are **not shippable**.

---

## ğŸ”— MCP â†” KFM evidence pipeline

KFM uses a strict evidence pipeline:

**ETL â†’ Catalogs (STAC/DCAT/PROV) â†’ Graph â†’ APIs â†’ UI â†’ Story Nodes â†’ Focus Mode**

So for MCP work:

- âœ… Experiment reports live here: `mcp/experiments/...`
- âœ… Run receipts live here: `mcp/runs/...`
- âœ… Evidence artifacts live here: `data/processed/...`
- âœ… Evidence artifacts must be:
  - cataloged (STAC/DCAT) ğŸ§¾
  - lineage-linked (PROV) ğŸ§¬
  - integrated through governed contracts (API boundary) ğŸ”’

```mermaid
flowchart LR
  EXP["ğŸ§ª EXP report"] --> RUN["ğŸƒ RUN receipt"]
  RUN --> OUT["ğŸ“¦ data/processed outputs"]
  OUT --> CAT["ğŸ—‚ï¸ STAC/DCAT/PROV"]
  CAT --> GR["ğŸ•¸ï¸ Graph"]
  GR --> API["ğŸ›¡ï¸ APIs"]
  API --> UI["ğŸ—ºï¸ UI / Story / Focus"]
```

---

## ğŸ§© Domain checklists

Use the checklist that matches your work:

### ğŸ—ºï¸ GIS / Remote Sensing
- [ ] CRS documented (EPSG + axis order)
- [ ] Georeferencing method + control points documented
- [ ] Fit error/RMS recorded (if applicable)
- [ ] Raster outputs are COGs / tiled (with parameters)
- [ ] Vector outputs validate (geometry validity, topology as needed)

### ğŸ” OCR / NLP
- [ ] Input corpus + sampling documented
- [ ] Labeling rules / evaluation rubric included
- [ ] Precision/recall (or spot-check protocol) documented
- [ ] Failure classes logged (scan quality, fonts, ambiguity)

### ğŸ“Š Statistics / Inference
- [ ] Outcomes + units defined
- [ ] Assumptions checked (independence, distribution, etc.)
- [ ] Effect sizes reported (not just p-values)
- [ ] Multiple comparisons handled (or explicitly scoped)

### ğŸ›°ï¸ Modeling & Simulation
- [ ] Assumptions enumerated explicitly
- [ ] Validation approach documented (comparisons, backtests)
- [ ] Uncertainty quantified or bounded
- [ ] Results reported with uncertainty context

---

## ğŸ” Safety & hygiene

- ğŸš« Donâ€™t store secrets, tokens, keys, or sensitive PII in `mcp/`
- ğŸ§½ Redact logs before committing if they contain identifiers, endpoints, or sensitive paths
- ğŸ§Š Prefer immutable receipts: new run folder > editing old run folder
- ğŸ§­ If superseded, mark as `superseded` and link the replacement

---

## ğŸ¤ PR / review checklist

When your PR includes experiments, runs, or evidence:

- [ ] EXP report added/updated (`mcp/experiments/...`)
- [ ] RUN receipt folder added (`mcp/runs/...`) with `MANIFEST.md`
- [ ] Evidence outputs stored under `data/processed/...`
- [ ] STAC/DCAT/PROV pointers added (IDs or paths)
- [ ] AI involvement labeled (if applicable)
- [ ] Reproduction steps included (1â€“4 steps; copy/paste runnable)
- [ ] No secrets / no sensitive leaks in logs or outputs
- [ ] Reviewer can reproduce within ~30 minutes

> [!TIP]
> A good review comment is: **â€œI reproduced this and got the same outputs.â€** âœ…

---

## ğŸ•°ï¸ Version history

| Version | Date | Summary | Author |
|---:|---|---|---|
| v1.1.0 | 2026-01-06 | Clarified nonâ€‘negotiables + pipeline linkage; added run receipt template, PR checklist, and workflow diagram. | KFM Engineering |
| v1.0.0 | 2025-12-31 | Initial MCP README: experiments, runs, SOPs, model cards, notebooks, safety rules. | KFM Engineering |

---

ğŸ§­ **Goal:** Make every output auditable and every method teachable.  
ğŸ§¾ **Promise:** If itâ€™s in production, it has a paper trail. âœ…
