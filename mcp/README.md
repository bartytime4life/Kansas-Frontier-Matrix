<!--
ğŸ“Œ MCP is KFMâ€™s canonical â€œmethods + receiptsâ€ boundary (not a data store, not runtime code).
ğŸ—“ï¸ Last updated: 2026-01-09
-->

# ğŸ§ª MCP â€” Master Coder Protocol  
### *Methods, Controls & Processes* ğŸ§¾âš™ï¸

![README](https://img.shields.io/badge/README-v1.2.0-8957e5)
![Docs-first](https://img.shields.io/badge/docs-documentation--first-blue)
![Reproducible](https://img.shields.io/badge/reproducible-audit--ready-success)
![Evidence](https://img.shields.io/badge/evidence-traceable-informational)
![Governance](https://img.shields.io/badge/governance-FAIR%2BCARE%20%2B%20Sovereignty-2ea043)
![Security](https://img.shields.io/badge/security-hostile--inputs%20%2B%20deny--by--default-red)
![KFM](https://img.shields.io/badge/KFM-Kansas%20Frontier%20Matrix-orange)

> **TL;DR:** `mcp/` is KFMâ€™s **lab notebook + playbook** ğŸ““ğŸ§   
> It holds **experiment protocols**, **run receipts**, **SOPs**, **model cards**, and **review checklists**â€”so every result can be **re-run, reviewed, and trusted** âœ…  
>  
> **MCP** is also shorthand for **Methods, Controls & Processes** in some internal docs.  
> In *this repo*, the canonical expansion remains **Master Coder Protocol** âœ… (same intent, same discipline).

> [!IMPORTANT]
> In this repo, **MCP = Master Coder Protocol** âœ…  
> **MCP â‰  Model Context Protocol** ğŸš« *(not what we mean here)*  
> Keep this distinction consistent in docs, PRs, issues, and commit messages.

---

## ğŸ”— Quick links
- ğŸ§­ Repo overview: **[`../README.md`](../README.md)**
- ğŸ§© Executable source boundary: **[`../src/README.md`](../src/README.md)**
- ğŸ“¦ Data + metadata boundary: **[`../data/README.md`](../data/README.md)**
- ğŸ“š Governed documentation boundary: **[`../docs/README.md`](../docs/README.md)** *(if present)*
- ğŸ¤ Collaboration & automation: **[`../.github/README.md`](../.github/README.md)** *(if present)*

---

## âš¡ Quick Nav
- [ğŸ§¾ Doc metadata](#-doc-metadata)
- [ğŸ§­ What MCP is](#-what-mcp-is)
- [ğŸš¦ Nonâ€‘negotiables](#-non-negotiables)
- [ğŸ Quick Start](#-quick-start)
- [ğŸ—‚ï¸ Directory layout](#ï¸-directory-layout)
- [ğŸ” The MCP workflow loop](#-the-mcp-workflow-loop)
- [âœ… Definition of done](#-definition-of-done)
- [ğŸ“¦ Required artifacts](#-required-artifacts)
- [ğŸ§ª Experiment reports](#-experiment-reports)
- [ğŸƒ Run receipts](#-run-receipts)
- [ğŸ§° SOPs](#-sops)
- [ğŸ§  Model cards](#-model-cards)
- [ğŸ““ Notebooks](#-notebooks)
- [ğŸ§­ Traceability matrix](#-traceability-matrix)
- [ğŸ§¯ Bad evidence protocol](#-bad-evidence-protocol)
- [ğŸ”— MCP â†” KFM evidence pipeline](#-mcp--kfm-evidence-pipeline)
- [ğŸ§© Domain checklists](#-domain-checklists)
- [ğŸ” Safety, privacy, licensing](#-safety-privacy-licensing)
- [âœ… QA, audits, and CI hooks](#-qa-audits-and-ci-hooks)
- [ğŸ¤ PR / review checklist](#-pr--review-checklist)
- [ğŸ“š Project reference library influence map](#-project-reference-library-influence-map)
- [ğŸ•°ï¸ Version history](#ï¸-version-history)

---

## ğŸ§¾ Doc metadata

| Field | Value |
|---|---|
| Doc | `mcp/README.md` |
| Status | Active âœ… |
| Last updated | **2026-01-09** |
| Audience | Contributors writing experiments, running jobs, training models, shipping evidence artifacts |
| Prime directive | If it changes â€œspatial truth,â€ it must be **traceable + reproducible + reviewable** |

---

## ğŸ§­ What MCP is

### âœ… MCP isâ€¦
A **governed method layer** that turns â€œwe tried somethingâ€ into **auditable science**:

- ğŸ§ª **Protocols** (what we intended to do + why)
- ğŸƒ **Receipts** (what we actually ran + how)
- ğŸ§° **SOPs** (repeatable procedures for risky/repeated work)
- ğŸ§  **Model cards** (responsible AI/ML usage)
- ğŸ§¾ **Review artifacts** (what was checked, by whom, and what failed)

### ğŸš« MCP is notâ€¦
- âŒ A data lake (thatâ€™s `data/`)
- âŒ A code dump (thatâ€™s `src/` and `web/`)
- âŒ A place for large outputs (store in `data/processed/**` + catalogs)
- âŒ A place for â€œunsourced narrativeâ€ (that belongs in Story Nodes with explicit evidence links)

---

## ğŸš¦ Nonâ€‘negotiables

These rules keep KFM **scientific**, **auditable**, and **governed**:

1. **Evidence lives in `data/` â€” not in `mcp/`.** ğŸ“¦  
   - `mcp/` = methods, decisions, receipts, checklists  
   - `data/processed/...` = outputs (**and they must be cataloged**)

2. **Protocol before run.** ğŸ§¾â¡ï¸ğŸƒ  
   If results could influence product decisions, public narratives, or model outputs: write an **EXP** first.

3. **No â€œmagic results.â€** ğŸª„ğŸš«  
   If you canâ€™t reproduce it using:
   - a commit hash
   - an environment snapshot
   - a config
   - linked inputs/outputs (catalog IDs)
   â€¦then itâ€™s not â€œdone.â€

4. **Immutable receipts.** ğŸ§¾  
   Donâ€™t edit a run receipt to â€œfix history.â€  
   Make a **new** run folder if anything changes.

5. **Label AI involvement.** ğŸ¤–  
   Any AI-assisted outputs must be labeled and provenance-linked.  
   (Focus Mode is **advisory-only** and must stay evidence-backed.)

6. **KFM pipeline order is sacred.** ğŸ§±  
   **ETL â†’ Catalogs (STAC/DCAT/PROV) â†’ Graph â†’ APIs â†’ UI â†’ Story Nodes â†’ Focus Mode**

7. **No privacy / sensitivity downgrade.** ğŸ”’  
   Outputs cannot be less restricted than inputs without an explicit, reviewed redaction step.

8. **Licensing isnâ€™t optional.** ğŸ§¾âš–ï¸  
   Every dataset / artifact must carry license + attribution requirements through catalogs and narratives.

> [!TIP]
> Motto: **â€œIf I canâ€™t reproduce it in 30 minutes, itâ€™s not complete.â€** â±ï¸âœ…

---

## ğŸ Quick Start

### 1) Create an experiment report ğŸ§ª
Add:
- `mcp/experiments/EXP-YYYY-MM-DD-<slug>.md`

### 2) Create a run receipt ğŸƒ
Add:
- `mcp/runs/RUN-YYYY-MM-DD-<slug>/`
  - config, env snapshot, logs, metrics, and `MANIFEST.md`

### 3) Store evidence outputs in the governed data layer ğŸ“¦
Put artifacts in:
- `data/processed/...` âœ…  
Then catalog them:
- **STAC/DCAT/PROV** ğŸ—‚ï¸ğŸ§¬

### 4) Link it to decisions ğŸ§­
Update traceability (recommended):
- `mcp/traceability/TRACEABILITY.md` *(or equivalent)*

> [!IMPORTANT]
> `mcp/` should stay **lightweight** and human-readable.  
> Large artifacts go to `data/processed/` (or object storage) and get catalog records.

---

## ğŸ—‚ï¸ Directory layout

```text
ğŸ“ mcp/
â”œâ”€â”€ ğŸ“„ README.md                 # you are here ğŸ‘‹
â”œâ”€â”€ ğŸ“ experiments/              # human-readable experiment reports ğŸ§ªğŸ§¾
â”œâ”€â”€ ğŸ“ runs/                     # immutable run receipts (configs, env, logs, metrics) ğŸƒğŸ§¾
â”œâ”€â”€ ğŸ“ sops/                     # Standard Operating Procedures (repeatable tasks) ğŸ§°
â”œâ”€â”€ ğŸ“ model_cards/              # model cards for any ML/AI used or trained ğŸ§ 
â”œâ”€â”€ ğŸ“ notebooks/                # tidy, reproducible notebooks ğŸ““
â”œâ”€â”€ ğŸ“ traceability/             # decision â†” evidence mapping ğŸ§­ (recommended)
â”œâ”€â”€ ğŸ“ reviews/                  # peer reproduction notes / review sign-offs ğŸ‘€ (recommended)
â””â”€â”€ ğŸ“ templates/                # optional local templates ğŸ§© (or use docs/templates/)
```

> [!NOTE]
> Repo implementations vary. If `traceability/` or `reviews/` doesnâ€™t exist yet, consider adding themâ€”  
> the design docs explicitly call out traceability + modular documentation as a core MCP promise. âœ…

---

## ğŸ” The MCP workflow loop

KFM work is **question â†’ protocol â†’ run â†’ evidence â†’ report â†’ review â†’ iterate**:

```mermaid
flowchart LR
  Q["â“ Question"] --> P["ğŸ§¾ Protocol (EXP)"]
  P --> R["ğŸƒ Run (RUN receipt)"]
  R --> E["ğŸ“¦ Evidence (data/processed + catalogs)"]
  E --> S["ğŸ§ª Summary report (interpretation + limits)"]
  S --> V["ğŸ‘€ Review (repro + governance)"]
  V --> Q
```

### ğŸ”¬ Scientific method alignment (what we document)
MCP is a practical â€œscientific method adapterâ€ for software + data work:

- **Observation / question** â†’ Why are we doing this?
- **Hypothesis** â†’ What do we expect to see?
- **Method** â†’ Exact procedure + configuration
- **Experiment** â†’ The run receipt (what happened)
- **Analysis** â†’ Metrics, plots, error checks
- **Conclusion** â†’ What we learned (with limits)
- **Iteration** â†’ Next experiment / pipeline change

---

## âœ… Definition of done

### âœ… MCP â€œdoneâ€ means: reproducible + governed
For any EXP/RUN that influences production pipelines, APIs, UI layers, Story Nodes, or Focus Mode:

- [ ] Front-matter complete + consistent (IDs, dates, owner, status)
- [ ] Claims link to evidence inputs/outputs (catalog pointers)
- [ ] Validation steps are listed and repeatable
- [ ] Governance + FAIR/CARE + sovereignty considerations stated (when applicable)
- [ ] Another contributor can reproduce results without tribal knowledge

### ğŸ§± Reproducibility levels (recommended)
- **L0** ğŸŸ¡: exploratory note (not decision-worthy)
- **L1** ğŸŸ : reproducible by author (config + env captured)
- **L2** ğŸŸ¢: reproducible by reviewer (independent re-run)
- **L3** ğŸ†: CI-backed / automated rerun (pipeline job + validators)

---

## ğŸ“¦ Required artifacts

### âœ… â€œReal workâ€ minimum bar
If an experiment influences decisions, pipelines, or published results, it must include:

- ğŸ§ª **Experiment report** â†’ `mcp/experiments/...`
- ğŸƒ **Run receipt** â†’ `mcp/runs/...`
- ğŸ”— **Code pointer** â†’ commit hash + entrypoint
- ğŸ§± **Environment snapshot** â†’ Docker image digest **or** lockfile/requirements
- ğŸ² **Seeds / determinism flags** (where applicable)
- ğŸ“¦ **Outputs stored as evidence** â†’ `data/processed/...`
- ğŸ—‚ï¸ **Catalog records** â†’ STAC/DCAT
- ğŸ§¬ **Lineage** â†’ PROV pointers (inputs + outputs)
- ğŸ‘€ **Review notes** â†’ reproducibility sign-off for L2/L3 work (recommended)

> [!WARNING]
> Avoid committing large binaries to `mcp/`.  
> If itâ€™s an â€œoutput,â€ it probably belongs in `data/processed/` with catalogs + lineage.

---

## ğŸ§ª Experiment reports

### ğŸ“› Naming convention
Use one pattern consistently:

- `EXP-YYYY-MM-DD-<short-slug>.md`  
  Example: `EXP-2026-01-02-ocr-ner-baseline.md`

### ğŸ·ï¸ Status values
- `draft` ğŸ“ â€” in progress
- `complete` âœ… â€” reproducible; linked receipts + evidence
- `superseded` ğŸ§¯ â€” replaced by a newer experiment

### ğŸ§¾ Experiment template (copy/paste)

```md
---
id: EXP-YYYY-MM-DD-<slug>
title: "<short, explicit title>"
date: YYYY-MM-DD
owner: "@github-handle"
status: draft | complete | superseded
repro_level: L0 | L1 | L2 | L3
risk_level: low | medium | high
ai_used: true | false
supersedes: []          # optional: [EXP-...]
superseded_by: []       # optional: [EXP-...]
tags: [gis, ocr, nlp, stac, dcat, prov, graph, sim, stats, web, security]
---

# Objective / Question â“
- What are we trying to learn or improve?

# Background / Prior Art ğŸ“š
- Links to prior experiments, issues, papers, notes, or domain docs.

# Hypothesis âœ…/âŒ
- What do we expect and why?

# Inputs (Evidence In) ğŸ—ƒï¸
- Dataset IDs + STAC/DCAT pointers.
- Sampling rules, inclusion/exclusion, time range, bbox.
- Licensing + sensitivity notes (if applicable).

# Method / Protocol ğŸ§¾
- Step-by-step procedure.
- Parameters + configs (link to run receipt config).
- Tools + versions (OS/GPU/driver notes if relevant).

# Run Receipt ğŸƒ
- Code commit: `abcdef1`
- Entrypoint: `src/...` or notebook path
- Run folder: `mcp/runs/RUN-YYYY-MM-DD-.../`
- Seeds: `...`
- Determinism flags: `...`

# Outputs (Evidence Out) ğŸ“¦
- Where outputs live (paths under `data/processed/...`)
- Catalog pointers:
  - STAC item(s): `...`
  - DCAT dataset: `...`
  - PROV bundle: `...`

# Results ğŸ“ˆ
- Metrics, charts, qualitative examples (keep small).
- Add 1â€“3 â€œsanity checkâ€ examples.

# Uncertainty, Bias, and Validation ğŸ”
- What could be wrong?
- Checks performed (spot checks, CV, error bounds, leakage checks).
- Bias risks / perspective gaps (especially for historical corpora).

# Interpretation ğŸ§ 
- What do results mean for KFM decisions?

# Decision / Next Steps ğŸ§­
- Adopt / iterate / abandon (and why).

# Reproducibility Checklist âœ…
- [ ] Parameters & configs documented
- [ ] Code committed + hash recorded
- [ ] Environment captured (Docker/lockfile)
- [ ] Seeds recorded (if applicable)
- [ ] Inputs/outputs linked via STAC/DCAT/PROV
- [ ] Reviewer can reproduce (for L2/L3)
```

> [!TIP]
> If you canâ€™t write the â€œUncertainty, Bias, and Validationâ€ section honestly, the experiment isnâ€™t finished. ğŸ”âœ…

---

## ğŸƒ Run receipts

Runs are the **receipt** for an experiment: what you ran, how you ran it, where outputs went, and what changed.

### ğŸ“› Naming convention
- `RUN-YYYY-MM-DD-<slug>/`

### ğŸ“¦ Suggested run folder contents
- `config/` ğŸ§¾ â€” YAML/JSON config used for the run
- `env/` ğŸ§± â€” `pip freeze`, lockfiles, Docker digest, OS info
- `logs/` ğŸªµ â€” structured logs (**redacted if needed**)
- `metrics/` ğŸ“ˆ â€” CSV/JSON metrics, evaluations
- `artifacts/` ğŸ§© â€” *small* artifacts (thumbnails, sample outputs)
- `MANIFEST.md` ğŸ§¾ â€” reproduction instructions + evidence links + checks performed

### ğŸ§¾ Minimal `MANIFEST.md` template (copy/paste)

```md
---
run_id: RUN-YYYY-MM-DD-<slug>
related_experiment: EXP-YYYY-MM-DD-<slug>
date: YYYY-MM-DD
owner: "@github-handle"

code:
  commit: abcdef1
  entrypoint: "src/pipelines/..."
  args: ["--config", "config/run.yml"]

environment:
  docker_image: "ghcr.io/org/project:tag@sha256:..."
  # or:
  requirements: "env/requirements.lock.txt"
  os: "..."
  cpu: "..."
  gpu: "..."  # optional

randomness:
  seeds: [123, 456]
  deterministic_flags: ["..."]

inputs:
  - stac: "data/stac/items/..."
  - dcat: "data/catalog/dcat/..."
  - prov: "data/prov/..."

outputs:
  - path: "data/processed/<domain>/<dataset>/..."
    stac_item: "data/stac/items/..."
    dcat: "data/catalog/dcat/..."
    prov: "data/prov/<run-id>.jsonld"

validation:
  - "schema validation: pass/fail"
  - "link checks: pass/fail"
  - "spot checks: ..."

notes: ""
---

# Summary ğŸ§¾
- What did this run do?

# Evidence outputs ğŸ“¦
- Where outputs are stored (`data/processed/...`) + catalog IDs

# How to reproduce ğŸ”
1. Checkout commit: `abcdef1`
2. Restore environment: ...
3. Run: ...
4. Validate: ...
```

> [!TIP]
> Treat run folders as **immutable receipts**.  
> New parameters â†’ new run folder âœ…

---

## ğŸ§° SOPs

SOPs turn â€œtribal knowledgeâ€ into a repeatable, reviewable procedure.  
Write an SOP whenever a task is repeated or risky: georeferencing, catalog publishing, redaction, OCR, tile generation, etc. ğŸ§¯

### SOP template (copy/paste)

```md
---
id: SOP-<topic>-v1
title: "<clear title>"
owner: "@github-handle"
last_updated: YYYY-MM-DD
risk_level: low | medium | high
---

# Purpose ğŸ¯
What this SOP accomplishes and when to use it.

# Scope âœ…
Whatâ€™s included / excluded.

# Prerequisites ğŸ§±
Accounts, tools, access, permissions.

# Tools & Versions ğŸ§°
Software + versions.

# Procedure ğŸ§­
1. Step...
2. Step...
3. Step...

# Verification âœ…
How to confirm it worked (checks + expected outputs).

# Troubleshooting ğŸ§¯
Common failure modes + fixes.

# Audit Trail ğŸ§¾
Example PRs / runs / experiments that used this SOP.
```

### â­ High-value SOPs to add (starter set)
- `sops/georeference_map.md` ğŸ—ºï¸ (control points, RMS error, CRS discipline)
- `sops/build_cog_and_tiles.md` ğŸ§Š (COG params, overviews, tile scheme)
- `sops/ocr_pipeline.md` ğŸ” (scan QA, language assumptions, error classes)
- `sops/publish_stac_dcat_prov.md` ğŸ—‚ï¸ğŸ§¬ (profiles, validation, link checks)
- `sops/catalog_qa_gate.md` âœ… (how to run CI-like catalog QA locally)
- `sops/postgis_import_index.md` ğŸ˜ (schemas, indexes, vacuum/analyze)
- `sops/redaction_and_sensitive_locations.md` ğŸ” (coarsen/offset rules, approvals)
- `sops/story_node_evidence_bundle.md` ğŸ“š (evidence pack for narratives + Focus Mode)

---

## ğŸ§  Model cards

Any ML/AI model used in KFM (trained or adopted) needs a model card:

- what it is
- what it was trained on / sourced from
- what it should be used for âœ…
- what it must **not** be used for ğŸš«
- known limitations, bias risks, failure modes âš ï¸
- provenance + licensing + governance labels ğŸ§¾ğŸ”’

### Model card template (copy/paste)

```md
---
model_id: MODEL-<name>-v<semver>
owner: "@github-handle"
date: YYYY-MM-DD
ai_used: true
source:
  type: trained | third_party
  license: "..."
  reference: "paper/link/registry id"
governance:
  sensitivity: public | restricted | confidential
  human_in_the_loop: required | recommended | optional
---

# Model overview ğŸ§ 
- What problem does it solve?

# Intended use âœ…
- Supported use-cases.

# Out-of-scope / prohibited use ğŸš«
- What it must not be used for.

# Training data ğŸ—ƒï¸
- Datasets used (STAC/DCAT pointers), sampling, labeling notes.
- Known gaps / perspective bias notes.

# Evaluation ğŸ“ˆ
- Metrics, test sets, qualitative examples.
- Calibration / uncertainty notes when applicable.

# Limitations & biases âš ï¸
- Known failure modes, bias risks, uncertainty notes.

# Governance & safety ğŸ”
- Any redaction rules or sensitivity constraints.
- How outputs are labeled in UI / Focus Mode.

# Reproducibility ğŸ§ª
- Training code commit hash
- Environment / hardware notes
- Hyperparameters / config
- Seeds
- Artifact pointers (weights, charts) stored under `data/processed/...` with catalogs
```

---

## ğŸ““ Notebooks

Notebooks are welcomeâ€”**but must be readable and reproducible**:

- Start with a markdown cell: **purpose + inputs + outputs**
- Keep outputs small *(no huge embedded blobs)*
- Prefer parameterized notebooks or export to scripts when it becomes â€œrealâ€
- If a notebook produces evidence artifacts:
  - store outputs in `data/processed/...`
  - link them from an experiment report + run receipt

> [!CAUTION]
> Notebooks that silently write files without catalogs + provenance are **not shippable**.

---

## ğŸ§­ Traceability matrix

Traceability is how MCP connects â€œworkâ€ to â€œwhy it mattersâ€:

- â“ Question / requirement  
- ğŸ§ª EXP protocol  
- ğŸƒ RUN receipt  
- ğŸ“¦ Evidence artifacts (+ STAC/DCAT/PROV)  
- ğŸ•¸ï¸ Graph IDs (if applicable)  
- ğŸ›¡ï¸ API endpoints (if applicable)  
- ğŸ“š Story Node(s) / Focus Mode (if applicable)

### âœ… Recommended traceability table (copy/paste)

```md
| Decision / Feature | EXP | RUN | Evidence outputs (data/processed) | Catalog pointers (STAC/DCAT/PROV) | Reviewer repro | Notes |
|---|---|---|---|---|---|---|
| "OCR treaties baseline for Land Treaties domain" | EXP-2026-01-02-... | RUN-2026-01-02-... | data/processed/historical/land-treaties/ocr/... | STAC: ... / DCAT: ... / PROV: ... | @reviewer âœ… | error classes logged |
```

> [!TIP]
> If a Story Node makes a claim, traceability must point to the evidence artifacts that support it. ğŸ“šğŸ§¾

---

## ğŸ§¯ Bad evidence protocol

KFM must be resilient to â€œbad evidenceâ€ (messy scans, biased corpora, incomplete sensor data, uncertain geocoding).

When evidence is questionable, MCP requires **restraint**:

1. **Data pruning** âœ‚ï¸  
   Exclude known-bad inputs (or flag them as â€œquarantinedâ€ until fixed).

2. **Inferential restraint** ğŸ§ â¬‡ï¸  
   Reduce the strength/scope of conclusions; report uncertainty explicitly.

3. **Executional restraint** ğŸ›‘  
   Limit downstream actions: donâ€™t ship to UI/Story/Focus until reviewed, or serve only aggregated/redacted views.

### âœ… Minimum â€œbad evidenceâ€ documentation
- Whatâ€™s wrong?
- How do we know?
- What we changed (or refused to change)
- What remains uncertain
- Who reviewed the restraint decision

---

## ğŸ”— MCP â†” KFM evidence pipeline

KFM uses a strict evidence pipeline:

**ETL â†’ Catalogs (STAC/DCAT/PROV) â†’ Graph â†’ APIs â†’ UI â†’ Story Nodes â†’ Focus Mode**

So for MCP work:

- âœ… Protocols live here: `mcp/experiments/...`
- âœ… Receipts live here: `mcp/runs/...`
- âœ… Evidence artifacts live here: `data/processed/...`
- âœ… Evidence artifacts must be:
  - cataloged (STAC/DCAT) ğŸ—‚ï¸
  - lineage-linked (PROV) ğŸ§¬
  - integrated through governed contracts (API boundary) ğŸ”’

```mermaid
flowchart LR
  EXP["ğŸ§ª EXP report"] --> RUN["ğŸƒ RUN receipt"]
  RUN --> OUT["ğŸ“¦ data/processed outputs"]
  OUT --> CAT["ğŸ—‚ï¸ STAC/DCAT/PROV"]
  CAT --> GR["ğŸ•¸ï¸ Graph"]
  GR --> API["ğŸ›¡ï¸ APIs"]
  API --> UI["ğŸ—ºï¸ UI / Story / Focus"]
```

---

## ğŸ§© Domain checklists

Use the checklist that matches your work:

### ğŸ—ºï¸ GIS / Remote Sensing
- [ ] CRS documented (EPSG + axis order)
- [ ] Georeferencing method + control points documented
- [ ] Fit error/RMS recorded (if applicable)
- [ ] Raster outputs are COGs / tiled (with parameters)
- [ ] Vector outputs validate (geometry validity, topology as needed)
- [ ] Symbology/aggregation choices documented if they change interpretation ğŸ¨

### ğŸ” OCR / NLP
- [ ] Input corpus + sampling documented
- [ ] Labeling rules / evaluation rubric included
- [ ] Precision/recall (or spot-check protocol) documented
- [ ] Failure classes logged (scan quality, fonts, ambiguity)
- [ ] Geoparsing uncertainty documented (ambiguous place names, gazetteer limits)

### ğŸ•¸ï¸ Graph analytics
- [ ] Graph schema/ontology version noted
- [ ] Metrics treated as **signals**, not facts (avoid over-interpretation)
- [ ] Provenance links from derived relations to source evidence
- [ ] No orphan IDs / referential integrity checks pass âœ…

### ğŸ“Š Statistics / Inference
- [ ] Outcomes + units defined
- [ ] Assumptions checked (independence, distribution, etc.)
- [ ] Effect sizes reported (not just p-values)
- [ ] Multiple comparisons handled (or explicitly scoped)
- [ ] Guardrails against optional stopping / publication bias documented ğŸ§¯

### ğŸ›°ï¸ Modeling & Simulation
- [ ] Assumptions enumerated explicitly
- [ ] Verification & validation approach documented (V&V mindset)
- [ ] Sensitivity analysis for key parameters
- [ ] Uncertainty quantified or bounded
- [ ] Results reported with uncertainty context (not single â€œtruthâ€ numbers)

### ğŸŒ Web UI / Visualization (when experiments affect front-end behavior)
- [ ] Payload budgets considered (tiles, vector sizes, images)
- [ ] Offline/low-bandwidth considerations documented (if relevant) ğŸ“±
- [ ] Accessibility and audit logging expectations noted â™¿ï¸ğŸ§¾

---

## ğŸ” Safety, privacy, licensing

- ğŸš« Donâ€™t store secrets, tokens, keys, or sensitive PII in `mcp/`
- ğŸ§½ Redact logs before committing if they contain identifiers, endpoints, or sensitive paths
- ğŸ§Š Prefer immutable receipts: new run folder > editing old run folder
- ğŸ§­ If superseded, mark as `superseded` and link the replacement
- ğŸ—ºï¸ Sensitive locations: if a dataset could expose culturally sensitive or personal location data:
  - coarsen/offset/omit coordinates
  - require explicit permission & review before publishing
  - propagate sensitivity tags through catalogs and UI

> [!IMPORTANT]
> Licensing must travel with evidence. If you combine layers, the resulting artifact must still honor attribution and license constraints. âš–ï¸ğŸ§¾

---

## âœ… QA, audits, and CI hooks

### CI intent (minimum bar)
- ğŸ§¹ lint + formatting
- âœ… unit tests (where applicable)
- ğŸ§¾ schema validation (STAC/DCAT/PROV)
- ğŸ”— link checks (assets exist; IDs resolve)
- ğŸ” security scans (secrets; common foot-guns)
- ğŸ§· governance checks (classification propagation; redaction regressions)

### Periodic audits (recommended)
- Quarterly: sample 3 completed EXP/RUN pairs â†’ verify re-run works end-to-end
- Before release: audit â€œhigh-impactâ€ artifacts (models, major new datasets, story bundles)
- After incidents: add an SOP + regression checks

---

## ğŸ¤ PR / review checklist

When your PR includes experiments, runs, or evidence:

- [ ] EXP report added/updated (`mcp/experiments/...`)
- [ ] RUN receipt folder added (`mcp/runs/...`) with `MANIFEST.md`
- [ ] Evidence outputs stored under `data/processed/...`
- [ ] STAC/DCAT/PROV pointers added (IDs or paths)
- [ ] AI involvement labeled (if applicable)
- [ ] Reproduction steps included (1â€“4 steps; copy/paste runnable)
- [ ] No secrets / no sensitive leaks in logs or outputs
- [ ] Reviewer can reproduce (required for L2/L3 work)

> [!TIP]
> A great review comment is: **â€œI reproduced this and got the same outputs.â€** âœ…

---

## ğŸ“š Project reference library influence map

> [!NOTE]
> These project files inform *how we design and review* MCP artifacts: reproducibility, governance, security, modeling rigor, statistical discipline, scaling, and visualization constraints.

<details>
<summary><strong>ğŸ“¦ Expand: Reference library â†’ what it influences in <code>mcp/</code></strong></summary>

| Project file | Primary lens | How it upgrades MCP |
|---|---|---|
| `Scientific Method _ Research _ Master Coder Protocol Documentation.pdf` | ğŸ”¬ Scientific method + documentation system | Reinforces protocol-first workflow, domain modules, traceability matrices, and â€œliving documentationâ€ discipline. |
| `Kansas Frontier Matrix (KFM) â€“ Comprehensive Technical Documentation.docx` | ğŸ§­ System blueprint | Aligns MCP with KFMâ€™s pipeline order, governance, QA gates, Story Nodes, and Focus Mode evidence rules. |
| `Kansas-Frontier-Matrix_ Open-Source Geospatial Historical Mapping Hub Design.pdf` | ğŸ—ï¸ Platform design | Clarifies end-to-end architecture (ingest â†’ catalogs â†’ AI/analysis â†’ UI), and why provenance must be first-class. |
| `Kansas-Frontier-Matrix Design Audit â€“ Gaps and Enhancement Opportunities.pdf` | ğŸ§¯ Reality check | Highlights where MCP must be operational (actual SOPs, glossary, model cards, experiment logsâ€”no â€œpaper MCPâ€). |
| `MARKDOWN_GUIDE_v13.md.gdoc` | ğŸ“˜ Repo-level invariants | Defines evidence-first + contract-first doctrine, Story Node/Focus constraints, and definition-of-done patterns for governed docs. |
| `Scientific Modeling and Simulation_ A Comprehensive NASA-Grade Guide.pdf` | ğŸ§ª V&V discipline | Shapes simulation experiment logging, V&V framing, uncertainty, sensitivity analysis, and documentation rigor. |
| `Understanding Statistics & Experimental Design.pdf` | ğŸ“Š Rigor + bias | Encourages guarding against optional stopping/publication bias and documenting assumptions + effect sizes. |
| `regression-analysis-with-python.pdf` + `Regression analysis using Python - slides-linear-regression.pdf` | ğŸ“ˆ Baselines + diagnostics | Improves reproducible modeling baselines and diagnostic reporting in EXP/RUN artifacts. |
| `think-bayes-bayesian-statistics-in-python.pdf` | ğŸ² Uncertainty | Encourages explicit priors, posterior uncertainty reporting, and calibrated decisions under uncertainty. |
| `graphical-data-analysis-with-r.pdf` | ğŸ“‰ EDA instincts | Reinforces visual sanity checks and artifact detection before publishing evidence. |
| `Cloud-Based Remote Sensing with Google Earth Engine-Fundamentals and Applications.pdf` | ğŸ›°ï¸ EO workflows | Informs remote sensing SOPs (export patterns, time-series handling) and treating derived indices as evidence artifacts. |
| `python-geospatial-analysis-cookbook.pdf` | ğŸ—ºï¸ GIS engineering | Guides CRS hygiene, vector/raster IO, PostGIS integration, and safe geoprocessing SOPs. |
| `making-maps-a-visual-guide-to-map-design-for-gis.pdf` | ğŸ¨ Cartography ethics | Reminds that visualization choices shape meaning; demands documentation of map design decisions. |
| `Mobile Mapping_ Space, Cartography and the Digital - 9789048535217.pdf` | ğŸ“± Mobile/offline constraints | Encourages tiling, caching, and offline-aware documentation for downstream UX and performance. |
| `responsive-web-design-with-html5-and-css3.pdf` | ğŸŒ Real-device constraints | Pushes MCP to capture payload/latency constraints and test on realistic device assumptions. |
| `webgl-programming-guide-interactive-3d-graphics-programming-with-webgl.pdf` | ğŸ§Š GPU/3D | Motivates explicit coordinate conventions, LOD/tiling decisions, and 3D evidence display constraints. |
| `compressed-image-file-formats-jpeg-png-gif-xbm-bmp.pdf` | ğŸ–¼ï¸ Image pipelines | Shapes SOPs for thumbnails, compression, and safe handling of complex formats. |
| `PostgreSQL Notes for Professionals - PostgreSQLNotesForProfessionals.pdf` | ğŸ˜ Data store discipline | Informs SOPs around schemas, indexing, migrations, and reproducible data loading. |
| `Scalable Data Management for Future Hardware.pdf` | âš™ï¸ Performance + streaming | Encourages documenting performance experiments, concurrency assumptions, and future streaming ingestion patterns. |
| `Data Spaces.pdf` | ğŸ”— Interop & federation | Supports catalog-as-interface thinking and future multi-region/federated evidence workflows. |
| `Spectral Geometry of Graphs.pdf` | ğŸ•¸ï¸ Graph theory | Encourages careful interpretation of graph metrics and provenance for derived relations. |
| `Generalized Topology Optimization for Structural Design.pdf` | ğŸ§® Optimization workflows | Suggests structuring optimization experiments as reproducible jobs with clear objectives/constraints. |
| `Principles of Biological Autonomy - book_9780262381833.pdf` | ğŸ§  Systems thinking | Promotes feedback-loop awareness and stability thinking when documenting pipelines and governance. |
| `Introduction to Digital Humanism.pdf` | â¤ï¸ Human-centered ethics | Reinforces transparency, accountability, and dignity in governance + AI documentation. |
| `On the path to AI Lawâ€™s prophecies...pdf` | âš–ï¸ AI governance + restraint | Informs bad-evidence handling (data pruning + inferential/executional restraint) and AI output labeling. |
| `Gray Hat Python...pdf` + `ethical-hacking-and-countermeasures...pdf` | ğŸ›¡ï¸ Security mindset | Guides hostile-input posture, threat modeling, and defensive review of parsers/pipelines/services. |
| `concurrent-real-time-and-distributed-programming-in-java...pdf` | ğŸ§µ Concurrency | Encourages careful worker/job design, race-condition awareness, and reproducible concurrency tests. |
| Programming bundles (`A...pdf`, `B-C...pdf`, `D-E...pdf`, `F-H...pdf`, `I-L...pdf`, `M-N...pdf`, `O-R...pdf`, `S-T...pdf`, `U-X...pdf`) | ğŸ§° Polyglot reference | Supports language/tooling best practices while keeping KFMâ€™s boundary invariants intact. |

</details>

---

## ğŸ•°ï¸ Version history

| Version | Date | Summary | Author |
|---:|---|---|---|
| v1.2.0 | 2026-01-09 | Upgraded MCP to align with v13 evidence-first/contract-first doctrine: added definition-of-done, reproducibility levels, traceability matrix, bad-evidence restraint protocol, expanded governance/licensing/sensitive-location guidance, and an updated reference-library influence map. | KFM Engineering |
| v1.1.0 | 2026-01-06 | Clarified nonâ€‘negotiables + pipeline linkage; added run receipt template, PR checklist, and workflow diagram. | KFM Engineering |
| v1.0.0 | 2025-12-31 | Initial MCP README: experiments, runs, SOPs, model cards, notebooks, safety rules. | KFM Engineering |

---

ğŸ§­ **Goal:** Make every output auditable and every method teachable.  
ğŸ§¾ **Promise:** If itâ€™s in production, it has a paper trail. âœ…
