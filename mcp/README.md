# ğŸ§ª MCP â€” Master Coder Protocol  
### *Methods & Computational Experiments* ğŸ§¾âš™ï¸

![Docs-first](https://img.shields.io/badge/docs-documentation--first-blue)
![Reproducible](https://img.shields.io/badge/reproducible-audit--ready-success)
![Evidence](https://img.shields.io/badge/evidence-traceable-informational)
![KFM](https://img.shields.io/badge/KFM-Kansas%20Frontier%20Matrix-orange)

> **TL;DR:** `mcp/` is the projectâ€™s **lab notebook + playbook** ğŸ““ğŸ§   
> It holds experiment reports, run receipts, SOPs, notebooks, and model cardsâ€”so every result can be **re-run, reviewed, and trusted** âœ…

> [!IMPORTANT]
> In this repo, **MCP = Master Coder Protocol** (not â€œModel Context Protocolâ€).  
> Keep this distinction consistent in docs, PRs, and issues.

---

## ğŸ§­ Quick Start (do this when you run anything â€œrealâ€)

**Create an experiment report** ğŸ§ª  
- Add: `mcp/experiments/EXP-YYYY-MM-DD-<slug>.md`

**Create a run receipt** ğŸƒ  
- Add: `mcp/runs/RUN-YYYY-MM-DD-<slug>/` with config, env snapshot, logs, metrics, and a `MANIFEST.md`

**Store evidence outputs elsewhere** ğŸ“¦  
- Put artifacts in: `data/processed/...` (not in `mcp/`)
- Catalog + provenance: **STAC/DCAT/PROV** ğŸ§¾ğŸ§¬

> **Bar:** â€œIf I canâ€™t reproduce it in 30 minutes, itâ€™s not done.â€ â±ï¸

---

## ğŸ¯ Why this folder exists

Kansas Frontier Matrix (KFM) is built to be:

- **Evidence-first** ğŸ§¾
- **Reproducible** ğŸ”
- **Audit-ready** âœ…
- **Human-reviewable** ğŸ‘€

`mcp/` keeps the â€œhow we did itâ€ artifacts in one place:

- ğŸ§ª **Experiments:** what we tried + why + what happened  
- ğŸƒ **Runs:** concrete run metadata/artifacts (configs, seeds, logs, metrics)  
- ğŸ§° **SOPs:** repeatable procedures for recurring tasks  
- ğŸ§  **Model Cards:** responsible documentation for ML models  
- ğŸ““ **Notebooks:** exploratory work that should be readable & reproducible  
- ğŸ§© **Templates:** optional local templates (if not using `docs/templates/`)

If youâ€™re new, start here:
- `../docs/MASTER_GUIDE_v13.md` ğŸ“Œ *(canonical pipeline & repo structure)*
- `../CONTRIBUTING.md` ğŸ¤ *(how to contribute safely & consistently)*

---

## ğŸ—‚ï¸ Directory layout

```text
mcp/
â”œâ”€â”€ README.md                 # you are here ğŸ‘‹
â”œâ”€â”€ experiments/              # human-readable experiment reports ğŸ§¾
â”œâ”€â”€ runs/                     # run artifacts + metadata (configs, logs, metrics) ğŸƒ
â”œâ”€â”€ sops/                     # Standard Operating Procedures (SOPs) ğŸ§°
â”œâ”€â”€ model_cards/              # model cards for any ML/AI used or trained ğŸ§ 
â”œâ”€â”€ notebooks/                # exploratory notebooks (kept tidy & reproducible) ğŸ““
â””â”€â”€ templates/                # optional: local templates ğŸ§©
```

> [!NOTE]
> **Rule of thumb:** `mcp/` documents *methods & decisions*.  
> **Data products** (including AI/analysis outputs) belong in `data/processed/...` and must be cataloged (STAC/DCAT/PROV).

---

## ğŸ” The MCP workflow loop

When you do *anything* that affects evidence (data, analysis outputs, models), follow this loop:

1. **Ask a question** â“ *(What are we trying to learn or improve?)*  
2. **Write the protocol** ğŸ§¾ *(What exactly will we do? Variables? Controls?)*  
3. **Run it** ğŸƒ *(Capture configs, versions, seeds, environment)*  
4. **Publish evidence artifacts** ğŸ“¦ *(store outputs as datasets + catalogs + provenance)*  
5. **Write the report** ğŸ§ª *(interpret results; record limitations & next steps)*  
6. **Review** âœ… *(someone else can reproduce it from your documentation)*

---

## âœ… Minimum required artifacts (for any â€œrealâ€ experiment)

When an experiment goes beyond a quick local poke, it must include:

- ğŸ§¾ **Experiment Report** in `mcp/experiments/â€¦`
- ğŸƒ **Run Record** in `mcp/runs/â€¦` *(or a link to an external run folder)*
- ğŸ”— **Code pointer:** commit hash + main entrypoint script/notebook
- ğŸ§± **Environment pointer:** Docker image tag/digest OR `requirements*.txt` / `environment.yml`
- ğŸ² **Randomness controls:** seeds + deterministic flags (when applicable)
- ğŸ“¦ **Evidence outputs:** stored under `data/processed/...` *(not inside `mcp/`)*
- ğŸ§¬ **Provenance links:** STAC/DCAT/PROV IDs/paths for inputs + outputs

> [!WARNING]
> Avoid committing large binaries to `mcp/`.  
> Keep `mcp/` lightweight and human-readable. Evidence belongs in governed data storage.

---

## ğŸ§ª Experiment reports

### ğŸ“› Naming convention
Pick one and be consistent:

- `EXP-YYYY-MM-DD-<short-slug>.md`  
  Example: `EXP-2026-01-02-ocr-ner-baseline.md`
- or numeric: `EXP-001-<short-slug>.md`

### ğŸ·ï¸ Status definitions (recommended)
- `draft` ğŸ“: in progress; may be missing evidence links
- `complete` âœ…: reproducible; evidence + run receipts linked
- `superseded` ğŸ§¯: replaced by a newer experiment

### ğŸ§¾ Experiment report template (copy/paste)

```md
---
id: EXP-YYYY-MM-DD-<slug>
title: "<short, explicit title>"
date: YYYY-MM-DD
owner: "@github-handle"
status: draft | complete | superseded
supersedes: []          # optional: [EXP-...]
superseded_by: []       # optional: [EXP-...]
tags: [gis, ocr, nlp, stac, dcat, prov, web, graph]
---

# Objective / Question â“
- What are we trying to learn or improve?

# Background / Prior Art ğŸ“š
- Links to prior experiments, issues, papers, or notes.

# Hypothesis âœ…/âŒ
- What do we expect and why?

# Data Used ğŸ—ƒï¸
- Inputs (STAC/DCAT references, dataset IDs, checksums if available).
- Any sampling/filter criteria.

# Method / Protocol ğŸ§¾
- Step-by-step procedure.
- Parameters and configs.
- Tools + versions (including OS/GPU if relevant).

# Run Record ğŸƒ
- Code commit: `abcdef1`
- Entrypoint: `src/pipelines/...` or notebook path
- Run folder: `mcp/runs/RUN-YYYY-MM-DD-.../`
- Seeds: `...`
- Duration: `...`

# Results ğŸ“ˆ
- Metrics, charts, qualitative examples.
- Link to produced evidence artifacts under `data/processed/...`

# Uncertainty & Validation ğŸ”
- What could be wrong?
- Sanity checks, cross-validation, spot-check counts, error bounds, etc.

# Interpretation ğŸ§ 
- What do the results mean for KFM?

# Decision / Next Steps ğŸ§­
- What do we do next?
- What should be repeated, scaled, or abandoned?

# Reproducibility Checklist âœ…
- [ ] All parameters & configs documented
- [ ] Code committed + hash recorded
- [ ] Environment captured (Docker/lockfile)
- [ ] Seeds recorded (if applicable)
- [ ] Inputs/outputs linked via STAC/DCAT/PROV
- [ ] Another person can re-run it using this doc
```

---

## ğŸƒ Runs

Runs are the **receipt** for an experiment: configs, logs, and machine outputs needed to reproduce.

### ğŸ“› Naming convention
`RUN-YYYY-MM-DD-<slug>/`

### ğŸ“¦ Suggested run folder contents
- `config/` ğŸ§¾ *(YAML/JSON config used for the run)*
- `env/` ğŸ§± *(pip freeze / conda export / docker image digest)*
- `logs/` ğŸªµ *(structured logs)*
- `metrics/` ğŸ“ˆ *(CSV/JSON metrics, eval outputs)*
- `artifacts/` ğŸ§© *(small artifacts like thumbnails, sample outputs)*
- `MANIFEST.md` ğŸ§¾ *(human-readable summary + links to evidence in `data/processed/...`)*

### ğŸ§¾ Minimal `MANIFEST.md` template (copy/paste)

```md
---
run_id: RUN-YYYY-MM-DD-<slug>
related_experiment: EXP-YYYY-MM-DD-<slug>
date: YYYY-MM-DD
owner: "@github-handle"
code_commit: abcdef1
entrypoint: "src/pipelines/..."
environment:
  docker_image: "ghcr.io/org/project:tag@sha256:..."
  # or:
  requirements: "requirements.txt"
randomness:
  seeds: [123, 456]
  deterministic_flags: ["torch.use_deterministic_algorithms=True"]
inputs:
  - stac: "..."
  - dcat: "..."
outputs:
  - path: "data/processed/<dataset-id>/..."
    stac_item: "..."
    prov: "..."
notes: ""
---

# Summary ğŸ§¾
- What did this run do?

# Where to find outputs ğŸ“¦
- `data/processed/...` links + catalog IDs

# How to reproduce ğŸ”
1. Checkout commit: `abcdef1`
2. Restore env: ...
3. Run: ...
4. Validate: ...
```

> [!TIP]
> Treat run folders as immutable receipts. If you change parameters, make a new run folder.

---

## ğŸ§° SOPs

SOPs turn â€œtribal knowledgeâ€ into a reproducible procedure.  
Write an SOP whenever a task is repeated or has meaningful risk (data integrity, georeferencing, catalog publishing, etc.).

### SOP template (copy/paste)

```md
---
id: SOP-<topic>-v1
title: "<clear title>"
owner: "@github-handle"
last_updated: YYYY-MM-DD
---

# Purpose ğŸ¯
What this SOP accomplishes and when to use it.

# Scope âœ…
Whatâ€™s included / excluded.

# Prerequisites ğŸ§±
Accounts, tools, data access, permissions.

# Tools & Versions ğŸ§°
List software + versions.

# Procedure ğŸ§­
1. Step...
2. Step...
3. Step...

# Verification âœ…
How to confirm it worked (checks, expected outputs).

# Troubleshooting ğŸ§¯
Common failure modes + fixes.

# Audit Trail ğŸ§¾
- Links to example PRs, experiment reports, or run folders that used this SOP.
```

**High-value SOPs for KFM (starter set):**
- `sops/georeference_map.md` ğŸ—ºï¸
- `sops/build_cog_tiles.md` ğŸ§±
- `sops/ocr_pipeline.md` ğŸ”
- `sops/publish_stac_dcat_prov.md` ğŸŒ
- `sops/train_or_update_model.md` ğŸ§ 

---

## ğŸ§  Model Cards

Any ML/AI model used in the pipeline (trained or adopted) needs a model card:
- what it is
- what it was trained on (or sourced from)
- what it should be used for âœ…
- what it should **not** be used for ğŸš«
- known limitations, bias risks, and failure modes âš ï¸

### Model card template (copy/paste)

```md
---
model_id: MODEL-<name>-v<semver>
owner: "@github-handle"
date: YYYY-MM-DD
source:
  type: trained | third_party
  license: "..."
  reference: "paper/link/registry id"
---

# Model overview ğŸ§ 
- What problem does it solve?

# Intended use âœ…
- Supported use-cases

# Out-of-scope / prohibited use ğŸš«
- What it must not be used for

# Training data ğŸ—ƒï¸
- Datasets used (STAC/DCAT references), sampling, labeling notes

# Evaluation ğŸ“ˆ
- Metrics, test sets, qualitative examples

# Limitations & biases âš ï¸
- Known failure modes, bias risks, uncertainty notes

# Reproducibility ğŸ§ª
- Training code commit hash
- Environment / hardware notes
- Hyperparameters / config
- Seeds
```

---

## ğŸ““ Notebooks

Notebooks are welcomeâ€”**but must be readable and reproducible**:

- Start with a markdown cell: **purpose + inputs + outputs**
- Keep output cells small *(no giant embedded blobs)*
- Prefer parameterized notebooks (or export to scripts) when a notebook becomes â€œrealâ€
- If a notebook produces evidence artifacts: store outputs in `data/processed/...` and link them from an experiment report ğŸ§¾

---

## ğŸ”— MCP â†” KFM pipeline (non-negotiable)

KFM uses a strict evidence pipeline:

**ETL â†’ STAC/DCAT/PROV catalogs â†’ Graph â†’ APIs â†’ UI â†’ Story Nodes**

So for MCP work:

- âœ… Experiment reports live here: `mcp/experiments/â€¦`
- âœ… Run receipts live here: `mcp/runs/â€¦`
- âœ… Evidence artifacts live here: `data/processed/...`
- âœ… Evidence artifacts must be:
  - cataloged in **STAC/DCAT** ğŸ§¾
  - linked with **PROV** lineage ğŸ§¬
  - integrated into graph/UI only through governed contracts ğŸ”’

This keeps â€œcool experimentsâ€ from turning into untraceable claims.

---

## ğŸ§© Domain checklists

Use the checklist that matches your work:

### ğŸ—ºï¸ GIS / Remote Sensing
- [ ] CRS documented (EPSG code + axis order)
- [ ] Georeferencing method + control points documented
- [ ] RMS / fit error recorded (if applicable)
- [ ] Raster outputs are COGs / tiled in a documented way
- [ ] Vector outputs validate (topology, geometry validity)

### ğŸ” OCR / NLP
- [ ] Input corpus + sampling documented
- [ ] Labeling rules / evaluation rubric included
- [ ] Precision/recall (or at least spot-check protocol) documented
- [ ] Known failure classes logged (fonts, scan quality, place-name ambiguity)

### ğŸ“Š Statistics / Inference
- [ ] Outcome variables + units defined
- [ ] Assumptions checked (normality, independence, etc.)
- [ ] Effect sizes reported (not just p-values)
- [ ] Multiple comparisons / researcher degrees of freedom handled

### ğŸ›°ï¸ Modeling & Simulation
- [ ] Assumptions enumerated explicitly
- [ ] Validation approach documented (comparisons, back-to-back tests)
- [ ] Uncertainty quantified or bounded where possible
- [ ] Results reported with error/uncertainty context (not just point estimates)

---

## ğŸ” Safety & hygiene rules

- ğŸš« Donâ€™t store secrets, tokens, private keys, or sensitive PII in `mcp/`
- ğŸ§½ Redact logs before committing if they contain identifiers or confidential paths
- ğŸ§Š Prefer immutable receipts: new run folder > editing old run folder
- ğŸ§­ If an experiment is superseded, mark it as `superseded` and link the successor

---

## ğŸ¤ Contributing

- See `../CONTRIBUTING.md`
- Security concerns: see `../.github/SECURITY.md`
- When in doubt: open an issue with an MCP stub (question + proposed experiment) ğŸ§¾

---

ğŸ§­ **Goal:** Make every output auditable and every method teachable.  
ğŸ§¾ **Promise:** If itâ€™s in production, it has a paper trail.
