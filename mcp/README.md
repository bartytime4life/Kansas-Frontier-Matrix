# ğŸ§ª MCP â€” Master Coder Protocol (Methods & Computational Experiments)

![Docs-first](https://img.shields.io/badge/docs-documentation--first-blue)
![Reproducible](https://img.shields.io/badge/reproducible-audit--ready-success)
![Evidence](https://img.shields.io/badge/evidence-traceable-informational)
![KFM](https://img.shields.io/badge/KFM-Kansas%20Frontier%20Matrix-orange)

> **TL;DR:** `mcp/` is the projectâ€™s â€œlab notebook + playbookâ€ ğŸ§¾  
> It holds experiment reports, run records, SOPs, notebooks, and model cardsâ€”so every result can be re-run, reviewed, and trusted.

âœ… **Important:** In this repo, **MCP = Master Coder Protocol** (not â€œModel Context Protocolâ€).

---

## ğŸ¯ Why this folder exists

Kansas Frontier Matrix (KFM) is built to be **evidence-first** and **reproducible**.  
This directory operationalizes that philosophy by keeping the â€œhow we did itâ€ artifacts in one place:

- **ğŸ§ª Experiments:** what we tried + why + what happened  
- **ğŸƒ Runs:** concrete run metadata/artifacts (configs, seeds, logs, metrics)  
- **ğŸ§° SOPs:** repeatable procedures for recurring tasks  
- **ğŸ§  Model Cards:** responsible documentation for ML models  
- **ğŸ““ Notebooks:** exploratory work that eventually becomes pipelines or reports

If youâ€™re new, start here:
- `../docs/MASTER_GUIDE_v13.md` ğŸ“Œ (canonical pipeline & repo structure)
- `../CONTRIBUTING.md` ğŸ¤ (how to contribute safely and consistently)

---

## ğŸ—‚ï¸ Directory layout

```text
mcp/
â”œâ”€â”€ README.md                 # you are here ğŸ‘‹
â”œâ”€â”€ experiments/              # human-readable experiment reports ğŸ§¾
â”œâ”€â”€ runs/                     # run artifacts + metadata (configs, logs, metrics) ğŸƒ
â”œâ”€â”€ sops/                     # Standard Operating Procedures (SOPs) ğŸ§°
â”œâ”€â”€ model_cards/              # model cards for any ML/AI used or trained ğŸ§ 
â”œâ”€â”€ notebooks/                # exploratory notebooks (kept tidy & reproducible) ğŸ““
â””â”€â”€ templates/                # optional: local templates (if not using docs/templates/) ğŸ§©
```

> **Rule of thumb:** `mcp/` documents *methods and decisions*.  
> **Data products** (including AI/analysis outputs) belong in `data/processed/...` and must be cataloged (STAC/DCAT/PROV).

---

## ğŸ” The MCP workflow

When you do *anything* that affects evidence (data, analysis outputs, models), follow this loop:

1. **Ask a question** â“ (What are we trying to learn or improve?)
2. **Write the protocol** ğŸ§¾ (What exactly will we do? What are the variables?)
3. **Run it** ğŸƒ (Capture configs, versions, seeds, environment)
4. **Publish the evidence artifact** ğŸ“¦ (store outputs as datasets + catalogs + provenance)
5. **Write the report** ğŸ§ª (interpret results; record limitations & next steps)
6. **Review** âœ… (another human can reproduce it from your documentation)

---

## âœ… Minimum required artifacts (for any â€œrealâ€ experiment)

When an experiment is beyond a quick local poke, it must include:

- **ğŸ§¾ Experiment Report** in `mcp/experiments/â€¦`
- **ğŸƒ Run Record** in `mcp/runs/â€¦` (or linked run folder)
- **ğŸ”— Code pointer:** commit hash + main entrypoint script/notebook
- **ğŸ§± Environment pointer:** Docker image tag OR `requirements*.txt`/`environment.yml`
- **ğŸ² Randomness controls:** seeds + deterministic flags (when applicable)
- **ğŸ“¦ Evidence outputs:** stored under `data/processed/...` (not inside `mcp/`)
- **ğŸ§¬ Provenance links:** STAC/DCAT/PROV IDs/paths for inputs + outputs

### â€œIf I canâ€™t reproduce it in 30 minutes, itâ€™s not done.â€ â±ï¸
Thatâ€™s the bar.

---

## ğŸ§ª Experiment reports

### ğŸ“› Naming convention
Pick one and be consistent:

- `EXP-YYYY-MM-DD-<short-slug>.md`  
  Example: `EXP-2026-01-02-ocr-ner-baseline.md`
- or numeric: `EXP-001-<short-slug>.md`

### ğŸ§¾ Experiment report template (copy/paste)

```md
---
id: EXP-YYYY-MM-DD-<slug>
title: "<short, explicit title>"
date: YYYY-MM-DD
owner: "@github-handle"
status: draft | complete | superseded
tags: [gis, ocr, nlp, stac, dcat, prov, web, graph]
---

# Objective / Question â“
- What are we trying to learn or improve?

# Background / Prior Art ğŸ“š
- Links to prior experiments, issues, papers, or notes.

# Hypothesis âœ…/âŒ
- What do we expect and why?

# Data Used ğŸ—ƒï¸
- Inputs (STAC/DCAT references, dataset IDs, checksums if available).
- Any sampling/filter criteria.

# Method / Protocol ğŸ§¾
- Step-by-step procedure.
- Parameters and configs.
- Tools + versions (including OS/GPU if relevant).

# Run Record ğŸƒ
- Code commit: `abcdef1`
- Entrypoint: `src/pipelines/...` or notebook path
- Run folder: `mcp/runs/RUN-YYYY-MM-DD-.../`
- Seeds: `...`
- Duration: `...`

# Results ğŸ“ˆ
- Metrics, charts, qualitative examples.
- Link to produced evidence artifacts under `data/processed/...`

# Uncertainty & Validation ğŸ”
- What could be wrong?
- Sanity checks, cross-validation, spot-check counts, error bounds, etc.

# Interpretation ğŸ§ 
- What do the results mean for KFM?

# Decision / Next Steps ğŸ§­
- What do we do next?
- What should be repeated, scaled, or abandoned?

# Reproducibility Checklist âœ…
- [ ] All parameters & configs documented
- [ ] Code committed + hash recorded
- [ ] Environment captured (Docker/lockfile)
- [ ] Seeds recorded (if applicable)
- [ ] Inputs/outputs linked via STAC/DCAT/PROV
- [ ] Another person can re-run it using this doc
```

---

## ğŸƒ Runs

Runs are the â€œreceiptâ€ for an experiment: configs, logs, and machine outputs needed to reproduce.

### ğŸ“› Naming convention
`RUN-YYYY-MM-DD-<slug>/`

### Suggested run folder contents
- `config/` (YAML/JSON config used for the run)
- `env/` (pip freeze, conda env export, docker image digest)
- `logs/` (structured logs)
- `metrics/` (CSV/JSON metrics, evaluation outputs)
- `artifacts/` (small artifacts like thumbnails, sample outputs)
- `MANIFEST.md` (human-readable summary + links to evidence artifacts in `data/processed/...`)

> âš ï¸ Avoid committing large binaries here.  
> Large outputs belong in the governed data pipeline (`data/processed/...`) and/or a release bundle (`releases/...`) using the repoâ€™s data/versioning strategy.

---

## ğŸ§° SOPs

SOPs turn â€œtribal knowledgeâ€ into a reproducible procedure.  
Write an SOP whenever a task is repeated or has meaningful risk (data integrity, georeferencing, catalog publishing, etc.).

### SOP template (copy/paste)

```md
---
id: SOP-<topic>-v1
title: "<clear title>"
owner: "@github-handle"
last_updated: YYYY-MM-DD
---

# Purpose ğŸ¯
What this SOP accomplishes and when to use it.

# Scope âœ…
Whatâ€™s included / excluded.

# Prerequisites ğŸ§±
Accounts, tools, data access, permissions.

# Tools & Versions ğŸ§°
List software + versions.

# Procedure ğŸ§­
1. Step...
2. Step...
3. Step...

# Verification âœ…
How to confirm it worked (checks, expected outputs).

# Troubleshooting ğŸ§¯
Common failure modes + fixes.

# Audit Trail ğŸ§¾
- Links to example PRs, experiment reports, or run folders that used this SOP.
```

**High-value SOPs for KFM (starter set):**
- `sops/georeference_map.md` ğŸ—ºï¸
- `sops/build_cog_tiles.md` ğŸ§±
- `sops/ocr_pipeline.md` ğŸ”
- `sops/publish_stac_dcat_prov.md` ğŸŒ
- `sops/train_or_update_model.md` ğŸ§ 

---

## ğŸ§  Model Cards

Any ML/AI model used in the pipeline (trained or adopted) needs a model card:

- what it is
- what it was trained on
- what it should be used for
- what it should **not** be used for
- known limitations, biases, and failure modes

### Model card template (copy/paste)

```md
---
model_id: MODEL-<name>-v<semver>
owner: "@github-handle"
date: YYYY-MM-DD
---

# Model overview ğŸ§ 
- What problem does it solve?

# Intended use âœ…
- Supported use-cases

# Out-of-scope / prohibited use ğŸš«
- What it must not be used for

# Training data ğŸ—ƒï¸
- Datasets used (STAC/DCAT references), sampling, labeling notes

# Evaluation ğŸ“ˆ
- Metrics, test sets, qualitative examples

# Limitations & biases âš ï¸
- Known failure modes, bias risks, uncertainty notes

# Reproducibility ğŸ§ª
- Training code commit hash
- Environment / hardware notes
- Hyperparameters / config
- Seeds
```

---

## ğŸ““ Notebooks

Notebooks are welcomeâ€”**but must be readable and reproducible**:

- Start with a markdown cell: purpose + inputs + outputs
- Keep output cells small (no massive embedded blobs)
- Prefer parameterized notebooks (or export to scripts) when a notebook becomes â€œrealâ€

> If a notebook produces an evidence artifact, it must follow the same rules: store outputs in `data/processed/...` and link them from an experiment report.

---

## ğŸ”— MCP â†” KFM pipeline (non-negotiable)

KFM has a strict evidence pipeline:

**ETL â†’ STAC/DCAT/PROV catalogs â†’ Graph â†’ APIs â†’ UI â†’ Story Nodes**

So for MCP work:

- **Experiment reports** live here (`mcp/experiments/â€¦`) âœ…
- **Evidence artifacts** live in `data/processed/...` âœ…
- Evidence artifacts must be:
  - cataloged in **STAC/DCAT** ğŸ§¾
  - linked with **PROV** lineage ğŸ§¬
  - integrated into graph / UI **only** through governed contracts

This keeps â€œcool experimentsâ€ from turning into untraceable claims.

---

## ğŸ§© Domain checklists

Use the checklist that matches your work:

### ğŸ—ºï¸ GIS / Remote Sensing
- [ ] CRS documented (EPSG code + axis order)
- [ ] Georeferencing method + control points documented
- [ ] RMS / fit error recorded (if applicable)
- [ ] Raster outputs are COGs / tiled in a documented way
- [ ] Vector outputs validate (topology, geometry validity)

### ğŸ” OCR / NLP
- [ ] Input corpus + sampling documented
- [ ] Labeling rules / evaluation rubric included
- [ ] Precision/recall (or at least spot-check protocol) documented
- [ ] Known failure classes logged (fonts, scan quality, place-name ambiguity)

### ğŸ“Š Statistics / Inference
- [ ] Outcome variables + units defined
- [ ] Assumptions checked (normality, independence, etc.)
- [ ] Effect sizes reported (not just p-values)
- [ ] Multiple comparisons / researcher degrees of freedom handled

### ğŸ›°ï¸ Modeling & Simulation
- [ ] Assumptions enumerated explicitly
- [ ] Validation approach documented (comparisons, back-to-back tests)
- [ ] Uncertainty quantified or bounded where possible
- [ ] Results reported with error/uncertainty context (not just point estimates)

---

## ğŸ¤ Contributing

- See `../CONTRIBUTING.md`
- Security concerns: see `../.github/SECURITY.md`
- When in doubt: open an issue with an MCP stub (question + proposed experiment)

---

ğŸ§­ **Goal:** Make every output auditable and every method teachable.