---
title: "ğŸ§ª MCP â€” Master Coder Protocol (Methods, Controls & Processes) ğŸ§¾âš™ï¸"
path: "mcp/README.md"
version: "v1.6.0"
last_updated: "2026-01-26"
status: "active"
doc_kind: "Directory README"
license: "CC-BY-4.0"
kfm_blueprint: "v13+"

# Repo doctrine (aligns with KFM redesign blueprint v13+)
doctrine:
  contract_first: true
  provenance_first: true
  evidence_first: true
  deny_by_default: true
  focus_mode_advisory_only: true
  atomic_promotion: true
  api_boundary_enforced: true

# Governance posture (FAIR+CARE + sovereignty-aware)
governance:
  fair_category: "FAIR+CARE+Sovereignty"
  care_label_default: "public"
  sensitivity_default: "public"
  classification_default: "open"
  ai_human_in_loop_default: "required"
  policy_packs_expected: true
  governance_ledger_expected: true

owners:
  primary: "KFM Engineering"
  reviewers: ["Maintainers", "Data Steward(s)", "Governance Council (when applicable)"]
---

# ğŸ§ª MCP â€” Master Coder Protocol  
### *Methods, Controls & Processes* ğŸ§¾âš™ï¸

![README](https://img.shields.io/badge/README-v1.6.0-8957e5)
![Blueprint](https://img.shields.io/badge/KFM%20Blueprint-v13%2B-orange)
![Docs-first](https://img.shields.io/badge/docs-documentation--first-blue)
![Contract-first](https://img.shields.io/badge/contracts-schema--first-1f6feb)
![Provenance-first](https://img.shields.io/badge/provenance-first-7b42f6)
![Evidence](https://img.shields.io/badge/evidence-STAC%2BDCAT%2BPROV-informational)
![PROV](https://img.shields.io/badge/lineage-W3C%20PROV-7b42f6)
![Governance](https://img.shields.io/badge/governance-FAIR%2BCARE%20%2B%20Sovereignty-2ea043)
![Policy](https://img.shields.io/badge/policy-OPA%20%2B%20Conftest-important)
![Security](https://img.shields.io/badge/security-hostile--inputs%20%2B%20deny--by--default-red)
![Supply%20Chain](https://img.shields.io/badge/supply--chain-SBOM%20%2B%20SLSA%20%2B%20attestations-success)
![KFM](https://img.shields.io/badge/KFM-Kansas%20Frontier%20Matrix-orange)

> **TL;DR:** `mcp/` is KFMâ€™s **methods + receipts** layer ğŸ““ğŸ§   
> It holds **protocols**, **run receipts**, **SOPs**, **model cards**, **gate reports**, and **review artifacts**â€”so every result can be **re-run, reviewed, and trusted** âœ…  
>
> **v13+ alignment:** KFM is **evidenceâ€‘first**, **catalogâ€‘driven**, **APIâ€‘centric**, and **policyâ€‘gated**.  
> MCP is the paper trail that keeps those promises honest. ğŸ§¾ğŸ›¡ï¸

> [!IMPORTANT]
> In this repo, **MCP = Master Coder Protocol** âœ…  
> **MCP â‰  Model Context Protocol** ğŸš« *(not what we mean here)*  
> Keep this distinction consistent in docs, PRs, issues, and commit messages.

---

## ğŸ”— Quick links
- ğŸ§­ Repo overview: **[`../README.md`](../README.md)**
- ğŸ“š Docs boundary: **[`../docs/README.md`](../docs/README.md)** *(if present)*
- ğŸ§¾ Standards & profiles (STAC/DCAT/PROV, ontology, schemas): **[`../docs/standards/`](../docs/standards/)** *(if present)*
- ğŸ§© Templates (governed formats for docs, Story Nodes, API changes): **[`../docs/templates/`](../docs/templates/)** *(if present)*
- ğŸ› Governance (FAIR/CARE/Sovereignty, review gates): **[`../docs/governance/`](../docs/governance/)** *(if present)*
- ğŸ§¬ Pipelines boundary (contract portal): **[`../pipelines/`](../pipelines/)** *(if present)*
- ğŸ§° Scripts boundary (automation): **[`../scripts/`](../scripts/)** *(if present)*
- ğŸ§© Executable source boundary: **[`../src/`](../src/)** *(if present)*
- ğŸ“¦ Data + metadata boundary: **[`../data/README.md`](../data/README.md)** *(if present)*
- ğŸ§ª Notebooks boundary (lab bench): **[`../notebooks/`](../notebooks/)** *(if present)*
- ğŸ§° Validators & tooling: **[`../tools/`](../tools/)** *(if present)*
- âœ… Tests & CI gates: **[`../tests/`](../tests/)** *(if present)*
- ğŸ¤ CI/CD & policies: **[`../.github/`](../.github/)** *(workflows, security policy, automation)*

---

## âš¡ Quick Nav
- [ğŸ§¾ Doc metadata](#-doc-metadata)
- [ğŸ§­ What MCP is](#-what-mcp-is)
- [ğŸ§· Repo invariants](#-repo-invariants)
- [ğŸ§± MCP artifacts](#-mcp-artifacts-types-ids-and-immutability)
- [ğŸš¦ Nonâ€‘negotiables](#-non-negotiables)
- [ğŸ Golden paths](#-golden-paths)
- [ğŸ—‚ï¸ Directory layout](#-directory-layout)
- [ğŸ” The MCP workflow loop](#-the-mcp-workflow-loop)
- [ğŸš¥ Detect â†’ Validate â†’ Promote](#-detect--validate--promote)
- [ğŸ” Policy packs & gate reports](#-policy-packs--gate-reports)
- [ğŸ¤– Watcher â†’ Planner â†’ Executor receipts](#-watcher--planner--executor-receipts)
- [ğŸ§¬ DevOps provenance & supply chain](#-devops-provenance--supply-chain)
- [ğŸ§¾ Front matter + schemas](#-front-matter--schemas-machine-readable-mcp)
- [âœ… Definition of done](#-definition-of-done)
- [ğŸ“¦ Required artifacts](#-required-artifacts)
- [ğŸ§ª Experiment reports](#-experiment-reports)
- [ğŸƒ Run receipts](#-run-receipts)
- [ğŸ“ˆ Performance & scalability experiments](#-performance--scalability-experiments)
- [ğŸ§° SOPs](#-sops)
- [ğŸ§  Model cards](#-model-cards)
- [ğŸ““ Notebooks](#-notebooks)
- [ğŸ§­ Traceability matrix](#-traceability-matrix)
- [ğŸ§¯ Bad evidence protocol](#-bad-evidence-protocol)
- [ğŸ”— MCP â†” KFM evidence pipeline](#-mcp--kfm-evidence-pipeline)
- [ğŸ§© Domain checklists](#-domain-checklists)
- [ğŸ” Safety, privacy, licensing](#-safety-privacy-licensing)
- [ğŸ›¡ï¸ Threat model & hostile inputs](#-threat-model--hostile-inputs)
- [âœ… QA, audits, and CI hooks](#-qa-audits-and-ci-hooks)
- [ğŸ¤ PR / review checklist](#-pr--review-checklist)
- [ğŸ“š Project reference library influence map](#-project-reference-library-influence-map)
- [ğŸ§¾ Reference library index](#-reference-library-index)
- [ğŸ•°ï¸ Version history](#-version-history)

---

## ğŸ§¾ Doc metadata

| Field | Value |
|---|---|
| Doc | `mcp/README.md` |
| Status | Active âœ… |
| Last updated | **2026-01-26** |
| Audience | Contributors authoring experiments, running jobs, shipping evidence, publishing Story Nodes, training/using models |
| Prime directive | If it changes â€œspatial truth,â€ it must be **traceable + reproducible + reviewable** |
| KFM canon (v13+) | **ETL/Streaming â†’ Catalogs (STAC/DCAT/PROV) â†’ Graph â†’ API â†’ UI â†’ Story Nodes â†’ Focus Mode** |
| Special doctrine | **Policy packs gate publishing** + **atomic promotion** + **AI is advisory-only and must cite** |
| API boundary rule | UI never queries Neo4j (or other stores) directlyâ€”**only through governed APIs** |

---

## ğŸ§­ What MCP is

### âœ… MCP isâ€¦
A **governed method layer** that turns â€œwe tried somethingâ€ into **auditable science** ğŸ§¾âœ…

- ğŸ§ª **Protocols** (what we intended to do + why)
- ğŸƒ **Receipts** (what actually ran + how to reproduce)
- ğŸ§° **SOPs** (repeatable procedures for risky/repeated work)
- ğŸ§  **Model cards** (responsible AI/ML use + limits)
- ğŸ” **Gate reports** (policy/contract checks proving it was safe to publish)
- ğŸ‘€ **Review artifacts** (what was checked, by whom, what failed)
- ğŸ§­ **Traceability** (decision â†” evidence â†” catalogs â†” provenance â†” PRs)

### ğŸš« MCP is notâ€¦
- âŒ A data lake (thatâ€™s `data/`)
- âŒ A code dump (thatâ€™s `src/`, `pipelines/`, `web/`)
- âŒ A place for large binaries (store in `data/processed/**` + catalogs)
- âŒ A place for â€œunsourced narrativeâ€ (Story Nodes must cite evidence artifacts)
- âŒ A runtime governance ledger (that lives in system telemetry / governance ledger), but MCP **references** ledger IDs when relevant

> [!TIP]
> MCP exists to make â€œresultsâ€ **explainable and replayable**â€”not merely impressive. ğŸ§¾ğŸ§ 

---

## ğŸ§· Repo invariants

These are the â€œyou can build anything, but not *like that*â€ constraints.  
They align MCP with KFMâ€™s v13+ blueprint: **catalog-driven**, **API-centric**, **policy-enforced**, and **transparent**.

### ğŸ§± Invariant 1 â€” Pipeline ordering is absolute
**ETL/Streaming â†’ Catalogs (STAC/DCAT/PROV) â†’ Graph â†’ API â†’ UI â†’ Story Nodes â†’ Focus Mode**  
No skipping steps. No â€œtemporary UI demoâ€ without catalogs + lineage.

### ğŸ§¬ Invariant 2 â€” Provenance-first publishing (including streaming)
Anything â€œpublishedâ€ (referenced by graph, APIs, UI, Story Nodes, dashboards, or Focus Mode) must have:
- a **catalog record** (STAC/DCAT) ğŸ—‚ï¸  
- a **PROV lineage record** ğŸ§¬  
â€¦**before** it is used downstream.

> Streaming isnâ€™t exempt: snapshots/aggregations must be traceable (windowing + cadence + lineage).

### ğŸ§¾ Invariant 3 â€” Contracts are first-class
Schemas/profiles/contracts are interfacesâ€”treat them like APIs.  
If it canâ€™t validate, it canâ€™t ship. ğŸ§±âœ…

### ğŸ§· Invariant 4 â€” API boundary is non-negotiable
The UI must never query Neo4j/PostGIS/search indexes directly.  
All access goes through the governed API layer so access control, redaction, and schema consistency are enforced centrally. ğŸ›¡ï¸

### ğŸ” Invariant 5 â€” ETL must be deterministic + idempotent
Given the same inputs + config:
- same IDs, same hashes, same catalogs
- safe to rerun without double-writing or corrupting state
- automation uses **idempotency keys** + has a **kill switch** ğŸ§¯

### ğŸ“¦ Invariant 6 â€” Work vs processed areas are distinct
- `data/work/**` = scratch/intermediate (safe to delete, not â€œevidenceâ€) ğŸ§ª
- `data/processed/**` = final evidence artifacts (versioned, cataloged, lineage-linked) ğŸ“¦ğŸ—‚ï¸ğŸ§¬

### ğŸš€ Invariant 7 â€” Promotion is atomic
**Promote =** `data/processed` outputs + **STAC/DCAT** + **PROV** + *(optional)* graph ingest bundle  
as one consistent publish step.  
If any gate fails â†’ publish remains staged. No half-publishes. ğŸ§¾âœ…

### ğŸ” Invariant 8 â€” Policy packs gate publishing
Publishing must pass automated policy checks (OPA/Conftest style), including:
- contracts/schema validation
- STAC/DCAT/PROV validity + link integrity
- no sensitivity downgrade
- license/attribution propagation
- Story/Focus citation coverage checks (when narrative/AI exists)
- supply-chain + secrets scanning (when relevant)

### ğŸ§Š Invariant 9 â€” Dual-format evidence stays one truth
When a domain needs both **analysis format** and **UI format**:
- produce both (e.g., **GeoParquet** for analytics + **PMTiles** for map rendering) ğŸ§Š
- catalog both under the same dataset identity/metadata
- keep provenance common and explicit (same inputs, same pipeline run)

### ğŸ¤– Invariant 10 â€” Focus Mode is advisory-only and evidence-bound
Focus Mode can **suggest** and **summarize**. It cannot â€œdeclare truthâ€ unless it:
- cites evidence artifacts (catalog IDs / graph entities)  
- passes governance checks (content + sensitivity rules)  
- shows uncertainty/refuses when evidence is insufficient

### ğŸ§¾ Invariant 11 â€” Everything decision-worthy gets a receipt
Every decision-worthy run captures:
- input/output hashes (or stable IDs) ğŸ”¢
- config + environment snapshot ğŸ§±
- commit hash + entrypoint ğŸ§©
- gate report(s) ğŸ”
- catalog IDs + PROV pointers ğŸ—‚ï¸ğŸ§¬
- governance ledger references when available ğŸ›

---

## ğŸ§± MCP artifacts (types, IDs, and immutability)

MCP stays reliable because artifacts are **typed**, **named**, and **handled correctly**.

| Artifact | Prefix / ID pattern | Where | Immutable? | Purpose |
|---|---|---|---:|---|
| ğŸ§ª Experiment protocol | `EXP-YYYY-MM-DD-<slug>.md` | `mcp/experiments/` | âš ï¸ Mutable while `draft` | Pre-register intent, assumptions, validation plan |
| ğŸƒ Run receipt | `RUN-YYYY-MM-DD-<slug>/` | `mcp/runs/` | âœ… Yes | What executed + how to reproduce |
| ğŸ” Gate report | `GATE-YYYY-MM-DD-<slug>.md` *(or `.json`)* | `mcp/gates/` *(recommended)* | âœ… Yes | Proof of policy/contract checks (OPA/Conftest + schema + link integrity) |
| ğŸš€ Promotion receipt | `PROMOTE-YYYY-MM-DD-<slug>/` *(optional)* | `mcp/promotions/` *(optional)* | âœ… Yes | Atomic publish record: what changed in evidence + catalogs |
| ğŸ§¬ DevOps provenance bundle | `DEVPROV-YYYY-MM-DD-<slug>.jsonld` *(optional)* | `mcp/dev_prov/` *(or `data/prov/dev/`)* | âœ… Yes | PR/build provenance (who/what built this, linked to datasets/runs) |
| ğŸ§° SOP | `SOP-<topic>-v<semver>.md` | `mcp/sops/` | âœ… Versioned | Repeatable procedures (risky/frequent tasks) |
| ğŸ§  Model card | `MODEL-<name>-v<semver>.md` | `mcp/model_cards/` | âœ… Versioned | Intended use, limits, datasets, governance for ML/AI |
| ğŸ“„ Dataset datasheet *(recommended)* | `DATASET-<name>-v<semver>.md` | `mcp/datasheets/` *(optional)* | âœ… Versioned | Ethical/quality notes for curated datasets |
| ğŸ“š Glossary / concept notes *(recommended)* | `GLOSSARY.md` *(or `TERM-*.md`)* | `mcp/glossary/` *(optional)* | âœ… Append-only mindset | Shared vocabulary across domains |
| ğŸ‘€ Review note | `REV-YYYY-MM-DD-<slug>.md` | `mcp/reviews/` *(recommended)* | âœ… Yes | Independent reproduction + governance sign-off |
| ğŸ§­ Traceability | `TRACEABILITY.md` *(or `TRACE-*.md`)* | `mcp/traceability/` | âœ… Append-only mindset | Decision/feature â†’ EXP/RUN â†’ evidence + catalogs + PRs |
| ğŸ§¯ Incident / anomaly | `INC-YYYY-MM-DD-<slug>.md` *(optional)* | `mcp/incidents/` | âœ… Yes | Post-mortems and regression gates |

> [!IMPORTANT]
> **Run receipts and gate reports are immutable.**  
> If anything changes (inputs, params, code, env, policy pack) â†’ create a **new** artifact and link it. âœ…

---

## ğŸš¦ Nonâ€‘negotiables

These rules keep KFM **scientific**, **auditable**, and **governed**:

1) **Evidence lives in `data/` â€” not in `mcp/`.** ğŸ“¦  
   - `mcp/` = methods, decisions, receipts, checklists  
   - `data/processed/...` = outputs (**must be cataloged + lineage-linked**)

2) **Contract-first by default.** ğŸ§¾ğŸ§±  
   If you publish an artifact, you must also publish the contract it conforms to (schema/profile/version).

3) **Provenance-first publishing (always).** ğŸ§¬  
   If itâ€™s referenced by graph/API/UI/Story/Focus, it must have STAC/DCAT + PROV before use.

4) **API boundary is enforced.** ğŸ›¡ï¸  
   UI/clients must not bypass governance by reading databases directly.

5) **Policy packs gate shipping.** ğŸ”  
   If a policy pack says â€œno,â€ it doesnâ€™t shipâ€”no â€œtemporaryâ€ bypass.

6) **Protocol before run (when it matters).** ğŸ§¾â¡ï¸ğŸƒ  
   If results influence product decisions, public narratives, pipelines, or models: write an **EXP** first.

7) **No â€œmagic results.â€** ğŸª„ğŸš«  
   If you canâ€™t reproduce it using commit hash + env + config + evidence pointers (+ hashes), itâ€™s not done.

8) **Immutable receipts.** ğŸ§¾  
   Donâ€™t edit a receipt to â€œfix history.â€ Create a new receipt and link it.

9) **No privacy / sensitivity downgrade.** ğŸ”’  
   Outputs cannot be less restricted than inputs without an explicit, reviewed redaction step.

10) **Licensing isnâ€™t optional.** âš–ï¸  
   License + attribution must propagate through catalogs, narratives, exports, and AI answers.

11) **Stable IDs are correctness features.** ğŸ·ï¸  
   IDs should be stable across reruns when inputs havenâ€™t changedâ€”treat IDs like API contracts.

> [!TIP]
> Motto: **â€œIf I canâ€™t reproduce it in 30 minutes, itâ€™s not complete.â€** â±ï¸âœ…

---

## ğŸ Golden paths

### ğŸŒ¿ Path A â€” â€œIâ€™m adding a dataset / layerâ€
1. ğŸ§² Intake source â†’ record checksum(s) + license + care label  
2. âš™ï¸ Run deterministic ETL â†’ write intermediates to `data/work/`  
3. ğŸ“¦ Write final artifacts to `data/processed/` *(versioned)*  
4. ğŸ—‚ï¸ Publish **STAC + DCAT** metadata  
5. ğŸ§¬ Publish **PROV** (inputs + activity + outputs)  
6. ğŸ” Run gates (contracts + catalogs + policy packs) â†’ write `GATE-...`  
7. ğŸƒ Record `RUN-...` receipt + update `TRACEABILITY.md`  
8. ğŸš€ Promote atomically (optional `PROMOTE-...`) â†’ then ingest to Graph/API

### ğŸ“š Path B â€” â€œIâ€™m shipping a Story Nodeâ€
1. âœï¸ Author story Markdown + storyboard JSON (declarative map/timeline state)  
2. ğŸ”— Ensure every claim cites cataloged evidence (no orphan links)  
3. ğŸ” Run Story gates (link integrity + citation coverage + sensitivity propagation)  
4. ğŸƒ Record receipt + review note if story is public-facing

### ğŸ¤– Path C â€” â€œIâ€™m updating Focus Mode / AI behaviorâ€
1. ğŸ§  Update model/prompt/retrieval rules + record exact version hashes  
2. ğŸ” Ensure policies enforce: citations required + refusal under uncertainty  
3. ğŸ§ª Run evaluation harness (citation coverage, refusal tests, bias checks)  
4. ğŸ§¾ Publish model card update + gate report + traceability entry

---

## ğŸ—‚ï¸ Directory layout

```text
ğŸ“ mcp/
â”œâ”€â”€ ğŸ“„ README.md                 # you are here ğŸ‘‹
â”œâ”€â”€ ğŸ“ experiments/              # experiment protocols ğŸ§ªğŸ§¾
â”œâ”€â”€ ğŸ“ runs/                     # immutable run receipts ğŸƒğŸ§¾
â”œâ”€â”€ ğŸ“ gates/                    # immutable gate reports ğŸ”âœ… (recommended)
â”œâ”€â”€ ğŸ“ promotions/               # atomic publish receipts ğŸš€ (optional)
â”œâ”€â”€ ğŸ“ dev_prov/                 # PR/build provenance bundles ğŸ§¬ (optional)
â”œâ”€â”€ ğŸ“ sops/                     # Standard Operating Procedures ğŸ§°
â”œâ”€â”€ ğŸ“ model_cards/              # model cards for ML/AI used or trained ğŸ§ 
â”œâ”€â”€ ğŸ“ datasheets/               # dataset datasheets (optional but valuable) ğŸ“„
â”œâ”€â”€ ğŸ“ glossary/                 # shared vocabulary across domains (optional) ğŸ“š
â”œâ”€â”€ ğŸ“ notebooks/                # tidy, reproducible notebooks ğŸ““
â”œâ”€â”€ ğŸ“ traceability/             # decision â†” evidence mapping ğŸ§­ (recommended)
â”œâ”€â”€ ğŸ“ reviews/                  # repro notes / governance sign-offs ğŸ‘€ (recommended)
â”œâ”€â”€ ğŸ“ incidents/                # post-mortems / anomaly reports ğŸ§¯ (optional)
â””â”€â”€ ğŸ“ templates/                # local templates ğŸ§© (or use docs/templates/)
```

> [!NOTE]
> Repo implementations vary. If `gates/`, `traceability/`, or `reviews/` donâ€™t exist yet, add themâ€”  
> policy packs + traceability + reproduction notes are core v13+ promises.

---

## ğŸ” The MCP workflow loop

KFM work is: **question â†’ protocol â†’ run â†’ gates â†’ evidence â†’ publish â†’ review â†’ iterate** ğŸ”

```mermaid
flowchart LR
  Q["â“ Question"] --> P["ğŸ§¾ Protocol (EXP)"]
  P --> R["ğŸƒ Run (RUN receipt)"]
  R --> G["ğŸ” Gates (policy + contracts)"]
  G --> E["ğŸ“¦ Evidence (data/processed + catalogs + PROV)"]
  E --> S["ğŸ§  Summary / Interpretation (limits + uncertainty)"]
  S --> V["ğŸ‘€ Review (repro + governance)"]
  V --> Q
```

---

## ğŸš¥ Detect â†’ Validate â†’ Promote

KFM risk comes from â€œhalf-publishedâ€ artifacts. MCP enforces a controlled rhythm:

1) **Detect** ğŸ•µï¸  
   Identify new inputs/changes (sources updated, schema drift, new tiles, new docs).

2) **Validate** âœ…  
   Run fast gates:
   - contract/schema checks (domain + metadata)
   - STAC/DCAT validation
   - PROV presence + link integrity
   - governance propagation (no downgrade)
   - security scans (secrets, unsafe deps, unsafe parsing)
   - **policy packs** (OPA/Conftest) ğŸ”

3) **Promote** ğŸš€  
   Only after validation:
   - write evidence to `data/processed/**`
   - write STAC/DCAT/PROV (provenance-first)
   - optionally export graph ingest bundles
   - record `RUN` + `GATE` + traceability update

> [!TIP]
> Treat â€œpromotionâ€ like a release: **atomic publish or nothing.** ğŸ§¾âœ…

---

## ğŸ” Policy packs & gate reports

Policy is a **first-class publish gate** (not a reviewer â€œnice-to-haveâ€).  
We expect:
- policy logic in versioned **OPA/Rego** packs
- execution via **Conftest** (or equivalent) in CI/local
- a **gate report** proving what ran and what passed

### ğŸ§° Gate taxonomy (recommended)
| Gate type | What it protects | Typical tooling |
|---|---|---|
| ğŸ§¾ Contract validation | Schema compatibility and drift control | JSON Schema / Pydantic / Great Expectations |
| ğŸ—‚ï¸ Catalog validity | STAC/DCAT structure + required fields | stac-validator, JSON Schema, custom checks |
| ğŸ§¬ PROV integrity | Lineage completeness + link integrity | JSON-LD validation, link checkers |
| ğŸ›¡ï¸ API boundary | Prevent bypass of governance | integration tests, lint rules |
| ğŸ” Governance propagation | No sensitivity downgrade, license propagation | OPA/Conftest policies |
| ğŸ¤– AI behavior | Citation coverage + refusal + hedging rules | evaluation harness + OPA |
| ğŸ§· Supply chain | SBOM + signed attestations + pinned deps | SLSA, Sigstore, SBOM tooling |
| ğŸ§¨ Security | Secrets, unsafe deps, hostile inputs | secret scanners, SAST, sandbox rules |

### âœ… What belongs in a gate report
- policy pack version (and commit hash)
- checks executed (schemas, link integrity, downgrade prevention, license propagation)
- inputs evaluated (catalog IDs / files)
- results (pass/fail + reasons)
- runner (human, CI, Wâ€‘Pâ€‘E Executor)
- pointers to logs (redacted if needed)

### ğŸ“„ Minimal `GATE-...` template (copy/paste)

```md
---
gate_id: GATE-YYYY-MM-DD-<slug>
date: YYYY-MM-DD
owner: "@github-handle"
scope: ["data", "catalogs", "prov", "graph", "api", "ui", "story", "ai"]
policy_pack:
  name: "kfm-policy-pack"
  version: "vX.Y.Z"
  commit: "<sha>"
runner:
  type: local | ci | wpe_executor
  run_id: "RUN-YYYY-MM-DD-<slug>"   # optional
inputs:
  - "data/stac/..."
  - "data/catalog/dcat/..."
  - "data/prov/..."
results:
  status: pass | fail
  checks:
    - name: "stac_validate"
      status: pass
    - name: "prov_present"
      status: pass
    - name: "no_sensitivity_downgrade"
      status: pass
    - name: "license_propagation"
      status: pass
    - name: "ai_citation_coverage"
      status: pass
notes: ""
---

# Summary ğŸ”âœ…
- What was evaluated and what changed?

# Findings ğŸ§¾
- Failures/warnings (if any) + remediation links

# Evidence ğŸ”—
- Pointers to relevant RUN + PROMOTE receipts
```

> [!IMPORTANT]
> If the gate fails, the publish is invalid. Fix â†’ rerun â†’ new gate report. âœ…

---

## ğŸ¤– Watcher â†’ Planner â†’ Executor receipts

KFMâ€™s roadmap uses a **Watcherâ€“Plannerâ€“Executor (Wâ€‘Pâ€‘E)** pattern for safe automation ğŸ¤ğŸ¤–

- **Watcher** observes and emits immutable events
- **Planner** proposes deterministic plans (often PR-ready diffs)
- **Executor** opens PRs / triggers pipeline execution **under the same gates as humans**
- **Kill switch** must exist to halt all agent actions immediately ğŸ§¯
- **Idempotency keys** prevent duplicate PRs/loops ğŸ”ğŸš«
- Agents are policy-constrained (OPA rules apply to bots, too) ğŸ”

### âœ… How MCP records Wâ€‘Pâ€‘E activity
Wâ€‘Pâ€‘E is â€œjust another run,â€ so it gets receipts:
- Watch events â†’ `mcp/runs/.../events/`
- Planner output â†’ `mcp/runs/.../plans/` (diffs, patches)
- Executor action â†’ `mcp/runs/.../` (logs + PR link) + **gate report**
- Traceability links: event â†’ plan â†’ PR â†’ merge â†’ artifacts

> [!TIP]
> Treat Wâ€‘Pâ€‘E like a â€œrobot teammateâ€: reproducible, reviewable, policy-gated. ğŸ¤ğŸ¤–âœ…

---

## ğŸ§¬ DevOps provenance & supply chain

KFM extends provenance to development and deployment: **DevOps â†’ PROV** ğŸ§¬  
So you can answer:
- Which code version produced this dataset?
- Who reviewed and approved the change?
- Which policy pack version gated publishing?

### âœ… Where to store DevOps PROV
Preferred:
- `data/prov/dev/DEVPROV-...jsonld`

Acceptable:
- `mcp/dev_prov/DEVPROV-...jsonld`

### ğŸ” Supply-chain expectations (recommended)
- SBOM attached to PRs/releases
- checksums for generated outputs
- signed build attestations
- pinned dependencies + vulnerability checks

> [!NOTE]
> This is compatible with high-assurance supply chain ideas (SLSA levels, Sigstore attestations) without forcing any single tool choice.

---

## ğŸ§¾ Front matter + schemas (machine-readable MCP)

MCP artifacts are human-first, but should be machine-readable for dashboards, validation, and CI.

### âœ… Front matter conventions (recommended)
All MCP artifacts should start with YAML front matter including:
- stable ID (`EXP-...`, `RUN-...`, `GATE-...`, `SOP-...`, `MODEL-...`)
- date, owner, status
- care label / classification
- AI involvement (if applicable)
- links to evidence (catalog IDs or paths)
- contracts + lineage pointers (STAC/DCAT/PROV)
- policy pack version + gate report IDs
- dev provenance pointers (optional)

### â­ â€œGoverned docâ€ extras (recommended)
- `doc_uuid` (stable UUID)
- `commit_sha` (when tied to a release)
- `doc_integrity_checksum` (optional)
- `policy_pack_version`
- `governance_ledger_ref` *(pointer only)*

### ğŸ” Minimal example
```yaml
---
id: EXP-2026-01-26-example
doc_uuid: "b0b6d0a5-4f4e-4e5b-9f25-1a2b1b9b0a7e"
title: "Example MCP artifact"
date: 2026-01-26
owner: "@github-handle"
status: draft
repro_level: L1
risk_level: medium
care_label: public
ai_used: false

policy_pack_version: "vX.Y.Z"
gate_reports: ["GATE-2026-01-26-example"]

contracts:
  - "schemas/mcp/exp.schema.json"
catalog_pointers:
  stac: ["stac://collection-id", "stac://item-id"]
  dcat: ["data/catalog/dcat/dataset.jsonld"]
  prov: ["data/prov/PROV-2026-01-26-example.jsonld"]
dev_prov: ["data/prov/dev/DEVPROV-2026-01-26-pr-123.jsonld"]
---
```

---

## âœ… Definition of done

### âœ… MCP â€œdoneâ€ means: reproducible + governed
For any EXP/RUN affecting production pipelines, APIs, UI layers, Story Nodes, dashboards, or Focus Mode:

- [ ] Front matter complete + consistent (IDs, dates, owner, status)
- [ ] Claims link to evidence inputs/outputs (**catalog pointers**)
- [ ] Contracts + lineage pointers present (STAC/DCAT/PROV)
- [ ] Policy pack gates executed and recorded (**gate report present**)
- [ ] Validation steps listed and repeatable
- [ ] Governance + FAIR/CARE + sovereignty considerations stated (when applicable)
- [ ] Another contributor can reproduce without tribal knowledge
- [ ] If AI was involved, human sign-off recorded

### ğŸ§± Reproducibility levels (recommended)
- **L0** ğŸŸ¡: exploratory note (not decision-worthy)
- **L1** ğŸŸ : reproducible by author (config + env captured)
- **L2** ğŸŸ¢: reproducible by reviewer (independent re-run)
- **L3** ğŸ†: CI-backed rerun (pipeline job + validators + policy packs)

---

## ğŸ“¦ Required artifacts

### âœ… â€œReal workâ€ minimum bar
If an experiment influences decisions, pipelines, published results, Story Nodes, or Focus Mode:

- ğŸ§ª Experiment report â†’ `mcp/experiments/...`
- ğŸƒ Run receipt â†’ `mcp/runs/...` (with `MANIFEST.md`)
- ğŸ” Gate report â†’ `mcp/gates/...` *(or in run folder)*
- ğŸ”— Code pointer â†’ commit hash + entrypoint
- ğŸ§± Environment snapshot â†’ Docker digest **or** lockfile/requirements
- ğŸ² Seeds/determinism flags (when applicable)
- ğŸ”¢ Input/output hashes (recommended)
- ğŸ“¦ Outputs in `data/processed/...`
- ğŸ—‚ï¸ Catalog records â†’ STAC/DCAT
- ğŸ§¬ Lineage â†’ PROV pointers
- ğŸ‘€ Review notes â†’ for L2/L3 (recommended)
- ğŸ§­ Traceability entry â†’ decision/feature â†” EXP/RUN/GATE â†” evidence

---

## ğŸ§ª Experiment reports

### ğŸ“› Naming convention
- `EXP-YYYY-MM-DD-<short-slug>.md`  
  Example: `EXP-2026-01-02-ocr-ner-baseline.md`

### ğŸ·ï¸ Status values
- `draft` ğŸ“ â€” in progress
- `complete` âœ… â€” reproducible; linked receipts + evidence + gate report
- `superseded` ğŸ§¯ â€” replaced by a newer experiment

### ğŸ§¾ Experiment template (copy/paste)

```md
---
id: EXP-YYYY-MM-DD-<slug>
title: "<short, explicit title>"
date: YYYY-MM-DD
owner: "@github-handle"
status: draft | complete | superseded
repro_level: L0 | L1 | L2 | L3
risk_level: low | medium | high
ai_used: true | false
care_label: public | restricted | confidential
tags: [gis, ocr, nlp, stac, dcat, prov, graph, sim, stats, web, security]

policy_pack_version: "vX.Y.Z"
gate_reports: []

contracts:
  - "schemas/mcp/exp.schema.json"
---

# Objective / Question â“
- What are we trying to learn or improve?

# Background / Prior Art ğŸ“š
- Links to prior experiments, issues, Story Nodes, or system docs.

# Hypothesis âœ…/âŒ
- What do we expect and why?

# Variables & Controls ğŸ›ï¸
- Variables changed
- Baselines/controls
- What stays fixed

# Inputs (Evidence In) ğŸ—ƒï¸
- Dataset IDs + STAC/DCAT pointers
- Sampling rules, time range, bbox
- Licensing + sensitivity notes

# Method / Protocol ğŸ§¾
- Step-by-step procedure
- Parameters + configs (link to run config)
- Tools + versions (OS/GPU/driver notes if relevant)

# Validation Plan âœ…
- What fails fast?
- What warns?
- What sanity checks must pass?
- What policy checks must pass?

# Run Receipt ğŸƒ
- Code commit: `abcdef1`
- Entrypoint: `src/...` or notebook path
- Run folder: `mcp/runs/RUN-YYYY-MM-DD-.../`
- Gate report: `mcp/gates/GATE-YYYY-MM-DD-...`
- Seeds: `...`

# Outputs (Evidence Out) ğŸ“¦
- Output paths under `data/processed/...`
- Catalog pointers:
  - STAC: `...`
  - DCAT: `...`
  - PROV: `...`

# Results ğŸ“ˆ
- Metrics/charts + 1â€“3 small sanity examples

# Uncertainty, Bias, and Validation ğŸ”
- What could be wrong?
- Checks performed (spot checks, leakage checks, error bounds)
- Bias risks / perspective gaps (esp. historical corpora)

# Interpretation ğŸ§ 
- What do results mean for KFM decisions?

# Decision / Next Steps ğŸ§­
- Adopt / iterate / abandon (and why)

# Reproducibility Checklist âœ…
- [ ] Parameters & configs documented
- [ ] Code committed + hash recorded
- [ ] Environment captured (Docker/lockfile)
- [ ] Inputs/outputs linked via STAC/DCAT/PROV
- [ ] Gate report recorded
- [ ] Reviewer can reproduce (for L2/L3)
```

---

## ğŸƒ Run receipts

Runs are the receipt for an experiment: what ran, how, where outputs went, and what changed.

### ğŸ“› Naming convention
- `RUN-YYYY-MM-DD-<slug>/`

### ğŸ“¦ Suggested run folder contents
- `config/` ğŸ§¾ â€” YAML/JSON config used
- `env/` ğŸ§± â€” lockfiles, Docker digest, OS info
- `logs/` ğŸªµ â€” structured logs (redacted if needed)
- `metrics/` ğŸ“ˆ â€” eval metrics (CSV/JSON)
- `artifacts/` ğŸ§© â€” small artifacts (thumbnails, samples)
- `gates/` ğŸ” â€” raw gate outputs (recommended)
- `events/` ğŸ‘€ â€” watcher events (if Wâ€‘Pâ€‘E)
- `plans/` ğŸ—ºï¸ â€” planner diffs/patches (if Wâ€‘Pâ€‘E)
- `hashes/` ğŸ”¢ â€” input/output checksums (recommended)
- `MANIFEST.md` ğŸ§¾ â€” reproduction instructions + evidence links

### ğŸ§¾ Minimal `MANIFEST.md` template (copy/paste)

```md
---
run_id: RUN-YYYY-MM-DD-<slug>
run_type: standard | benchmark | streaming | backfill | hotfix
related_experiment: EXP-YYYY-MM-DD-<slug>
date: YYYY-MM-DD
owner: "@github-handle"
env: dev | staging | prod
care_label: public | restricted | confidential

code:
  commit: abcdef1
  entrypoint: "src/pipelines/..."
  args: ["--config", "config/run.yml"]
  dirty_worktree: false

policy:
  policy_pack_version: "vX.Y.Z"
  gate_report: "mcp/gates/GATE-YYYY-MM-DD-<slug>.md"

environment:
  docker_image: "ghcr.io/org/project:tag@sha256:..."     # preferred
  # or:
  requirements: "env/requirements.lock.txt"
  os: "..."
  cpu: "..."
  gpu: "..."  # optional

randomness:
  seeds: [123, 456]
  deterministic_flags: ["..."]

contracts:
  - "schemas/<domain>/...schema.json"
  - "schemas/mcp/run.schema.json"

inputs:
  - catalog_ref: "stac://<collection_or_item_id>"
    sha256: "..."
  - dcat: "data/catalog/dcat/<dataset>.jsonld"
  - prov: "data/prov/<prior-run>.jsonld"

outputs:
  - path: "data/processed/<domain>/<dataset>/..."
    sha256: "..."
    stac_item: "data/stac/items/..."
    dcat: "data/catalog/dcat/..."
    prov: "data/prov/<run-id>.jsonld"

governance:
  ledger_ref: ""   # pointer only (optional)

validation:
  - "contract validation: pass/fail"
  - "schema validation (STAC/DCAT/PROV): pass/fail"
  - "link checks: pass/fail"
  - "classification propagation: pass/fail"
  - "spot checks: ..."

notes: ""
---

# Summary ğŸ§¾
- What did this run do?

# Evidence outputs ğŸ“¦
- Where outputs are stored (`data/processed/...`) + catalog IDs

# How to reproduce ğŸ”
1. Checkout commit: `abcdef1`
2. Restore environment: ...
3. Run: ...
4. Validate (contracts + catalogs + lineage + policy): ...
```

> [!TIP]
> New params, new inputs, new env, or new policy pack version â†’ new run folder âœ…

---

## ğŸ“ˆ Performance & scalability experiments

Performance claims can influence architecture decisions, so they still need receipts + gates.

### âœ… When to treat a run as a benchmark
- COG/PMTiles generation tuning (compression, overviews, tile size) ğŸ§Š
- PostGIS indexing/migrations affecting query latency ğŸ˜
- Graph ingest optimizations / analytics jobs ğŸ•¸ï¸
- Focus Mode latency or citation coverage regressions ğŸ¤–
- Offline pack generation (bundle size, time-to-build, cache correctness) ğŸ“¦

### ğŸ§¾ Benchmark add-ons (recommended in `MANIFEST.md`)
```yaml
performance:
  workload:
    description: "API tile requests @ zoom 6â€“12 for 3 AOIs"
    concurrency: 25
    warm_cache: true
    duration_s: 600
  dataset:
    size_bytes: 123456789
    feature_count: 987654
  system:
    cpu_cores: 16
    ram_gb: 64
    storage: "nvme"
  results:
    p50_ms: 120
    p95_ms: 410
    p99_ms: 900
    throughput_rps: 85
  artifacts:
    - "metrics/latency_histogram.csv"
    - "logs/explain_analyze.txt"
```

---

## ğŸ§° SOPs

SOPs turn â€œtribal knowledgeâ€ into repeatable procedures.  
Write an SOP whenever a task is repeated or risky: data intake, catalog publishing, redaction, OCR, tile generation, policy packs, Wâ€‘Pâ€‘E operations, offline packs, AR alignment, etc.

### SOP template (copy/paste)

```md
---
id: SOP-<topic>-v1.0.0
title: "<clear title>"
owner: "@github-handle"
last_updated: YYYY-MM-DD
risk_level: low | medium | high
care_label: public | restricted | confidential
---

# Purpose ğŸ¯
What this SOP accomplishes and when to use it.

# Scope âœ…
Whatâ€™s included / excluded.

# Prerequisites ğŸ§±
Accounts, tools, access, permissions.

# Tools & Versions ğŸ§°
Software + versions.

# Procedure ğŸ§­
1. Step...
2. Step...
3. Step...

# Verification âœ…
How to confirm it worked (checks + expected outputs).

# Troubleshooting ğŸ§¯
Common failure modes + fixes.

# Audit Trail ğŸ§¾
Example PRs / runs / experiments that used this SOP.
```

### â­ High-value SOPs to add (starter set)
- `sops/data_intake_connectors.md` ğŸ§² (source manifests, checksums, quarantine, streaming snapshots)
- `sops/publish_stac_dcat_prov.md` ğŸ—‚ï¸ğŸ§¬ (profiles, validation, link checks)
- `sops/policy_packs_and_gate_reports.md` ğŸ” (OPA/Conftest usage, failure triage)
- `sops/wpe_agents.md` ğŸ¤– (idempotency keys, kill switch, human approvals)
- `sops/offline_data_packs.md` ğŸ“¦ (bundle creation, size budgets, validation)
- `sops/ar_overlay_alignment.md` ğŸ•¶ï¸ (coordinate alignment, uncertainty, safety constraints)
- `sops/devops_prov_supply_chain.md` ğŸ§¬ (SBOM, attestations, linking to runs/datasets)
- `sops/redaction_and_sensitive_locations.md` ğŸ”’ (coarsening, approvals, redaction provenance)

---

## ğŸ§  Model cards

Any ML/AI model used in KFM needs a model card:
- what it is / where it came from
- intended use âœ… / prohibited use ğŸš«
- training data pointers (STAC/DCAT)
- evaluation + known failure modes
- governance labels + sensitivity constraints
- **citation coverage expectations** (Focus Mode must cite)
- drift/bias monitoring plan

### Model card template (copy/paste)

```md
---
model_id: MODEL-<name>-v<semver>
owner: "@github-handle"
date: YYYY-MM-DD
ai_used: true
source:
  type: trained | third_party
  license: "..."
  reference: "paper/link/registry id"
datasets:
  - "stac://<collection_or_item_id>"
  - "dcat://<dataset_id>"
governance:
  care_label: public | restricted | confidential
  human_in_the_loop: required | recommended | optional
  redaction_required: yes | no
evaluation:
  citation_coverage_required: true
  refusal_policy: "refuse-if-no-evidence"
monitoring:
  drift_checks: "monthly"
  bias_checks: "release-gated"
---

# Model overview ğŸ§ 
- What problem does it solve?

# Intended use âœ…
- Supported use-cases.

# Out-of-scope / prohibited use ğŸš«
- What it must not be used for.

# Training data ğŸ—ƒï¸
- Datasets (STAC/DCAT), sampling, labeling notes.
- Known gaps / perspective bias notes.

# Evaluation ğŸ“ˆ
- Metrics, test sets, qualitative examples.
- Citation coverage checks + refusal tests.

# Limitations & biases âš ï¸
- Known failure modes, bias risks, uncertainty notes.

# Governance & safety ğŸ”
- Redaction rules or sensitivity constraints.
- How outputs are labeled in UI / Focus Mode.

# Reproducibility ğŸ§ª
- Training code commit hash
- Environment / hardware notes
- Hyperparameters / config
- Seeds
- Artifact pointers stored under `data/processed/...` with catalogs + lineage
```

---

## ğŸ““ Notebooks

Notebooks are welcomeâ€”**but must be readable and reproducible**:
- start with purpose + inputs + outputs
- keep outputs small (no huge embedded blobs)
- prefer parameterized notebooks or export to scripts when it becomes â€œrealâ€
- if a notebook produces evidence:
  - store outputs in `data/processed/...`
  - link from EXP + RUN
  - policy-gate before publishing

---

## ğŸ§­ Traceability matrix

Traceability connects â€œworkâ€ to â€œwhy it mattersâ€:
- requirement/question
- EXP
- RUN
- GATE
- evidence outputs (+ STAC/DCAT/PROV)
- graph IDs (if applicable)
- API endpoints (if applicable)
- Story Node(s) / Focus Mode behavior (if applicable)
- DevOps PROV (optional)

### âœ… Recommended traceability table (copy/paste)

```md
| Decision / Feature | EXP | RUN | GATE | Evidence outputs (data/processed) | Catalog pointers (STAC/DCAT/PROV) | PR/Build PROV | Reviewer repro | Notes |
|---|---|---|---|---|---|---|---|---|
| "Offline pack for County X (1930s)" | EXP-... | RUN-... | GATE-... | data/processed/packs/county-x/1930s/... | STAC: ... / DCAT: ... / PROV: ... | DEVPROV: ... | @reviewer âœ… | size budget |
```

---

## ğŸ§¯ Bad evidence protocol

KFM must be resilient to â€œbad evidenceâ€ (messy scans, biased corpora, incomplete sensors, uncertain geocoding).  
When evidence is questionable, MCP requires restraint:

1) **Quarantine lane** ğŸš§  
   Keep known-bad/uncertain inputs staged until validated.

2) **Inferential restraint** ğŸ§ â¬‡ï¸  
   Reduce scope/strength of conclusions; report uncertainty.

3) **Executional restraint** ğŸ›‘  
   Donâ€™t ship to UI/Story/Focus until reviewed, or ship only aggregated/redacted views.

4) **Community verification (optional)** ğŸ¤  
   If used, it must have its own receipts, gates, and bias checks.

### âœ… Minimum documentation
- whatâ€™s wrong + how we know
- what we changed (or refused to change)
- what remains uncertain
- who reviewed the restraint decision
- whether gates/policies were updated

---

## ğŸ”— MCP â†” KFM evidence pipeline

KFM uses a strict evidence pipeline:

**ETL/Streaming â†’ Catalogs (STAC/DCAT/PROV) â†’ Graph â†’ API â†’ UI â†’ Story Nodes â†’ Focus Mode**

So for MCP work:
- âœ… Protocols: `mcp/experiments/...`
- âœ… Receipts: `mcp/runs/...`
- âœ… Gate proofs: `mcp/gates/...`
- âœ… Evidence artifacts: `data/processed/...` (plus catalogs + PROV)

```mermaid
flowchart LR
  EXP["ğŸ§ª EXP report"] --> RUN["ğŸƒ RUN receipt"]
  RUN --> GATE["ğŸ” GATE report"]
  GATE --> OUT["ğŸ“¦ data/processed outputs"]
  OUT --> CAT["ğŸ—‚ï¸ STAC/DCAT"]
  CAT --> PROV["ğŸ§¬ PROV lineage"]
  PROV --> GR["ğŸ•¸ï¸ Graph"]
  GR --> API["ğŸ›¡ï¸ APIs"]
  API --> UI["ğŸ—ºï¸ UI"]
  UI --> STORY["ğŸ“š Story Nodes"]
  STORY --> FM["ğŸ¤– Focus Mode (evidence-only)"]
```

---

## ğŸ§© Domain checklists

Use the checklist that matches your work:

### ğŸ—ºï¸ GIS / Remote Sensing
- [ ] CRS documented (EPSG + axis order)
- [ ] Georeferencing method + control points documented
- [ ] Fit error/RMS recorded (if applicable)
- [ ] Raster outputs are COGs / tiled (params documented)
- [ ] Vector outputs validate (geometry validity)
- [ ] Derived indices treated as evidence artifacts (params documented)
- [ ] Symbology/aggregation choices documented if they change interpretation ğŸ¨
- [ ] Catalog pointers included (STAC/DCAT) + lineage (PROV) ğŸ—‚ï¸ğŸ§¬
- [ ] Gate report recorded ğŸ”âœ…

### ğŸ” OCR / NLP
- [ ] Input corpus + sampling documented
- [ ] Labeling rules / rubric included
- [ ] Precision/recall (or spot-check protocol) documented
- [ ] Failure classes logged (scan quality, fonts, ambiguity)
- [ ] Geoparsing uncertainty documented (ambiguous place names, gazetteer limits)
- [ ] AI outputs include citations to source docs/entities (or refuse)

### ğŸ•¸ï¸ Graph analytics
- [ ] Graph schema/ontology version noted
- [ ] Metrics treated as signals, not facts (avoid over-interpretation)
- [ ] Provenance links from derived relations to source evidence
- [ ] No orphan IDs; referential integrity checks pass âœ…
- [ ] Derived relations are explainable (store â€œwhyâ€ pointers)

### ğŸ“¡ Streaming / Live data & dashboards
- [ ] Snapshot/aggregation cadence documented (windowing + retention)
- [ ] Each snapshot/aggregate has PROV activity + inputs
- [ ] â€œNowâ€ vs â€œhistoricalâ€ semantics explicit
- [ ] Quarantine behavior defined for anomalies/outliers
- [ ] Publish remains policy-gated (no exemptions)

### ğŸ“š Story Nodes & narratives
- [ ] Every claim cites evidence artifacts (STAC/DCAT/PROV pointers)
- [ ] Story config references valid layer IDs and time ranges
- [ ] Moderation workflow recorded (review note or PR approvals)
- [ ] Export includes citations/attribution automatically
- [ ] Link integrity checks pass (no dead dataset/story refs)

### ğŸ¤– Focus Mode / AI assistance
- [ ] Answers are evidence-bound (citations required)
- [ ] Refusal/uncertainty behavior tested
- [ ] Governance checks applied (sensitivity + policy rules)
- [ ] Drift/bias monitoring plan exists for deployed models
- [ ] Ledger references recorded when relevant (pointer only)

### ğŸ•¶ï¸ AR / Immersive / 4D experiments
- [ ] Coordinate alignment assumptions explicit (units, axis order, altitude datum)
- [ ] Uncertainty bounds disclosed (no â€œpreciseâ€ overlays without error bars)
- [ ] Sensitive locations protected (coarsen/offset/omit)
- [ ] Offline pack constraints documented if used in field mode
- [ ] Overlays remain evidence-cited (no â€œmagic AR factsâ€)

---

## ğŸ” Safety, privacy, licensing

- ğŸš« Donâ€™t store secrets/tokens/keys/PII in `mcp/`
- ğŸ§½ Redact logs before committing if they contain identifiers/endpoints/sensitive paths
- ğŸ§Š Prefer immutable receipts: new run folder > editing old run folder
- ğŸ§­ If superseded, mark `superseded` and link replacement
- ğŸ—ºï¸ Sensitive locations / culturally sensitive knowledge:
  - coarsen/offset/omit coordinates as required
  - require explicit permission & review before publishing
  - propagate sovereignty tags through catalogs, APIs, and UI
- âš–ï¸ Licensing must travel with evidence (derived artifacts must honor upstream constraints)

> [!TIP]
> Privacy is not only about raw dataâ€”**outputs can leak too**.  
> For high-risk domains, prefer aggregation, query auditing, and inference control gates.

---

## ğŸ›¡ï¸ Threat model & hostile inputs

KFM handles hostile-by-default artifacts: PDFs, images, archives, scraped web content.  
Treat every input as potentially maliciousâ€”even if it â€œlooks like a map.â€ ğŸ§¨

### âœ… Minimum defensive posture
- ğŸ§· Deny-by-default parsing (allow only expected MIME types/structures)
- ğŸ§ª Sandbox risky parsers (no network, least privilege)
- ğŸ§Š Decompression bomb limits (ZIP expansion, image decode caps, PDF object limits)
- ğŸ” Metadata stripping (avoid EXIF/path leaks)
- ğŸ§¾ Log redaction (no secrets, no tokens)
- âœ… Dependency hygiene (SBOMs, pinned versions, scanning)
- ğŸ¤– Prompt injection awareness (prompts are hostile input)
- ğŸ§± AI isolation: treat AI services as untrusted code (least privilege, protected creds, rate limits)

---

## âœ… QA, audits, and CI hooks

### CI intent (minimum bar)
- ğŸ§¹ lint + formatting (docs + code)
- âœ… unit tests (where applicable)
- ğŸ§¾ contract validation (schemas/profiles)
- ğŸ—‚ï¸ STAC/DCAT/PROV validation
- ğŸ”— link checks (assets exist; IDs resolve)
- ğŸ” policy pack eval (OPA/Conftest)
- ğŸ§· governance checks (classification propagation; redaction regressions)
- ğŸ¤– AI checks (citation coverage + refusal policy) when AI outputs exist
- ğŸ§¬ supply chain checks (SBOM, attestations) where required

### Weekly / regular audits (recommended)
- ğŸ•¸ï¸ Graph audit: dangling refs, required props, uniqueness constraints
- ğŸ—‚ï¸ Metadata audit: DCAT required fields, PROV structure validity
- ğŸ“š Story audit: Story Node references still valid (no references to deleted items)
- â³ Temporal audit: no timestamps outside stated coverage
- ğŸ” Permission audit: sensitive datasets not exposed in public UI
- ğŸ¤– AI audit sampling: Focus Mode logs for citation compliance + refusals
- ğŸ“ˆ Telemetry review: performance anomalies, misuse spikes

> [!TIP]
> Every incident should produce a new regression gate. ğŸ§¯âœ…

---

## ğŸ¤ PR / review checklist

When your PR includes experiments, runs, stories, or evidence:

- [ ] EXP report added/updated (if decision-relevant)
- [ ] RUN receipt folder added with `MANIFEST.md`
- [ ] Gate report recorded (or raw gate outputs in run folder)
- [ ] Evidence outputs stored under `data/processed/...` (or linked object storage)
- [ ] STAC/DCAT/PROV pointers included
- [ ] Policy packs pass
- [ ] AI involvement labeled (if applicable)
- [ ] Reproduction steps included (copy/paste runnable)
- [ ] No secrets / no sensitive leaks in logs or outputs
- [ ] Reviewer can reproduce (required for L2/L3)
- [ ] Story Nodes cite evidence artifacts and pass moderation checks

> [!TIP]
> A great review comment is: **â€œI reproduced this and got the same outputs.â€** âœ…

---

## ğŸ“š Project reference library influence map

> These project files inform how we design and review MCP artifacts: reproducibility, governance, policy enforcement, AI transparency, UI/story discipline, streaming receipts, and roadmap alignment.

<details>
<summary><strong>ğŸ“¦ Expand: Reference library â†’ what it influences in <code>mcp/</code></strong></summary>

| Project file | Primary lens | How it upgrades MCP |
|---|---|---|
| `MARKDOWN_GUIDE_v13.md.gdoc` | ğŸ§­ v13 invariants | Canon pipeline ordering, API boundary rule, provenance-first, deterministic ETL, evidence-first narrative |
| `ğŸ“š Kansas Frontier Matrix (KFM) â€“ Expanded Technical & Design Guide.pdf` | ğŸ§± System onboarding | Provenance-first philosophy, evidence-first triplet (STAC+DCAT+PROV), clean layered architecture, Focus Mode advisory-only |
| `Kansas Frontier Matrix (KFM) â€“ Comprehensive Technical Documentation.pdf` | âš™ï¸ Implementation detail | Hybrid DB strategy (PostGIS + graph + search), dual-format outputs (GeoParquet + PMTiles), supply-chain provenance patterns |
| `Kansas Frontier Matrix (KFM) â€“ Comprehensive Architecture, Features, and Design.pdf` | ğŸ§­ Architecture & governance | Governance ledger concept, immutable audit logs, privacy classification patterns, model/simulation governance |
| `Kansas Frontier Matrix (KFM) â€“ AI System Overview ğŸ§­ğŸ¤–.pdf` | ğŸ¤– Evidence-bound AI | Retrieval-first QA, citations enforced, governance check layer via policy engine, Wâ€‘Pâ€‘E safeguards (idempotency + kill switch + policy constraints) |
| `Kansas Frontier Matrix (KFM) â€“ Comprehensive UI System Overview (Technical Architecture Guide).pdf` | ğŸ—ºï¸ UI + Story Nodes | Story Node format (Markdown + JSON), declarative storyboard playback, moderation via PR workflow, citation UX patterns |
| `Kansas Frontier Matrix (KFM) â€“ Comprehensive Platform Overview and Roadmap.pdf` | ğŸ—ºï¸ Roadmap & audits | Weekly integrity audits, SHACL/JSON Schema checks, policy pack expansion, telemetry review practices |
| `Kansas-Frontier-Matrix Design Audit â€“ Gaps and Enhancement Opportunities.pdf` | ğŸ§¯ Design gaps | Highlights modularity/API boundaries, extension patterns, and where MCP receipts/gates close gaps |
| `Kansas-Frontier-Matrix_ Open-Source Geospatial Historical Mapping Hub Design.pdf` | ğŸ§¾ Repo practices | Model cards + datasheets, glossary discipline, issue/PR templates, versioning and traceability culture |
| `AI Concepts & more.pdf` | ğŸ§  AI shelf | Reliability & safety practices for AI/LLMs; supports stronger evaluation + prompt security posture |
| `Data Managment-Theories-Architures-Data Science-Baysian Methods-Some Programming Ideas.pdf` | âš™ï¸ Systems/methods shelf | Data architecture patterns, statistical rigor, uncertainty reporting; supports deterministic ETL + auditability |
| `Maps-GoogleMaps-VirtualWorlds-Archaeological-Computer Graphics-Geospatial-webgl.pdf` | ğŸŒ 3D/WebGL shelf | WebGL/virtual worlds concepts; supports AR/3D checklists + performance receipts |
| `Mapping-Modeling-Python-Git-HTTP-CSS-Docker-GraphQL-Data Compression-Linux-Security.pdf` | ğŸ§° Engineering shelf | Reproducible builds, Docker hygiene, API discipline, compression/performance notes |
| `Geographic Information-Security-Git-R coding-SciPy-MATLAB-ArcGIS-Apache Spark-Type Script-Web Applications.pdf` | ğŸ§° Applied stack shelf | GIS + security + scientific tooling references; informs SOPs and run receipts across toolchains |
| `Various programming langurages & resources 1.pdf` | ğŸ§° Polyglot shelf | Practical references (Git, shell, languages) supporting reproducible run discipline |
| `Data Mining Concepts & applictions.pdf` | ğŸ” Privacy methods | Reinforces privacy gates: aggregation, query auditing, inference control, differential privacy where needed |

</details>

---

## ğŸ§¾ Reference library index

This is the local reference library shaping MCP.  
Use it as scaffoldingâ€”**not** as a substitute for evidence artifacts. ğŸ“šğŸ§¾

<details>
<summary><strong>ğŸ“š Expand: Full index of consulted project files</strong></summary>

### ğŸ§­ Core KFM canon + blueprint (v13+)
- `MARKDOWN_GUIDE_v13.md.gdoc`
- `ğŸ“š Kansas Frontier Matrix (KFM) â€“ Expanded Technical & Design Guide.pdf`
- `Kansas Frontier Matrix (KFM) â€“ Comprehensive Technical Documentation.pdf`
- `Kansas Frontier Matrix (KFM) â€“ Comprehensive Architecture, Features, and Design.pdf`
- `Kansas Frontier Matrix (KFM) â€“ AI System Overview ğŸ§­ğŸ¤–.pdf`
- `Kansas Frontier Matrix (KFM) â€“ Comprehensive UI System Overview (Technical Architecture Guide).pdf`
- `Kansas Frontier Matrix (KFM) â€“ Comprehensive Platform Overview and Roadmap.pdf`

### ğŸ§¯ Audits & historical design docs
- `Kansas-Frontier-Matrix Design Audit â€“ Gaps and Enhancement Opportunities.pdf`
- `Kansas-Frontier-Matrix_ Open-Source Geospatial Historical Mapping Hub Design.pdf`

### ğŸ“¦ Reference shelves (background, not canon)
- `AI Concepts & more.pdf`
- `Data Managment-Theories-Architures-Data Science-Baysian Methods-Some Programming Ideas.pdf`
- `Maps-GoogleMaps-VirtualWorlds-Archaeological-Computer Graphics-Geospatial-webgl.pdf`
- `Mapping-Modeling-Python-Git-HTTP-CSS-Docker-GraphQL-Data Compression-Linux-Security.pdf`
- `Geographic Information-Security-Git-R coding-SciPy-MATLAB-ArcGIS-Apache Spark-Type Script-Web Applications.pdf`
- `Various programming langurages & resources 1.pdf`
- `Data Mining Concepts & applictions.pdf`

</details>

---

## ğŸ•°ï¸ Version history

| Version | Date | Summary | Author |
|---:|---|---|---|
| v1.6.0 | 2026-01-26 | Upgraded alignment to v13+ invariants: clarified **API boundary rule**, added **work vs processed** guidance, formalized **dual-format evidence** (analytics + UI tiles), expanded **Wâ€‘Pâ€‘E safeguards** (idempotency + kill switch + policy constraints), added **supply-chain provenance** expectations (SBOM/attestations), refreshed reference index to match current project files. | KFM Engineering |
| v1.5.0 | 2026-01-19 | Aligned MCP with KFM v13+ architecture: added **policy packs + gate reports**, clarified **atomic promotion** and **streaming provenance**, added **Watcherâ€“Plannerâ€“Executor receipts** and optional **DevOpsâ†’PROV** bundles, expanded Story/Focus/Offline/AR guidance, and updated reference influence map. | KFM Engineering |
| v1.4.0 | 2026-01-13 | Brought MCP in line with contract-first + provenance-first doctrine: added repo invariants, strengthened front matter (doc UUID + care_label), expanded run receipts to include hashes + contract validation, added benchmark/performance receipt guidance, and added threat-model/hostile-input section. | KFM Engineering |
| v1.3.0 | 2026-01-11 | Tightened MCP into a typed, machine-readable â€œmethods + receiptsâ€ layer: added artifact/ID table, Detectâ†’Validateâ†’Promote guidance, front matter + schema notes, expanded run manifest to include policy checks/attestation hooks, added incident/review artifact guidance. | KFM Engineering |
| v1.2.0 | 2026-01-09 | Upgraded MCP to align with evidence-first/contract-first doctrine: added definition-of-done, reproducibility levels, traceability matrix, bad-evidence restraint protocol, expanded governance/licensing/sensitive-location guidance. | KFM Engineering |
| v1.1.0 | 2026-01-06 | Clarified nonâ€‘negotiables + pipeline linkage; added run receipt template, PR checklist, and workflow diagram. | KFM Engineering |
| v1.0.0 | 2025-12-31 | Initial MCP README: experiments, runs, SOPs, model cards, notebooks, safety rules. | KFM Engineering |

---

ğŸ§­ **Goal:** Make every output auditable and every method teachable.  
ğŸ§¾ **Promise:** If itâ€™s in production, it has a paper trail. âœ…