<!-- According to a document from 2026-01-22: this template follows the KFM â€œSTAC + DCAT + PROVâ€ evidence-first, catalog-driven approach. -->

# ğŸ—ƒï¸ DCAT Datasets â€” Experiment Report Template

`ğŸ§ª TEMPLATE` `ğŸ“¦ dcat:Dataset` `ğŸ”— STAC + PROV` `âœ… CI / Policy Gates` `ğŸ§­ KFM-aligned`

This folder is the **dataset-level discovery layer** for an experiment report.  
If your experiment **uses**, **produces**, or **cites** a dataset, it should have a **DCAT dataset entry** here.

**What you get (when you do this right):**
- ğŸ” **Discoverability** (search + filters + external harvesters)
- ğŸ§¾ **Attribution & licensing** (UI â€œSource: â€¦â€ + exports)
- â›“ï¸ **Traceability** (every dataset links to STAC + PROV)
- ğŸ¤– **Explainable AI** (Focus Mode citations, no-source = no-answer)

---

## ğŸ“ Folder Contract

```text
dcat/
  datasets/
    README.md
    dataset--<dataset_id>.jsonld          # âœ… preferred (JSON-LD)
    dataset--<dataset_id>.ttl             # optional (Turtle)
  catalog.jsonld                          # optional roll-up catalog
```

> âœ… Rule of thumb: If it can show up in the UI, in search, in a report, or in Focus Mode as something you can citeâ€¦ it should have a DCAT entry.

---

## ğŸ§  Core Concepts

### Dataset vs Distribution
- **Dataset** = the â€œwhatâ€  
  Human-facing description + governance + scope (spatial/temporal/theme).
- **Distribution** = the â€œhow to access itâ€  
  Download file, tile endpoint, API query, STAC collection, OCI artifact, offline pack, etc.

### Why DCAT lives next to STAC & PROV
This template assumes a **3-layer evidence spine**:

- **STAC** â†’ geospatial asset indexing (spatial/temporal + asset pointers)  
- **DCAT** â†’ dataset catalog (discovery + license + publisher + keywords)  
- **PROV** â†’ lineage (inputs â†’ activities â†’ outputs, plus agents/run IDs)

That triad is the â€œno mystery layersâ€ rule: every dataset is **findable** and **auditable**.

---

## ğŸ·ï¸ Naming & IDs

### âœ… Canonical Dataset ID (stable + versioned)
Use a stable ID (example pattern):

- `kfm.ks.landcover.2000_2020.v1`

**Recommended filename:**
- `dataset--kfm.ks.landcover.2000_2020.v1.jsonld`

> ğŸ” If the meaning changes, bump the version (`v2`) and create a new dataset file.  
> âœ… IDs should be immutable; history lives in versioning + provenance.

### ğŸ§¾ Required Local Extensions
At minimum, datasets should carry:

- `kfm:dataset_id` (canonical ID)
- `kfm:classification` (`public` | `internal` | `restricted` | â€¦)

You can extend further (sovereignty/sensitivity, uncertainty flags, etc.) via your project DCAT profile.

---

## âœ… Minimum Required Fields (Checklist)

- [ ] `@id` (stable URI/URN for the dataset)
- [ ] `@type: dcat:Dataset`
- [ ] `kfm:dataset_id`
- [ ] `dct:title`
- [ ] `dct:description`
- [ ] `dct:publisher`
- [ ] `dct:license`
- [ ] `dcat:keyword` (â‰¥ 3 recommended)
- [ ] `dct:spatial` (bbox/region)
- [ ] `dct:temporal` (start/end)
- [ ] `kfm:classification`
- [ ] `dcat:distribution` (**must include STAC + PROV at minimum**)

---

## ğŸ”— Cross-Linking Rules (STAC + PROV)

### Required distributions
Every dataset should include distributions for:

1) **STAC Collection** (and/or key STAC Item(s))  
2) **PROV bundle** (JSON-LD)

Optionally include:
- processed file downloads (GeoParquet/COG/CSV/etc.)
- tile endpoints (vector/raster)
- API endpoints for queryable tables
- OCI artifact references (oras + cosign)
- offline packs (PMTiles, packaged COG sets, etc.)
- model cards / scenario docs for simulations

---

## ğŸ§© Template: Minimal JSON-LD Dataset

<details>
<summary>ğŸ“„ Click to expand: dataset JSON-LD skeleton (copy/paste)</summary>

```json
{
  "@context": {
    "dcat": "http://www.w3.org/ns/dcat#",
    "dct": "http://purl.org/dc/terms/",
    "prov": "http://www.w3.org/ns/prov#",
    "kfm": "urn:kfm:terms:"
  },
  "@id": "urn:kfm:dataset:kfm.ks.landcover.2000_2020.v1",
  "@type": "dcat:Dataset",

  "kfm:dataset_id": "kfm.ks.landcover.2000_2020.v1",
  "kfm:classification": "public",

  "dct:title": "Kansas Landcover 2000â€“2020",
  "dct:description": "Landcover classifications for Kansas from 2000 through 2020, published as annual rasters and derived summaries.",
  "dct:publisher": { "@id": "urn:kfm:org:kansas-gis" },
  "dct:license": { "@id": "urn:license:CC-BY-4.0" },

  "dcat:keyword": ["kansas", "landcover", "remote sensing", "raster"],

  "dct:spatial": {
    "@type": "dct:Location",
    "dcat:bbox": "POLYGON((...))"
  },
  "dct:temporal": {
    "@type": "dct:PeriodOfTime",
    "dcat:startDate": "2000-01-01",
    "dcat:endDate": "2020-12-31"
  },

  "dcat:distribution": [
    {
      "@type": "dcat:Distribution",
      "dct:title": "STAC collection",
      "dcat:mediaType": "application/json",
      "dcat:accessURL": "../../stac/collections/kfm.ks.landcover.2000_2020.v1.json"
    },
    {
      "@type": "dcat:Distribution",
      "dct:title": "PROV lineage bundle (JSON-LD)",
      "dcat:mediaType": "application/ld+json",
      "dcat:accessURL": "../../prov/kfm.ks.landcover.2000_2020.v1.prov.jsonld"
    }
  ]
}
```

</details>

---

## ğŸ§  Themes, Ontology & Concept Nodes

To make datasets searchable and narratable, tag them consistently.

**Recommended pattern**
- Use `dcat:theme` to point to stable concept IDs (concept nodes / controlled vocabulary).
- Keep `dcat:keyword` for human-friendly search phrases.

Example snippet:

```json
{
  "dcat:theme": [
    { "@id": "urn:kfm:concept:landcover" },
    { "@id": "urn:kfm:concept:remote_sensing" }
  ],
  "dcat:keyword": ["landcover", "classification", "satellite imagery"]
}
```

> ğŸ§© This keeps â€œthemesâ€ machine-stable (good for graph + AI) while keywords stay flexible (good for UX).

---

## ğŸ§¬ Reproducibility Hooks (dev_prov-friendly)

Even though lineage â€œlivesâ€ in PROV, itâ€™s useful to leave breadcrumbs in DCAT for quick inspection:

- pipeline run ID
- commit hash
- PR/review reference
- scenario/model card reference (for simulations)

Example add-on:

```json
{
  "dct:provenance": {
    "@id": "urn:kfm:prov:bundle:kfm.ks.landcover.2000_2020.v1"
  },
  "prov:wasGeneratedBy": {
    "@id": "urn:kfm:prov:activity:run_2026_01_22T1200Z"
  }
}
```

---

## ğŸ“¦ Distribution Patterns (Pick what fits)

### 1) File download (static artifact)
Use when you have a stable file (GeoParquet, CSV, COG, etc.).

```json
{
  "@type": "dcat:Distribution",
  "dct:title": "Processed GeoParquet (authoritative)",
  "dcat:mediaType": "application/x-parquet",
  "dcat:downloadURL": "../../artifacts/processed/kfm.ks.landcover.2000_2020.v1.geoparquet"
}
```

### 2) API access (queryable / real-time)
Use for tables served through an API (PostGIS-backed, timeseries, etc.).

```json
{
  "@type": "dcat:Distribution",
  "dct:title": "API query endpoint (GeoJSON)",
  "dcat:mediaType": "application/geo+json",
  "dcat:accessURL": "/api/v1/query?table=geo_counties"
}
```

> âš ï¸ â€œLiveâ€ sources are still **cataloged** here; the runtime query is separate, but identity/licensing/classification still come from DCAT.

### 3) Tiles / map delivery
Use for web-friendly serving (vector tiles, raster tiles, etc.).

```json
{
  "@type": "dcat:Distribution",
  "dct:title": "Vector tiles (XYZ)",
  "dcat:mediaType": "application/x-protobuf",
  "dcat:accessURL": "/tiles/kfm.ks.landcover/{z}/{x}/{y}.pbf"
}
```

### 4) Offline packs (field mode)
Use for field-ready bundles (PMTiles, packaged rasters, etc.).

```json
{
  "@type": "dcat:Distribution",
  "dct:title": "Offline PMTiles pack",
  "dcat:mediaType": "application/vnd.pmtiles",
  "dcat:downloadURL": "../../offline/packs/kfm.ks.landcover.2000_2020.v1.pmtiles"
}
```

### 5) OCI artifacts (reproducible, signed)
Use when the dataset is published as an OCI artifact (oras) and signed (cosign).

```json
{
  "@type": "dcat:Distribution",
  "dct:title": "OCI artifact (oras)",
  "dcat:mediaType": "application/vnd.oci.image.manifest.v1+json",
  "dcat:accessURL": "oci://registry.example/kfm/landcover:2000_2020-v1"
}
```

---

## â± Rapid Data, Simulations & Scenario Outputs

If your experiment outputs scenarios (future projections, â€œwhat-ifâ€ runs, digital-twin slices):

- treat each scenario output as a **Dataset**
- include a distribution for the **model card / scenario notes**
- include PROV details (inputs, parameters, run IDs)

**UI hint:** scenario compare becomes easier when each scenario is a separately cataloged dataset.

---

## ğŸ›¡ï¸ Sensitivity, Sovereignty & Ethics

If a dataset has restrictions (private land monitors, culturally sensitive locations, etc.):

- set `kfm:classification` appropriately
- add explicit access notes (policy tags / redaction expectations)
- ensure derived distributions do not leak sensitive coordinates
- prefer generalized/blurred spatial coverage in public-facing metadata when needed

> ğŸ”’ Principle: **no bypassing catalogs + provenance required** applies to sensitive and real-time layers too.

---

## ğŸ¤– AI + UI Expectations

Your DCAT record is used by:

- ğŸ—ºï¸ UI legend & layer info (â€œSource: â€¦â€ pulled from metadata)
- ğŸ” dataset discovery & faceted search (keywords/themes/spatiotemporal)
- ğŸ§­ Focus Mode citations (no-source = no-answer)
- ğŸ§¬ knowledge graph ingestion (Dataset nodes come from DCAT)

So keep:
- titles human-readable
- descriptions crisp but complete
- keywords consistent
- licenses explicit
- classification accurate (drives redaction + display rules)

---

## âœ… Validation & CI Gates (Metadata-as-Code)

Before merging:
- run schema validation (JSON Schema / SHACL)
- run policy checks (license present, classification present, provenance linked)
- ensure STAC + PROV paths resolve
- fail closed (no â€œbest effortâ€ publishing)

Suggested local workflow:
```bash
# Example commands â€” wire these to your repo tooling
make validate-dcat
make validate-stac
make validate-prov
make policy-check
```

---

## ğŸ§¾ PR Reviewer Checklist

- [ ] DCAT file added/updated under `dcat/datasets/`
- [ ] `kfm:dataset_id` matches filename and STAC/PROV IDs
- [ ] `dct:license` is present and correct
- [ ] `kfm:classification` is present and correct
- [ ] `dcat:distribution` includes STAC + PROV links
- [ ] If AI assisted, it is labeled + provenance is captured
- [ ] If sensitive, redaction rules are documented and tested

---

## ğŸ§µ Bonus Pattern: â€œNarratives & Answers are Datasets Tooâ€
If your experiment report produces:
- an analysis narrative
- an AI-generated answer bundle
- a story node / pulse thread / derived interpretation

â€¦treat it as a **dataset**, publish it here, and link the supporting evidence via PROV.

That keeps â€œreport artifactsâ€ citeable and auditable just like â€œdata artifactsâ€.

---

## ğŸ“š Project Docs This README Aligns With (KFM Bundle)

- `ğŸ“š Kansas Frontier Matrix (KFM) Data Intake â€“ Technical & Design Guide.pdf`
- `Kansas Frontier Matrix (KFM) â€“ Comprehensive Architecture, Features, and Design.pdf`
- `Kansas Frontier Matrix (KFM) â€“ AI System Overview ğŸ§­ğŸ¤–.pdf`
- `Kansas Frontier Matrix â€“ Comprehensive UI System Overview.pdf`
- `Kansas Frontier Matrix (KFM) â€“ Comprehensive Technical Documentation.pdf`
- `ğŸŒŸ Kansas Frontier Matrix â€“ Latest Ideas & Future Proposals.docx.pdf`
- `Innovative Concepts to Evolve the Kansas Frontier Matrix (KFM).pdf`
- `Additional Project Ideas.pdf`
- `AI Concepts & more.pdf` (PDF portfolio)
- `Maps-GoogleMaps-VirtualWorlds-Archaeological-Computer Graphics-Geospatial-webgl.pdf` (PDF portfolio)
- `Various programming langurages & resources 1.pdf` (PDF portfolio)
- `Data Managment-Theories-Architures-Data Science-Baysian Methods-Some Programming Ideas.pdf` (PDF portfolio)
