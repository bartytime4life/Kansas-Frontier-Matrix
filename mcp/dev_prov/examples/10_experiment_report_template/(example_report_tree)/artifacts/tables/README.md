# ğŸ“Š Tables (Experiment Artifacts)

![Artifact](https://img.shields.io/badge/artifact-tables-2ea44f) ![Format](https://img.shields.io/badge/formats-csv%20%7C%20tsv%20%7C%20parquet%20%7C%20md-blue) ![Goal](https://img.shields.io/badge/goal-reproducible%20%26%20auditable-purple)

> [!NOTE]
> This folder stores **tabular outputs** produced by an experiment run (metrics, comparisons, traceability, audits).  
> Keep tables **portable**, **deterministic**, and **provenance-linked** â€” â€œnumbers with receiptsâ€ ğŸ§¾âœ¨

---

## âœ… What belongs here?

Put a table here when it:
- backs a claim in the experiment report (â€œModel A improved recall by 6%â€)
- summarizes results across runs / datasets / configs
- acts as a â€œledgerâ€ linking artifacts together (traceability, model registry, dataset manifest)
- is something youâ€™d want to plot later (dashboards, regression tracking, drift monitoring)

---

## ğŸ§± Recommended file layout

```text
ğŸ“ artifacts/
  ğŸ“ tables/
    ğŸ“„ README.md
    ğŸ“„ tables_manifest.yml            # âœ… index of tables + provenance pointers
    ğŸ“„ TBL-001_traceability_matrix.csv
    ğŸ“„ TBL-010_metrics_summary.csv
    ğŸ“„ TBL-020_ablation_study.csv
    ğŸ“„ TBL-030_error_slices.csv
    ğŸ“„ TBL-040_model_registry.csv
    ğŸ“„ TBL-900_governance_redactions.csv
    ğŸ“ _snapshots/                    # optional: exact â€œas-usedâ€ exports
```

> [!TIP]
> If the table is **big** or **multi-typed**, prefer **Parquet** (and/or GeoParquet for geospatial).  
> If the table is **human-first**, a Markdown table is fine â€” but still index it in the manifest.

---

## ğŸ§¾ Table manifest (required)

Every table in this folder should be listed in **`tables_manifest.yml`** so the experiment report (and tooling) can discover and validate artifacts.

### Minimal manifest shape

```yaml
tables:
  - table_id: "TBL-010"
    title: "Metrics Summary"
    file: "TBL-010_metrics_summary.csv"
    purpose: "Primary results table for the report"
    produced_by:
      run_id: "RUN-2026-01-22T031500Z"
      code_ref: "git:commit:<sha>"
      pipeline: "mcp/dev_prov/examples/10_experiment_report_template"
    inputs:
      - kind: "dataset"
        ref: "dcat:<dataset-id-or-uri>"
      - kind: "artifact"
        ref: "../figures/FIG-010_roc_curve.png"
    provenance:
      prov_ref: "../prov/PROV-RUN-2026-01-22T031500Z.jsonld"
      evidence_manifest_ref: "../evidence/EM-010.yaml"
    governance:
      sensitivity: "public"
      care_label: "none"
      contains_pii: false
    integrity:
      sha256: "<optional-but-nice>"
```

> [!IMPORTANT]
> Keep the **data table clean** (columns that matter to analysis).  
> Put â€œmetaâ€ in the **manifest** and/or a **PROV/evidence manifest** sidecar.

---

## ğŸ·ï¸ Naming conventions

### File naming
Use stable, sortable, greppable names:

- `TBL-###_<short_slug>.<ext>`
- Prefer `snake_case` for slugs
- Keep IDs stable once referenced in a report

Examples:
- `TBL-001_traceability_matrix.csv`
- `TBL-010_metrics_summary.csv`
- `TBL-040_model_registry.csv`
- `TBL-900_governance_redactions.csv`

### Column naming
- `snake_case`
- include units in the header when ambiguous: `area_sq_km`, `temp_c`, `lat_deg`
- avoid â€œmagicâ€ abbreviations unless documented

---

## ğŸ“¦ Preferred formats (pick the lightest tool that works)

| Format | Use when | Pros | Watch-outs |
|---|---|---|---|
| `.csv` | default | universal, diffable-ish | escaping/encoding; no types |
| `.tsv` | many text fields | fewer quoting headaches | still no types |
| `.parquet` | large / typed / repeated | compact, typed, fast | not as human-friendly |
| `.md` | tiny, human-first tables | reads great in GitHub | not ideal for stats/plots |
| `.xlsx` | **avoid** (unless required) | stakeholder convenience | not diff-friendly; always export csv too |

---

## ğŸ§ª â€œStandard tablesâ€ we expect to see

### 1) Traceability matrix (high value)
A birds-eye table that connects **experiment â†’ hypothesis/feature â†’ code â†’ data/model â†’ result references**.

Suggested columns:
- `experiment_id`
- `hypothesis_or_feature`
- `code_version`
- `data_version`
- `model_version`
- `result_refs` (paths/IDs to figures/tables/artifacts)

---

### 2) Metrics summary (primary results)
Suggested columns:
- `run_id`, `model_id`, `dataset_id`, `split`
- `metric_name`, `metric_value`
- `ci_low`, `ci_high` (optional)
- `notes`

---

### 3) Ablation / baseline comparison
Suggested columns:
- `variant_id`, `variant_desc`
- `baseline_metric`, `candidate_metric`, `delta`
- `n`, `stat_test`, `p_value` (optional)

---

### 4) Error slices / fairness / robustness
Suggested columns:
- `slice_key`, `slice_value`
- `count`, `metric_value`
- `risk_notes`, `mitigation_ref`

> [!CAUTION]
> If slices could reveal sensitive attributes or locations, aggregate/blur/redact and document it in `TBL-900_*`.

---

### 5) Model registry (when you train/produce models)
A simple â€œtable of modelsâ€ so any model can be traced back to the conditions that produced it:
- `model_id`
- `training_data_ref` (+ version)
- `code_ref`
- `params_ref`
- `evaluation_ref`

---

## ğŸ” Determinism rules (so diffs donâ€™t lie)

- **Sort rows** by a stable key (e.g., `run_id`, `model_id`, `metric_name`)
- **Round floats** consistently (pick a precision and stick to it)
- **Use UTF-8**
- **No hidden filters** (if exporting from notebooks/spreadsheets, double-check)

---

## ğŸ›¡ï¸ Safety, governance, and redaction

If the experiment touches sensitive info (PII, vulnerable locations, restricted sources):
- **do not** store raw sensitive rows here
- store aggregated/blurred outputs + a short explanation table like:
  - `TBL-900_governance_redactions.csv`

Recommended columns:
- `artifact_id`, `risk_type`, `what_was_removed`, `why`, `approved_by`, `policy_ref`

---

## ğŸ§© How tables get used downstream (UI + reports)

Tables in this folder may be:
- embedded in the experiment report as â€œresultsâ€
- plotted into figures
- promoted into dashboards/time-series monitoring
- indexed for search and future â€œevidence manifestsâ€

> [!TIP]
> If a table becomes a long-lived product (not just a one-off experiment), consider â€œpromotingâ€ it into the platformâ€™s dataset/catalog patterns (with STAC/DCAT/PROV + policy gates).

---

## ğŸ”— How to reference tables from the experiment report

In your experiment report Markdown:

```md
See the full metrics breakdown in: [TBL-010 Metrics Summary](./artifacts/tables/TBL-010_metrics_summary.csv)
```

For wide tables:
- link to the file
- add a short top-line summary in the report body

---

## âœ… Definition of Done (DoD) for tables âœ…

- [ ] Table is listed in `tables_manifest.yml`
- [ ] Filename follows `TBL-###_<slug>.<ext>`
- [ ] Rows are deterministically sorted
- [ ] Units/definitions are clear (or linked)
- [ ] Provenance pointers exist (run id, code ref, input refs)
- [ ] No sensitive data leaks (or redaction table included)
- [ ] Report links to the table from the relevant section

---

## ğŸ§  Pro tips (tiny things that save hours)

- Keep an â€œID columnâ€ even if it feels redundant (future joins will thank you ğŸ™)
- Prefer â€œlong formâ€ for metrics (`metric_name`, `metric_value`) when you expect new metrics
- When comparing runs, store both:
  - the **raw metrics table**
  - the **comparison/delta table** (so your narrative is directly supported)

---
