# ğŸ“š Artifact Catalogs â€” *Experiment Report Spine* (STAC Â· DCAT Â· PROV)

![MCP](https://img.shields.io/badge/MCP-dev__prov-6e40c9?style=flat)
![Template](https://img.shields.io/badge/example-10__experiment__report__template-0ea5e9?style=flat)
![Provenance](https://img.shields.io/badge/provenance-first-22c55e?style=flat)
![STAC](https://img.shields.io/badge/STAC-geospatial-2563eb?style=flat)
![DCAT](https://img.shields.io/badge/DCAT-JSON--LD-f97316?style=flat)
![PROV](https://img.shields.io/badge/W3C%20PROV-lineage-111827?style=flat)
![FAIR+CARE](https://img.shields.io/badge/FAIR%20%2B%20CARE-governed-14b8a6?style=flat)

> âœ… This folder contains the **machine-readable catalogs** that make an experiment report **reproducible, auditable, and linkable** (from raw sources â†’ processing â†’ datasets â†’ narratives/claims).

---

## ğŸ§­ What lives in `artifacts/catalogs/`?

This directory is the **catalog snapshot** for an experiment report run. Think of it as:

- ğŸ§¾ **Metadata catalogs** for inputs & outputs (STAC / DCAT / PROV)
- ğŸ§· **Indexes/manifests** that glue everything together (what files exist, where they are, and how to resolve IDs)
- ğŸ›¡ï¸ **Validation + policy evidence** (optional but recommended) to prove gates passed
- ğŸ” **Attestations** (optional) to prove integrity and provenance of large/binary artifacts stored elsewhere

> ğŸ’¡ **Rule of thumb:** store **metadata** here, not large data. Big files belong in `artifacts/data/` (or an OCI registry) and are referenced *from* these catalogs.

---

## ğŸ—ºï¸ Quick map

- **Humans** read the report narrative elsewhere (`report.md`, `sections/`, etc.)
- **Machines** read this folder to:
  - rebuild a run ğŸ§ª
  - verify sources ğŸ”
  - trace lineage ğŸ§¬
  - enforce policy ğŸ”’
  - power UI + AI citations ğŸ§ ğŸ—ºï¸

---

## ğŸ“¦ Recommended layout (template-friendly)

> This template supports flexible layouts, but the structure below is the â€œhappy pathâ€ for KFM-style provenance-first reporting.

```text
ğŸ“¦ artifacts/catalogs/
â”œâ”€ ğŸ“„ README.md                      â† you are here
â”œâ”€ ğŸ“„ index.json                     â† one-file entrypoint (recommended)
â”‚
â”œâ”€ ğŸ“‚ stac/                          â† spatial/temporal asset catalog
â”‚  â”œâ”€ ğŸ“„ collection.json
â”‚  â””â”€ ğŸ“‚ items/
â”‚     â”œâ”€ ğŸ“„ <item-id>.json
â”‚     â””â”€ ğŸ“„ ...
â”‚
â”œâ”€ ğŸ“‚ dcat/                          â† dataset discovery catalog (JSON-LD)
â”‚  â”œâ”€ ğŸ“„ dataset.jsonld
â”‚  â””â”€ ğŸ“„ distributions.jsonld        â† optional split-out
â”‚
â”œâ”€ ğŸ“‚ prov/                          â† provenance bundles (JSON-LD)
â”‚  â”œâ”€ ğŸ“„ run.prov.jsonld
â”‚  â””â”€ ğŸ“„ activities/                 â† optional per-step provenance
â”‚     â”œâ”€ ğŸ“„ <activity-id>.prov.jsonld
â”‚     â””â”€ ğŸ“„ ...
â”‚
â”œâ”€ ğŸ“‚ manifests/                     â† â€œwhat happenedâ€ & â€œwhat changedâ€
â”‚  â”œâ”€ ğŸ“„ run_manifest.json           â† inputs/outputs/tooling counts + hashes
â”‚  â”œâ”€ ğŸ“„ params.json                 â† the experiment config snapshot
â”‚  â””â”€ ğŸ“„ environment.json            â† runtime env / container image / versions
â”‚
â”œâ”€ ğŸ“‚ evidence/                      â† narrative evidence inventories
â”‚  â”œâ”€ ğŸ“„ EM-<id>.yaml                 â† evidence manifest(s) for claims/plots
â”‚  â””â”€ ğŸ“„ ...
â”‚
â”œâ”€ ğŸ“‚ checks/                        â† proof gates passed (optional but ğŸ”¥)
â”‚  â”œâ”€ ğŸ“„ schema_validation.json
â”‚  â”œâ”€ ğŸ“„ policy_gate.conftest.json
â”‚  â”œâ”€ ğŸ“„ graph_health.json
â”‚  â””â”€ ğŸ“„ qa_summary.md
â”‚
â””â”€ ğŸ“‚ security/                      â† supply-chain & artifact integrity
   â”œâ”€ ğŸ“„ sbom.spdx.json
   â””â”€ ğŸ“‚ attestations/
      â”œâ”€ ğŸ“„ provenance.intoto.json
      â””â”€ ğŸ“„ cosign.bundle.json
```

---

## ğŸ§© Core catalogs (required)

These are the **boundary artifacts** that connect the pipeline to downstream systems (graph/API/UI/AI).

| Catalog | What it answers | Typical format | Minimum expectation |
|---|---|---:|---|
| ğŸ›°ï¸ **STAC** | â€œWhat is this geospatial asset? Where/when? What files?â€ | JSON | Has spatial/temporal extents + assets + links |
| ğŸ—‚ï¸ **DCAT** | â€œHow do I discover this dataset? What is the license? How do I access it?â€ | JSON-LD | Has title/desc/license/keywords + distributions |
| ğŸ§¬ **PROV** | â€œHow was this produced? From what sources? By who/what?â€ | JSON-LD | Lists inputs, activities, agents, outputs |

> ğŸ“Œ In KFM-style systems, **nothing becomes â€œpublishedâ€** unless these catalogs exist and cross-link correctly.

---

## ğŸ”— Cross-link rules (donâ€™t break the chain)

Your report will be more than pretty text if the catalogs link *both directions*:

### âœ… Minimum cross-links (recommended)
- **DCAT â†’ STAC**: each dataset distribution points to the STAC collection and/or item(s)
- **DCAT â†’ PROV**: dataset record links to `prov/run.prov.jsonld` (or per-step provenance)
- **STAC â†’ DCAT**: collection/item includes a link/reference back to the dataset record
- **STAC â†’ PROV**: item/collection includes a link/reference to provenance
- **PROV â†’ STAC/DCAT**: provenance bundle references the IDs used by STAC/DCAT (so lineage is resolvable)

### ğŸ§  Why this matters
- The **graph loader** can ingest STAC/DCAT/PROV consistently
- The **UI** can show â€œSource: â€¦â€ and â€œHow was this made?â€ at click-time
- The **AI** can produce answers with **tight citations** (and log provenance for dynamic queries)

---

## ğŸ§¾ `index.json` (highly recommended)

To keep the report template automation simple, provide a single entrypoint:

```jsonc
{
  "report_run_id": "RUN-2026-01-22T00-00-00Z-acde123",
  "catalogs": {
    "stac_collection": "stac/collection.json",
    "stac_items_dir": "stac/items/",
    "dcat_dataset": "dcat/dataset.jsonld",
    "prov_run": "prov/run.prov.jsonld",
    "run_manifest": "manifests/run_manifest.json"
  },
  "checks": {
    "schema_validation": "checks/schema_validation.json",
    "policy_gate": "checks/policy_gate.conftest.json"
  }
}
```

> ğŸ”¥ If you only have time for **one** extra file beyond STAC/DCAT/PROV, make it `index.json`.

---

## ğŸ§ª Experiment report integration (dev_prov template)

This folder enables the experiment report template to be:

- ğŸ§· **Self-contained**: the exact metadata used at run-time is preserved
- â™»ï¸ **Reproducible**: you can re-run with the same catalogs + params
- ğŸ§± **Composable**: charts, tables, narratives, and map layers can all point to stable IDs

### ğŸ§  Suggested mental model
```mermaid
flowchart LR
  A["ğŸ§¾ Raw Inputs"] --> B["ğŸ› ï¸ Pipeline / Experiment Steps"]
  B --> C["ğŸ›°ï¸ STAC"]
  B --> D["ğŸ—‚ï¸ DCAT"]
  B --> E["ğŸ§¬ PROV"]
  C --> F["ğŸ•¸ï¸ Graph / Index"]
  D --> F
  E --> F
  F --> G["ğŸ—ºï¸ UI / Maps"]
  F --> H["ğŸ§  AI (Focus Mode)"]
  H --> I["ğŸ“„ Report Narrative + Citations"]
```

---

## ğŸ§· Evidence manifests (recommended for claims, charts, narratives)

When the report includes **claims** (especially AI-assisted text), attach a structured evidence inventory:

- ğŸ“„ `evidence/EM-*.yaml` lists **each evidence item**, how it was used, and how to fetch it
- ğŸ§¬ PROV then ties the narrative activity to those evidence entities

This enables:
- â€œShow me all evidence behind Figure 3â€
- â€œWhich reports used dataset X?â€
- Automated review workflows (missing citations â†’ fail closed)

---

## ğŸ” Policy gates & validation outputs (optional, but strong default)

Place gate results under `checks/` so reviewers can validate trust **without rerunning** the pipeline:

- âœ… schema validation (STAC/DCAT/PROV)
- âœ… license + metadata completeness checks
- âœ… sensitivity/classification checks (public vs restricted)
- âœ… AI citation presence checks
- âœ… graph integrity checks (no orphan entities / broken references)

> ğŸ›¡ï¸ If your CI enforces â€œfail closed,â€ store the output here so the report has an *audit trail*.

---

## ğŸ“¦ OCI artifact distribution (optional, for large/binary artifacts)

If your pipeline outputs large artifacts (PMTiles, COGs, GeoParquet, models):

- Store artifacts in an **OCI registry** (content-addressable digests)
- Sign with **Cosign**
- Attach **PROV JSON-LD** and **SBOM** as referrers/attestations
- Keep the *references* here in STAC/DCAT (e.g., `distribution.oci`)

This turns your report into a verifiable â€œreceiptâ€ for exactly what bytes were produced.

---

## ğŸ·ï¸ Naming & versioning conventions

**Stable IDs** are your friend:

- `report_run_id`: include timestamp + short commit SHA
- Dataset IDs: consistent `domain:name:version` (or your projectâ€™s URI scheme)
- Evidence manifests: `EM-<id>.yaml` (where `<id>` can be a short hash or sequential id)

> âœ… Bonus: include canonical hashes in run manifests to enable idempotency + replay.

---

## âœ… Completion checklist

- [ ] `stac/collection.json` exists
- [ ] `dcat/dataset.jsonld` exists
- [ ] `prov/run.prov.jsonld` exists
- [ ] Cross-links resolve (STAC â†” DCAT â†” PROV)
- [ ] `manifests/run_manifest.json` present (or equivalent)
- [ ] `index.json` present
- [ ] If the report contains claims/figures: `evidence/EM-*.yaml` present
- [ ] If gating is enabled: `checks/*` captured
- [ ] If OCI used: attestation references captured

---

## ğŸ“š Glossary (fast)

- **STAC**: geospatial item/collection catalog (assets + where/when)
- **DCAT**: discovery catalog for datasets (license, keywords, distributions)
- **PROV**: lineage graph (inputs â†’ activity â†’ outputs + agents)
- **Evidence Manifest**: structured â€œbibliographyâ€ for claims/figures/narratives
- **Run Manifest**: pipeline run ledger (params, tools, inputs/outputs, hashes)

---

## ğŸ”­ Future-ready extensions (safe to add later)

- ğŸ§  **Conceptual Attention Nodes** (catalog-level thematic pivots)
- ğŸ“ **Pulse Threads** (time-sensitive geotagged narrative updates)
- ğŸ§ª **Weekly Graph Health Check outputs** (metadata about metadata)
- ğŸ§¾ **Report-to-dataset â€œimpact trackingâ€** (stories affected by dataset changes)

---

## ğŸ”— Related KFM-inspired references (for maintainers)

- ğŸ“¥ Data Intake philosophy (provenance-first, determinism, policy)  
- ğŸ—ºï¸ UI + AI usage patterns (citations, map layers, focus mode)  
- ğŸ›¡ï¸ Security / supply chain / attestation patterns (SBOM, signing, governance)  
- ğŸ§  Evidence-first narrative patterns (Story Nodes, manifests, PROV edges)

> Keep this README concise for contributorsâ€”put deep details in the project docs (profiles, schemas, policy packs).
