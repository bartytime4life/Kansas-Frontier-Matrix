# ğŸ“Š Results â€” Metrics

![Template](https://img.shields.io/badge/template-experiment__report-blue)
![Artifact](https://img.shields.io/badge/artifact-metrics-brightgreen)
![Provenance](https://img.shields.io/badge/provenance-required-orange)
![Formats](https://img.shields.io/badge/formats-json%20%7C%20csv%20%7C%20parquet%20%7C%20ndjson-informational)

Quantitative, machine-readable outputs for this experiment live here âœ…  
Think: **quality**, **performance**, **reliability**, **governance**, **AI**, and **UX** measurements â€” all tied back to a **run ID + config hash + provenance chain**.

> [!IMPORTANT]
> In KFM, metrics are not â€œnice-to-have.â€ Theyâ€™re part of the **audit trail** (telemetry + provenance-first + policy gates).  
> If you canâ€™t trace a metric back to its inputs and method, treat it as **non-compliant**.

---

## ğŸ§­ What belongs in `/results/metrics/`

**Put here:**
- ğŸ“ˆ **Summary metrics** (KPI-style, per-run aggregates)
- ğŸ§ª **Evaluation metrics** (model, retrieval, classification, geospatial accuracy, etc.)
- ğŸ§± **Data intake & validation metrics** (completeness, schema/PROV checks, failures)
- ğŸ•¸ï¸ **Graph health metrics** (node/edge deltas, orphan nodes, schema drift)
- âš¡ **Performance telemetry snapshots** (latency, runtime, memory, energy usage)
- ğŸ§‘â€ğŸ’» **UI & map rendering metrics** (tile generation time, FPS, interaction latency)
- ğŸ” **Governance metrics** (policy gate pass/fail counts, sensitivity handling status)

**Do _not_ put here:**
- âŒ Raw datasets (those belong in `data/raw` / `data/processed`)
- âŒ Charts-only without underlying numbers (charts go to `results/figures/`, but back them with metrics here)
- âŒ Anything containing sensitive or restricted data at record-level granularity

---

## ğŸ—‚ï¸ Recommended directory layout

```text
results/
  metrics/
    README.md
    metric_spec.yaml              # optional but strongly recommended ğŸ“
    metrics_summary.json          # required âœ… (human-friendly + machine-readable)
    metrics_long.csv              # recommended (tidy/long format) ğŸ“Š
    metrics_wide.csv              # optional (one row per run)
    metrics_timeseries.ndjson     # optional (append-only event/time-series) â±ï¸
    metrics_checks.json           # optional (policy + gate outcomes)
    plots/                        # optional (generated visuals)
      *.png
      *.svg
```

> [!TIP]
> Prefer **one canonical summary file** (for reports + dashboards) and **one canonical long-format file** (for slicing + comparisons).

---

## ğŸ” How metrics flow through the report

```mermaid
flowchart LR
  A[Experiment Run ğŸ§ª] --> B[metrics_long.csv ğŸ“Š]
  A --> C[metrics_summary.json âœ…]
  A --> D[metrics_timeseries.ndjson â±ï¸]
  C --> E[Policy Gate Outcomes ğŸ”]
  C --> F[Experiment Report Narrative ğŸ“]
  B --> G[Dashboards / Plots ğŸ“ˆ]
  C --> H[Provenance + Governance Ledger ğŸ§¾]
```

---

## ğŸ§© MetricSpec (recommended): make metrics â€œcontract-firstâ€

Create **`metric_spec.yaml`** to define:
- metric IDs + meaning
- units + direction (higher/lower is better)
- required segmentation (by county/time/layer/model/etc.)
- expected ranges / thresholds (for CI + policy checks)

Example:

```yaml
schema_version: kfm.metric_spec.v1
experiment_id: "<EXP_ID>"
owner: "<TEAM_OR_PERSON>"
metrics:
  - id: kfm.qa.metadata_completeness
    display_name: "Metadata completeness (required fields present)"
    unit: ratio
    direction: higher_better
    thresholds:
      - type: min
        value: 0.95
    tags: [qa, intake, governance]

  - id: kfm.ai.citation_coverage
    display_name: "Focus Mode citation coverage"
    unit: ratio
    direction: higher_better
    thresholds:
      - type: min
        value: 0.90
    tags: [ai, focus_mode, provenance]

  - id: kfm.perf.pipeline_runtime_seconds
    display_name: "Pipeline runtime"
    unit: s
    direction: lower_better
    tags: [perf, etl]
```

> [!NOTE]
> This aligns with KFMâ€™s **policy-gated** mindset: schema validation, STAC/DCAT/PROV completeness, license presence, sensitivity classification, and provenance completeness â€” plus AI outputs requiring citations. âœ…

---

## âœ… Required file: `metrics_summary.json`

This should be the â€œfront doorâ€ to your metrics.

### Required keys (minimum contract)
- `schema_version`
- `experiment_id`
- `run.run_id`
- `run.config_hash` (or equivalent)
- `run.git.commit` (or equivalent)
- `provenance.inputs[]` (what was measured)
- `provenance.outputs[]` (what was generated)
- `metrics[]` (the actual measurements)

Example:

```json
{
  "schema_version": "kfm.metrics.v1",
  "experiment_id": "EXP-010",
  "run": {
    "run_id": "run_2026-01-22T19-12-00Z_6f8a",
    "started_at": "2026-01-22T19:12:00Z",
    "ended_at": "2026-01-22T19:15:03Z",
    "config_hash": "sha256:...",
    "git": { "commit": "abcdef1", "dirty": false }
  },
  "provenance": {
    "inputs": [
      { "asset_id": "data/processed/<dataset>.parquet", "sha256": "..." }
    ],
    "outputs": [
      { "path": "results/metrics/metrics_long.csv", "sha256": "..." }
    ],
    "governance_ledger_ref": "govlog:<optional>"
  },
  "metrics": [
    {
      "id": "kfm.qa.metadata_completeness",
      "value": 0.97,
      "unit": "ratio",
      "direction": "higher_better"
    },
    {
      "id": "kfm.ai.citation_coverage",
      "value": 0.92,
      "unit": "ratio",
      "direction": "higher_better"
    },
    {
      "id": "kfm.perf.pipeline_runtime_seconds",
      "value": 183.4,
      "unit": "s",
      "direction": "lower_better"
    }
  ],
  "gate_outcomes": [
    { "gate": "schema_validation", "status": "pass" },
    { "gate": "prov_completeness", "status": "pass" },
    { "gate": "focus_mode_citations", "status": "pass" }
  ]
}
```

---

## ğŸ“„ Recommended file: `metrics_long.csv` (tidy format)

Use **one row per metric observation**, so you can slice by scope and segment.

Suggested columns:
- `experiment_id`
- `run_id`
- `metric_id`
- `value`
- `unit`
- `direction`
- `scope_type` (global / dataset / layer / county / graph / ui / ai)
- `scope_id` (e.g., dataset_id, layer_id, county_fips, graph_subgraph_id)
- `time_start`, `time_end` (if applicable)
- `notes` (short, optional)

---

## â±ï¸ Optional file: `metrics_timeseries.ndjson` (append-only)

Use this when:
- your pipeline emits events (ingestion gates, retries, failures)
- you track latency over time
- you store â€œtelemetry-likeâ€ stream data snapshots

Format: **NDJSON**, one event per line (easy to append + audit).

Example line:
```json
{"ts":"2026-01-22T19:12:10Z","run_id":"run_...","metric_id":"kfm.perf.api_latency_ms","value":243,"unit":"ms","scope_type":"api","scope_id":"/tiles/{z}/{x}/{y}.pbf"}
```

> [!TIP]
> This mirrors KFMâ€™s ingestion telemetry approach: append-only logs that feed dashboards/audits.

---

## ğŸ§° Suggested metric catalog (KFM-aligned)

### 1) ğŸ“¥ Data intake & catalog QA
- `kfm.qa.metadata_completeness` (required fields present)
- `kfm.qa.license_presence_rate`
- `kfm.qa.checksum_verified_rate`
- `kfm.qa.schema_validation_failures`
- `kfm.qa.prov_missing_count`
- `kfm.ingest.bytes_processed`
- `kfm.ingest.records_processed`
- `kfm.ingest.failures_by_stage` (gate / transform / load)

### 2) ğŸ•¸ï¸ Knowledge graph health checks
- `kfm.graph.node_count_total`
- `kfm.graph.edge_count_total`
- `kfm.graph.node_count_delta`
- `kfm.graph.orphan_node_count`
- `kfm.graph.property_schema_drift_count`
- `kfm.graph.top_degree_p95`

> [!NOTE]
> Graph health routines should flag â€œsilentâ€ breakage (orphans, drift, constraint failures) before it hits UI/AI.

### 3) ğŸ¤– AI / Focus Mode evaluation & governance
- `kfm.ai.citation_coverage` âœ… (policy-critical)
- `kfm.ai.refusal_rate` (healthy if policy triggers)
- `kfm.ai.answer_latency_ms`
- `kfm.ai.model_energy_wh` (or joules)
- `kfm.ai.hallucination_flags_count` (if you have a checker)
- `kfm.ai.retrieval_hit_rate_at_k`
- `kfm.ai.mrr_at_k`

> [!WARNING]
> If AI output cannot cite sources, the correct behavior is **refusal/uncertainty** â€” not guessing.

### 4) ğŸ—ºï¸ UI + mapping performance (2D/3D/WebGL)
- `kfm.ui.time_to_interactive_ms`
- `kfm.ui.tile_request_p95_ms`
- `kfm.ui.webgl_fps_median`
- `kfm.ui.layer_toggle_latency_ms`
- `kfm.ui.story_playback_drop_rate`
- `kfm.ui.accessibility_score` (if using automated checks)

### 5) ğŸ” Security, privacy, and sensitivity handling
- `kfm.gov.sensitive_records_redacted_count`
- `kfm.gov.coordinate_generalization_applied_rate`
- `kfm.gov.policy_gate_failures_by_type`
- `kfm.gov.audit_log_events_count`

> [!IMPORTANT]
> When working with sensitive cultural/archaeological data, store only **aggregated** metrics (counts, rates), never raw coordinates or identifying fields.

### 6) ğŸŒ Federation / multi-region readiness (optional)
- `kfm.fed.cross_catalog_query_latency_ms`
- `kfm.fed.schema_compatibility_score`
- `kfm.fed.remote_hub_availability_rate`

---

## ğŸ§ª Model evaluation metrics quick reference (if applicable)

If your experiment involves classification/retrieval, include:
- Precision / Recall / F1 (macro + weighted when relevant)
- Confusion matrix (as data, not just an image)
- Calibration / confidence metrics (if using probabilistic outputs)
- Segment metrics (by county/time/layer/domain)

---

## ğŸ” Validation & CI hooks (recommended)

Automate checks so metrics donâ€™t drift into chaos:

- âœ… `metric_spec.yaml` exists (for serious experiments)
- âœ… `metrics_summary.json` matches schema
- âœ… all metrics have units + direction
- âœ… all metrics reference a `run_id`
- âœ… provenance references exist for the inputs (dataset/model/config)
- âœ… policy gates outcomes recorded (pass/fail)
- âœ… no sensitive data leakage in metrics artifacts

---

## âœ… Checklist (copy into your PR)

- [ ] `metrics_summary.json` added/updated  
- [ ] `metrics_long.csv` added/updated (or Parquet equivalent)  
- [ ] Run metadata included (`run_id`, `config_hash`, code version)  
- [ ] Provenance pointers included (inputs + outputs)  
- [ ] Policy/gate outcomes recorded (especially citations for AI outputs)  
- [ ] No sensitive data leaked (only aggregated stats)  
- [ ] Figures (if any) backed by numbers in this folder  

---

## ğŸ“š Related docs (design intent)

This template is aligned with KFMâ€™s broader principles:
- provenance-first + evidence-backed UX ğŸ§¾
- policy gates + â€œfail closedâ€ quality bar ğŸ”
- telemetry/observability with run IDs + config hashes ğŸ“¡
- Focus Mode citations + governance ledger ğŸ¤–
- federation-ready schemas + reusable UI architecture ğŸŒ
- community & governance maturity over time ğŸ¤
