# ğŸ§ªğŸ“¦ `results/` â€” Experiment Outputs (MCP â€¢ Dev Prov â€¢ Template)

Welcome to the **Results** folder for the `10_experiment_report_template` example tree. This directory is where an experimentâ€™s **final, reviewable outputs** live: metrics, artifacts, provenance, and â€œwhat changed + why it mattersâ€.

> ğŸ§­ Rule of thumb: **If it influenced a decision, it belongs here** (or is referenced from here).

---

## ğŸ—ºï¸ Reading order (recommended)

1. âœ… **`summary.md`** â€” the â€œexecutive summaryâ€ (what youâ€™d paste into a PR)
2. ğŸ“Š **`metrics/`** â€” numbers + comparisons (with confidence & caveats)
3. ğŸ§¾ **`provenance/`** â€” lineage, evidence manifests, and run metadata
4. ğŸ§ª **`artifacts/`** â€” plots, maps, exports, screenshots, samples
5. ğŸ§° **`logs/`** â€” debugging & audit trail (only whatâ€™s needed)

---

## ğŸŒ² Suggested results tree (example)

```text
results/
  README.md                # (you are here)
  summary.md               # human-first, 1â€“2 pages max
  changelog.md             # notable diffs vs baseline / previous runs (optional)

  run/
    run_manifest.json      # run_id, timestamp, git sha, command, params, inputs hash
    env.txt                # python/node versions, platform, key libs
    requirements.txt       # pip freeze (or lockfile reference)
    seeds.json             # all RNG seeds used
    timings.csv            # stage timings (ETL, retrieval, model, validation)
    warnings.md            # known issues, policy waivers, edge cases

  metrics/
    metrics.json           # primary metrics (machine-readable)
    metrics.md             # human-readable highlights + interpretation
    ablation.csv           # optional: ablations table
    confusion_matrix.csv   # optional: classification style tasks
    drift_report.json      # optional: drift/bias checks summary

  artifacts/
    figures/
      *.png
      *.svg
    exports/
      *.geojson
      *.parquet
      *.pmtiles
      *.csv
    samples/
      example_inputs/
      example_outputs/
    notebooks/
      *.ipynb
      *.html

  provenance/
    prov.jsonld            # lineage (W3C PROV)
    evidence_manifest.json # â€œclaims -> sources -> transforms -> outputsâ€
    catalogs/
      stac/
        collection.json
        item_*.json
      dcat/
        dataset.jsonld
    governance/
      policy_eval.json     # policy-as-code results (allow/deny + reasons)
      approvals.md         # reviewer sign-offs (if needed)
      ledger_ref.json      # pointer/ID into an immutable governance ledger (if used)
```

> ğŸ§  Tip: keep file names boring + sortable. Prefer:  
> `YYYY-MM-DD__exp-####__short_slug/â€¦` (if you later nest multiple runs under `results/`).

---

## âœ… Minimum â€œDefinition of Doneâ€ (DoD) for results

Use this as a checklist before you call a run â€œrealâ€ (template-friendly âœ…).

- [ ] **`summary.md`** exists and answers: *What did we test? What changed? What won? What broke?*
- [ ] **`metrics/metrics.json`** exists (machine-readable) and includes:
  - [ ] baseline reference (or â€œN/Aâ€ with reason)
  - [ ] confidence / uncertainty where applicable
  - [ ] dataset/version IDs (not just filenames)
- [ ] **`run/run_manifest.json`** exists and includes:
  - [ ] `run_id`
  - [ ] git commit SHA
  - [ ] command line (or entrypoint) used
  - [ ] key parameters + seeds
  - [ ] input digests / checksums
- [ ] **`provenance/prov.jsonld`** exists (or a clearly documented equivalent)
- [ ] **Policy checks are recorded** (`provenance/governance/policy_eval.json`)
- [ ] **Artifacts are referenced** (not orphaned): `summary.md` links to the important plots/exports
- [ ] **Repro path is stated**: â€œhow to rerun thisâ€ in 3â€“6 lines (even if itâ€™s slow)

---

## ğŸ§· What goes in `summary.md` (keep it sharp)

A good `summary.md` is a PR-ready narrative:

- ğŸ¯ **Goal** (what hypothesis / decision is this run answering?)
- ğŸ§° **Method** (what changed: data, model, prompt, pipeline stage?)
- ğŸ§ª **Results** (key numbers + key artifacts)
- ğŸ§  **Interpretation** (why it happened, tradeoffs, failure modes)
- ğŸ” **Next steps** (ship / iterate / rollback)
- âš ï¸ **Risk & governance notes** (sensitivity, CARE constraints, policy flags)

---

## ğŸ§¾ Provenance expectations (evidence-first, not vibes)

This project favors **traceable outputs**. Treat provenance like a first-class deliverable:

- **Every derived file** should be traceable to:
  - source inputs
  - transforms/configs
  - tool versions + parameters
  - responsible agent/person (human or automated)
- If your experiment produces â€œanswer-likeâ€ content (summaries, narratives, classifications), store:
  - **the evidence manifest**
  - **the policy evaluation**
  - **the exact inputs retrieved** (or immutable references/digests)

> If you canâ€™t reproduce it, itâ€™s not a result â€” itâ€™s a screenshot.

---

## ğŸ›¡ï¸ Policy & QA gates (how to think about failures)

Use **gates** to keep bad outputs from becoming â€œofficialâ€:

- Metadata completeness (schemas, required fields)
- License presence (no unknown license)
- Sensitivity classification (and correct handling)
- Provenance completeness (no missing lineage)
- AI outputs: citations/evidence required (or output is rejected)

When a gate fails:
- Put the failure details in `provenance/governance/policy_eval.json`
- Summarize in `summary.md` under **âš ï¸ Known Issues**
- If you had to override anything, record the **who/why/when** in `approvals.md`

---

## ğŸ§  Results that feed UI/Storytelling (if applicable)

If your experiment outputs Story Nodes, guided tours, map states, or similar UI content:

- store the **rendered artifacts** (screenshots, exports)
- store the **source content** (Markdown + JSON configs) as part of the experiment artifacts
- store a **preview recipe** (how a reviewer opens/validates it locally)

---

## ğŸ“Œ Tips for clean diffs & easy review

- âœ… Prefer **small, legible artifacts** (downsampled previews + a link/ref to the full resolution)
- âœ… Use **CSV/JSON** for metrics so CI can diff and chart them
- âœ… Keep logs â€œthinâ€: errors + warnings + key stage summaries
- âœ… Always include **baselines** when possible
- âœ… Always include **one â€œgolden exampleâ€** input/output pair for sanity checking

---

## ğŸ”§ Quick â€œHow to reproduceâ€ template (paste into `summary.md`)

```bash
# 1) set up env
# (fill in: conda/uv/pip/npm)
<SETUP_COMMANDS>

# 2) run the experiment
<RUN_COMMAND> --config <PATH> --seed <N>

# 3) validate results (schemas + policy)
<VALIDATE_COMMAND>

# 4) regenerate artifacts (optional)
<RENDER_COMMAND>
```

---

## ğŸ§° Appendix: `run_manifest.json` (starter schema)

```json
{
  "run_id": "2026-01-22__exp-0010__example_slug",
  "timestamp_utc": "2026-01-22T00:00:00Z",
  "git": { "repo": "Kansas-Frontier-Matrix", "commit": "<sha>", "dirty": false },
  "entrypoint": "<command or workflow name>",
  "params": { "key": "value" },
  "seeds": { "python": 0, "numpy": 0, "torch": 0 },
  "inputs": [
    { "name": "dataset_x", "ref": "<stac/dcat id>", "sha256": "<digest>" }
  ],
  "outputs": [
    { "path": "metrics/metrics.json", "sha256": "<digest>" }
  ]
}
```

---

<details>
<summary>ğŸ“š Why this structure exists (click to expand)</summary>

This results layout is designed to support:

- ğŸ” **Reproducibility** (run manifests + deterministic configs)
- ğŸ§¾ **Auditability** (provenance + governance outputs)
- ğŸ§ª **Comparability** (metrics in diff-friendly formats)
- ğŸ§‘â€âš–ï¸ **Reviewability** (summaries and curated artifacts, not raw dumps)
- ğŸ—ºï¸ **Geospatial + narrative workflows** (exports + story content + previews)

</details>
