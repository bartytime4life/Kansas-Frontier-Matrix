# ğŸ—‚ï¸ Registry â€” Experiment Report Tree (dev_prov)

![provenance](https://img.shields.io/badge/provenance-STAC%2FDCAT%2FPROV-2b6cb0)
![policy](https://img.shields.io/badge/policy-OPA%2FConftest%20gated-6b46c1)
![ledger](https://img.shields.io/badge/log-append--only%20ledger-0f766e)
![artifacts](https://img.shields.io/badge/artifacts-digests%20%26%20signing-9a3412)
![template](https://img.shields.io/badge/template-experiment__report__tree-334155)

> ğŸ“ **Folder:** `mcp/dev_prov/examples/10_experiment_report_template/(example_report_tree)/registry/`  
> ğŸ¯ **Purpose:** Machine-readable **source of truth** for experiments, runs, artifacts, and provenance links â€” so â€œwhat happened?â€ is always answerable.

---

## ğŸ§  What this folder is (and isnâ€™t)

### âœ… This registry **is**
- A **thin, query-friendly index** of *what* exists in the experiment tree (IDs, paths, digests, versions, metadata).
- A **provenance router** that points to the required evidence chain (STAC/DCAT/PROV) and run manifests.
- An **automation-friendly contract** used by agents (Watcher â†’ Planner â†’ Executor) and CI policy gates.
- The â€œglue layerâ€ between human reports ğŸ“ and machine-auditable artifacts ğŸ§¾.

### ğŸš« This registry is **not**
- A dumping ground for binaries (models, rasters, PDFs, zips).  
  âœ Store binaries in `../artifacts/` (or an OCI registry) and reference them here by digest.
- A second copy of your report.  
  âœ Reports live in `../experiments/` (or `../reports/`) â€” registry only links + summarizes.

---

## ğŸ§­ Design principles (KFM-flavored)

> [!IMPORTANT]
> **No artifact without provenance. No provenance without policy. No policy without tests.** ğŸ§·

- **Evidence-first**: treat experiment outputs (including AI outputs) as first-class â€œevidence artifacts.â€
- **Fail closed**: if required metadata is missing (license, classification, PROV links), the registry entry is invalid.
- **Append-only mentality**: donâ€™t â€œrewrite history.â€ Deprecate + supersede.
- **Stable IDs + content digests**: humans remember IDs, machines trust hashes.
- **UI-ready transparency**: registry fields should support â€œLayer Provenanceâ€, â€œAudit Panelsâ€, and export attributions.

---

## ğŸ§± Expected layout (template)

> Your exact filenames may differ â€” this is the **contract shape** this example intends.

```text
registry/
â”œâ”€ ğŸ“„ README.md                         # you are here
â”œâ”€ ğŸ“„ registry.index.json               # top-level index (fast lookup)
â”œâ”€ ğŸ“„ registry.index.ndjson             # optional append-only event stream
â”œâ”€ ğŸ“ experiments/                      # one folder per experiment (stable ID)
â”‚  â”œâ”€ ğŸ“ EXP-0001/                      
â”‚  â”‚  â”œâ”€ ğŸ“„ entry.json                  # experiment summary + links
â”‚  â”‚  â”œâ”€ ğŸ“„ prov.jsonld                 # PROV for the experiment/report itself
â”‚  â”‚  â”œâ”€ ğŸ“„ runs.json                   # run list (run_id â†’ manifest + artifacts)
â”‚  â”‚  â””â”€ ğŸ“„ metrics.json                # metrics snapshot (optionally DVC-friendly)
â”‚  â””â”€ ğŸ“ EXP-0002/
â”‚     â””â”€ ...
â”œâ”€ ğŸ“ schemas/                          # JSON Schemas used by CI + tooling
â”‚  â”œâ”€ ğŸ“„ experiment.entry.schema.json
â”‚  â”œâ”€ ğŸ“„ run.manifest.schema.json
â”‚  â””â”€ ğŸ“„ artifact.ref.schema.json
â””â”€ ğŸ“ policies/                         # optional: local policy helpers for this example
   â””â”€ ğŸ“„ README.md
```

---

## ğŸ“Œ Canonical IDs & naming conventions

### ğŸ§ª Experiment IDs
Format (recommended):
- `EXP-YYYYMMDD-###` âœ… (sortable + unique)  
  Example: `EXP-20260122-001`

Or (smaller template):
- `EXP-0001`, `EXP-0002`, â€¦

### ğŸƒ Run IDs
Format (recommended):
- `RUN-<UTC timestamp>-<short digest>`  
  Example: `RUN-2026-01-22T21-04-12Z-8a31c2f`

### ğŸ“¦ Artifact IDs
- Prefer content-addressed IDs: `sha256:<digest>`  
- If stored in an OCI registry, keep the **OCI digest** and **ref** (tag/URL) as separate fields.

---

## ğŸ—ƒï¸ Registry files: what goes where

| File | What it does | Must contain |
|---|---|---|
| `registry.index.json` | Fast lookup for tooling/UI | list of experiment IDs + pointers |
| `experiments/<EXP>/entry.json` | â€œCardâ€ for an experiment | goals, method, inputs, outputs, decision |
| `experiments/<EXP>/prov.jsonld` | Provenance for the *experiment/report* | agents, activities, used/generated entities |
| `experiments/<EXP>/runs.json` | Run ledger | run_id â†’ manifest digest + artifacts |
| `experiments/<EXP>/metrics.json` | Metrics snapshot | key metrics + thresholds + comparisons |
| `schemas/*.schema.json` | Validation contract | used by CI + policy gates |

---

## ğŸ§¬ Minimum required fields (experiment entry)

> [!TIP]
> Think of `entry.json` as the registry equivalent of an â€œExperiment Report cover page.â€  
> The full narrative still lives in the report markdown.

```json
{
  "experiment_id": "EXP-20260122-001",
  "title": "Compare NER approach A vs B for place-name extraction",
  "status": "completed",
  "owners": ["@you", "@kfm-bot"],
  "created_utc": "2026-01-22T21:04:12Z",
  "tags": ["nlp", "ner", "gazetteer", "kansas"],
  "goals": [
    "Increase correct place-name extraction without increasing false positives"
  ],
  "method_summary": "Ran two pipelines with identical inputs; compared precision/recall and downstream graph link rate.",
  "inputs": {
    "datasets": [
      {
        "dcat_id": "dcat:us_ks_historical_docs_v3",
        "stac_collection": "stac:docs_scans_1900s",
        "prov_entity": "prov:entity:raw_docs_bundle_sha256_..."
      }
    ],
    "code": {
      "git_commit": "abcdef123456",
      "pipeline_entrypoint": "src/pipelines/nlp/extract_places.py"
    },
    "environment": {
      "container_image": "ghcr.io/your-org/kfm-pipeline:1.2.3",
      "requirements_lock": "sha256:..."
    }
  },
  "outputs": {
    "artifacts": [
      {
        "artifact_type": "csv",
        "path": "../artifacts/EXP-20260122-001/places_extracted.csv",
        "digest": "sha256:...",
        "stac_item": "stac:item:places_extracted_v1",
        "prov_entity": "prov:entity:places_extracted_sha256_..."
      }
    ],
    "metrics_ref": "metrics.json",
    "runs_ref": "runs.json"
  },
  "decision": {
    "outcome": "adopted",
    "why": "Model B improved F1 by +0.12 and reduced false positives in county-level aggregation.",
    "followups": [
      "Add bias check for indigenous place names",
      "Update model card + pipeline docs"
    ]
  },
  "links": {
    "report_md": "../experiments/EXP-20260122-001/README.md",
    "prov_jsonld": "prov.jsonld"
  }
}
```

---

## ğŸ§¾ Run manifests (determinism + audit)

> [!NOTE]
> A **run manifest** is your â€œreceipt.â€ It records inputs, outputs, tool versions, counts, and policy-relevant facts.

Suggested manifest fields:
- `idempotency_key` (derived from canonicalized JSON)
- `seed` / `clock_mode` (for deterministic simulations)
- `input_digests[]` and `output_digests[]`
- `tool_versions` (compiler/runtime/libs)
- `source_urls[]` (if pulling from upstream sources)
- `summary_counts` (records in/out, errors)

Example skeleton:
```json
{
  "run_id": "RUN-2026-01-22T21-04-12Z-8a31c2f",
  "idempotency_key": "sha256:...",
  "seed": 1337,
  "clock_mode": "virtual",
  "tool_versions": {
    "python": "3.12.1",
    "postgis": "3.x",
    "pipeline": "kfm-pipeline@abcdef123456"
  },
  "input_digests": ["sha256:..."],
  "output_digests": ["sha256:..."],
  "source_urls": ["https://example.gov/data/file.csv"],
  "summary_counts": {
    "records_in": 120034,
    "records_out": 118992,
    "errors": 0
  },
  "prov_activity": "prov:activity:run_8a31c2f"
}
```

---

## ğŸ›¡ï¸ Policy gates & validation expectations

### CI checks this registry should be friendly to âœ…
- **Schema validation** (JSON Schema / AJV)
- **Policy-as-code** (OPA/Rego via Conftest)
- **Provenance completeness** (STAC/DCAT/PROV links exist for new evidence artifacts)
- **License presence** (no dataset/artifact entry without license)
- **Sensitivity classification present** (and handled correctly)
- **â€œNo bypassâ€ ordering rules** (no graph/UI-facing artifacts without catalog + provenance)

> [!WARNING]
> If you canâ€™t prove provenance, **donâ€™t merge** the registry entry.  
> Create a draft entry and mark it `status: blocked` instead.

---

## ğŸ“¦ Artifact storage strategies

### Option A: Repo-relative artifacts (simple)
- Store artifacts under `../artifacts/<EXP>/...`
- Reference by:
  - relative path
  - `sha256` digest
  - optional STAC/DCAT/PROV pointers

### Option B: OCI registry artifacts (scalable + signed)
- Push artifacts to an OCI registry (ORAS).
- Attach:
  - signatures (Cosign)
  - provenance attestations (in-toto style)
  - PROV JSON-LD as a â€œreferrerâ€ object
- Registry entry should carry:
  - `oci_ref` (tag/ref)
  - `oci_digest` (immutable)
  - signature verification metadata (issuer/key)

---

## ğŸ—ºï¸ How this connects to the rest of KFM

```mermaid
flowchart LR
  A[ğŸ§ª Experiment Report (human)] --> B[ğŸ—‚ï¸ Registry (machine index)]
  B --> C[ğŸ§¾ Run Manifests (audit receipts)]
  C --> D[â›“ï¸ PROV / STAC / DCAT (evidence chain)]
  D --> E[ğŸ§  Graph + Search]
  E --> F[ğŸ—ºï¸ UI (Layer Provenance / Audit Panels)]
  F --> G[ğŸ¤– Focus Mode (cite-or-refuse)]
```

- The **UI** can display experiment outcomes like a â€œdashboardâ€ without guessing.
- **Focus Mode** can safely reference experiment results because the registry enforces that theyâ€™re evidence-linked.
- **Governance ledgers** (append-only logs) are consistent with this registryâ€™s â€œreceiptâ€ model.

---

## ğŸ§° Quick workflow: adding a new experiment (template)

1. **Create report**  
   - Copy the experiment template into `../experiments/<EXP-ID>/README.md`  
   - Fill sections: *Goals â†’ Data Used â†’ Method â†’ Results â†’ Interpretation* ğŸ§ª

2. **Create registry entry**  
   - `registry/experiments/<EXP-ID>/entry.json`
   - Add `prov.jsonld` for the experiment/report itself
   - Add `runs.json` and/or run manifest references

3. **Record artifacts**  
   - Put outputs in `../artifacts/<EXP-ID>/...` (or OCI)
   - Compute digests and link them in `entry.json`

4. **Pass gates**  
   - Validate schemas
   - Run Conftest policies
   - Ensure STAC/DCAT/PROV pointers exist for evidence artifacts

5. **Append, donâ€™t rewrite**  
   - If correcting: mark old entry as `deprecated` and add a new one that supersedes it.

---

## ğŸ“š Project doc inputs this registry is aligned with

> This example registry structure is intentionally consistent with the broader KFM docs on:  
> provenance-first pipeline, policy gates, evidence artifacts, experiment reporting, and UI transparency.

<details>
<summary>ğŸ“– Click to expand the full project-files list</summary>

- ğŸ“˜ **Kansas Frontier Matrix (KFM) â€“ Comprehensive Technical Documentation**  
- ğŸ§­ **Kansas Frontier Matrix (KFM) â€“ AI System Overview**  
- ğŸ—ï¸ **Kansas Frontier Matrix (KFM) â€“ Comprehensive Architecture, Features, and Design**  
- ğŸ–¥ï¸ **Kansas Frontier Matrix â€“ Comprehensive UI System Overview**  
- ğŸ“¥ **Kansas Frontier Matrix (KFM) Data Intake â€“ Technical & Design Guide**  
- ğŸ’¡ **Innovative Concepts to Evolve the Kansas Frontier Matrix (KFM)**  
- â• **Additional Project Ideas**  
- ğŸŒŸ **Kansas Frontier Matrix â€“ Latest Ideas & Future Proposals**  
- ğŸ§© **AI Concepts & more** *(PDF portfolio reference library)*  
- ğŸ—ºï¸ **Maps / GoogleMaps / VirtualWorlds / Archaeology / Computer Graphics / Geospatial WebGL** *(PDF portfolio reference library)*  
- ğŸ§° **Various programming languages & resources** *(PDF portfolio reference library)*  
- ğŸ—„ï¸ **Data Management / Theories / Architectures / Data Science / Bayesian Methods** *(PDF portfolio reference library)*  

</details>

---

## ğŸ§¾ Glossary (tiny but useful)

- **STAC**: â€œWhere/when + assetsâ€ catalog for spatial/temporal data  
- **DCAT**: dataset-level catalog metadata  
- **PROV**: lineage graph of *used/generated/agents/activities*  
- **W-P-E**: Watcher â†’ Planner â†’ Executor automation pattern  
- **Fail closed**: missing metadata = blocked merge  
- **Evidence artifact**: any derived output that must move through the same pipeline as â€œreal dataâ€

---

### âœ… Definition of Done (DoD) for registry entries

- [ ] Entry has a stable `experiment_id`
- [ ] Inputs list includes dataset identifiers (and provenance pointers where required)
- [ ] Outputs list includes artifact digests (+ location)
- [ ] A run manifest exists for any non-trivial execution
- [ ] License + sensitivity classification fields are present where applicable
- [ ] Schema + policy checks pass (CI green) âœ…
- [ ] Report markdown is linked (human narrative exists)

---

ğŸ’¬ **Rule of thumb:** If someone canâ€™t reproduce it, audit it, or cite itâ€¦ itâ€™s not â€œin the systemâ€ yet.
