# ğŸ§¬ dev_prov Examples (MCP) â€” Evidence-First Templates

![MCP](https://img.shields.io/badge/MCP-Master%20Coder%20Protocol-3b82f6)
![Provenance](https://img.shields.io/badge/Provenance-STAC%20%2B%20DCAT%20%2B%20PROV-10b981)
![Policy](https://img.shields.io/badge/Policy-OPA%20%2F%20Rego%20%2B%20Conftest-f97316)
![Supply%20Chain](https://img.shields.io/badge/Supply%20Chain-SLSA%20%2B%20Sigstore%20%2B%20Cosign-8b5cf6)
![Status](https://img.shields.io/badge/Status-Templates%20%2F%20Copy--Paste-64748b)

> **Goal:** Give you *copy/paste* example artifacts for **development provenance** in Kansas Frontier Matrix (KFM) / Kansas-Matrix-System.  
> These examples encode the projectâ€™s nonâ€‘negotiables: **pipeline ordering**, **evidence-first publishing**, **FAIR+CARE governance**, and **auditable automation**.

---

## ğŸ§­ What â€œdev_provâ€ covers

**dev_prov** is the â€œpaper trail engineâ€ for the platform â€” *datasets, narratives, and even devops events* are treated as queryable provenance. That means:

- ğŸ—ºï¸ **Every dataset/evidence artifact** publishes the **Evidence Triplet**:
  - **STAC** (asset-level metadata)
  - **DCAT** (catalog/discovery-level metadata)
  - **W3C PROV** (lineage: inputs â†’ processing â†’ outputs â†’ agents)
- ğŸ§¾ **Every run** emits an auditable **Run Manifest** (inputs, outputs, parameters, environment, checks)
- ğŸ§  **Focus Mode outputs** (AI answers / dynamic queries) still log provenance + citations
- ğŸ§ª **AI/analysis outputs** are first-class â€œevidence artifactsâ€ (not â€œmagic textâ€)
- ğŸ§± **Policy-as-code** blocks merges if provenance is missing or governance rules are violated
- ğŸ“¦ **Release artifacts** can be shipped as OCI artifacts and signed/attested (supply chain integrity)

---

## ğŸ§± Canonical pipeline ordering (invariants)

KFMâ€™s pipeline order is **inviolable**:

> **ETL â†’ Catalogs (STAC/DCAT/PROV) â†’ Graph â†’ API â†’ UI â†’ Story Nodes â†’ Focus Mode**  
> No stage consumes data that hasnâ€™t crossed the previous boundary with its formal artifacts. âœ…

```mermaid
flowchart LR
  A["ğŸ“¥ ETL (data/raw âœ data/work âœ data/processed)"] --> B["ğŸ›°ï¸ STAC (data/stac/...)"]
  A --> C["ğŸ—‚ï¸ DCAT (data/catalog/dcat/ or data/catalogs/...)"]
  A --> D["ğŸ§¾ PROV (data/prov/...)"]

  B --> G["ğŸ•¸ï¸ Neo4j Graph (references catalogs)"]
  C --> G
  D --> G

  G --> H["ğŸ›¡ï¸ API Layer (contracts + redaction + auth)"]
  H --> I["ğŸ—ºï¸ UI (MapLibre/Cesium + panels)"]
  I --> J["ğŸ“– Story Nodes (governed narratives)"]
  J --> K["ğŸ¤– Focus Mode (evidence-linked answers)"]
```

---

## ğŸ“ Expected folder layout (this directory)

```text
mcp/dev_prov/examples/
â”œâ”€ ğŸ“„ README.md                         # ğŸ‘ˆ you are here ğŸ“Œ How to use the examples + recommended learning order
â”œâ”€ ğŸš€ 00_quickstart_minimal_triplet/     # Smallest end-to-end â€œevidence tripletâ€ example (claim + citation + artifact)
â”œâ”€ ğŸ›°ï¸ 01_dataset_evidence_triplet/       # Dataset-focused triplet: source â†’ processed artifact â†’ catalog/prov links
â”œâ”€ ğŸ” 02_run_manifest_and_hashing/       # Run manifest + checksum patterns (inputs/outputs, sha256, reproducibility)
â”œâ”€ ğŸ¬ 03_story_node_evidence_manifest/   # Story Node evidence manifest: citations, media, layer refs, policy gates
â”œâ”€ ğŸ§µ 04_pulse_thread_evidence/          # Pulse thread example: short updates + evidence bundle + lineage pointers
â”œâ”€ ğŸ”— 05_github_pr_to_prov/              # Link a GitHub PR to PROV/receipts (ids, commits, approvals, traceability)
â”œâ”€ ğŸ” 06_focus_mode_answer_log/          # Focus Mode answer logging: citations required + redaction notices + receipts
â”œâ”€ ğŸ“¡ 07_streaming_ingest_stub_prov/     # Streaming ingest stub: event traces + backpressure notes + partial lineage
â”œâ”€ ğŸ“¦ 08_oci_artifact_distribution/      # OCI distribution example: artifact manifest + digests + publication record
â”œâ”€ ğŸš¦ 09_policy_pack_smoke_tests/        # Policy pack smoke tests: known-pass/known-fail fixtures for gates
â”œâ”€ ğŸ§ª 10_experiment_report_template/     # Experiment report example: meta, evidence, metrics, and PROV bundle
â””â”€ â™»ï¸ _shared/
   â”œâ”€ ğŸ“ schemas/                        # Shared schema snippets used by multiple examples (avoid duplication)
   â””â”€ ğŸ§© snippets/                       # Reusable fragments (YAML/JSON/MD) for copy/paste into new examples
```

> If your repo currently uses slightly different paths, keep the **artifact intent** identical and update the paths consistently (see â€œGotchasâ€ below).

---

## ğŸ§° Example index

| # | Example | You learn / produce | Key outputs |
|---:|---|---|---|
| 00 | âš¡ Quickstart Minimal Triplet | â€œHello worldâ€ provenance | STAC + DCAT + PROV |
| 01 | ğŸ›°ï¸ Dataset Evidence Triplet | A real dataset with full linkage | STAC Item/Collection + DCAT Dataset + PROV bundle |
| 02 | ğŸ§¾ Run Manifest + Hashing | Reproducibility + canonical digests | `run_manifest.json` + canonical JSON digest |
| 03 | ğŸ“– Story Node Evidence Manifest | Evidence-first narrative packaging | `story.md` + evidence manifest + PROV link |
| 04 | ğŸ’“ Pulse Thread Evidence | Live â€œpulseâ€ entries with review gates | pulse entry JSON + evidence refs + PROV |
| 05 | ğŸ”€ GitHub PR â†’ PROV | DevOps events become provenance graph | PR activity + commit entities + agents |
| 06 | ğŸ¤– Focus Mode Answer Log | AI answers become evidence artifacts | answer log JSON + citations + PROV usage |
| 07 | â± Streaming Ingest â€œStub PROVâ€ | Real-time data still obeys rules | streaming STAC + minimal PROV + update strategy |
| 08 | ğŸ“¦ OCI Artifact Distribution | Ship + sign datasets like containers | ORAS push + cosign signatures + attestations |
| 09 | ğŸ›¡ï¸ Policy Pack Smoke Tests | Fail-closed governance | conftest/rego checks + examples |
| 10 | ğŸ§ª Experiment Report Template | MCP scientific-method logging | experiment README + protocol + outputs |

---

# âœ… Example 00 â€” Quickstart Minimal Triplet

This is the *smallest valid* proof that your pipeline can publish **boundary artifacts**.

**Definition of done** âœ…  
- [ ] STAC exists (item or collection)
- [ ] DCAT dataset exists
- [ ] PROV lineage exists
- [ ] All 3 link to each other (IDs + URLs/paths)
- [ ] Policy Pack passes (even minimal)

<details>
<summary>ğŸ›°ï¸ Minimal STAC Item (example)</summary>

```json
{
  "type": "Feature",
  "stac_version": "1.0.0",
  "id": "kfm.ks.demo.minimal_asset.v1",
  "collection": "kfm.ks.demo.minimal_asset",
  "geometry": { "type": "Point", "coordinates": [-96.0, 39.0] },
  "bbox": [-96.0, 39.0, -96.0, 39.0],
  "properties": {
    "datetime": "2026-01-20T00:00:00Z",
    "title": "KFM Minimal Asset (Demo)",
    "kfm:prov_ref": "data/prov/kfm.ks.demo.minimal_asset.v1.prov.jsonld"
  },
  "assets": {
    "data": {
      "href": "data/processed/demo/minimal_asset/minimal_asset_v1.geojson",
      "type": "application/geo+json",
      "roles": ["data"]
    }
  }
}
```
</details>

<details>
<summary>ğŸ—‚ï¸ Minimal DCAT Dataset (example)</summary>

```json
{
  "@context": {
    "dcat": "http://www.w3.org/ns/dcat#",
    "dct": "http://purl.org/dc/terms/",
    "prov": "http://www.w3.org/ns/prov#"
  },
  "@type": "dcat:Dataset",
  "@id": "urn:kfm:dataset:kfm.ks.demo.minimal_asset.v1",
  "dct:title": "KFM Minimal Asset (Demo)",
  "dct:license": "CC-BY-4.0",
  "dcat:distribution": [
    {
      "@type": "dcat:Distribution",
      "dct:format": "STAC",
      "dcat:accessURL": "data/stac/items/kfm.ks.demo.minimal_asset.v1.json"
    }
  ],
  "prov:wasGeneratedBy": "urn:kfm:activity:run:demo_minimal_v1"
}
```
</details>

<details>
<summary>ğŸ§¾ Minimal PROV JSON-LD (example)</summary>

```json
{
  "@context": { "prov": "http://www.w3.org/ns/prov#", "dct": "http://purl.org/dc/terms/" },
  "@graph": [
    {
      "@id": "urn:kfm:entity:data:raw:demo_source_v1",
      "@type": "prov:Entity",
      "dct:description": "Raw demo source input",
      "prov:value": "sha256:REPLACE_ME"
    },
    {
      "@id": "urn:kfm:activity:run:demo_minimal_v1",
      "@type": "prov:Activity",
      "prov:used": ["urn:kfm:entity:data:raw:demo_source_v1"],
      "prov:generated": ["urn:kfm:entity:data:processed:kfm.ks.demo.minimal_asset.v1"],
      "prov:startedAtTime": "2026-01-20T00:00:00Z"
    },
    {
      "@id": "urn:kfm:agent:human:you",
      "@type": "prov:Agent",
      "prov:label": "Developer"
    }
  ]
}
```
</details>

---

# ğŸ›°ï¸ Example 01 â€” Dataset Evidence Triplet (STAC + DCAT + PROV)

This example shows **full cross-layer linkage** and â€œgraph-readyâ€ metadata.

## ğŸ§© Linking rules (donâ€™t skip these)

- **STAC** must point to the actual assets in `data/processed/**`
- **DCAT** must link to STAC (and/or distribution download URLs)
- **PROV** must link raw inputs â†’ work â†’ processed outputs, including **run ID** and/or **git commit**
- The **graph** should reference these catalog IDs (not duplicate payloads)

> Treat STAC/DCAT/PROV as **boundary artifacts**. Downstream systems only trust whatâ€™s behind those boundaries.

---

# ğŸ§¾ Example 02 â€” Run Manifest + Canonical Digest

This example implements an auditable run log with **repeatable hashing**.

## ğŸ“„ `run_manifest.json` (shape)

```json
{
  "run_id": "2026-01-20T20-00Z__usgs_nwis_ingest",
  "pipeline": "hydro/usgs_nwis_ingest",
  "code_ref": {
    "git_commit": "REPLACE_WITH_SHA",
    "repo": "REPLACE_WITH_REPO_URL"
  },
  "inputs": [
    { "uri": "data/raw/hydro/usgs_nwis/source_dump.json", "sha256": "..." }
  ],
  "outputs": [
    { "uri": "data/processed/hydro/river_gauges/river_gauges_v1.geoparquet", "sha256": "..." },
    { "uri": "data/stac/items/kfm.ks.hydro.river_gauges.v1.json", "sha256": "..." },
    { "uri": "data/prov/kfm.ks.hydro.river_gauges.v1.prov.jsonld", "sha256": "..." }
  ],
  "params": { "epsg": 4326, "dedupe": true },
  "environment": { "python": "3.12.x", "container_image": "ghcr.io/ORG/IMAGE@sha256:..." },
  "timestamps": { "started": "2026-01-20T19:55:12Z", "ended": "2026-01-20T20:02:41Z" }
}
```

## ğŸ”’ Canonical digest (why we care)

If two developers (or CI bots) run the same pipeline with the same inputs, the **manifest digest** should match.

<details>
<summary>ğŸ Minimal hashing snippet (illustrative)</summary>

```python
# PSEUDOCODE: Use RFC 8785 JSON canonicalization in your implementation.
import json, hashlib

def canonical_json_bytes(obj: dict) -> bytes:
    # Replace with RFC 8785 canonicalization (stable key order, number formatting, etc.)
    return json.dumps(obj, sort_keys=True, separators=(",", ":"), ensure_ascii=False).encode("utf-8")

manifest = json.load(open("data/audits/RUN_ID/run_manifest.json", "r", encoding="utf-8"))
digest = hashlib.sha256(canonical_json_bytes(manifest)).hexdigest()
print("canonical_digest_sha256:", digest)
```
</details>

> ğŸ§  TIP: Put run manifests in `data/audits/<run_id>/run_manifest.json` and store digests alongside them for easy verification.

---

# ğŸ“– Example 03 â€” Story Node Evidence Manifest

Story Nodes are **governed narrative artifacts**: every claim points to evidence in catalogs.

## ğŸ“„ `story.md` + evidence manifest (pattern)

- `story.md` (human-readable + machine-ingestible)
- `evidence.yaml` (structured list of citations, assets, checksums)
- Optional: `story.prov.jsonld` (story as an Entity; evidence as used Entities; author/reviewer Agents)

<details>
<summary>ğŸ§¾ Example evidence manifest (YAML)</summary>

```yaml
story_id: kfm.story.ks.historical.santa_fe_trail.segment_a.v1
title: "Santa Fe Trail â€” Segment A (Draft)"
status: "needs_review"
claims:
  - id: claim-001
    text: "This segment follows the corridor between X and Y (approx.)."
    evidence:
      - stac_item_id: kfm.ks.historical_maps.santa_fe_trail_1859.v1
        asset_key: geotiff
        locator: "map sheet 2, annotation A"
        checksum: "sha256:REPLACE_ME"
  - id: claim-002
    text: "A military outpost existed near Z during the 18xx period."
    evidence:
      - dcat_dataset_id: urn:kfm:dataset:kfm.ks.archives.fort_records.v1
        locator: "doc page 12"
        checksum: "sha256:REPLACE_ME"
```
</details>

> âœ… **Policy expectation:** â€œEvidence-first narrativeâ€ â€” **no unsourced claims** in Story Nodes or Focus Mode.

---

# ğŸ’“ Example 04 â€” Pulse Thread Evidence (review-gated)

A â€œPulse Threadâ€ is a **time-based feed** entry (an alert, anomaly, update, event) thatâ€™s still evidence-linked.

Use this when:
- â± streaming data has notable events
- ğŸ§ª a model run flags an anomaly
- ğŸ›°ï¸ a new dataset version is published
- ğŸ§¯ a governance warning occurs

<details>
<summary>ğŸ’“ Pulse entry (example)</summary>

```json
{
  "pulse_id": "pulse.kfm.ks.hydro.kansas_river.level_spike.v1",
  "timestamp": "2026-01-20T20:00:00Z",
  "geo": { "type": "Point", "coordinates": [-95.67, 39.05] },
  "headline": "Kansas River gauge spike detected",
  "status": "needs_review",
  "evidence": {
    "stac_item_id": "kfm.ks.hydro.river_gauge_readings.2026-01-20T20-00Z",
    "prov_bundle": "data/prov/kfm.ks.hydro.river_gauge_readings.2026-01-20T20-00Z.prov.jsonld",
    "query": "SELECT max(value) ... WHERE station_id='TOPEKA' ..."
  },
  "governance": {
    "classification": "public",
    "notes": "If station is private/sensitive, downgrade visibility or obfuscate."
  }
}
```
</details>

---

# ğŸ”€ Example 05 â€” GitHub PR â†’ PROV (DevOps as provenance)

Treat code evolution like data evolution: PRs become provenance activities, commits become entities, humans/bots become agents.

**Why this matters:** You can answer questions like:
- â€œWhich PR produced this dataset version and who reviewed it?â€
- â€œWhich PRs touched the hydrology pipeline last year?â€

<details>
<summary>ğŸ§¾ PR-as-PROV JSON-LD (example)</summary>

```json
{
  "@context": { "prov": "http://www.w3.org/ns/prov#" },
  "@graph": [
    {
      "@id": "urn:kfm:activity:pr:1234",
      "@type": "prov:Activity",
      "prov:label": "GitHub PR #1234",
      "prov:used": ["urn:kfm:entity:git:commit:abc123", "urn:kfm:entity:git:commit:def456"],
      "prov:wasAssociatedWith": ["urn:kfm:agent:github:author", "urn:kfm:agent:github:ci-bot"]
    },
    {
      "@id": "urn:kfm:entity:git:commit:merge789",
      "@type": "prov:Entity",
      "prov:wasGeneratedBy": "urn:kfm:activity:pr:1234"
    }
  ]
}
```
</details>

> âœ… **Optional invariant:** CI can fail if a merged PR is missing required provenance nodes/edges.

---

# ğŸ¤– Example 06 â€” Focus Mode Answer Log (AI outputs as evidence artifacts)

Even when Focus Mode runs a *dynamic query* (or RAG retrieval), the answer should:
- cite sources (catalog IDs or endpoints),
- log which exact reading/time was used,
- and produce a provenance record.

<details>
<summary>ğŸ§  Focus Mode answer log (example)</summary>

```json
{
  "answer_id": "kfm.focus.answer.2026-01-20T20-01Z.topeka_gauge.v1",
  "question": "Whatâ€™s the current water level at the Kansas River gauge in Topeka?",
  "timestamp": "2026-01-20T20:01:00Z",
  "map_context": {
    "bbox": [-96.0, 38.8, -95.3, 39.3],
    "layers": ["hydro:river_gauges:realtime"]
  },
  "retrieval": [
    { "type": "graph_lookup", "id": "station:TOPEKA_KANSAS_RIVER_GAUGE" },
    { "type": "postgis_query", "id": "qry:latest_reading:TOPEKA", "result_sha256": "..." }
  ],
  "answer": {
    "text": "As of 2026-01-20 20:00Z, the water level at Topeka is X ft.",
    "citations": [
      "urn:kfm:dataset:usgs_realtime_water_data",
      "kfm.ks.hydro.river_gauge_readings.2026-01-20T20-00Z"
    ]
  },
  "prov_ref": "data/prov/kfm.focus.answer.2026-01-20T20-01Z.topeka_gauge.v1.prov.jsonld"
}
```
</details>

---

# â± Example 07 â€” Streaming ingest â€œstub PROVâ€ (still governed)

Streaming data is â€œmany small datasets over time.â€  
The platform can generate:
- a **DCAT dataset** representing the stream,
- **STAC items** per micro-batch (or per time bucket),
- and **PROV** that captures the query/collector activity (with timestamps).

âœ… Policy expectation: real-time layers still require at least *minimal provenance* before UI use.

---

# ğŸ“¦ Example 08 â€” OCI artifact distribution (ORAS + Cosign)

For â€œNASA-gradeâ€ reproducibility and sharing:
- package dataset outputs + catalogs + provenance as an **OCI artifact**
- sign it (Cosign)
- attach attestations (SLSA-style provenance, SBOMs, etc.)

<details>
<summary>ğŸ“¦ Example ORAS/Cosign flow (illustrative)</summary>

```bash
# PSEUDOCODE â€” adapt to your registry + media types
oras push ghcr.io/ORG/kfm-datasets/river_gauges:v1 \
  --artifact-type application/vnd.kfm.dataset \
  data/processed/hydro/river_gauges/river_gauges_v1.geoparquet:application/x-parquet \
  data/stac/items/kfm.ks.hydro.river_gauges.v1.json:application/json \
  data/prov/kfm.ks.hydro.river_gauges.v1.prov.jsonld:application/ld+json

cosign sign --yes ghcr.io/ORG/kfm-datasets/river_gauges:v1

# Optional: attach attestations (run manifest / SLSA provenance)
cosign attest --yes \
  --predicate data/audits/RUN_ID/run_manifest.json \
  --type slsa-provenance \
  ghcr.io/ORG/kfm-datasets/river_gauges:v1
```
</details>

---

# ğŸ›¡ï¸ Example 09 â€” Policy Pack smoke tests (OPA/Rego + Conftest)

Your CI should fail if:
- STAC/DCAT/PROV is missing
- classification is downgraded
- sources/citations are missing
- sensitive locations appear without obfuscation/redaction
- API boundary rule is violated (UI must not query graph directly)

<details>
<summary>ğŸ§  Tiny illustrative Rego rule (concept only)</summary>

```rego
package kfm.provenance

deny[msg] {
  input.changed_processed_assets == true
  not input.has_prov
  msg := "Missing PROV lineage for processed asset change"
}

deny[msg] {
  input.classification_output == "public"
  input.classification_input == "restricted"
  msg := "Classification downgrade detected (restricted -> public)"
}
```
</details>

---

# ğŸ§ª Example 10 â€” Experiment report template (MCP scientific method)

When doing NLP extraction, OCR ingestion, georeferencing comparisons, model selection, etc. â€” log it like a lab notebook:

- question/problem
- hypothesis
- methods
- data used
- results + artifacts
- conclusion
- next steps

<details>
<summary>ğŸ§ª Minimal experiment README (template skeleton)</summary>

```markdown
# EXP-00X â€” Title

## Objective ğŸ¯
(What question are we answering?)

## Data ğŸ“¦
- Dataset IDs:
- Versions:
- Provenance refs:

## Method ğŸ§°
- Steps:
- Parameters:
- Environment:

## Results ğŸ“Š
- Outputs:
- Metrics:
- Links to artifacts:

## Interpretation ğŸ§ 
(What did we learn?)

## Next Steps ğŸ”
(What do we do next?)
```
</details>

---

## âš ï¸ Gotchas (common drift points)

- **DCAT path:** Some docs reference `data/catalogs/` while v13 conventions also show `data/catalog/dcat/`. Pick one canonical location in your repo and make policies enforce it consistently.
- **â€œEvidence artifactâ€ means everything:** AI-generated layers, OCR corpora, simulation outputs â€” all must publish STAC/DCAT/PROV and be governed like â€œnormalâ€ datasets.
- **No narrative without evidence:** Story Nodes and Focus Mode must cite cataloged sources and identify AI-generated text clearly.
- **Sovereignty/classification propagation:** Outputs must not be *less restricted* than inputs. Redact/obfuscate where needed.

---

## ğŸ§³ Appendix â€” Using the â€œProgramming Booksâ€ PDF Portfolios

Some project PDFs are **portfolios** containing multiple embedded books/resources (they may show â€œopen in Acrobatâ€ if viewed as plain text). You can still access embedded files locally.

### ğŸ” List embedded documents
```bash
pdfdetach -list "Various programming langurages & resources 1.pdf"
pdfdetach -list "Data Managment-Theories-Architures-Data Science-Baysian Methods-Some Programming Ideas.pdf"
pdfdetach -list "Maps-GoogleMaps-VirtualWorlds-Archaeological-Computer Graphics-Geospatial-webgl.pdf"
pdfdetach -list "AI Concepts & more.pdf"
```

### ğŸ“¤ Extract one embedded file
```bash
# Example: extract file number 13 (name varies)
pdfdetach -save 13 -o extracted_book.pdf "Maps-GoogleMaps-VirtualWorlds-Archaeological-Computer Graphics-Geospatial-webgl.pdf"
```

> These portfolios are meant to serve as the projectâ€™s â€œğŸ“š built-in reference libraryâ€ for geospatial, data engineering, CI/CD, cryptography/signing, and AI governance.

---

## ğŸ“š Sources & design docs used for these examples

### âœ… Core KFM design (primary)
- ğŸ“¥ Data intake philosophy, pipeline gating, streaming + Focus Mode provenance â†’  [oai_citation:0â€¡ğŸ“š Kansas Frontier Matrix (KFM) Data Intake â€“ Technical & Design Guide.pdf](file-service://file-EbUCdsJMbu5KwpoKMrLrgj)  [oai_citation:1â€¡ğŸ“š Kansas Frontier Matrix (KFM) Data Intake â€“ Technical & Design Guide.pdf](file-service://file-EbUCdsJMbu5KwpoKMrLrgj)
- ğŸ§­ AI system overview (Watcherâ€“Plannerâ€“Executor, supply chain, governance) â†’  [oai_citation:2â€¡Kansas Frontier Matrix (KFM) â€“ AI System Overview ğŸ§­ğŸ¤–.pdf](file-service://file-Pv8eev6RWvCKrGCXyzY7zg)
- ğŸ§± Architecture & design (FAIR+CARE, mandatory provenance, AI-enhanced ingestion) â†’  [oai_citation:3â€¡Kansas Frontier Matrix (KFM) â€“ Comprehensive Architecture, Features, and Design.pdf](file-service://file-4Umt1yHoGKicdmLWzFJ9sC)
- ğŸ—ºï¸ UI system overview (mobile/offline, AR future, governed UX expectations) â†’  [oai_citation:4â€¡Kansas Frontier Matrix â€“ Comprehensive UI System Overview.pdf](file-service://file-KcBQruYcoFVDEixzzRHTwt)
- ğŸ§¾ Technical documentation (docs-first, governance, SOPs, long-form architecture notes) â†’  [oai_citation:5â€¡Kansas Frontier Matrix (KFM) â€“ Comprehensive Technical Documentation.pdf](file-service://file-AkqwUuYPp5zePf7pv5SMxi)
- ğŸŒŸ Latest ideas & proposals (PRâ†’PROV, policy pack tightening) â†’  [oai_citation:6â€¡ğŸŒŸ Kansas Frontier Matrix â€“ Latest Ideas & Future Proposals.docx.pdf](file-service://file-SQ3f7ve8SGiusT6ThZEuCe)
- ğŸ’¡ Innovative concepts (GeoXAI, cultural protocols, sensitivity-aware handling, crowdsourced QA) â†’  [oai_citation:7â€¡Innovative Concepts to Evolve the Kansas Frontier Matrix (KFM).pdf](file-service://file-G71zNoWKxsoSW44iwZaaCC)  [oai_citation:8â€¡Innovative Concepts to Evolve the Kansas Frontier Matrix (KFM).pdf](file-service://file-G71zNoWKxsoSW44iwZaaCC)
- ğŸ§  Additional ideas (run manifests, canonical digests, OCI artifact distribution, evidence manifests) â†’  [oai_citation:9â€¡Additional Project Ideas.pdf](file-service://file-Pc2GNivcrHBeKjBQksLC3T)

### ğŸ§¾ MCP / documentation protocol references
- ğŸ“˜ Master guide v13 patterns & invariants (pipeline ordering, boundary artifacts, evidence artifacts) â†’  [oai_citation:10â€¡MARKDOWN_GUIDE_v13.md.gdoc](file-service://file-UYVruFXfueR8veHMUKeugU)
- ğŸ§ª Scientific Method / Research / MCP documentation approach â†’  [oai_citation:11â€¡Scientific Method _ Research _ Master Coder Protocol Documentation.pdf](file-service://file-HTpax4QbDgguDwxwwyiS32)
- ğŸ—‚ï¸ Hub design doc (MCP-compatible templates, SOPs, experiment reports) â†’  [oai_citation:12â€¡Kansas-Frontier-Matrix_ Open-Source Geospatial Historical Mapping Hub Design.pdf](file-service://file-64djFYQUCmxN1h6L6X7KUw)

### ğŸ“š Reference-library portfolios (embedded multi-doc PDFs)
- ğŸ§  AI reference portfolio â†’  [oai_citation:13â€¡AI Concepts & more.pdf](file-service://file-K6BctJjeUwvyCahLf9qdwr)
- ğŸ—ºï¸ Maps / WebGL / geospatial reference portfolio â†’  [oai_citation:14â€¡Kansas Frontier Matrix â€“ Comprehensive UI System Overview.pdf](file-service://file-KcBQruYcoFVDEixzzRHTwt)
- ğŸ§° Programming languages & tooling reference portfolio â†’  [oai_citation:15â€¡Innovative Concepts to Evolve the Kansas Frontier Matrix (KFM).pdf](file-service://file-G71zNoWKxsoSW44iwZaaCC)
- ğŸ—„ï¸ Data management / architecture / data science reference portfolio â†’  [oai_citation:16â€¡Kansas Frontier Matrix (KFM) â€“ AI System Overview ğŸ§­ğŸ¤–.pdf](file-service://file-Pv8eev6RWvCKrGCXyzY7zg)
