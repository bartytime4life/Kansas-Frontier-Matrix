# ğŸ§ª Fail Fixtures â€” Dev Provenance Policy Pack (OPA/Rego + Conftest)

![OPA](https://img.shields.io/badge/OPA-Open%20Policy%20Agent-7B42BC?logo=openpolicyagent&logoColor=white)
![Conftest](https://img.shields.io/badge/Conftest-Policy%20Tests-111827)
![Rego](https://img.shields.io/badge/Rego-Policy%20as%20Code-0EA5E9)
![Fail Closed](https://img.shields.io/badge/Gate-fail--closed-DC2626)
![Provenance First](https://img.shields.io/badge/Provenance-first-10B981)

> âš ï¸ **This folder is intentionally â€œbad.â€** Everything here should **fail** policy checks.  
> It exists to prove our governance gates catch violations before anything becomes â€œrealâ€ in KFM.

---

## ğŸ“ What this folder is

This directory contains **negative test fixtures** for the KFM policy pack â€” files that violate rules on purpose, so CI can confirm we **block** them.

Typical uses:
- âœ… Prevent regressions (a rule should *stay enforced*)
- âœ… Validate new rules (add a new fail fixture first)
- âœ… Document edge cases (why something is rejected)

---

## ğŸ—‚ï¸ Recommended layout

```text
ğŸ“¦ mcp/dev_prov/policies/
â”£â”â” ğŸ“œ rego/                     # policy rules (*.rego)
â”£â”â” ğŸ§ª fixtures/
â”ƒ   â”£â”â” âœ… pass/                 # fixtures that MUST pass
â”ƒ   â”—â”â” âŒ fail/                 # fixtures that MUST fail (you are here)
â”ƒ       â”£â”â” ğŸ§¾ metadata/         # DCAT/STAC/contract violations
â”ƒ       â”£â”â” â›“ï¸ provenance/        # PROV / lineage / run-manifest violations
â”ƒ       â”£â”â” ğŸ” security/         # secrets, tokens, unsafe URLs, etc.
â”ƒ       â”£â”â” ğŸ¤– ai/               # AI-output policies (citations, labels, etc.)
â”ƒ       â”£â”â” ğŸ§¬ graph/            # graph integrity / orphan checks (if modeled as files)
â”ƒ       â”—â”â” ğŸ“¦ supply_chain/     # signing / OCI / artifact integrity checks
```

> If your repo layout differs, keep the **intent**: *fail fixtures are grouped by what theyâ€™re testing*.

---

## â–¶ï¸ How to run locally

From the repo root (adjust paths if needed):

```bash
# Run ONLY the fail fixtures (expect failures)
conftest test \
  -p mcp/dev_prov/policies/rego \
  mcp/dev_prov/policies/fixtures/fail

# Run pass fixtures (expect success)
conftest test \
  -p mcp/dev_prov/policies/rego \
  mcp/dev_prov/policies/fixtures/pass
```

Pro tip:
- Fail fixtures should produce **stable, readable** failure messages.
- Pass fixtures should remain **minimal** to avoid â€œaccidentalâ€ failures.

---

## ğŸ§© What we typically enforce (and therefore break here)

Below is a â€œmenuâ€ of common policy categories. Your actual `.rego` rules define truth; fixtures prove it.

### 1) ğŸ§¾ Metadata completeness (FAIR)
Fail cases usually include:
- Missing `license` (or not an approved SPDX string)
- Missing `publisher/provider` fields
- Missing spatial/temporal coverage in STAC/DCAT-like records
- Missing dataset ID / version / classification tags

**Example filenames**
- `metadata/MISSING__license__dcat.json`
- `metadata/INVALID__spdx__stac_item.json`

---

### 2) â›“ï¸ Provenance & lineage (Evidence-first)
Fail cases usually include:
- Processed data changed without a matching PROV update
- PROV entity missing `wasDerivedFrom` / `used` / `wasGeneratedBy`
- Run manifests missing checksums or canonical digests
- â€œOrphanedâ€ lineage nodes (e.g., activity not linked to inputs/outputs)

**Example filenames**
- `provenance/MISSING__prov_links__prov.jsonld`
- `provenance/ORPHAN__activity__prov.jsonld`
- `provenance/MISSING__checksums__run_manifest.json`

---

### 3) ğŸ” Security hygiene
Fail cases usually include:
- Obvious secrets (AWS keys, JWT-like strings, private tokens)
- Disallowed external endpoints
- Unsafe config patterns (e.g., debug flags in prod configs)

**Example filenames**
- `security/SECRET__aws_key__dataset.json`
- `security/SECRET__jwt__config.yml`

---

### 4) ğŸ¤– AI governance (Focus Mode / generated content)
Fail cases usually include:
- AI output missing citations
- Missing â€œAI-generatedâ€ labeling/metadata (if required)
- Output references a dataset without provenance/citation anchors

**Example filenames**
- `ai/MISSING__citations__answer.json`
- `ai/MISSING__ai_label__story_node.md`

---

### 5) ğŸ“¦ Supply chain / artifact integrity
Fail cases usually include:
- Artifact references missing digest pins
- Missing signature/attestation (if required)
- OCI artifact metadata missing required referrers

**Example filenames**
- `supply_chain/MISSING__cosign_sig__artifact_ref.yml`
- `supply_chain/MISSING__digest__oci_distribution.yml`

---

## ğŸ§  Naming convention (strongly recommended)

Use names that explain **what should fail** and **why**:

```text
<AREA>/<STATUS>__<RULE_OR_CONCEPT>__<SHORT_CASE>.<ext>

# Examples:
metadata/MISSING__license__dcat.json
provenance/INVALID__prov_chain__prov.jsonld
security/SECRET__jwt__env.json
ai/MISSING__citations__answer.json
```

Why this works:
- ğŸ” Greppable
- ğŸ“š Self-documenting
- ğŸ§¯ Easy to map to policy IDs later

---

## ğŸ§· Fixture design rules (keep them sharp âœ‚ï¸)

âœ… **Minimize**: smallest file that reproduces the failure  
âœ… **One reason to fail**: avoid multi-fail â€œsoupâ€ unless youâ€™re explicitly testing bundling  
âœ… **Stable failures**: donâ€™t depend on time, randomness, or network  
âœ… **Readable**: prefer short JSON/YAML with comments (where allowed)  
âœ… **No real secrets**: even in fixtures â€” use obviously fake patterns

---

## â• Adding a new fail fixture (Golden Path ğŸ†)

1. **Pick one rule** youâ€™re testing (or one new rule youâ€™re introducing)
2. Create the smallest violating artifact in the right subfolder
3. Run:
   ```bash
   conftest test -p mcp/dev_prov/policies/rego mcp/dev_prov/policies/fixtures/fail
   ```
4. Confirm:
   - It fails for the expected reason
   - The error message is understandable
5. (Optional but ğŸ”¥) Add a matching **pass** fixture demonstrating the compliant version

---

## ğŸ§¯ Troubleshooting

- **â€œFail fixture passedâ€** â†’ a regression (or your fixture didnâ€™t actually violate the rule)
- **â€œPass fixture failedâ€** â†’ your rule became stricter (or fixture needs updating)
- **Lots of failures at once** â†’ fixture might be breaking multiple rules; simplify it

---

## ğŸ“š Project reference library (why these rules exist)

These policy fixtures are aligned with KFMâ€™s core principles:
- ğŸ§¾ **FAIR + CARE** governance
- â›“ï¸ **Provenance-first / evidence-first publishing**
- ğŸ” **Fail-closed gates**
- ğŸ¤– **AI outputs must stay citeable and auditable**
- ğŸ—ºï¸ **UI must surface provenance (â€œmap behind the mapâ€)**
- ğŸ“¦ **Supply chain integrity for artifacts**

Key docs / packs in this repoâ€™s broader library (see `/mnt/data` uploads in this project workspace):
- ğŸ“˜ Kansas Frontier Matrix â€” Comprehensive Architecture, Features, and Design
- ğŸ“— Kansas Frontier Matrix â€” AI System Overview
- ğŸ“™ Kansas Frontier Matrix â€” Comprehensive UI System Overview
- ğŸ“• KFM Data Intake â€” Technical & Design Guide
- ğŸ’¡ Innovative Concepts to Evolve KFM
- ğŸ§  AI Concepts & more
- ğŸ—ºï¸ Maps/GoogleMaps/VirtualWorlds/Archaeological/Computer Graphics/Geospatial/WebGL
- ğŸ§° Various programming languages & resources
- ğŸ—„ï¸ Data Management / Architectures / Data Science / Bayesian Methods
- ğŸ§ª Additional Project Ideas
- ğŸŒŸ Latest Ideas & Future Proposals

---

### âœ… Youâ€™re in the right place ifâ€¦
- Youâ€™re building **new policies** and want confidence theyâ€™ll be enforced
- Youâ€™re strengthening provenance/security/AI governance and need **tests that prove the gate closes**
- Youâ€™re documenting â€œwhy we reject thisâ€ with a concrete artifact

Happy breaking things (safely) ğŸ˜ˆâœ¨
