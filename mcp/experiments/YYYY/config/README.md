# ğŸ§ª MCP Experiments â€” Config (YYYY)

![MCP](https://img.shields.io/badge/MCP-Master%20Coder%20Protocol-blueviolet)
![KFM](https://img.shields.io/badge/KFM-Kansas%20Frontier%20Matrix-orange)
![Configs](https://img.shields.io/badge/Scope-configs%20only-2ea44f)
![Provenance](https://img.shields.io/badge/Provenance-STAC%20%2B%20DCAT%20%2B%20PROV-success)
![Governance](https://img.shields.io/badge/Governance-OPA%20%2B%20Conftest-informational)
![Data](https://img.shields.io/badge/Data-append--only-critical)
![Year](https://img.shields.io/badge/Year-YYYY-lightgrey)

> [!TIP]
> Replace `YYYY` with the 4-digit year (e.g., `2026`). Year folders keep experiments chronological, reviewable, and â€œappend-onlyâ€ by default. ğŸ“†âœ…

---

## ğŸ¯ What this folder is

This directory is the **source of truth for experiment configuration** in the MCP workflow.

An **experiment config** is a declarative spec that answers:

- ğŸ§  **What are we trying to prove?** (hypothesis + success criteria)
- ğŸ“¥ **What inputs are we using?** (datasets, docs, queries, ontology fragments)
- ğŸ§± **What components are we touching?** (pipeline â†”ï¸ graph â†”ï¸ UI â†”ï¸ Focus Mode)
- ğŸ§¾ **What governance must pass?** (license, sensitivity, policy gates)
- ğŸ“¦ **What must be produced?** (run manifest, STAC/DCAT/PROV, evidence manifests, report)

> [!IMPORTANT]
> **This folder is configs only.**  
> Put run outputs (reports, logs, artifacts, screenshots) in `../runs/` (or whatever the repo standard is). Keep `config/` deterministic and review-friendly. ğŸ§ŠğŸ§¾

---

## ğŸ§­ KFM â€œGolden Rulesâ€ for experiments

These are the invariants we design configs around:

1. **Provenance-first** ğŸ§¾  
   Every new dataset, layer, or derived output is traceable end-to-end (inputs â†’ transforms â†’ outputs).

2. **Evidence-first narratives** ğŸ”  
   Story Nodes and AI answers should be backed by explicit evidence inventories (citations you can verify).

3. **Append-only publishing** â•  
   Donâ€™t silently rewrite historical facts or artifacts. Publish versioned additions.

4. **Policy-as-code gates** ğŸš¦  
   Governance is enforced in CI (fail-closed). If policy fails, the experiment doesnâ€™t ship.

5. **Reproducible runs** â™»ï¸  
   Pin tool versions, record seeds, and generate a run manifest so others can reproduce results.

---

## ğŸ—‚ï¸ Recommended layout

> [!NOTE]
> Your repo may vary â€” this is a â€œgolden pathâ€ layout that plays nicely with MCP templates and KFMâ€™s auditability goals.

```text
mcp/
â””â”€ ğŸ§ª experiments/
   â””â”€ ğŸ“… YYYY/
      â”œâ”€ âš™ï¸ config/                           # ğŸ‘ˆ you are here ğŸ“Œ Declared configs + templates + schemas (PR-reviewed)
      â”‚  â”œâ”€ ğŸ“„ README.md                      # ğŸ“˜ How experiment configs work, review rules, and how runs are produced
      â”‚  â”œâ”€ ğŸ§ª experiments/                   # â€œDeclaredâ€ experiment configs (committed + PR-reviewed; stable IDs)
      â”‚  â”‚  â”œâ”€ ğŸ§¾ YYYY-001-example.yaml        # Example declared experiment (id, hypothesis, inputs, params, expected proofs)
      â”‚  â”‚  â””â”€ ğŸ§¾ YYYY-002-focus-mode-rag.yaml # Declared experiment for Focus Mode/RAG evaluation (gates + metrics)
      â”‚  â”œâ”€ ğŸ§© templates/                     # Copy/paste starters for new experiment declarations
      â”‚  â”‚  â”œâ”€ ğŸ§©ğŸ§¾ minimal.yaml               # Minimal config (enough to run + produce receipts)
      â”‚  â”‚  â””â”€ ğŸ§©ğŸ§¾ full.yaml                  # Full config (datasets, metrics, evidence, publish steps)
      â”‚  â””â”€ ğŸ“ schemas/                       # Validation contracts enforced by CI/gates
      â”‚     â”œâ”€ ğŸ“ğŸ§¾ experiment.schema.json     # Schema for declared experiment configs (YAML validated against this)
      â”‚     â””â”€ ğŸ“ğŸ§¾ evidence-manifest.schema.json # Schema for evidence manifests produced by runs
      â””â”€ ğŸƒ runs/                             # Outputs (append-only): each run is immutable once recorded
         â””â”€ ğŸ·ï¸ YYYY-001-example__RUNID/        # One run folder per execution (id + run identifier)
            â”œâ”€ ğŸ§¾ run_manifest.json            # Run ledger: who/what/when + commands + env + inputs/outputs + hashes
            â”œâ”€ ğŸ“ report.md                    # Human report: results, metrics, limitations, and evidence links
            â”œâ”€ ğŸ“¦ artifacts/                  # Produced artifacts (figures, tables, exports, logs; keep access-safe)
            â””â”€ ğŸ—‚ï¸ catalogs/                   # Generated STAC/DCAT/PROV for this run (or pointers to canonical catalogs)
```

---

## ğŸ§© Config contract (opinionated, but practical)

### âœ… Required top-level blocks

| Block | Why it exists | Typical reviewer questions |
|------|----------------|----------------------------|
| `meta` | identity + lifecycle | â€œIs this named well and scoped?â€ |
| `hypothesis` | what weâ€™re proving | â€œWhat does success look like?â€ |
| `scope` | which KFM subsystems | â€œDoes this touch UI? AI? ingestion?â€ |
| `inputs` | datasets + docs + queries | â€œAre inputs licensed and stable?â€ |
| `pipeline` | transforms + steps | â€œDeterministic? Idempotent?â€ |
| `provenance` | STAC/DCAT/PROV + run ledger | â€œCan we trace outputs?â€ |
| `governance` | FAIR/CARE + sensitivity | â€œCan this be public?â€ |
| `evaluation` | metrics + acceptance | â€œWhat numbers decide go/no-go?â€ |
| `outputs` | where artifacts land | â€œWhereâ€™s the report & manifest?â€ |
| `policy` | CI gates / constraints | â€œFail-closed? Which checks?â€ |

---

## ğŸ§¬ Canonical fields (suggested schema)

Below is a **human-first** schema sketch (not strict JSON Schema). Treat it as the â€œshapeâ€ to implement.

```yaml
meta:
  id: "YYYY-001-example"
  title: "Example experiment: Story Node + Provenance panel"
  owners: ["@your-handle"]
  status: "proposed"  # proposed|active|completed|archived|promoted
  created: "YYYY-MM-DD"
  tags: ["ui", "story-nodes", "provenance", "mcp"]

hypothesis:
  statement: >
    If Story Nodes require an evidence manifest and PROV bundle,
    then user trust + auditability will improve without slowing authoring too much.
  success_criteria:
    - "100% of Story Node claims have evidence_manifest entries"
    - "PROV bundle links story -> evidence -> generation activity"
    - "No policy gate failures in CI"

scope:
  components:
    - ui.story_nodes
    - ui.provenance_panel
    - graph.prov_edges
  risk_level: "medium"   # low|medium|high
  rollout: "demo-only"   # demo-only|dev|staging|prod-candidate

inputs:
  datasets:
    - id: "kfm.ks.example.dataset.v1"
      type: "stac+geoparquet"
      license: "CC-BY-4.0"
      where: "data/processed/example/"
  documents:
    - id: "doc.example.1908.newspaper"
      where: "docs/sources/example/1908_newspaper.pdf"
  queries:
    - id: "q.max_gauge_1908"
      text: "max flood gauge height in 1908"
      expected_source: "kfm.ks.hydro.gauges.v2"

pipeline:
  mode: "dry-run" # dry-run|run|ci
  steps:
    - name: "validate-inputs"
    - name: "generate-stac"
    - name: "generate-dcat"
    - name: "generate-prov"
    - name: "graph-upsert"
  determinism:
    seed: 42
    pinned_versions: true

provenance:
  require_triplet: true           # STAC + DCAT + PROV
  log_dynamic_queries: true       # important for Focus Mode / on-demand retrieval
  run_manifest:
    enabled: true
    canonicalize_json: true       # RFC 8785 style canonicalization (recommended)
    include_tool_versions: true
    include_source_urls: true

governance:
  classification: "public"        # public|restricted|sensitive
  fair:
    findable: true
    accessible: true
    interoperable: true
    reusable: true
  care:
    collective_benefit: true
    authority_to_control: "review-required"
    responsibility: true
    ethics: "no-sensitive-sites-exposed"

evaluation:
  metrics:
    - name: "policy_gate_pass_rate"
      target: "100%"
    - name: "provenance_completeness"
      target: ">= 0.99"
    - name: "ui_story_playback_smoothness"
      target: ">= 60fps median on dev machine"
  acceptance:
    decision: "maintainer-review" # or "auto-merge" for very low-risk changes

outputs:
  run_dir: "mcp/experiments/YYYY/runs/{{id}}__{{run_id}}/"
  report: "report.md"
  run_manifest: "run_manifest.json"
  catalogs_dir: "catalogs/"
  artifacts_dir: "artifacts/"
  screenshots_dir: "artifacts/screenshots/"

policy:
  fail_closed: true
  checks:
    - "schema.validate"
    - "rego.conftest"
    - "license.present"
    - "no_secrets"
    - "provenance.triplet_present"
```

---

## ğŸ§ª Templates you should keep in `templates/`

### ğŸŸ¢ Minimal (fast to propose)
Use for quick PoCs that still respect governance:

- `meta`
- `hypothesis`
- `scope`
- `inputs` (even if tiny)
- `outputs`
- `policy` (fail-closed)

### ğŸŸ£ Full (run-ready)
Use for anything that touches ingestion, AI behavior, public UI, or release candidates:

- everything in the schema sketch
- explicit `run_manifest`
- explicit `provenance.require_triplet`
- explicit `evaluation.metrics`

---

## ğŸš¦ Validation & CI expectations

> [!IMPORTANT]
> A config that doesnâ€™t validate is not a config â€” itâ€™s a comment. ğŸ˜„  
> Treat configs like code: typed, validated, and policy-gated.

Recommended gates:

- âœ… **Schema validation** (JSON Schema / Pydantic model)
- âœ… **Policy Pack** (OPA Rego via Conftest)
- âœ… **Secrets / license checks**
- âœ… **Provenance completeness checks**
- âœ… **Idempotency / determinism checks** (seed pinned, tool versions present)

---

## ğŸ§¾ Outputs every â€œrealâ€ experiment should produce

### 1) ğŸ§¾ Run Manifest (`run_manifest.json`)
A single ledger file capturing:
- `run_id`, `run_time`
- `idempotency_key`
- `canonical_digest`
- `source_urls`
- `tool_versions`
- `summary_counts` (records in/out, errors)

### 2) ğŸ“¦ Catalog Triplet
- ğŸ›°ï¸ STAC (spatiotemporal + assets)
- ğŸ—ƒï¸ DCAT (dataset discovery metadata)
- ğŸ”— PROV (lineage: entities + activities + agents)

### 3) ğŸ§· Evidence Manifest(s)
For Story Nodes and AI-facing outputs:
- sources + stable IDs/checksums
- queries used (if applicable)
- transformations performed
- mapping from narrative claims â†’ evidence items

> [!TIP]
> If you canâ€™t point to the evidence manifest, the feature isnâ€™t ready for users. âœ…

---

## ğŸ§  Special notes for AI / Focus Mode experiments

If your config toggles AI behavior, make it explicit:

- retrieval mode (graph / catalogs / docs / hybrid)
- citation required âœ…
- dynamic query logging enabled âœ…
- safeguards for uncertainty (especially simulations)

Keep the AI honest:
- It can *summarize*, but it must always be able to *trace*. ğŸ§­

---

## ğŸ—ºï¸ Special notes for UI experiments (2D/3D, Story Nodes, AR)

UI-facing experiment configs should declare:

- ğŸŒ 2D/3D mode expectations (MapLibre / Cesium usage)
- ğŸ•°ï¸ timeline / temporal filters (what â€œtimeâ€ means for the data)
- ğŸ§© layer toggles and provenance panel behavior (â€œmap behind the mapâ€)
- ğŸ“– Story Node packs (Markdown + JSON config + evidence manifest)
- ğŸ“± mobile/offline expectations (if relevant)
- ğŸ¥½ AR â€œscenesâ€ as a *filtered, decluttered* subset (AR cannot show everything)

---

## ğŸ¤– W-P-E automation hooks (optional but powerful)

If youâ€™re using Watcherâ€“Plannerâ€“Executor agents for upkeep:

- Watcher emits a signed event
- Planner drafts a config change (often as a PR)
- Executor runs the experiment + attaches proof (tests, manifests)

Configs should include enough metadata so W-P-E can act safely:
- owners / reviewers
- risk level
- gates to enforce
- â€œauto-merge allowedâ€ only for truly low-risk changes

---

## ğŸ§° Experiment categories you can standardize

Use these tags in `meta.tags` for sorting and dashboards:

- `data-intake` ğŸ“¥
- `pipeline` âš™ï¸
- `catalogs` ğŸ›°ï¸
- `provenance` ğŸ§¾
- `graph` ğŸ§ 
- `focus-mode` ğŸ¤–
- `story-nodes` ğŸ“–
- `ui` ğŸ—ºï¸
- `real-time` âš¡
- `simulations` ğŸ§ª
- `ar` ğŸ¥½
- `governance` ğŸš¦
- `storage-oci` ğŸ“¦

---

## ğŸ“š Project reference library (useful when writing configs)

These project docs drove the conventions above (keep them close while authoring configs):

### Core KFM design ğŸ“Œ
- **KFM â€“ Comprehensive Technical Documentation** (repo map, API patterns, system components)
- **KFM â€“ Comprehensive Architecture, Features, and Design** (stack + roadmap)
- **KFM â€“ Comprehensive UI System Overview** (2D/3D maps, Story Nodes, Focus Mode, AR)
- **KFM â€“ AI System Overview** (Focus Mode + knowledge graph + hybrid retrieval)
- **KFM Data Intake â€“ Technical & Design Guide** (pipeline philosophy, standards, governance)

### Innovation / roadmap boosters ğŸš€
- **Innovative Concepts to Evolve KFM**
- **KFM â€“ Latest Ideas & Future Proposals**
- **Additional Project Ideas**

### Resource packs ğŸ§ ğŸ“¦
> Some of these are PDF portfolios / multi-document bundles (best opened in Acrobat/Reader).

- **AI Concepts & more** (reference bundle)
- **Various programming languages & resources** (reference bundle)
- **Data Management / Theories / Architectures / Bayesian Methods** (reference bundle)
- **Maps / Google Maps / Virtual Worlds / Archaeological CG / Geospatial WebGL** (reference bundle)

---

## ğŸ§· MCP appendix: â€œDefinition of Doneâ€ for configs

An experiment config is **Done âœ…** when:

- [ ] It validates against `schemas/experiment.schema.json`
- [ ] It passes policy gates (fail-closed)
- [ ] It declares clear success criteria + evaluation metrics
- [ ] It produces a run manifest + catalog triplet (if it generates outputs)
- [ ] It has an associated run report in `../runs/`
- [ ] It is reviewable (no giant opaque blobs; prefer small, composable specs)

---

## ğŸ§¾ MCP appendix: Expected outputs (per experiment)

At minimum:

- ğŸ“„ Config file (`config/experiments/*.yaml`)
- ğŸ“„ Report (`runs/.../report.md`)
- ğŸ§¾ Run manifest (`runs/.../run_manifest.json`)

When the experiment affects published data/UI:

- ğŸ“¦ STAC + DCAT + PROV
- ğŸ§· Evidence manifest(s)
- ğŸ“¸ UI captures (screenshots / short clips) as artifacts

---

## ğŸ›Ÿ If youâ€™re unsure, do this first

1) Copy `templates/minimal.yaml` â†’ `experiments/YYYY-###-your-slug.yaml`  
2) Fill in `meta`, `hypothesis`, `inputs`, `policy`  
3) Run local validation âœ…  
4) Open PR early and iterate in public ğŸ‘€

Because in KFM, **review is a feature** â€” itâ€™s part of provenance. ğŸ§¾âœ¨
