# ğŸ§ª MCP Experiments â€” YYYY

![Year](https://img.shields.io/badge/Year-YYYY-informational)
![MCP](https://img.shields.io/badge/MCP-Methods%20%7C%20Controls%20%7C%20Processes-blueviolet)
![KFM](https://img.shields.io/badge/Project-Kansas%20Frontier%20Matrix%20(KFM)-brightgreen)
![Principle](https://img.shields.io/badge/Principle-Evidence--First-orange)
![Governance](https://img.shields.io/badge/Governance-FAIR%20%2B%20CARE-2ea44f)
![CI](https://img.shields.io/badge/CI-Policy%20Gates%20(Fail--Closed)-red)

> **This folder is the year-based â€œlab notebookâ€ for KFM experiments under MCP (Methods, Controls & Processes).**  
> Every experiment here should be **reproducible**, **evidence-backed**, and **reviewable** â€” the goal is to turn ideas into validated building blocks (or retire them fast, with receipts ğŸ§¾).

> [!TIP]
> Replace `YYYY` with the actual year (e.g., `2026`) and keep one README per year so we can build an *audit trail over time*.

---

## ğŸ§­ Quick Links

- ğŸ§© **MCP Home:** `../../README.md`
- ğŸ§ª **Experiments Home:** `../README.md`
- ğŸ§± **Architecture / Design:** `../../../docs/`
- âœ… **SOPs & Governance:** `../../sops/`
- ğŸ§° **Templates:** `../../templates/`
- ğŸ“¦ **Artifacts / Releases:** `../../../artifacts/` (or your chosen registry + index)

---

## ğŸ¯ What â€œCountsâ€ as an Experiment?

If it changes **outputs, decisions, or user experience** and youâ€™re not 100% sure what will happen, itâ€™s an experiment:

- ğŸ¤– **AI / Focus Mode**: prompt routing, retrieval strategies, citation enforcement, safety controls, eval harnesses
- ğŸ§¬ **Graph + Ontology**: schema evolution, relationship rules, entity resolution, â€œconcept nodesâ€
- ğŸ›°ï¸ **Geo + Data**: ingestion transforms, new sources, projections, tiling formats, PMTiles/COG/GeoParquet pipelines
- ğŸ—ºï¸ **UI / Storytelling**: story playback UX, timeline behavior, layer controls, Story Builder prototypes, AR scenes
- ğŸ§ª **Ops + Reliability**: monitoring, drift detection, graph health checks, CI policy gates, deterministic pipelines

---

## ğŸ§  MCP Experiment Principles

> [!IMPORTANT]
> **If it canâ€™t be reproduced, it didnâ€™t happen.**  
> **If it canâ€™t be cited, itâ€™s a hypothesis â€” not a fact.**

**Golden rules (non-negotiable):**
- ğŸ§¾ **Evidence-first**: claims must trace to datasets, queries, or sources.
- ğŸ§¬ **Provenance-first**: every output must be linked to inputs + process.
- ğŸ§± **Deterministic-by-default**: idempotent runs, stable IDs, repeatable results.
- ğŸ”’ **Policy gates fail closed**: missing licenses/metadata/provenance = no merge.
- ğŸ§‘â€âš–ï¸ **Human-in-the-loop**: automation can propose; humans approve what becomes â€œrealâ€.

---

## ğŸ—‚ï¸ Year Folder Layout

This is the recommended structure for `mcp/experiments/YYYY/`:

```text
mcp/experiments/YYYY/
â”œâ”€â”€ README.md                # ğŸ“Œ You are here (year index + rules)
â”œâ”€â”€ _assets/                 # ğŸ–¼ï¸ Shared images, diagrams, screenshots for this year
â”œâ”€â”€ _reports/                # ğŸ“š Year-level rollups (quarterly, annual summaries)
â”œâ”€â”€ EXP-001-short-slug/      # ğŸ§ª Individual experiment folders
â”œâ”€â”€ EXP-002-short-slug/
â””â”€â”€ EXP-999-template/        # ğŸ§° Optional: frozen copy of the template for easy cloning
```

---

## ğŸ§¾ Experiment Index (Update This As You Go)

> Keep this table current. It becomes the â€œmap of workâ€ for the year.

| ID | Title | Domain | Status | Start | Owner(s) | Key Result |
|---:|---|---|---|---|---|---|
| EXP-001 | _TBD_ | AI / Data / UI / Ops | Proposed | YYYY-MM-DD | @you | _TBD_ |
| EXP-002 | _TBD_ |  |  |  |  |  |

**Status vocabulary**
- ğŸŸ¡ **Proposed** â†’ ğŸŸ¦ **Running** â†’ ğŸŸ¢ **Completed** â†’ âš« **Archived**
- ğŸ” **Superseded** is allowed (link to replacement experiment)

---

## ğŸš€ Start a New Experiment (5-Minute â€œKickoffâ€)

1. ğŸ†• Create a folder: `EXP-###-short-slug/`
2. ğŸ§ª Copy the template files (see below) into the folder
3. âœï¸ Write the **question + hypothesis + success metrics**
4. ğŸ§± Pin the environment (container/requirements) + record dataset versions
5. â–¶ï¸ Run it, produce artifacts + provenance, write results
6. âœ… Open a PR with:
   - a short summary
   - output links / artifact digests
   - what you learned
   - what you recommend next

---

## ğŸ§° Experiment Folder Template

```text
EXP-###-short-slug/
â”œâ”€â”€ README.md                 # ğŸ§  1-page executive summary (what/why/result)
â”œâ”€â”€ protocol.md               # ğŸ§ª MCP protocol (question â†’ method â†’ analysis)
â”œâ”€â”€ experiment.yaml           # ğŸ—‚ï¸ machine-readable metadata (IDs, inputs, metrics)
â”‚
â”œâ”€â”€ env/                      # ğŸ§± reproducible environment
â”‚   â”œâ”€â”€ Dockerfile            # (preferred) or
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ environment.yml
â”‚
â”œâ”€â”€ src/                      # ğŸ§© scripts / modules
â”œâ”€â”€ notebooks/                # ğŸ““ exploratory work (must still be reproducible)
â”œâ”€â”€ configs/                  # âš™ï¸ config files used in runs
â”‚
â”œâ”€â”€ data/                     # ğŸ§Š keep light â€” store pointers, not massive blobs
â”‚   â”œâ”€â”€ inputs/               # dataset pointers + manifests (no raw dumps unless small)
â”‚   â””â”€â”€ outputs/              # small derived outputs or index files
â”‚
â”œâ”€â”€ outputs/                  # ğŸ“ˆ figures, tables, intermediate results
â”œâ”€â”€ report/                   # ğŸ§¾ final narrative results (what happened + why)
â”‚   â””â”€â”€ RESULTS.md
â”‚
â”œâ”€â”€ logs/                     # ğŸ§µ run logs (NDJSON recommended)
â”œâ”€â”€ audits/                   # ğŸ” run manifests + policy reports (if used)
â”‚   â””â”€â”€ run_manifest.json
â”‚
â”œâ”€â”€ prov/                     # ğŸ§¬ provenance bundles (PROV JSON-LD, etc.)
â”œâ”€â”€ stac/                     # ğŸ›°ï¸ STAC Items/Collections for experiment artifacts
â”œâ”€â”€ dcat/                     # ğŸ—ƒï¸ DCAT dataset records (if experiment creates datasets)
â”‚
â””â”€â”€ artifacts/                # ğŸ“¦ artifact references (OCI digests, checksums, SBOMs)
    â””â”€â”€ artifact_index.md
```

---

## ğŸ§¾ `experiment.yaml` (Suggested Minimal Schema)

> Keep it boring and consistent â€” it enables automation later ğŸ¤–

```yaml
id: EXP-###-short-slug
year: YYYY
title: "..."
status: proposed|running|completed|archived|superseded
owners:
  - handle: "@yourname"
domains: [ai, ui, pipeline, graph, ops, governance]

question: "What are we trying to learn?"
hypothesis: "If we do X, then Y will improve because Z."
success_metrics:
  - id: metric.example
    target: ">= 0.95"
    measure: "definition + calculation method"

inputs:
  datasets:
    - id: dataset.catalog.id
      license: "..."
      sensitivity: public|restricted|confidential
      pointers:
        - "stac/item.json"
        - "dcat/dataset.json"
  configs:
    - "configs/config.yaml"

environment:
  container_image: "ghcr.io/ORG/IMAGE:tag"
  container_digest: "sha256:..."
  tool_versions:
    python: "3.11.x"
    gdal: "..."
    node: "..."

runs:
  - run_id: "YYYYMMDD-HHMM-<short>"
    seed: 42
    manifest: "audits/run_manifest.json"
    prov_bundle: "prov/prov.jsonld"
    outputs:
      - "report/RESULTS.md"
      - "outputs/"

artifacts:
  - kind: dataset|model|tiles|report
    location: "artifacts/artifact_index.md"

decision:
  outcome: adopt|iterate|reject|park
  promoted_to:
    - "ADR-###"
    - "pipeline/<name>"
notes: "Any extra context"
```

---

## ğŸ§ª `protocol.md` (MCP Scientific Method Skeleton)

```markdown
# Protocol â€” EXP-### (Title)

## 1) Question
- ...

## 2) Hypothesis
- ...

## 3) Method (Plan)
- Inputs:
- Controls / baselines:
- Variables:
- Procedure:
- Risks & mitigations:
- Governance constraints (licenses/sensitivity/ethics):

## 4) Execution
- Run commands / steps:
- Environments used:
- Run IDs:

## 5) Analysis
- Metrics:
- Statistical notes:
- Failure modes:
- Surprises:

## 6) Conclusion
- Supported? (Y/N/Partial)
- Recommendation: adopt / iterate / reject / park
- Follow-ups:
```

---

## ğŸ”’ Governance, Sensitivity, and â€œDo No Harmâ€ ğŸ›¡ï¸

Experiments can touch sensitive things (precise locations, restricted datasets, cultural data, PII).  
**Every experiment must declare:**
- ğŸ” **Sensitivity level** (public/restricted/confidential)
- ğŸ§­ **Location handling** (exact vs generalized)
- ğŸ“œ **License & reuse constraints**
- ğŸ¤ **CARE alignment** (authority to control, responsibility, ethics)

> [!CAUTION]
> If an experiment uses or generates sensitive location data, store *generalized* outputs by default and keep exact geometry behind access controls.

---

## âœ… Policy Gates (Fail-Closed)

Treat CI as a safety rail, not a speed bump.

**Common gates we enforce:**
- âœ… every dataset has a license + sensitivity tag
- âœ… no secrets / tokens / private keys in repo
- âœ… required metadata present (STAC/DCAT/PROV where applicable)
- âœ… provenance links exist (no orphan outputs)
- âœ… experiment outputs reference their run manifest + environment

> [!NOTE]
> Gates should be easy to satisfy *when you do the right thing* â€” painful only when you skip steps.

---

## ğŸ§¬ Provenance & Reproducibility Contract

### Minimum â€œRepro Packâ€ for any significant run
- ğŸ§¾ `audits/run_manifest.json` (inputs/outputs/tool versions/summary counts)
- ğŸ§¬ `prov/` bundle linking inputs â†’ process â†’ outputs
- ğŸ§± pinned environment (container digest preferred)
- ğŸ§  results narrative (`report/RESULTS.md`)
- ğŸ“¦ artifact references (checksums or OCI digests)

### Recommended: Run Manifest Pattern
Store a **self-identifying** run manifest:
- canonicalize JSON (stable hashing)
- compute a SHA-256 `canonical_digest`
- use the digest as an immutable run identity

This makes experiments idempotent and auditable.

---

## ğŸ“¦ Artifact Storage (When Git Isnâ€™t Enough)

For large artifacts (PMTiles, GeoParquet, model weights, big rasters):
- âœ… Store **pointers + metadata** in Git
- ğŸ“¦ Store binaries in a registry/object store (OCI registry recommended)
- ğŸ” Sign artifacts when possible (provenance + integrity)

Keep `artifacts/artifact_index.md` human-readable, e.g.:

```markdown
# Artifacts â€” EXP-###

| Kind | Name | Ref | Digest | Notes |
|---|---|---|---|---|
| tiles | surficial_geology.pmtiles | ghcr.io/...:2026-01-11 | sha256:... | Kansas statewide |
| dataset | hydro_obs.geoparquet | ghcr.io/...:v1 | sha256:... | hourly ingest |
```

---

## ğŸ¤– Human-in-the-Loop Automation (Watcher â†’ Planner â†’ Executor)

KFM automation should behave like an **assistant**, not an unaccountable actor:

- ğŸ‘€ **Watcher**: detects triggers (new data, anomalies, broken SLAs, drift)
- ğŸ§  **Planner**: proposes an experiment plan + expected outcomes
- ğŸ› ï¸ **Executor**: runs the plan in a controlled environment, produces artifacts + provenance
- âœ… **Human**: reviews, signs off, merges/promotes

> [!TIP]
> If a watcher auto-generates content (e.g., a â€œpulseâ€), require a reviewer and attach the evidence manifest.

---

## ğŸ§© KFM-Aligned Experiment Themes (Good Targets)

### ğŸŒŠ 1) Real-time â€œPulseâ€ content + anomaly detection
- Geotagged short narratives tied to regions/datasets
- Watchers trigger based on sensor anomalies or pattern detection
- Must include evidence manifests + provenance

### ğŸ§  2) Conceptual Attention Nodes + thematic navigation
- Promote â€œconceptsâ€ to first-class graph entities (drought, biodiversity, railroadsâ€¦)
- Evaluate: discovery UX, query relevance, AI grounding quality

### ğŸ—ºï¸ 3) Story Nodes & Story Builder
- Stories are file-based (Markdown + JSON config)
- Experiments can test: citation UX, step playback, timeline sync, authoring workflow

### ğŸ›°ï¸ 4) Temporal simulation & AR / hybrid storytelling
- â€œ4Dâ€ thinking: time-travel through layers, guided tours, scenario playback
- Prototype AR â€œscenesâ€ that reuse existing APIs/graph data

### ğŸ“¦ 5) Deterministic ingestion patterns
- exactly-once ingestion, idempotent merges, stable identifiers
- evaluate: rerun safety, duplication rate, latency, lineage completeness

---

## ğŸ“ Metrics & Evaluation (Pick Metrics Before You Run)

**AI / Focus Mode**
- citation coverage (% statements with support)
- groundedness / factuality (human or benchmark eval)
- refusal rate (correct refusals vs false refusals)
- latency (P50/P95)
- drift (quality over time)

**Pipelines**
- idempotency (same inputs â†’ same outputs)
- schema validation pass rate
- provenance completeness (no orphan outputs)
- SLA freshness (time since last update)

**UI**
- time-to-insight (task completion time)
- error rate / confusion points
- â€œtrust actionsâ€ (did users click evidence?)
- accessibility checks

**Graph**
- orphan node rate
- query performance (p95)
- entity resolution precision/recall

---

## ğŸ Promotion Path: From Experiment â†’ Product

When an experiment â€œwinsâ€ ğŸŸ¢:

1. ğŸ§¾ Write a short decision summary in `report/RESULTS.md`
2. ğŸ§± Create/Update a **Design Pack** or ADR describing the promoted change
3. âœ… Add tests + policy rules + docs
4. ğŸšš Wire it into ingestion / graph / API / UI
5. ğŸ“£ Announce via release notes / changelog

When it â€œlosesâ€ âš«:
- archive it with the results + why, and link any follow-on ideas

---

## ğŸ§¬ Lifecycle Diagram (MCP-Friendly)

```mermaid
flowchart TD
  A[Question ğŸ§ ] --> B[Hypothesis ğŸ’¡]
  B --> C[Protocol + Metrics ğŸ§ª]
  C --> D[Implement + Pin Env ğŸ§±]
  D --> E[Run + Log + Manifest ğŸ§¾]
  E --> F[Artifacts + PROV + STAC/DCAT ğŸ“¦ğŸ§¬]
  F --> G[Analysis + Results ğŸ“ˆ]
  G --> H{Decision âœ…}
  H -->|Adopt| I[Design Pack / ADR ğŸ§±]
  H -->|Iterate| C
  H -->|Reject| J[Archive âš«]
  I --> K[Promote to Pipelines/UI/Graph ğŸš€]
```

---

## ğŸ“š Suggested Year Rollups (Optional, But Powerful)

Add quarterly/annual rollups to `_reports/`:
- âœ… what shipped
- âŒ what failed + why
- ğŸ“ˆ what metrics improved
- ğŸ§­ what to do next year

---

## ğŸ§¾ Mini-Checklist (Before You Merge)

- [ ] `experiment.yaml` completed (inputs, metrics, owners, status)
- [ ] environment pinned (container digest or frozen deps)
- [ ] run manifest present + stable run_id
- [ ] provenance bundle present (or explicitly â€œN/Aâ€ with reason)
- [ ] artifacts referenced (checksums/digests)
- [ ] results written in `report/RESULTS.md`
- [ ] sensitivity + license declared
- [ ] no large raw blobs committed accidentally

---

## ğŸ§° FAQ

**Q: Where do big files go?**  
A: Prefer artifact storage (OCI registry/object store). Keep Git for manifests, indexes, and small samples.

**Q: Can notebooks live here?**  
A: Yes â€” but they must be runnable, versioned, and backed by pinned environments.

**Q: Do we really need provenance for UI experiments?**  
A: If the UI experiment generates claims, derived datasets, or publishable narratives â†’ yes.  
If itâ€™s purely layout/interaction and doesnâ€™t generate new data, provenance can be â€œN/Aâ€ (document why).

---

## ğŸ·ï¸ Glossary (Tiny, But Useful)

- **MCP**: Methods, Controls & Processes (the operational backbone)
- **PROV**: provenance graph describing how outputs were produced
- **STAC/DCAT**: structured catalogs for data + assets
- **Evidence Manifest**: explicit list of sources supporting claims/narratives
- **OCI Artifacts**: content-addressed packages (optionally signed)

---

_âœ… Keep it tight. Keep it honest. Keep it reproducible._ ğŸš€
