# ğŸ§ª 05_simulation â€” Scenario Modeling & Simulation Notebooks

![Jupyter](https://img.shields.io/badge/Jupyter-Notebooks-F37626?logo=jupyter&logoColor=white)
![Deterministic](https://img.shields.io/badge/Deterministic-seeds%20%2B%20locks-success)
![Evidence Triplet](https://img.shields.io/badge/Evidence-STAC%20%2B%20DCAT%20%2B%20PROV-blue)
![PR First](https://img.shields.io/badge/Workflow-PR--first%20publishing-informational)
![Policy](https://img.shields.io/badge/Governance-Policy%20Pack-purple)

> [!IMPORTANT]
> **In KFM, simulations are â€œpublishable evidence artifacts,â€ not one-off plots.**  
> Your run is only *officially usable* by Graph/API/UI **after** it is **(1) reproducible**, **(2) provenance-linked**, **(3) policy-checked**, and **(4) promoted** from the sandbox into published storage.

---

## ğŸ§­ Quick Links

- ğŸ§° **Simulation runbook:** `data/work/sims/README.md`  
- ğŸ§¬ **Evidence-first data lifecycle:** `docs/MASTER_GUIDE_v13.md`  
- ğŸ—ºï¸ **UI expectations (layers, provenance, scenario compare):** see â€œUI System Overviewâ€ docs  
- ğŸ§  **AI + notebooks + PR-first modeling:** see â€œAI System Overviewâ€ docs  

---

## ğŸ¯ What lives in this folder

This folder is the **notebook workspace** for building and validating â€œwhat-ifâ€ models and simulations that can become **first-class KFM layers** (after review + promotion).

Typical notebook families:

- ğŸ§© **Scenario definition** (inputs, parameters, uncertainty model, seeds)
- âš™ï¸ **Simulation execution** (e.g., `kfm-sim-run` or domain-specific model runners)
- âœ… **Verification & validation** (sanity checks, regression tests, comparisons vs observed data)
- ğŸ² **Uncertainty quantification** (ensembles, sensitivity analysis, stability checks)
- ğŸ“¦ **Publishing** (dual-format artifacts + STAC/DCAT/PROV + PR automation hooks)
- ğŸ—ºï¸ **UI readiness** (tiles, styling hints, time extent, scenario labels, provenance summaries)

> [!NOTE]
> KFMâ€™s philosophy is **â€œPR-first modelingâ€**: simulation results are treated like code contributionsâ€”**reviewable diffs**, **auditable provenance**, and **merge-gated** publishing. ğŸ§¾âœ…

---

## ğŸ§± Where simulations fit in the KFM pipeline

```mermaid
flowchart LR
  A[ğŸ“¥ Inputs\nSTAC/DCAT pinned datasets] --> B[ğŸ§ª Run in Sandbox\n data/work/sims/<model>/<run_id>/]
  B --> C[âœ… V&V + UQ\nQA report + stability notes]
  C --> D[ğŸ“¦ Package Outputs\nanalysis + tiles + metadata]
  D --> E[ğŸ§¾ Evidence Triplet\nSTAC + DCAT + PROV]
  E --> F[ğŸ”’ Policy Gates\nclassification/license/safety]
  F --> G[ğŸ“Œ Promote\n data/processed/...]
  G --> H[ğŸ•¸ Graph Ingest\nNeo4j lineage + links]
  H --> I[ğŸ§  API\nserved w/ governance]
  I --> J[ğŸ—ºï¸ UI\nlayers + scenario compare]
```

---

## ğŸ“¦ What you must produce

| Artifact ğŸ§© | Why it exists ğŸ§  | Typical location ğŸ“ | Used by ğŸ”— |
|---|---|---|---|
| **Run Manifest** (`manifest.yaml/json`) | Captures scenario + parameters + seeds | `data/work/sims/.../<run_id>/` | Humans + CI |
| **Input Lock** (`inputs.lock.*`) | Pins dataset versions/hashes | `data/work/sims/.../<run_id>/` | Repro + PROV |
| **Env Lock** (`env.lock.*`) | Pins software/container versions | `data/work/sims/.../<run_id>/` | Repro + CI |
| **Outputs (analysis)** | Analytics-friendly results | `data/processed/...` | API/Graph/UI |
| **Outputs (viz)** | Map-friendly tiles (fast UI) | `data/processed/...` | UI |
| **QA Report** (`qa.md/html`) | V&V + UQ summary | `data/work/sims/.../<run_id>/qa/` | Reviewers |
| **STAC + DCAT + PROV** | Evidence-first publishing | `data/stac/`, `data/catalogs/`, `data/prov/` | Graph/API/UI |
| **Model Card** (`MODEL_CARD.md`) | Assumptions + limits | `data/work/sims/.../<run_id>/` (+ published copy) | Trust + governance |

> [!TIP]
> Use the **â€œdual-format packagingâ€** pattern:  
> âœ… **GeoParquet / Parquet / NetCDF** (analysis) + âœ… **PMTiles / COG / tiles** (visualization) â€” from the **same** deterministic pipeline. ğŸ“¦ğŸ—ºï¸

---

## ğŸš€ Golden Path workflow (recommended)

### 1) Pick a baseline + pin it ğŸ“Œ
- Select upstream datasets from KFM catalogs (STAC/DCAT IDs preferred).
- Create an **input lock** (hashes / IDs / timestamps).
- Assign a **classification** level (public vs restricted).

### 2) Define a scenario (as data, not prose) ğŸ§¾
Create a scenario config describing:
- Model name + version
- Parameter set
- Random seeds (or deterministic seed derivation)
- Time window + region
- Uncertainty model (single run vs ensemble)

### 3) Run in the sandbox ğŸ§ª
- Write outputs to: `data/work/sims/<model>/<run_id>/`
- Save:
  - logs
  - manifest
  - locks
  - raw outputs
  - QA artifacts
- Never point UI or downstream analysis to sandbox outputs.

### 4) Verify + validate âœ…
Minimum expectations:
- smoke checks (shape, bounds, NaNs, CRS/time sanity)
- regression test(s) on known scenario(s)
- calibration/validation split documented (if applicable)

### 5) UQ + sensitivity ğŸ²
Minimum deliverables:
- parameter sensitivity for key knobs
- ensemble statistics (mean/variance or quantiles)
- â€œwhere it breaksâ€ notes (instability regions)

### 6) Package + catalog ğŸ“¦
Produce **publishable artifacts** + **evidence triplet**:
- analysis output (GeoParquet/Parquet/etc)
- map output (PMTiles/COG/etc)
- STAC Collection + Items
- DCAT dataset record
- PROV JSON-LD lineage

### 7) Gate + promote ğŸ”’â¡ï¸ğŸ“Œ
- Pass policy checks (license, attribution, sensitivity)
- Promote vetted outputs into `data/processed/`
- Open a PR containing:
  - processed outputs
  - catalogs (STAC/DCAT/PROV)
  - QA + model card summary

---

## ğŸ“ Recommended folder layout

```text
ğŸ§ª data/
â”œâ”€ ğŸ§° work/
â”‚  â””â”€ ğŸ§ª sims/
â”‚     â””â”€ ğŸ§  <model_name>/
â”‚        â””â”€ ğŸ·ï¸ <run_id>/
â”‚           â”œâ”€ ğŸ§¾ manifest.yaml
â”‚           â”œâ”€ ğŸ”’ inputs.lock.json
â”‚           â”œâ”€ ğŸ§· env.lock.txt
â”‚           â”œâ”€ ğŸ§  MODEL_CARD.md
â”‚           â”œâ”€ ğŸ“œ logs/
â”‚           â”œâ”€ âœ… qa/
â”‚           â”œâ”€ ğŸ§¬ prov/              # draft / run-local lineage
â”‚           â”œâ”€ ğŸ—ºï¸ stac/              # draft / run-local STAC
â”‚           â””â”€ ğŸ“¦ outputs/
â”‚              â”œâ”€ ğŸ“Š analysis/       # parquet/netcdf/etc
â”‚              â””â”€ ğŸ—ºï¸ viz/            # pmtiles/cog/etc
â””â”€ ğŸ“Œ processed/
   â””â”€ ğŸ§  sims/
      â””â”€ <model_name>/
         â””â”€ <dataset_version>/
            â”œâ”€ ğŸ“Š analysis/
            â””â”€ ğŸ—ºï¸ viz/
```

> [!WARNING]
> **No â€œmystery data.â€** If itâ€™s not cataloged (STAC/DCAT) and lineage-traced (PROV), itâ€™s not official KFM output.

---

## ğŸ§¾ Run manifest template (copy/paste)

```yaml
run_id: "2026-01-20__drought_bayes__v0.1.0__ks_statewide"
model:
  name: "drought_frequency_bayes"
  version: "0.1.0"
  repo_ref: "<git sha or tag>"
scenario:
  description: "Bayesian drought frequency projection under scenario set A"
  region: "Kansas"
  time_window:
    start: "1900-01-01"
    end: "2040-12-31"
inputs:
  - id: "<stac_item_or_dcat_id>"
    uri: "<path-or-url>"
    hash: "<sha256>"
parameters:
  seed: 1337
  draws: 2000
  burn_in: 500
  key_assumptions:
    - "Stationarity within regime windows"
uq:
  mode: "ensemble"
  ensemble_size: 50
  summary_stats: ["mean", "variance", "p10", "p50", "p90"]
outputs:
  analysis:
    format: "geoparquet"
  viz:
    format: "pmtiles"
governance:
  classification: "public"
  license: "<spdx id>"
  attribution: ["<source names>"]
promotion:
  target: "data/processed/sims/drought_frequency_bayes/<dataset_version>/"
```

---

## âœ… Verification, Validation & UQ expectations

> [!NOTE]
> The bar here is intentionally â€œscientificâ€: deterministic runs, traceable inputs, and documented assumptions. ğŸ§‘â€ğŸ”¬ğŸ§¾

### Minimum V&V checklist
- [ ] Deterministic seed strategy documented
- [ ] Inputs pinned (hashes + IDs)
- [ ] Environment pinned (container digest or lockfile)
- [ ] Regression test(s) for at least one known scenario
- [ ] Output sanity checks (CRS, time range, bounds, null rates)
- [ ] Validation vs observed data (when possible) with notes
- [ ] Model card includes limitations + misuse warnings

### Minimum UQ deliverables
- [ ] Sensitivity sweep of key parameters
- [ ] Ensemble summary (mean/variance or quantiles)
- [ ] Stability map/notes (where outputs become unreliable)
- [ ] Scenario labeling consistent with UI compare mode

---

## ğŸ—ºï¸ Making simulation results UI-ready

To show up cleanly in KFMâ€™s UI (2D/3D/timeline/story mode):

- ğŸ§­ **Time-aware:** include temporal extent + time-step semantics
- ğŸ—‚ï¸ **Layer-ready:** provide symbology hints (or a style stub) + legend-friendly attributes
- ğŸ§¾ **Provenance-forward:** users should be able to inspect **source, license, and preparation summary**
- ğŸ”€ **Scenario compare-friendly:** outputs should include a scenario ID and be compatible with layer toggles / split view
- ğŸ§Š **Performance-aware:** prefer prebuilt tiles for large geometry layers

> [!TIP]
> If youâ€™re generating a â€œprojectionâ€ layer, label it clearly as such (e.g., `type: projection`, `confidence: p10/p50/p90`).  
> KFMâ€™s UI aims to make â€œwhy this map looks this wayâ€ inspectable at any time. ğŸ§¾ğŸ”

---

## ğŸ” Governance & safety for simulations

> [!WARNING]
> **Model outputs can leak sensitive information** even when raw inputs are protected. Treat simulation results as **equal-or-higher sensitivity** than the inputs.

Practical rules:
- ğŸ”’ Apply the same Policy Pack rules to simulation output PRs.
- ğŸ§¾ Always produce PROV lineage: outputs must link to **inputs + code + parameters + agent**.
- ğŸš« Donâ€™t bypass catalogs or API governance.
- ğŸ•µï¸ Consider **query auditing / inference control** patterns for sensitive derived results (especially if outputs enable reverse inference).
- ğŸ§¹ Redact or generalize sensitive fields **before** promotion.

---

## ğŸ§‘â€ğŸ’» Notebook hygiene (MCP standards)

Keep notebooks clean, rerunnable, and reviewable:

- âœ… â€œRestart kernel & run allâ€ should succeed
- ğŸ§  Avoid hidden state; write functions/modules when complexity grows
- ğŸ§· Record seeds, parameters, versions at the top
- ğŸ““ Begin with a short purpose block; end with a summary + next steps
- ğŸ§ª Prefer small, testable pieces; add regression checks when you can

---

## ğŸ¤– Automation hooks (optional, but encouraged)

KFM supports the idea that simulations can be run by automation **but published only by review**:

- ğŸ‘€ **Watcher** detects new input data or a new scenario request  
- ğŸ§  **Planner** produces a plan + manifest  
- âš™ï¸ **Executor** runs the model and opens a PR containing artifacts + catalogs  

> [!IMPORTANT]
> Automation may run simulations, but **must not auto-promote to official outputs without review**. PR-first remains the gate. âœ…ğŸ”’

---

## ğŸ“š Reference library (project docs)

These notebooks align with the projectâ€™s core design documents:

- ğŸ“˜ **KFM Data Intake â€“ Technical & Design Guide** (evidence-first publishing, simulation workflow, governance)
- ğŸ§­ **KFM Comprehensive Architecture, Features, and Design** (simulation engines + `kfm-sim-run`)
- ğŸ¤– **KFM AI System Overview** (domain models + notebook-driven, PR-first modeling)
- ğŸ—ºï¸ **KFM Comprehensive UI System Overview** (scenario visualization, timeline, 3D/AR direction)
- ğŸ§° **KFM Comprehensive Technical Documentation** (reproducible research integration, notebook launching ideas)
- ğŸŒŸ **Latest Ideas & Future Proposals** (dual-format packaging pattern for large datasets)
- ğŸ’¡ **Innovative Concepts** + **Additional Project Ideas** (4D digital twin, immersive/AR futures, governance automation)
- ğŸ—ƒï¸ **Data Management / Bayesian / Data Mining library** (uncertainty modeling + privacy considerations)
- ğŸŒ **Maps / WebGL / Virtual Worlds library** (geospatial rendering + immersive visualization)
- ğŸ§‘â€ğŸ’» **Programming Languages & Resources library** (numerical/simulation tooling references)

---

## âœ… â€œReady to publishâ€ checklist (PR gate)

Before you open a promotion PR:

- [ ] Run manifest present + complete
- [ ] Inputs pinned (hashes/IDs)
- [ ] Environment pinned (lockfile/container)
- [ ] Seeds recorded (deterministic run)
- [ ] QA report written (V&V + UQ summary)
- [ ] Model card includes assumptions + limits
- [ ] Dual-format outputs produced (analysis + viz)
- [ ] STAC Collection + Items created
- [ ] DCAT dataset record created
- [ ] PROV JSON-LD created and links inputs â†’ activity â†’ outputs
- [ ] Policy checks pass (license, attribution, sensitivity)
- [ ] No UI/analysis points directly to `data/work/sims/`

---

## ğŸ¤ Contributing a new simulation notebook

1. ğŸ“„ Add notebook(s) with clear numbering: `01_define_scenario.ipynb`, `02_run_model.ipynb`, `03_uq.ipynb`, `04_package_publish.ipynb`
2. ğŸ§¾ Add/extend a manifest schema if your model needs new fields
3. âœ… Add at least one regression test path (tiny scenario is fine!)
4. ğŸ“¦ Ensure outputs follow the publish checklist
5. ğŸ”€ Open PR with a short reviewer guide + expected outputs

---

**Next folder in the notebook sequence:** keep moving ğŸ‘‰ (analysis â†’ simulation â†’ publish) ğŸš€
