# ğŸ§© MCP Traceability Dashboards â€” Scripts

![MCP](https://img.shields.io/badge/MCP-Methods%2C%20Controls%20%26%20Processes-1f6feb?style=for-the-badge)
![Traceability](https://img.shields.io/badge/Traceability-Provenance%20%2B%20Telemetry%20%2B%20Policy-0ea5e9?style=for-the-badge)
![Evidence-First](https://img.shields.io/badge/Evidence--First-No%20Mystery%20Layers-22c55e?style=for-the-badge)
![Fail Closed](https://img.shields.io/badge/Governance-Fail%20Closed-f97316?style=for-the-badge)
![Reproducible](https://img.shields.io/badge/Reproducibility-Run%20Manifest%20%2B%20Canonical%20Digest-9333ea?style=for-the-badge)

> ğŸ§­ **Goal:** Turn KFMâ€™s *evidence chain* (STAC/DCAT/PROV + run manifests + telemetry + policy outcomes) into **dashboards + reports** that are *auditable, reproducible, and boringly dependable*.  
> If a metric canâ€™t point back to evidenceâ€¦ **we donâ€™t graph it.** âœ…

---

<details>
<summary>ğŸ—‚ï¸ Table of Contents</summary>

- [âœ¨ What lives here](#-what-lives-here)
- [ğŸ§  Mental model](#-mental-model)
- [ğŸ”— Inputs and evidence chain](#-inputs-and-evidence-chain)
- [ğŸ“¦ Outputs](#-outputs)
- [ğŸ—ºï¸ Folder layout](#ï¸-folder-layout)
- [ğŸš€ Quickstart](#-quickstart)
- [ğŸ“Š Script catalog](#-script-catalog)
- [ğŸ§¾ Script standards](#-script-standards)
- [â±ï¸ Schedules and runbooks](#ï¸-schedules-and-runbooks)
- [ğŸ§· Adding a new dashboard script](#-adding-a-new-dashboard-script)
- [ğŸ›¡ï¸ Security privacy and redaction](#ï¸-security-privacy-and-redaction)
- [ğŸ§¯ Troubleshooting](#-troubleshooting)
- [ğŸ“š Design inputs and reference library](#-design-inputs-and-reference-library)

</details>

---

## âœ¨ What lives here

This folder contains **dashboards scripts** for the **MCP Traceability** stack.

These scripts generate:
- ğŸ“ˆ **Operational dashboards** (ingestion health, graph health, policy gate status, CI signal)
- â›“ **Chain-of-custody dashboards** (what changed, who/what changed it, which outputs are affected)
- ğŸ¤– **AI governance dashboards** (citation coverage, drift/bias signals, OPA runtime denies)
- ğŸŒ± **Sustainability dashboards** (resource usage, energy/carbon telemetry if captured)
- ğŸ§µ **Pulse / health-check reports** (short, human-readable summaries + machine-readable metrics)

**Big idea:** The KFM UI can show *a map behind the map* (layer provenance panels, export attributions, etc.).  
These scripts produce the **same truth** but formatted for maintainers, reviewers, and auditors.

---

## ğŸ§  Mental model

KFM has a strict â€œevidence-firstâ€ pipeline ordering:

> **ETL â†’ Catalogs (STAC/DCAT/PROV) â†’ Graph â†’ API â†’ UI â†’ Story Nodes â†’ Focus Mode (AI)**

This folder lives in **MCP** land (Methods, Controls & Processes), so we treat dashboards as:
- ğŸ“ a *control surface* (SLOs, gates, and audits),
- ğŸ§¾ a *documentation artifact* (reports that can be reviewed),
- ğŸ” a *feedback loop* (what to fix next, with receipts).

---

## ğŸ”— Inputs and evidence chain

Dashboards must derive from **auditable sources**, typically:

### ğŸ—ƒï¸ Metadata backbone
- **DCAT** (dataset-level catalog / discoverability)
- **STAC** (spatiotemporal collections and items)
- **PROV** (lineage: inputs â†’ activities â†’ outputs)

### ğŸ§¾ Run-level audit trail
- **Run manifests** (per pipeline run)  
  - includes timestamps, tool versions, inputs/outputs, summary counts
  - includes a **canonical digest** so the manifest â€œfingerprints itselfâ€

### ğŸ“‰ Telemetry & logs
- Append-only NDJSON telemetry (pipeline events, governance telemetry, health checks)
- OpenTelemetry (if integrated): traces/spans, latency, error rates, etc.

### âš–ï¸ Policy outcomes
- **OPA / Conftest** results (CI policy gates)
- Runtime policy denies (e.g., block AI output if citations missing)

### ğŸ§  Optional stores (read-only)
- Graph DB queries (Neo4j health and integrity)
- PostGIS / time-series stores (for â€œlive layersâ€ metrics)

> [!IMPORTANT]
> Dashboards are **not** an alternate source of truth.  
> They are a **projection** of the evidence chain.

---

## ğŸ“¦ Outputs

All scripts should produce **both**:
- ğŸ§‘â€ğŸ”§ **Human outputs** (fast to scan)
- ğŸ¤– **Machine outputs** (dashboards / alerts / automation)

### âœ… Recommended output bundle per run
- `summary.md` â€” short narrative summary (what changed, what failed, whatâ€™s next)
- `metrics.json` â€” structured metrics for dashboards
- `index.csv` â€” append-friendly rollup row (for simple trending)
- `run_manifest.json` â€” reproducibility + custody
- `prov.jsonld` *(optional but encouraged)* â€” PROV for the dashboard build itself

> [!TIP]
> If a script only emits one format, pick **JSON** and have a follow-up step render Markdown/HTML.

---

## ğŸ—ºï¸ Folder layout

Typical layout (adjust to repo reality):

```text
ğŸ“¦ mcp/
 â””â”€â”€ ğŸ§­ traceability/
     â””â”€â”€ ğŸ“Š dashboards/
         â”œâ”€â”€ ğŸ§° scripts/               # ğŸ‘ˆ you are here
         â”‚   â”œâ”€â”€ README.md
         â”‚   â”œâ”€â”€ python/               # optional: Python CLIs
         â”‚   â”œâ”€â”€ node/                 # optional: TS/Node CLIs
         â”‚   â””â”€â”€ lib/                  # shared helpers (manifest, prov, io)
         â”œâ”€â”€ ğŸ“ˆ dashboards/            # dashboard definitions (Grafana JSON, etc.)
         â”œâ”€â”€ ğŸ§¾ reports/               # generated summaries (optional)
         â””â”€â”€ ğŸ“¦ out/                   # generated machine artifacts (optional)
```

---

## ğŸš€ Quickstart

### 1) Pick your runtime ğŸ§ª

Most KFM pipeline tooling is **Python-first**, but dashboards can be Python or Node.

- ğŸ Python recommended for: parsing catalogs/PROV, QA scans, CSV/JSON reporting
- ğŸŸ¦ Node/TS recommended for: dashboard JSON manipulation, static site builds, UI-ish bundling

### 2) Python setup (example)

```bash
python -m venv .venv
source .venv/bin/activate
python -m pip install -U pip
python -m pip install -r requirements.txt
```

### 3) Run a single script (pattern)

```bash
python python/graph_health_check.py \
  --out ../../reports/graph_health/$(date +%Y-%m-%dT%H%M%SZ)
```

### 4) Run â€œeverythingâ€ (pattern)

```bash
./run_all.sh --out ../out/$(date +%Y-%m-%d)
```

> [!NOTE]
> This README defines the **contract**. If your repo uses different entrypoints (Makefile, task runner, Poetry, npm), keep the contract but adapt the commands.

---

## ğŸ“Š Script catalog

Below is the **recommended** canonical set of scripts for traceability dashboards.

> ğŸ§­ Naming: `verb_noun_scope` (snake_case), with stable â€œdashboard IDsâ€ baked into metrics.

### ğŸ§¬ 1) Catalog + provenance QA

#### `stac_dcat_prov_linkcheck`
**Purpose:** Validate that every dataset has the required cross-links:
- DCAT â†’ STAC distribution
- STAC â†’ asset URLs / tiles
- PROV â†’ inputs + activities + outputs
- Optional: â€œno mystery layersâ€ checks (UI layers must link to catalogs)

**Emits:** `metrics.json`, `summary.md`  
**Dashboards:** âœ… Metadata Completeness, ğŸ”— Link Integrity

---

### ğŸ—ƒï¸ 2) Ingestion telemetry rollups

#### `telemetry_rollup`
**Purpose:** Aggregate append-only NDJSON telemetry into:
- ingest success/failure rates
- bytes processed
- top error classes
- policy deny counts
- â€œtime to publishâ€ lag

**Emits:** `metrics.json`, `index.csv`  
**Dashboards:** ğŸ“¥ Intake Health, ğŸš¦ Gate Status

---

### ğŸ§  3) Focus Mode governance metrics (AI)

#### `focus_citation_coverage`
**Purpose:** Compute metrics such as:
- % of answers with â‰¥1 citation
- % blocked/denied by policy (missing citations, sensitive content)
- drift indicators (citation coverage drops, accuracy regressions if you have eval sets)
- bias flags (if detectors exist)

**Emits:** `metrics.json`, `summary.md`  
**Dashboards:** ğŸ¤– AI Trust, ğŸ§¾ Audit Readiness

---

### ğŸ§· 4) Policy gate reporting (CI + runtime)

#### `policy_gate_report`
**Purpose:** Normalize Conftest/OPA outputs into dashboards:
- which policy IDs fired
- what file paths triggered them
- waivers usage (if supported)
- trend lines per policy ID

**Emits:** `metrics.json`, `summary.md`  
**Dashboards:** âš–ï¸ Governance, ğŸ”’ Security Posture

---

### ğŸ•¸ï¸ 5) Graph integrity (weekly health check)

#### `graph_health_check`
**Purpose:** Run weekly Neo4j integrity checks:
- node/edge counts + deltas
- orphaned nodes
- invalid relationship types
- schema drift signals
- backup verification results (if hooked in)

**Emits:** timestamped folder:
- `summary.md`
- `index.csv`
- optional: `details.json` (top offenders / sample IDs)

**Dashboards:** ğŸ•¸ï¸ Graph Health, ğŸ§¯ Data Quality

> [!TIP]
> Keep the checks **fast** and the output **actionable**:
> - one â€œheadlineâ€ section
> - one â€œtop 5 things to fixâ€
> - one â€œwhat changed since last runâ€

---

### ğŸ“¦ 6) Artifact registry audit (OCI + signatures)

#### `oci_artifacts_audit`
**Purpose:** Validate â€œartifact distributionâ€ claims:
- referenced OCI artifacts exist (tilesets, COGs, model files)
- digests match
- Cosign signatures present/valid (if required)
- referrers / attestations present (SLSA/SBOM if enabled)

**Emits:** `metrics.json`, `summary.md`  
**Dashboards:** ğŸ“¦ Supply Chain, âœ… Reproducibility

---

### ğŸ§µ 7) Pulse threads (optional but powerful)

#### `pulse_thread_metrics`
**Purpose:** Roll up â€œPulseâ€ health checks into:
- fast â€œheartbeatâ€ charts
- MTTR / incident counts
- open issues spawned by checks

**Emits:** `metrics.json`, `index.csv`, `summary.md`  
**Dashboards:** ğŸ§µ Pulse, ğŸš‘ Reliability

---

## ğŸ§¾ Script standards

Every script in this folder should follow these **non-negotiables**:

### âœ… Inputs
- **Read-only** evidence inputs (catalogs, logs, manifests)
- Explicit `--since` / `--until` (or `--window`) where relevant
- Optional `--strict` mode for CI (fail on warnings)

### âœ… Outputs
- Deterministic output paths (`--out`)
- Machine output (`metrics.json`) + human output (`summary.md`)
- A `run_manifest.json` that captures:
  - run ID
  - tool versions
  - inputs/outputs
  - counts/errors
  - canonical digest

### âœ… Determinism
- No nondeterministic timestamps inside the *content* unless they are captured as fields and included in digest intentionally
- Stable ordering (sorted keys / sorted rows)
- Prefer canonical JSON for hashing

### âœ… Exit codes
- `0` = OK
- `1` = failed checks / policy violations
- `2` = partial success / degraded mode (optional; document it per script)

---

## â±ï¸ Schedules and runbooks

Recommended cadence:

| Cadence | What | Why |
|---:|---|---|
| Every 10 minutes â± | Telemetry health check / rollup | detect ingestion outages fast |
| Nightly ğŸŒ™ | Metadata QA + linkcheck | prevent rot (broken links, missing fields) |
| Weekly ğŸ“… | Graph health check | catch drift, orphans, schema surprises |
| Per PR âœ… | Policy gate report | fail closed before merge |
| Per release ğŸš€ | OCI/cosign audit | supply chain + reproducibility proof |

> [!WARNING]
> If a check canâ€™t be performed (missing access, broken dependency), default is **fail closed** for CI gating scripts.

---

## ğŸ§· Adding a new dashboard script

Use this checklist to keep everything consistent:

- [ ] Create the script under `python/` or `node/`
- [ ] Define a stable **dashboard metric ID** namespace (e.g., `kfm.trace.graph.orphans`)
- [ ] Emit `metrics.json` + `summary.md`
- [ ] Emit / update `run_manifest.json`
- [ ] Add a â€œhow to runâ€ snippet in this README
- [ ] Add to CI schedule if it is a gate or heartbeat
- [ ] Add (or update) a policy rule if this metric is a governance requirement
- [ ] Ensure sensitive data is redacted (no leaking classified IDs/coords into public dashboards)

> [!TIP]
> If youâ€™re unsure whether a new metric belongs here:  
> **If it helps explain â€œwhy we trust this output,â€ it belongs here.** â›“ï¸

---

## ğŸ›¡ï¸ Security privacy and redaction

Dashboards are powerfulâ€¦ and dangerous if they leak sensitive info. Follow these rules:

### ğŸš« Donâ€™t leak
- secrets, tokens, internal URLs
- sensitive coordinates or protected site locations
- raw user queries (unless aggregated + policy-approved)

### ğŸ§¼ Redact by default
- show counts and rates
- show hashed IDs or sampled IDs only in restricted outputs
- keep â€œpublic dashboardsâ€ and â€œinternal dashboardsâ€ separated

### ğŸ§¾ Prefer auditability
- keep raw evidence in controlled stores
- keep dashboards as projections (derived data)

---

## ğŸ§¯ Troubleshooting

### â€œPolicy gates failing but I donâ€™t know whyâ€ ğŸ˜µâ€ğŸ’«
- Run `policy_gate_report` locally with `--explain`
- Ensure you have the same policy pack version as CI
- Look for missing required files (PROV/STAC/DCAT)

### â€œGraph check is slowâ€ ğŸŒ
- Add query timeouts
- Prefer count-based checks + sampled deep checks
- Cache last-run baseline metrics to compute deltas cheaply

### â€œTelemetry NDJSON is hugeâ€ ğŸ“š
- Add windowing (`--since` / `--until`)
- Maintain daily partitions
- Roll up to `index.csv` for dashboards and keep raw for audits

---

## ğŸ“š Design inputs and reference library

This README is derived from (and should remain aligned with) the KFM design docs and reference packs:

### ğŸ“„ Core KFM docs (project truth)
- ğŸ“˜ **Comprehensive Technical Documentation**
- ğŸ§± **Comprehensive Architecture, Features, and Design**
- ğŸ§­ **AI System Overview (Focus Mode + governance)**
- ğŸ–¥ï¸ **Comprehensive UI System Overview**
- ğŸ“¥ **Data Intake â€“ Technical & Design Guide**
- ğŸŒŸ **Latest Ideas & Future Proposals**
- ğŸ’¡ **Additional Project Ideas**
- ğŸ§  **Innovative Concepts to Evolve KFM**

### ğŸ§° Reference portfolios (supporting literature)
- ğŸ¤– **AI Concepts & more** *(PDF portfolio â€” AI/ML reference library)*
- ğŸ—ºï¸ **Maps / Google Maps / Virtual Worlds / WebGL** *(PDF portfolio â€” geospatial + visualization refs)*
- ğŸ§± **Data Management / Architectures / Bayesian Methods** *(PDF portfolio â€” data engineering refs)*
- ğŸ§‘â€ğŸ’» **Various programming languages & resources** *(PDF portfolio â€” programming reference library)*

> [!NOTE]
> These portfolios are not â€œrequired reading,â€ but they inform standards and implementation choices (CI/CD, observability, geoprocessing, visualization, governance, etc.).

---

### âœ… Definition of done (for this folder)

When `mcp/traceability/dashboards/scripts` is â€œdone enoughâ€, we should be able to answer:
- â€œIs the system healthy?â€ âœ…
- â€œWhat changed?â€ âœ…
- â€œCan we reproduce it?â€ âœ…
- â€œIs it compliant?â€ âœ…
- â€œIf something is wrong, what exactly do we fix next?â€ âœ…

ğŸ§­ Onward.
