<div align="center">

# ğŸ§¬ Traceability Dashboards ğŸ“Š  
**Master Coder Protocol (MCP) â€¢ Provenance â€¢ Governance â€¢ Observability**

![MCP](https://img.shields.io/badge/MCP-Traceability-2ea44f?style=flat-square)
![PROV](https://img.shields.io/badge/PROV-Lineage-blue?style=flat-square)
![STAC](https://img.shields.io/badge/STAC-Catalogs-8e44ad?style=flat-square)
![DCAT](https://img.shields.io/badge/DCAT-Metadata-5b2c6f?style=flat-square)
![Neo4j](https://img.shields.io/badge/Neo4j-Graph-f39c12?style=flat-square)
![OPA](https://img.shields.io/badge/OPA-Policy-as--Code-c0392b?style=flat-square)
![FAIR+CARE](https://img.shields.io/badge/FAIR%20%2B%20CARE-Governance-16a085?style=flat-square)

<sub>Measure lineage. Prove provenance. Ship with confidence. âœ…</sub>

</div>

---

## ğŸ¯ Purpose

This directory is the **dashboard layer** for Kansas Frontier Matrix (KFM) traceability.

Dashboards here exist to answer (quickly, repeatably, and audibly):

- **What changed?**
- **Why did it change?**
- **Who/what changed it?**
- **Can we trust the result?**
- **Which downstream artifacts are impacted?**

This aligns with KFMâ€™s provenance-first / evidence-first posture: data, narratives, and AI outputs should be explainable back to sources, transformations, and policy decisions.

---

## ğŸ§­ Where dashboards sit in the KFM stack

```mermaid
flowchart LR
  S[ğŸŒ Sources] --> IG[ğŸšª Ingestion Gate]
  IG --> CAT[ğŸ—‚ï¸ Catalogs - STAC / DCAT]
  CAT --> PROV[ğŸ§¬ PROV bundles]
  CAT --> G[ğŸ•¸ï¸ Knowledge Graph - Neo4j]
  G --> API[ğŸ§° API Layer]
  API --> UI[ğŸ—ºï¸ UI - Timeline - Layers]
  API --> FM[ğŸ¤– Focus Mode]

  IG --> TEL[ğŸ“¨ Telemetry / Logs]
  CAT --> TEL
  PROV --> TEL
  G --> TEL
  FM --> TEL

  TEL --> DSH[ğŸ“Š Dashboards]
  DSH --> ALRT[ğŸš¨ Alerts - Issues - Reviews]
```

> ğŸ”’ **Golden rule:** dashboards must respect KFMâ€™s governance model (classification, audit logging, provenance). Dashboards should not become a â€œbackdoor UIâ€ that bypasses the API boundary or policy checks.

---

## ğŸ—ºï¸ Dashboard catalog

| Dashboard | What it proves âœ… | Typical signals | Audience |
|---|---|---|---|
| ğŸšª Intake & ETL Ops | deterministic & repeatable runs | success %, runtimes, retries, checksums, quarantines | maintainers |
| ğŸ—‚ï¸ Catalog Coverage | data is discoverable + attributable | STAC/DCAT completeness, missing fields, broken links | curators |
| ğŸ§¬ Provenance & Lineage | outputs have explainable â€œwhyâ€ | PROV edges, run manifests, evidence manifests | researchers |
| ğŸ•¸ï¸ Graph Health | graph is consistent + queryable | constraints, schema drift, orphan nodes, deltas | platform |
| ğŸ›¡ï¸ Governance & Policy | rules are enforced | policy denies, redactions, publication blocks, sensitive access | governance |
| ğŸ¤– Focus Mode QA | AI stays tethered to evidence | citation coverage, drift signals, override rates | AI reviewers |
| ğŸ§¾ Story Node Evidence | narratives are verifiable | evidence manifest coverage, stale evidence, missing PROV | historians |
| ğŸ§‘â€ğŸ¤â€ğŸ§‘ Community / Moderation | contributions are reviewable | pending reviews, license flags, provenance gaps | maintainers |
| ğŸŒ± Sustainability & Cost | scaling stays responsible | compute time, storage growth, cost per run | platform |

---

## ğŸ§© â€œDashboards-as-codeâ€ layout (recommended)

```text
mcp/traceability/dashboards/
â”œâ”€ ğŸ“„ README.md                 â† you are here
â”œâ”€ ğŸ“Š dashboards/               # dashboard definitions (Grafana/Superset/etc.)
â”œâ”€ ğŸ§ª queries/                  # Cypher / SQL / PromQL (and query notes)
â”œâ”€ ğŸ§¬ schemas/                  # telemetry + manifest schemas (JSON Schema)
â”œâ”€ ğŸ§¾ reports/                  # generated markdown reports (checked in)
â”œâ”€ ğŸ› ï¸ scripts/                  # collectors + exporters
â””â”€ ğŸ–¼ï¸ screenshots/              # docs screenshots (optional)
```

If your repo already has a different structure, keep this README and update the tree to match reality.

---

## ğŸ§¾ Canonical traceability artifacts (what dashboards should read)

Dashboards should **prefer artifact-first truth** over â€œlive-only metricsâ€:

- **Telemetry event log** (append-only): pipeline + agent events captured for observability and governance  
- **Run manifests**: each pipeline run emits a `run_manifest.json` (inputs, outputs, checksums, versions, policy decisions)  
- **Evidence manifests**: story nodes and research narratives reference evidence inventories (YAML) + compact PROV bundles  
- **Graph health reports**: periodic checks produce human-readable reports (e.g., `docs/reports/qa/graph_health/summary.md` + `index.csv`)  
- **Governance ledger**: immutable log of key actions (ingestion, redactions, publication blocks, AI outputs)

---

## ğŸ§¬ Telemetry contract (minimum viable fields)

Keep telemetry boring, structured, and grepâ€‘able ğŸ§±

```json
{
  "ts": "2026-01-21T12:34:56Z",
  "level": "info",
  "run_id": "RUN-20260121-123456-abcdef",
  "stage": "ingestion_gate | etl | catalog | graph | api | ui | focus_mode",
  "event": "dataset_ingested | policy_blocked | graph_health_check | ai_answer_generated",
  "status": "ok | warn | error | blocked",
  "dataset_id": "kfm:dataset:â€¦",
  "artifact": {
    "path": "data/processed/â€¦",
    "digest": "sha256:â€¦"
  },
  "policy": {
    "decision": "allow | deny",
    "rule_id": "OPA:â€¦",
    "reason": "â€¦"
  },
  "provenance": {
    "prov_bundle": "prov/â€¦jsonld",
    "evidence_manifest": "evidence/EM-84.yaml"
  },
  "actor": {
    "type": "human | agent | ci",
    "id": "github:@user | service:etl"
  }
}
```

---

## ğŸ§  Dashboards you should expect to exist (and what to measure)

<details>
<summary><strong>ğŸšª Intake & ETL Ops</strong></summary>

**Questions**
- Which runs failed, and why?
- Are we deterministic (same inputs â†’ same outputs)?
- Are we accumulating technical debt (manual overrides, retries, drift)?

**KPIs**
- Run success rate / failure reasons
- Median + p95 runtime per stage
- Input/output checksum mismatches
- â€œQuarantineâ€ rate (items held for review)

**Primary inputs**
- Telemetry NDJSON
- `run_manifest.json`
- ingestion gate decision logs

</details>

<details>
<summary><strong>ğŸ—‚ï¸ Catalog Coverage (STAC / DCAT)</strong></summary>

**Questions**
- Are datasets discoverable and attributed?
- Are spatial/temporal extents present?
- Do we have broken asset links?

**KPIs**
- % of items meeting required metadata
- Missing bbox/time/license fields
- Broken asset counts
- Duplicate IDs / conflicting versions

</details>

<details>
<summary><strong>ğŸ§¬ Provenance & Lineage (PROV)</strong></summary>

**Questions**
- Can we answer â€œwhy is this in the graph?â€
- Can we reproduce the transformation chain?

**KPIs**
- % outputs with a PROV bundle
- # activities per output (lineage depth)
- â€œUnknown transformationâ€ count (outputs with missing/opaque steps)

</details>

<details>
<summary><strong>ğŸ•¸ï¸ Graph Health & Schema Drift (Neo4j)</strong></summary>

**Questions**
- Do constraints/indexes match the expected schema?
- Are there orphan nodes / broken relationships?
- Is the graph growing in a healthy way?

**KPIs**
- Orphan count by node type
- Constraint violations
- Relationship deltas per ingestion run
- Weekly â€œhealth scoreâ€ trend

**Outputs**
- Human-readable reports in `docs/reports/qa/graph_health/`
- Optional Prometheus/PromQL-style metrics for alerting

</details>

<details>
<summary><strong>ğŸ›¡ï¸ Governance & Policy (OPA / Policy-as-code)</strong></summary>

**Questions**
- What was blocked and why?
- Are we seeing repeated license issues?
- Are sensitive-data controls being triggered?

**KPIs**
- Policy denies by rule_id
- Redaction events over time
- Publication-block rate
- Sensitive access events (by dataset / actor type)

</details>

<details>
<summary><strong>ğŸ¤– Focus Mode QA (Citations â€¢ Bias â€¢ Drift)</strong></summary>

**Questions**
- Are answers citation-backed?
- Are we drifting (retrieval quality / model behavior)?
- Are humans overriding the model (and why)?

**KPIs**
- % answers with â‰¥1 citation
- â€œNeeds reviewâ€ rate
- Drift indicators (retrieval coverage changes)
- Override/appeal rate (human feedback loops)

</details>

<details>
<summary><strong>ğŸ§¾ Story Node Evidence Coverage</strong></summary>

**Questions**
- Which story nodes are â€œevidence completeâ€?
- Which nodes cite stale evidence (updated datasets)?
- Which nodes have evidence manifests but no PROV bundle?

**KPIs**
- % nodes with `evidence_manifest` present
- # evidence items per node (median/p95)
- Staleness score (evidence versions vs current dataset versions)

</details>

<details>
<summary><strong>ğŸŒ± Sustainability & Cost</strong></summary>

**Questions**
- What does each pipeline run cost (time/storage)?
- Where is the bottleneck?
- Are we scaling responsibly?

**KPIs**
- CPU/GPU time per pipeline stage
- Storage growth by data layer (raw/processed/tiles/graph exports)
- Cost per â€œnew dataset acceptedâ€
- Cache hit rates (if enabled)

</details>

---

## ğŸ§° Contribution workflow (add or improve a dashboard)

1. **Start from a question** (not a chart): what decision will this dashboard enable?
2. **Define the truth source** (artifact > log > live query).
3. **Add the query** under `queries/` (Cypher/SQL/PromQL/etc.).
4. **Add the dashboard definition** under `dashboards/` (or your dashboard systemâ€™s equivalent).
5. **Document panels + thresholds** in a short README next to the dashboard definition.
6. **Update the Traceability Matrix** (feature/experiment â†’ code/data/model/artifacts).
7. **Ship an evidence artifact**: screenshot, report snippet, or generated markdown report.

### âœ… Definition of Done checklist

- [ ] Clear question + audience
- [ ] Data sources documented (with paths + IDs)
- [ ] Query is versioned and reproducible
- [ ] No policy bypass (classification + audit preserved)
- [ ] Alerts are actionable (not noisy)
- [ ] Includes a â€œhow to debugâ€ panel / runbook link
- [ ] Traceability matrix updated

---

## ğŸ” Security, privacy, and cultural governance

Dashboards are **power tools** ğŸ§¯ â€” treat them like production features:

- Obey classification & redaction rules (FAIR + CARE alignment)
- Avoid exposing sensitive datasets or locations in public dashboards
- Log dashboard access for auditability (who viewed what, when)
- Prefer aggregated metrics where possible

---

## ğŸ“š Reference library (project files)

These documents informed the conventions and expectations baked into this dashboard layer:

- **Kansas Frontier Matrix (KFM) â€“ Comprehensive Technical Documentation** (architecture + evidence artifacts)
- **Kansas Frontier Matrix (KFM) â€“ Comprehensive Architecture, Features, and Design** (observability + performance)
- **ğŸ“š KFM Data Intake â€“ Technical & Design Guide** (ingestion gate + telemetry + validation)
- **Kansas Frontier Matrix (KFM) â€“ AI System Overview ğŸ§­ğŸ¤–** (Focus Mode QA + governance ledger)
- **Kansas Frontier Matrix â€“ Comprehensive UI System Overview** (UI hooks + future dashboard extensions)
- **Kansas-Frontier-Matrix: Open-Source Geospatial Historical Mapping Hub Design** (repo structure + MCP templates)
- **Scientific Method / Research / Master Coder Protocol Documentation** (traceability matrix + experiment rigor)
- **Innovative Concepts to Evolve KFM** (simulation + digital twin + advanced UX concepts)
- **Additional Project Ideas** (graph health checks, policy packs, artifact signing, PRâ†’PROV concepts)
- **ğŸŒŸ KFM â€“ Latest Ideas & Future Proposals** (ongoing integration ideas + workflow expansion)
- **AI Concepts & more / Data Management / Maps-WebGL / Programming Resources** (background references & implementation ideas)
- **Data Mining Concepts & Applications** (data cleaning + quality + privacy concepts)
- **KFM Python Geospatial Analysis Cookbook** (PostGIS + spatial analysis patterns)

---

## ğŸ—‚ï¸ Glossary

- **MCP**: Master Coder Protocol (documentation-first, reproducible, auditable)
- **STAC/DCAT**: metadata catalogs for geospatial datasets
- **PROV**: provenance model (entities, activities, agents) for lineage
- **OPA**: Open Policy Agent (policy-as-code)
- **Wâ€‘Pâ€‘E agents**: watcher/planner/executor orchestration pattern for pipelines
- **Focus Mode**: AI-assisted research & Q/A layer grounded in KFM sources

---

<sub>ğŸ§© Tip: If youâ€™re unsure which dashboard to add, start with a **weekly report** (markdown in `reports/`) and evolve it into a live dashboard once the signals stabilize.</sub>

