---
title: "MCP Traceability Scripts"
path: "mcp/traceability/scripts/README.md"
version: "v0.1.0"
last_updated: "2026-01-21"
status: "active"
doc_kind: "README"
license: "CC-BY-4.0"
markdown_protocol_version: "1.0"
kfm_principles:
  - "provenance-first"
  - "contract-first"
  - "fail-closed policy gates"
---

# ğŸ§¾ MCP Traceability Scripts

![MCP](https://img.shields.io/badge/MCP-master%20coder%20protocol-8A2BE2)
![Traceability](https://img.shields.io/badge/traceability-evidence--first-blue)
![Provenance](https://img.shields.io/badge/provenance-enforced-brightgreen)
![STAC](https://img.shields.io/badge/metadata-STAC-orange)
![DCAT](https://img.shields.io/badge/metadata-DCAT-informational)
![PROV-O](https://img.shields.io/badge/metadata-PROV--O-yellow)
![OPA](https://img.shields.io/badge/policy-OPA%2FConftest-lightgrey)
![Supply%20Chain](https://img.shields.io/badge/supply%20chain-cosign%20%2B%20oras-black)

> **Chain-of-custody tooling** for Kansas Frontier Matrix (KFM): run manifests âœ STAC/DCAT/PROV âœ policy gates âœ signatures âœ publishable artifacts.  
> Goal: **no mystery layers** ğŸ•µï¸â€â™‚ï¸ğŸš« and **every insight comes with a footnote** ğŸ§·â›“ï¸.

> [!IMPORTANT]
> **Fail-closed by default** ğŸš¦: if we canâ€™t validate or prove provenance, it **doesnâ€™t ship**.

---

## ğŸ§­ Quick Navigation

- [ğŸ¯ What â€œtraceabilityâ€ means in KFM](#-what-traceability-means-in-kfm)
- [ğŸ“¦ What these scripts produce](#-what-these-scripts-produce)
- [ğŸ—‚ï¸ Folder layout](#ï¸-folder-layout)
- [âš¡ Quickstart](#-quickstart)
- [ğŸ” End-to-end workflows](#-end-to-end-workflows)
- [ğŸ§¾ Run Manifest format](#-run-manifest-format)
- [ğŸ§¬ STAC + DCAT + PROV wiring](#-stac--dcat--prov-wiring)
- [ğŸš¦ Policy gates](#-policy-gates)
- [ğŸ” Signing + OCI publishing](#-signing--oci-publishing)
- [ğŸ•¸ï¸ Graph registration + health checks](#ï¸-graph-registration--health-checks)
- [ğŸ§  AI traceability (Focus Mode + agents)](#-ai-traceability-focus-mode--agents)
- [ğŸ—ºï¸ Geospatial helpers](#ï¸-geospatial-helpers)
- [ğŸ§« Privacy + sensitivity checks](#-privacy--sensitivity-checks)
- [ğŸ“š Source docs used to build this README](#-source-docs-used-to-build-this-readme)

---

## ğŸ¯ What â€œtraceabilityâ€ means in KFM

Traceability is the **end-to-end proof trail** that links any KFM output back to:

- ğŸ“¥ **Inputs** (source URLs, archives, sensor feeds, documents, etc.)
- ğŸ§ª **Processing** (what ran, when, with which parameters and tool versions)
- ğŸ§¾ **Evidence metadata** (STAC/DCAT/PROV triplet)
- ğŸ·ï¸ **Governance** (license, sensitivity classification, FAIR+CARE/CARE flags)
- ğŸ‘¤ğŸ¤– **Actors** (human contributor, bot, Watcherâ€“Plannerâ€“Executor agent, Focus Mode invocation)
- ğŸ” **Integrity** (canonical hashes, signatures, immutable governance ledger entry)

This aligns with KFMâ€™s contract-first approach (metadata â€œdata contractsâ€ as source of truth), plus strict provenance/citation expectations for anything shown in the UI or AI assistant. ğŸ§±â›“ï¸

---

## ğŸ“¦ What these scripts produce

Most workflows produce a **Trace Bundle** (think: â€œevidence backpackâ€ ğŸ’) with:

- ğŸ§¾ `run_manifest.json` (canonicalized + hashed; idempotency key)
- ğŸ§¬ `prov.jsonld` (lineage; `prov:Entity`, `prov:Activity`, `prov:Agent`)
- ğŸ›°ï¸ STAC Items/Collections (`stac_item.json`, `collection.json`, assets)
- ğŸ—ƒï¸ DCAT dataset record (`dcat_dataset.jsonld`)
- ğŸ§· `evidence_manifest.yaml` (for Story Nodes / Pulse Threads / AI outputs)
- âœ… validation + policy reports (schema, license, sensitivity, provenance completeness)
- ğŸ” signatures + attestations (Cosign) + optional SBOM
- ğŸ“¦ optional OCI publishing metadata (ORAS media types, digest pinning)

---

## ğŸ—‚ï¸ Folder layout

> [!NOTE]
> This folder is the **scripts layer** for MCP traceability. If your repo uses a centralized CLI (e.g., `tools/cli.py`), treat these scripts as the implementation targets behind that CLI.

```text
ğŸ“¦ mcp/traceability/
â””â”€ ğŸ“ scripts/
   â”œâ”€ ğŸ“„ README.md
   â”œâ”€ ğŸ run_manifest.py                # create + hash run manifests (RFC 8785 canonical JSON)
   â”œâ”€ ğŸ emit_prov.py                   # build PROV-O JSON-LD from run + pipeline metadata
   â”œâ”€ ğŸ emit_stac.py                   # generate STAC Item/Collection + assets
   â”œâ”€ ğŸ emit_dcat.py                   # generate DCAT record + link to STAC/PROV
   â”œâ”€ ğŸ validate_bundle.py             # schema/spatial/license/provenance completeness checks
   â”œâ”€ ğŸ policy_gate.py                 # conftest/OPA runner wrapper (fail-closed)
   â”œâ”€ ğŸ graph_register.py              # mirror triplet into Neo4j (datasets/assets/activities)
   â”œâ”€ ğŸ graph_health_check.py          # scheduled integrity report (orphans, missing edges, drift)
   â”œâ”€ ğŸ evidence_manifest.py           # citations/evidence manifests for stories + AI outputs
   â”œâ”€ ğŸ ai_receipt.py                  # Focus Mode / agent action receipts + ledger payloads
   â”œâ”€ ğŸ privacy_scan.py                # optional: k-anonymity/l-diversity/t-closeness checks
   â”œâ”€ ğŸš sign_bundle.sh                 # cosign sign/verify artifacts (optional keyless)
   â”œâ”€ ğŸš publish_oci.sh                 # oras push/pull + attach provenance (optional)
   â””â”€ ğŸ§° extract_pdf_portfolio.sh        # helper: unpack PDF portfolios in /mnt/data (optional)
```

> [!TIP]
> If your repo already has these scripts elsewhere, use this README as the **contract** for what each command must guarantee (inputs, outputs, exit codes, side effects).

---

## âš¡ Quickstart

### âœ… Prereqs

- ğŸ Python 3.11+ (recommended for deterministic tooling + JSON schema validation)
- ğŸ§° `jq` / `yq` (optional but convenient)
- ğŸš¦ `conftest` + OPA (policy-as-code gates)
- ğŸ” `cosign` + ğŸ“¦ `oras` (optional, for supply-chain + OCI artifact distribution)
- ğŸ•¸ï¸ Neo4j client (optional, for graph registration + health checks)
- ğŸ—ºï¸ GDAL / `ogr2ogr` (optional, for geospatial ingest helpers)

### ğŸ”§ Environment variables (common)

```bash
# Core
export KFM_ENV=dev
export KFM_ACTOR="user:yourname"              # or agent:KFM_Bot / service:ci

# Storage / catalogs
export KFM_DATA_DIR="./data"

# Graph / DB (optional)
export NEO4J_URI="bolt://localhost:7687"
export NEO4J_USER="neo4j"
export NEO4J_PASS="password"
export POSTGIS_DSN="postgresql://user:pass@localhost:5432/kfm"

# Policy
export KFM_POLICY_DIR="./mcp/traceability/policy"

# OCI publish (optional)
export OCI_REGISTRY="ghcr.io/your-org/kfm"
export OCI_REPO="trace-bundles"
```

---

## ğŸ” End-to-end workflows

### 1) ğŸ“¥ Dataset pipeline run â†’ Trace Bundle (run manifest + triplet + gates)

**Intent:** Deterministic, idempotent pipelines that produce catalog-ready assets (e.g., GeoParquet/COGs/tiles), plus metadata and provenance.

```mermaid
flowchart LR
  A[ğŸ“¥ Source Data] --> B[ğŸ§ª Ingest/Transform]
  B --> C[ğŸ§¾ Run Manifest<br/>RFC 8785 + SHA-256]
  C --> D[ğŸ§¬ Emit STAC/DCAT/PROV]
  D --> E[ğŸš¦ Validate + Policy Gates<br/>(fail-closed)]
  E -->|pass| F[ğŸ” Sign (Cosign)]
  F --> G[ğŸ“¦ Publish (ORAS â†’ OCI)]
  G --> H[ğŸ•¸ï¸ Register in Graph]
  E -->|fail| X[ğŸ›‘ Block Merge/Publish]
```

**Typical command sequence (example):**
```bash
python run_manifest.py   --config pipelines/population/config.yaml --out data/audits/
python emit_stac.py      --run data/audits/<run_id>/run_manifest.json --out data/catalog/
python emit_dcat.py      --run data/audits/<run_id>/run_manifest.json --out data/catalog/
python emit_prov.py      --run data/audits/<run_id>/run_manifest.json --out data/provenance/

python validate_bundle.py --run data/audits/<run_id>/run_manifest.json
python policy_gate.py     --inputs data/catalog/ data/provenance/

# Optional supply-chain
./sign_bundle.sh          data/audits/<run_id>/
./publish_oci.sh          data/audits/<run_id>/ --registry "$OCI_REGISTRY" --repo "$OCI_REPO"
```

---

### 2) ğŸ“ Story Node / ğŸ«€ Pulse Thread â†’ Evidence Manifest + provenance hooks

Story outputs must remain **evidence-first**: citations and an evidence manifest that preserves raw references (dataset IDs, query params, timestamps, etc.).

```mermaid
flowchart LR
  S[ğŸ“ Narrative Content<br/>(Markdown/JSON)] --> M[ğŸ“ evidence_manifest.yaml]
  M --> V[âœ… Validate citations<br/>exist + resolvable]
  V --> P[ğŸ”— PROV snippet<br/>prov:Entity derivedFrom]
  P --> G[ğŸ•¸ï¸ Graph links<br/>Story/Pulse â†’ Datasets/Places]
  G --> UI[ğŸ§­ UI playback + provenance panel]
```

**Example:**
```bash
python evidence_manifest.py \
  --story docs/stories/dust_bowl.md \
  --out data/audits/<run_id>/evidence_manifest.yaml \
  --mode story-node

python validate_bundle.py --evidence data/audits/<run_id>/evidence_manifest.yaml
```

---

### 3) ğŸ§  Focus Mode answer â†’ receipt + governance ledger payload

Focus Mode answers are treated as **first-class artifacts**:
- they must include citations,
- they can be represented as derived entities with PROV links,
- they are logged to an immutable governance ledger with compliance metadata.

**Example:**
```bash
python ai_receipt.py \
  --question "How has drought impacted Kansas agriculture in the last decade?" \
  --context ./ui_state.json \
  --citations ./citations.json \
  --concepts drought,agriculture \
  --out data/audits/<run_id>/ai_answer_receipt.json
```

---

### 4) ğŸ•¸ï¸ Weekly graph health check (QA)

A scheduled job runs health checks via Cypher to catch:
- orphan nodes,
- missing lineage edges,
- broken references to catalog assets,
- stale/expired metadata.

**Example:**
```bash
python graph_health_check.py \
  --neo4j "$NEO4J_URI" \
  --out docs/reports/qa/graph_health/
```

---

## ğŸ§¾ Run Manifest format

KFMâ€™s ingestion patterns emphasize:
- deterministic/idempotent runs,
- JSON canonicalization (RFC 8785),
- SHA-256 digest stored back into the manifest (self-fingerprinting),
- manifests saved under `data/audits/<run_id>/run_manifest.json`.

### Minimal example (shape, not exact schema)

```json
{
  "run_id": "2026-01-21T03:14:15Z__population_county_1860_2020__v1",
  "run_time": "2026-01-21T03:14:15Z",
  "actor": "user:yourname",
  "pipeline": {
    "name": "population_county_1860_2020",
    "version": "1.0.0",
    "config_path": "pipelines/population/config.yaml"
  },
  "inputs": [
    {
      "role": "source_csv",
      "uri": "https://example.gov/census.csv",
      "retrieved_at": "2026-01-21T03:10:01Z",
      "sha256": "..."
    }
  ],
  "outputs": [
    {
      "role": "processed_table",
      "path": "data/processed/population_county_1860_2020.parquet",
      "sha256": "..."
    },
    {
      "role": "catalog_entry",
      "path": "data/catalog/datasets/population_county_1860_2020.json",
      "sha256": "..."
    }
  ],
  "tool_versions": {
    "python": "3.11.7",
    "gdal": "3.8.0"
  },
  "summary_counts": {
    "records_in": 123456,
    "records_out": 123456,
    "errors": 0
  },

  "canonical_digest": "sha256:<computed-after-RFC8785-canonicalization>",
  "idempotency_key": "sha256:<usually-same-as-canonical_digest>"
}
```

> [!TIP]
> The manifest digest doubles as an **activity identifier** that can be referenced in PROV and mirrored into Neo4j for exact â€œwhat produced this?â€ queries.

---

## ğŸ§¬ STAC + DCAT + PROV wiring

KFM uses a **triplet approach** (STAC + DCAT + PROV) and links them together:

- STAC can carry a PROV activity reference (or a version field that maps back to PROV).
- DCAT can point to STAC and PROV through distributions/relations.
- Neo4j mirrors the same evidence graph with nodes and edges (Dataset, Asset, Activity/Run, etc.).

> [!NOTE]
> Profiles matter: expect KFM-flavored profiles for STAC/DCAT/PROV (e.g., `kfm:dataset_id`, `kfm:classification`, sovereignty/sensitivity extensions, agent roles).

---

## ğŸš¦ Policy gates

Policy gates should run at checkpoints (ingestion, AI inference, content publication) and enforce at minimum:

- âœ… schema validation
- âœ… STAC/DCAT/PROV completeness
- âœ… license presence (no data without known license)
- âœ… sensitivity classification (and correct handling)
- âœ… provenance completeness (inputs + processing declared)
- âœ… **citations required for AI outputs** (refuse if unsourced)

> [!IMPORTANT]
> Philosophy: **fail closed** ğŸš«âœ…. If it canâ€™t be proven compliant, itâ€™s blocked.

### Conftest runner (example)

```bash
conftest test data/catalog/ data/provenance/ \
  --policy "$KFM_POLICY_DIR" \
  --output table
```

### Common â€œmust-haveâ€ rules ğŸ”’

- ğŸš« **Secrets scanning** (API keys/JWT patterns in JSON/YAML)
- ğŸ·ï¸ **License allowlist** (SPDX strings)
- ğŸ§­ **Required fields** (providers, spatial/temporal, attribution)
- ğŸª¶ **Indigenous data sovereignty flags** when applicable

---

## ğŸ” Signing + OCI publishing

KFMâ€™s traceability flow supports publishing bundles to an OCI registry:

- publish via **ORAS** (`oras push ...`) with typed media
- link the OCI artifact back into metadata (`distribution.oci` in STAC/DCAT)
- sign artifacts with **Cosign**
- optionally attach SBOM + provenance attestations

### Example ORAS push (pattern)

```bash
oras push "$OCI_REGISTRY/$OCI_REPO:run-<run_id>" \
  --manifest-config /dev/null:application/vnd.oci.empty.v1+json \
  data/audits/<run_id>/run_manifest.json:application/vnd.kfm.runmanifest+json \
  data/provenance/<run_id>.prov.jsonld:application/ld+json \
  data/catalog/datasets/<dataset_id>.stac.json:application/json \
  data/catalog/datasets/<dataset_id>.dcat.jsonld:application/ld+json
```

### Cosign sign/verify (pattern)

```bash
cosign sign "$OCI_REGISTRY/$OCI_REPO:run-<run_id>"
cosign verify "$OCI_REGISTRY/$OCI_REPO:run-<run_id>"
```

> [!TIP]
> Policy can require â€œall artifacts must be signed by approved identityâ€ before use.

---

## ğŸ•¸ï¸ Graph registration + health checks

### Graph registration

The trace bundle should register:

- `Dataset` node (DCAT-ish)
- `Asset` nodes (STAC-ish)
- `Activity/Run` nodes (PROV-ish)
- `Agent` nodes (human / bot / CI / Focus Mode / W-P-E agents)

â€¦and wire edges:
- `prov:used`, `prov:generated`, `prov:wasDerivedFrom`
- datasetâ†’asset membership
- story/pulseâ†’dataset/place references

### Graph health (scheduled QA)

A weekly report should cover:

- ğŸ§© missing links (dataset without prov activity; asset without dataset; story without evidence manifest)
- ğŸ§Ÿ orphans (nodes with no inbound/outbound relationships)
- ğŸ§ª stale metadata (missing checksums, missing licenses, missing sensitivity labels)
- ğŸ” drift (unexpected schema changes; policy failures)

Outputs should land in something like:
```text
docs/reports/qa/graph_health/graph_health_YYYYMMDD.md
docs/reports/qa/graph_health/graph_health_YYYYMMDD.json
```

---

## ğŸ§  AI traceability (Focus Mode + agents)

### Focus Mode receipts ğŸ§¾ğŸ¤–

The AI layer should produce machine-readable receipts that capture:

- the question + UI context (bbox/time/layers)
- citations (dataset IDs, graph nodes, document IDs)
- concept nodes used (Conceptual Attention Nodes)
- policy checks applied (citation coverage, sensitivity redactions)
- governance ledger payload (append-only signed record)

**Example receipt shape (minimal):**
```json
{
  "answer_id": "ai:focusmode:2026-01-21T03:22:01Z:abcd1234",
  "question": "What does this drought index layer show?",
  "ui_context": {
    "bbox": [-101.2, 36.8, -94.6, 40.0],
    "time": {"start": "2015-01-01", "end": "2025-12-31"},
    "active_layers": ["usdm_drought_index_v2"]
  },
  "citations": [
    {"kind": "dataset", "kfm_id": "dataset:usdm_drought_index_v2", "locator": "stac:...#asset:..."}
  ],
  "concepts": ["concept:drought", "concept:agriculture"],
  "policy": {
    "citation_required": true,
    "citation_coverage": 1.0,
    "sensitivity_redactions_applied": false
  },
  "prov": {
    "entity": "prov:Entity:ai_answer",
    "wasDerivedFrom": ["prov:Entity:dataset:usdm_drought_index_v2"],
    "activity": "prov:Activity:focusmode_inference",
    "agent": "prov:Agent:focusmode_v1"
  },
  "ledger_ref": "govledger:sha256:..."
}
```

### Watcherâ€“Plannerâ€“Executor agents ğŸ•µï¸â¡ï¸ğŸ§ â¡ï¸âš™ï¸

For automation, traceability scripts must also support:

- Watcher detection events (what changed / what triggered)
- Planner proposals (what will be done; why; required approvals)
- Executor actions (what actually ran; outputs; manifests; policy results)

> [!NOTE]
> Treat agent actions like pipelines: **manifested, validated, signed (optional), and attributable**.

---

## ğŸ—ºï¸ Geospatial helpers

KFM commonly uses:

- ğŸ§± **GeoParquet** for large vector datasets
- ğŸ›°ï¸ **COGs** (Cloud-Optimized GeoTIFF) for rasters
- ğŸ§© tiles / offline packs (e.g., PMTiles/MBTiles patterns)
- ğŸ—ƒï¸ PostGIS for spatial DB workflows (optional)

### Import helper patterns (PostGIS)

```bash
# Example pattern (choose your preferred toolchain)
shp2pgsql -s 4326 -I input.shp public.my_layer | psql "$POSTGIS_DSN"

# Or GDAL:
ogr2ogr -f "PostgreSQL" "PG:$POSTGIS_DSN" input.shp -nln public.my_layer
```

> [!TIP]
> If you generate tiles / offline packs, treat them as artifacts too: checksum + STAC asset entry + PROV lineage + license + signatures (optional).

---

## ğŸ§« Privacy + sensitivity checks

Optional (but recommended when handling sensitive data):

- ğŸ§â€â™€ï¸ k-anonymity / l-diversity / t-closeness checks
- ğŸ§¾ query auditing patterns (track access + transformations)
- ğŸ” classification enforcement (public/internal/restricted)

These checks should produce:
- a machine-readable report
- a policy gate result (pass/fail)
- provenance hooks showing transformations/redactions applied

---

## ğŸ“š Source docs used to build this README

### Core KFM design + governance
- ğŸ“˜ KFM Comprehensive Technical Documentation :contentReference[oaicite:0]{index=0}
- ğŸ—ï¸ KFM Comprehensive Architecture, Features, and Design :contentReference[oaicite:1]{index=1}
- ğŸ§­ğŸ¤– KFM AI System Overview :contentReference[oaicite:2]{index=2}
- ğŸ§© UI System Overview (Provenance panels, â€œmap behind the mapâ€) :contentReference[oaicite:3]{index=3}
- ğŸ“š Data Intake â€“ Technical & Design Guide (STAC/DCAT/PROV, evidence manifests) :contentReference[oaicite:4]{index=4}
- ğŸŒŸ Latest Ideas & Future Proposals (reproducible research + model outputs as traceable data) :contentReference[oaicite:5]{index=5}
- ğŸ’¡ Innovative Concepts to Evolve KFM :contentReference[oaicite:6]{index=6}
- ğŸ§  Additional Project Ideas (run manifests, policy-as-code, OCI publishing, Conceptual Attention Nodes) :contentReference[oaicite:7]{index=7}

### Reference libraries (PDF portfolios)
- ğŸ§  AI Concepts & more (portfolio) :contentReference[oaicite:8]{index=8}
- ğŸ—ƒï¸ Data Management / Architectures / Data Science / Bayesian Methods (portfolio) :contentReference[oaicite:9]{index=9}
- ğŸ—ºï¸ Maps + Google Maps + Virtual Worlds + Archaeological CG + Geospatial WebGL (portfolio) :contentReference[oaicite:10]{index=10}
- ğŸ§° Various programming languages & resources (portfolio) :contentReference[oaicite:11]{index=11}

---

## âœ… Contributing a new script (checklist)

> [!TIP]
> The fastest way to â€œdo traceability rightâ€ is to treat scripts like pipeline contracts: stable outputs + strict validation.

- [ ] Script has `--help` and non-zero exit on failure ğŸ§¯
- [ ] Emits or references a `run_manifest.json` ğŸ§¾
- [ ] Produces (or updates) STAC/DCAT/PROV artifacts ğŸ§¬
- [ ] Runs validation + policy gates (fail-closed) ğŸš¦
- [ ] Writes outputs into deterministic locations (`data/audits/`, `data/provenance/`, `data/catalog/`) ğŸ“‚
- [ ] If publishing, pins digests + signs artifacts (optional but preferred) ğŸ”
- [ ] Adds/updates schema + policy tests âœ…
- [ ] Updates this README (add the script + example usage) ğŸ“

