# ğŸ§¾ MCP Traceability â€” Kansas Frontier Matrix (KFM)

<p align="center">
  <img alt="Traceability" src="https://img.shields.io/badge/traceability-provenance%20first-brightgreen" />
  <img alt="Catalog" src="https://img.shields.io/badge/catalog-STAC%20%7C%20DCAT%20%7C%20PROV-blue" />
  <img alt="Policy" src="https://img.shields.io/badge/policy-OPA%20%2B%20Conftest-purple" />
  <img alt="AI" src="https://img.shields.io/badge/AI-citations%20required-orange" />
  <img alt="Governance" src="https://img.shields.io/badge/governance-FAIR%20%2B%20CARE-black" />
</p>

> **Mission:** Every dataset, layer, story, simulation, and AI answer must be traceable to **cataloged sources** and **provable processing** â€” so users can always see the **map behind the map** ğŸ—ºï¸âœ¨

---

## ğŸ” What this folder is

This directory is the **MCP (Master Coder Protocol)** home for **traceability** in KFM.

It defines *how we capture, validate, store, and surface*:

- ğŸ“¦ **Data lineage** (raw âœ processed âœ published)
- ğŸ§¬ **Provenance** (who/what/when/how produced an artifact)
- ğŸ” **Governance events** (approvals, policy outcomes, access constraints)
- ğŸ¤– **AI receipts** (what the assistant used + what it refused to invent)
- ğŸ§ª **Simulation & modeling run evidence** (inputs, code, parameters, outputs)
- ğŸ› ï¸ **DevOps lineage** (PRs/commits/releases as provenance data)

> [!IMPORTANT]
> **Traceability is a product feature** *and* a **hard gate**. If it canâ€™t be traced, it canâ€™t ship.

---

## ğŸ§­ Quick navigation

- âœ… **Nonâ€‘negotiables** â†’ [Traceability invariants](#-traceability-invariants)
- ğŸ§± **Architecture** â†’ [Endâ€‘toâ€‘end trace graph](#-end-to-end-trace-graph)
- ğŸ“¦ **Artifacts** â†’ [What we must generate](#-required-trace-artifacts)
- âš–ï¸ **Policy** â†’ [Policy Pack & CI gates](#ï¸-policy-pack--ci-gates)
- ğŸ¤– **AI** â†’ [Focus Mode receipts](#-focus-mode-traceability)
- ğŸ›°ï¸ **Streaming** â†’ [Realâ€‘time & sensors](#ï¸-streaming--real-time-traceability)
- ğŸ§ª **Simulations** â†’ [Model runs & kfm-sim-run](#-simulations--modeling-traceability)
- ğŸ§· **DevOps** â†’ [PR â†’ PROV](#-devops-traceability-pr--prov)
- ğŸ—‚ï¸ **Repo layout** â†’ [Suggested contents of this folder](#ï¸-suggested-folder-layout)

---

## âœ… Traceability invariants

These are the **MCP traceability guarantees** KFM aims to enforce across the platform:

### 1) ğŸš« No â€œmystery layersâ€
- Anything visible in the UI or referenced by AI must map back to **cataloged sources**.
- Every published layer must have **source + license + provenance + classification**.

### 2) ğŸ“š Metadata triplet is mandatory
Every dataset must have the **STAC + DCAT + PROV** alignment, linked across layers:

- **STAC**: assets + spatiotemporal extents + links to distributions
- **DCAT**: dataset-level discovery metadata, licensing, publisher, distributions
- **PROV**: the lineage graph (entities, activities, agents)

### 3) ğŸ§¾ Every meaningful output has a â€œreceiptâ€
If KFM produces something that looks authoritative (map tiles, story claims, AI answers, analysis outputs, simulation results), it must write an **evidence receipt** that can be audited.

### 4) ğŸ§¯ â€œFail closedâ€ policy gates
If required evidence is missing:
- The ingestion fails
- The publication fails
- The UI must not display it
- Focus Mode must refuse rather than fabricate

### 5) ğŸ§¬ Determinism & idempotency
Pipelines must be **config-driven and replayable**:
- same inputs + same code + same config â‡’ same outputs + same checksums (within expected tolerances)

### 6) ğŸªª Sensitivity & sovereignty are first-class trace data
Traceability includes not just â€œwhere fromâ€ but also:
- **constraints** (CARE / community protocols)
- **roles** (who can see what)
- **transformations** (obfuscation, redaction, aggregation)

---

## ğŸ§± End-to-end trace graph

KFM traceability spans the **canonical pipeline ordering**:

```mermaid
flowchart LR
  A[ğŸ“¥ Intake: Raw Sources] --> B[ğŸ§° ETL / Pipelines]
  B --> C[ğŸ“¦ Catalogs: STAC + DCAT + PROV]
  C --> D[ğŸ§  Knowledge Graph: Neo4j Lineage]
  D --> E[ğŸ”Œ API Layer: REST/GraphQL]
  E --> F[ğŸ—ºï¸ UI: 2D/3D Maps + Story Nodes]
  F --> G[ğŸ¤– Focus Mode: Evidence-backed AI]
  B --> H[ğŸ§¾ Governance Ledger: append-only]
  E --> H
  G --> H
```

**Core idea:** KFM is **catalog-driven** and **evidence-first**. The catalogs and provenance *are* the backbone that everything else consumes.

---

## ğŸ“¦ Required trace artifacts

This section describes the **minimum â€œreceipt setâ€** per dataset / layer / output.

### ğŸ—‚ï¸ Dataset artifacts (minimum)

| Artifact | Purpose | Typical location |
|---|---|---|
| `source.json` | Where it came from, retrieval method, timestamps, license URL, contact, terms | `data/raw/<dataset>/source.json` |
| `checksums.sha256` | Integrity & repeatability (raw + processed) | `data/raw/...` and/or `data/processed/...` |
| `data-contract.json` (or similar) | Schema + expectations + domain fields + provenance linkage | `data/contracts/...` |
| `stac/collection.json` + `stac/items/*.json` | Spatiotemporal assets & file pointers | `data/catalog/stac/...` |
| `dcat/dataset.jsonld` (or Turtle) | Discovery metadata & distributions | `data/catalog/dcat/...` |
| `prov/bundle.jsonld` | Entities/Activities/Agents + pipeline lineage | `data/provenance/...` |

> [!NOTE]
> KFM may define **KFM profiles** for STAC/DCAT/PROV (e.g., requiring `kfm:dataset_id`, `kfm:classification`, sovereignty fields, etc.).

### ğŸ§  Graph lineage artifacts (minimum)

Once imported into Neo4j, the trace should be navigable:

- Dataset node(s) (DCAT-derived)
- Asset node(s) (STAC-derived)
- Activity/Run node(s) (PROV-derived)
- Agent node(s) (human, bot, CI workflow, AI assistant)
- Edges:
  - `USED` (inputs)
  - `GENERATED` (outputs)
  - `ASSOCIATED_WITH` (who/what ran it)
  - `DERIVED_FROM` (output-to-input lineage)

### ğŸ—ºï¸ UI/Story artifacts (minimum)

| Output type | Required trace |
|---|---|
| Map layer in UI | Source attribution + dataset ID + link to PROV |
| Story Node claim | Inline citations to dataset/assets/graph entities |
| Export (image/data) | Auto-attached credits + provenance summary |

---

## ğŸ§¾ Traceability matrix (what must be traceable)

| Stage | Inputs | Outputs | Evidence written | â€œFail closedâ€ gate |
|---|---|---|---|---|
| ğŸ“¥ Intake | External source + retrieval method | Raw files | `source.json` + raw checksums | Missing license/terms |
| ğŸ§° ETL | Raw files + config + code commit | Processed artifacts | RunContext + processed checksums | Missing schema / contract |
| ğŸ“¦ Catalog | Processed artifacts | STAC/DCAT/PROV | Linked triplet | Missing cross-links |
| ğŸ§  Graph | Catalog triplet | Neo4j nodes/edges | Import logs + lineage edges | Graph missing PROV activity |
| ğŸ”Œ API | Graph + PostGIS | Responses | API logs + trace IDs | Unauthorized/sensitive access |
| ğŸ—ºï¸ UI | API responses | Map/Story views | Visible provenance links | â€œMystery layerâ€ detected |
| ğŸ¤– Focus Mode | Catalog/graph + UI context | Answer/refusal | Answer receipt + governance log | Missing citations |
| ğŸ›°ï¸ Streaming | New observations | Micro-batches | Append-only trace events | Missing classification |
| ğŸ§ª Simulation | Inputs + model code | Outputs | Run receipt + PROV | Missing assumptions |

---

## âš–ï¸ Policy Pack & CI gates

Traceability is enforced by **policy-as-code** (OPA/Rego + Conftest) and validation tooling.

### âœ… Minimum policy gates (expected)

- **Schema validation** for data contracts & outputs
- **STAC/DCAT/PROV completeness**
- **License presence** (no unknown license)
- **Sensitivity classification** present & consistent
- **Provenance completeness** (inputs + steps declared)
- **Focus Mode citations required** (refuse if not source-backed)
- **Security scans** (secrets, PII, sensitive coordinates, etc.)
- **Classification consistency** (no accidental â€œdowngradeâ€)

> [!TIP]
> Put the policy pack docs near: `api/scripts/policy/` and CI checks in `.github/workflows/`.

---

## ğŸ¤– Focus Mode traceability

Focus Mode is *not* â€œchat in a vacuum.â€ It is constrained by traceability:

### What Focus Mode must do âœ…

- **Always cites sources** (dataset IDs, docs, graph entities)
- Uses **UI context** (location, timeframe, active layers)
- Surfaces **Explainable AI** via audit panel:
  - what data influenced the answer
  - governance flags (sensitive notices, constraints)
- **Logs a receipt** to a governance ledger for audit

### What Focus Mode must never do ğŸš«

- invent facts when no evidence exists
- cite things it didnâ€™t actually use
- bypass the catalog/graph boundary

### ğŸ“„ â€œAI Answer Receiptâ€ (recommended structure)

Store a receipt for every answer (and refusal) so we can reproduce:

- `question`
- `ui_context` (bbox, time range, layer IDs)
- `retrieved_evidence` (dataset IDs, STAC items, graph entity IDs)
- `queries` (graph/API calls, parameters)
- `policy_results` (citations ok? sensitivity ok?)
- `response` + `citations`
- `governance_ledger_id`

<details>
<summary>ğŸ“¦ Example receipt (JSON)</summary>

```json
{
  "receipt_version": "1.0",
  "receipt_id": "kfm:receipt:01J2F...ULID",
  "created_at": "2026-01-19T12:34:56Z",
  "actor": { "type": "ai_agent", "id": "kfm:agent:focus-mode" },
  "question": "Whatâ€™s the current water level at the Kansas River gauge in Topeka?",
  "ui_context": {
    "bbox": [-96.0, 38.8, -95.0, 39.3],
    "time_range": ["2026-01-19T00:00:00Z", "2026-01-19T23:59:59Z"],
    "active_layers": ["kfm:layer:river_gauges_realtime"]
  },
  "evidence": [
    { "type": "dcat_dataset", "id": "kfm:dataset:usgs_nwis_realtime@v2026.01.19" },
    { "type": "graph_entity", "id": "kfm:station:usgs:06889500" }
  ],
  "queries": [
    { "type": "api_call", "endpoint": "/api/realtime/gauges/06889500/latest" }
  ],
  "policy": {
    "pack_version": "kfm-policy-pack@v13",
    "citations_required": true,
    "sensitivity_ok": true
  },
  "response": {
    "answer": "As of 8:00 PM today, the water level at Topeka is X feet.",
    "citations": [
      { "label": "USGS Real-time Water Data", "ref": "kfm:dataset:usgs_nwis_realtime@v2026.01.19" }
    ]
  },
  "governance_ledger_id": "kfm:ledger:01J2F..."
}
```

</details>

---

## ğŸ—ºï¸ UI traceability surfaces

The UI is expected to be **transparent by default**:

### Must-haves âœ…
- Layer Info popups show:
  - source attribution (from DCAT)
  - dataset version + classification
  - provenance link (to PROV bundle)
- Exports (screenshots, data downloads) carry:
  - credits + license
  - provenance summary

> [!NOTE]
> The UI should remain decoupled from the graph â€” it talks to the system via API contracts, not direct DB queries.

---

## ğŸ›°ï¸ Streaming & real-time traceability

Streaming data is treated as **many small ingests** over time:

- Watcher polls (or subscribes) at a short interval
- Micro-batch processes data into PostGIS (append-only where possible)
- Emits STAC items for observations (or time-windowed items)
- Ensures every streaming layer still has:
  - source attribution
  - classification rules
  - stub/rolling provenance

---

## ğŸ§ª Simulations & modeling traceability

Simulation/model outputs are treated as **first-class datasets**:

### Model run receipts should include
- input dataset IDs (prov:used)
- model code version (commit hash)
- parameters and random seeds
- output artifacts (rasters/tables) + checksums
- assumptions and validity notes
- reviewer approvals (if required)

> [!TIP]
> If you canâ€™t answer â€œwhich model run produced this output and with what assumptions?â€ then the simulation trace is incomplete.

---

## ğŸ§· DevOps traceability: PR â†’ PROV

KFM treats development history as provenance:

- PR = **PROV Activity**
- Commits = **PROV Entity**
- Authors/reviewers/CI bots = **PROV Agents**
- Relationships:
  - `prov:used` (PR uses commits)
  - `prov:wasAssociatedWith` (PR associated with author/CI)
  - `prov:wasGeneratedBy` (merge commit generated by PR)

This makes it possible to answer:
- â€œWhich code version produced this dataset?â€
- â€œWho reviewed the change that altered pipeline X?â€

---

## ğŸ§  Automation traceability: Watcherâ€“Plannerâ€“Executor (Wâ€‘Pâ€‘E)

Wâ€‘Pâ€‘E is a safety-first automation pattern:

- **Watcher** observes and emits immutable alerts/events
- **Planner** proposes a deterministic plan (often a PR)
- **Executor** opens PRs with evidence; does not bypass governance

Key properties:
- idempotent operations (replays produce same diffs)
- kill-switch for safety
- signed, auditable event records

---

## ğŸ§¯ Rollback & provenance repair

Mistakes happen â€” but rollbacks must remain traceable.

A rollback runbook should describe:
- how to revert data in PostGIS
- how to retract/update catalogs
- how to write a PROV record that marks the rollback (so lineage is preserved, not erased)

---

## ğŸ—‚ï¸ Suggested folder layout

This README is intentionally implementable. Suggested structure:

```text
mcp/
  traceability/
    README.md ğŸ§¾
    schemas/ ğŸ§¬
      trace_event.schema.json
      receipt.schema.json
      prov_bundle.profile.json
    policies/ âš–ï¸
      traceability.rego
      citations_required.rego
      sensitivity_rules.rego
    templates/ ğŸ§©
      TEMPLATE__TRACEABILITY_RECEIPT.md
      TEMPLATE__DATASET_LINEAGE.md
      TEMPLATE__MODEL_RUN_RECEIPT.md
    examples/ ğŸ§ª
      dataset_ingest_receipt.json
      focus_mode_answer_receipt.json
      pr_prov_record.jsonld
    scripts/ ğŸ› ï¸
      emit_trace.py
      verify_trace.py
      build_prov_bundle.py
      link_pr_to_prov.py
    dashboards/ ğŸ“Š
      traceability_health.md
```

---

## ğŸ§° Contributor checklist

When adding or changing anything that could affect user trust:

### âœ… For a new dataset
- [ ] `source.json` written (license + retrieval details)
- [ ] checksums created (raw + processed)
- [ ] data contract updated/validated
- [ ] STAC + DCAT + PROV generated and cross-linked
- [ ] Neo4j lineage query works end-to-end
- [ ] UI shows attribution + provenance link
- [ ] policy pack passes (including sensitivity)

### âœ… For a new AI feature
- [ ] citations requirement enforced
- [ ] refusal path documented (no evidence â‡’ no answer)
- [ ] answer receipt format implemented
- [ ] governance ledger entries recorded
- [ ] audit panel (or equivalent) can surface evidence used

### âœ… For a new pipeline/automation agent
- [ ] deterministic RunContext stored
- [ ] idempotency key / replay safety documented
- [ ] PR-based governance path (no silent changes)
- [ ] kill-switch is present and tested

---

## ğŸ“š Project reference library (source PDFs)

These project files define the traceability requirements and philosophy. Recommended to store under `docs/references/`:

### Core KFM design docs ğŸ“˜
- `Kansas Frontier Matrix (KFM) â€“ Comprehensive Technical Documentation.pdf`
- `Kansas Frontier Matrix (KFM) â€“ Comprehensive Architecture, Features, and Design.pdf`
- `Kansas Frontier Matrix (KFM) â€“ AI System Overview ğŸ§­ğŸ¤–.pdf`
- `Kansas Frontier Matrix â€“ Comprehensive UI System Overview.pdf`
- `ğŸ“š Kansas Frontier Matrix (KFM) Data Intake â€“ Technical & Design Guide.pdf`
- `ğŸŒŸ Kansas Frontier Matrix â€“ Latest Ideas & Future Proposals.docx.pdf`
- `Innovative Concepts to Evolve the Kansas Frontier Matrix (KFM).pdf`

### Open source / audit references ğŸ§­
- `Kansas-Frontier-Matrix_ Open-Source Geospatial Historical Mapping Hub Design.pdf`
- `Kansas-Frontier-Matrix Design Audit â€“ Gaps and Enhancement Opportunities.pdf`

### Research packs (PDF portfolios) ğŸ—ƒï¸
> These may contain embedded docs/resources and can be extracted into `docs/research/`:
- `AI Concepts & more.pdf`
- `Data Managment-Theories-Architures-Data Science-Baysian Methods-Some Programming Ideas.pdf`
- `Various programming langurages & resources 1.pdf`
- `Maps-GoogleMaps-VirtualWorlds-Archaeological-Computer Graphics-Geospatial-webgl.pdf`

---

## ğŸ§© Next steps (recommended)

1. âœ… Implement the **receipt schema** + validator
2. âœ… Stand up **policy pack** gates for provenance/citations/sensitivity
3. âœ… Build a **Trace Explorer UI** (dataset â†’ lineage graph â†’ exports)
4. âœ… Add **PR â†’ PROV** ingest into Neo4j
5. âœ… Add governance ledger storage + signing strategy

If you can click a layer and answer **â€œwhere did this come from?â€** in under 10 seconds â€” youâ€™re doing it right. ğŸš€
