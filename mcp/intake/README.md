# ğŸ“¥ MCP Intake â€” Data & Evidence Ingestion Runbook

![MCP](https://img.shields.io/badge/MCP-Intake%20SOP-blue)
![Provenance](https://img.shields.io/badge/Provenance-First-success)
![Catalogs](https://img.shields.io/badge/Metadata-STAC%20%7C%20DCAT%20%7C%20PROV-informational)
![Pipelines](https://img.shields.io/badge/ETL-Deterministic%20%26%20Idempotent-orange)
![Governance](https://img.shields.io/badge/Governance-FAIR%20%2B%20CARE-purple)

> [!IMPORTANT]
> **Nothing enters KFM â€œas a usable layer/evidence artifactâ€ until itâ€™s** (1) produced by a repeatable pipeline, and (2) published with **STAC + DCAT + PROV** boundary artifacts, and (3) passes governance + CI gates.

---

## ğŸ§­ What this SOP is

This README is the **MCP (Master Coder Protocol)** â€œintakeâ€ runbook for Kansas Frontier Matrix (KFM).

It defines the **canonical** way to bring anything into the platform:
- ğŸ—ºï¸ geospatial datasets (vector/raster/tiles)
- ğŸ“„ documents & corpora (OCR, text, reports)
- ğŸ§ª analysis outputs & simulations
- ğŸ¤– AI-generated â€œevidence artifactsâ€ (summaries, extracted entities, derived layers)

The core promise: **evidence-first publishing** â€” every user-facing layer, chart, and citation is traceable back to **raw inputs + deterministic transforms + provenance**.

---

## ğŸ§± Nonâ€‘negotiable invariants

> [!TIP]
> Print these. Treat them like build rules. âœ…

1) **Pipeline ordering is absolute**  
   `ETL â†’ Catalogs (STAC/DCAT/PROV) â†’ Graph â†’ API â†’ UI â†’ Story Nodes â†’ Focus Mode`

2) **Raw is immutable**  
   Raw snapshots are append-only. Never â€œfixâ€ raw by editing it in-place.

3) **No ad-hoc manual manipulation**  
   If a processed result is wrong: update code/config, re-run, re-publish.

4) **Catalog triplet required**  
   Every dataset/evidence artifact must have:
   - ğŸ›°ï¸ STAC Collection + Items (assets & spatiotemporal metadata)
   - ğŸ—‚ï¸ DCAT dataset entry (discovery + distributions)
   - ğŸ§¾ PROV bundle (lineage: inputs â†’ work â†’ outputs; agents; parameters)

5) **Graph stores relationships, not bulky payloads**  
   Neo4j nodes should **reference** catalog IDs (e.g., STAC Item IDs / persistent IDs), not embed full data.

6) **Governance & sovereignty propagate forward**  
   Outputs canâ€™t be less restricted than inputs; sensitive classifications carry through derivatives.

---

## ğŸ“Œ Table of contents

- [ğŸ§° Intake deliverables](#-intake-deliverables)
- [ğŸ—‚ï¸ Canonical repo layout](#ï¸-canonical-repo-layout)
- [ğŸ” Intake workflow](#-intake-workflow)
- [âœ… Definition of done](#-definition-of-done)
- [ğŸ§© Templates](#-templates)
- [ğŸ—ºï¸ Geospatial-specific guidance](#ï¸-geospatial-specific-guidance)
- [ğŸ“¡ Streaming / watcher intake](#-streaming--watcher-intake)
- [ğŸ¤– AI & evidence artifacts](#-ai--evidence-artifacts)
- [ğŸ§ª Validation & CI gates](#-validation--ci-gates)
- [ğŸ§¯ Troubleshooting](#-troubleshooting)
- [ğŸ“š Project file index](#-project-file-index)

---

## ğŸ§° Intake deliverables

| Deliverable âœ… | What it is | Why it exists |
|---|---|---|
| **Raw snapshot** | Immutable input files | Reproducibility & auditing |
| **Source receipt** | URL/license/citation/checksums/access date | Provenance + legal clarity |
| **Pipeline** | Config-driven deterministic transform | Repeatable builds |
| **Processed assets** | Publishable outputs (COG, GeoParquet, tiles, etc.) | UI/API performance |
| **STAC** | Collection + Item(s) | Spatial/temporal indexing + asset pointers |
| **DCAT** | Dataset record (JSON-LD) | Discovery + external harvesting |
| **PROV** | Activity bundle | Full lineage, agents, parameters |
| **Store ingest** (as needed) | PostGIS + Neo4j + search/vector index | Query speed + relationships |

---

## ğŸ—‚ï¸ Canonical repo layout

> [!NOTE]
> Folder names may vary slightly between deployments; **the intent does not**. Keep the â€œraw/work/processed + catalogsâ€ staging discipline.

```text
ğŸ“¦ repo-root/
â”œâ”€ ğŸ“ data/
â”‚  â”œâ”€ ğŸ“ raw/                  # âœ… immutable snapshots (append-only)
â”‚  â”œâ”€ ğŸ“ work/                 # ğŸ§ª intermediate/temporary (rebuildable)
â”‚  â”œâ”€ ğŸ“ processed/            # ğŸš€ publishable outputs (stable paths)
â”‚  â”œâ”€ ğŸ“ stac/
â”‚  â”‚  â”œâ”€ ğŸ“ collections/       # ğŸ›°ï¸ STAC Collections
â”‚  â”‚  â””â”€ ğŸ“ items/             # ğŸ›°ï¸ STAC Items
â”‚  â”œâ”€ ğŸ“ catalog/
â”‚  â”‚  â””â”€ ğŸ“ dcat/              # ğŸ—‚ï¸ DCAT datasets (JSON-LD)
â”‚  â”œâ”€ ğŸ“ prov/                 # ğŸ§¾ PROV bundles (or data/provenance/)
â”‚  â””â”€ ğŸ“ sources/              # ğŸ§¾ source receipts (per-source JSON + indexes)
â”‚
â”œâ”€ ğŸ“ pipelines/               # ğŸ§° ETL + normalization code/configs
â”‚  â””â”€ ğŸ“ <domain>/
â”‚     â””â”€ ğŸ“ <dataset_slug>/
â”‚
â”œâ”€ ğŸ“ docs/
â”‚  â””â”€ ğŸ“ data/
â”‚     â””â”€ ğŸ“ <domain>/
â”‚        â””â”€ ğŸ“ <dataset_slug>/
â”‚           â””â”€ ğŸ“„ README.md    # domain/dataset runbook
â”‚
â””â”€ ğŸ“ mcp/
   â””â”€ ğŸ“ intake/
      â””â”€ ğŸ“„ README.md          # ğŸ‘ˆ you are here
```

---

## ğŸ” Intake workflow

### 0) Pick a dataset identity ğŸ·ï¸
- **domain:** e.g. `historical`, `air-quality`, `soils`, `hydrology`
- **dataset_slug:** kebab-case, stable: `kansas-county-boundaries`
- **dataset_version:** `vYYYY-MM-DD` or semantic version (keep it consistent)

> [!CAUTION]
> If you canâ€™t clearly state **license + attribution**, youâ€™re not ready to intake.

---

### 1) Acquire & snapshot raw inputs ğŸ“¦
1. Create the dataset folder:
   - `data/raw/<domain>/<dataset_slug>/<dataset_version>/`
2. Download/copy raw inputs into that folder **unchanged**
3. Generate checksums (sha256 recommended)
4. Record retrieval context (URL, date, auth method if applicable)

Example (bash):
```bash
# Example only â€” adapt to your environment
mkdir -p data/raw/<domain>/<dataset_slug>/<dataset_version>
sha256sum data/raw/<domain>/<dataset_slug>/<dataset_version>/* > data/raw/<domain>/<dataset_slug>/<dataset_version>/SHA256SUMS.txt
```

---

### 2) Create a source receipt ğŸ§¾
Create one (or both) of:
- `data/sources/<dataset_slug>.source.json`
- `data/sources/sources.json` (index of sources)

Minimal receipt fields (guideline):
- `id`, `title`, `source_url`
- `retrieved_at` (ISO-8601)
- `license`, `attribution`, `citation`
- `checksums` (sha256 per file or per archive)
- `spatial_extent`, `temporal_extent` (if known)
- `sensitivity` / `classification` (governance)

> [!TIP]
> Treat the source receipt like a â€œdata bill of materialsâ€.

---

### 3) Build the pipeline (deterministic + idempotent) ğŸ§°
Create:
- `pipelines/<domain>/<dataset_slug>/` (code + configs)

Rules:
- **Deterministic:** same inputs â‡’ same outputs  
- **Idempotent:** re-run safely without duplicating/dirtying outputs
- **Logged:** capture run IDs, parameters, and input/output hashes
- **Stage-aware:** write intermediates to `data/work/`, finals to `data/processed/`

---

### 4) Produce publishable outputs ğŸš€
Write final outputs to:
- `data/processed/<domain>/<dataset_slug>/<dataset_version>/`

Guidelines:
- Prefer open + stable formats:
  - vector: GeoParquet / GeoPackage (depending on needs), GeoJSON for small
  - raster: COG (Cloud-Optimized GeoTIFF)
  - tiles: MBTiles / vector tiles (as needed)
- If you reproject, simplify, or normalize attributes: record it in PROV.

---

### 5) Emit boundary artifacts: STAC + DCAT + PROV ğŸ›°ï¸ğŸ—‚ï¸ğŸ§¾
Create:
- `data/stac/collections/<domain>__<dataset_slug>.collection.json`
- `data/stac/items/<domain>__<dataset_slug>__<dataset_version>__*.item.json`
- `data/catalog/dcat/<domain>__<dataset_slug>.dataset.jsonld`
- `data/prov/<domain>__<dataset_slug>__<dataset_version>.prov.jsonld` (or `data/provenance/`)

Cross-link expectations:
- STAC Item **must** point to processed assets (`data/processed/**` or stable storage URL)
- DCAT entry links to STAC and/or direct distributions
- PROV links the chain: raw â†’ work â†’ processed, and identifies run/config/commit

> [!IMPORTANT]
> These â€œboundary artifactsâ€ are the contract to downstream layers (graph â†’ API â†’ UI â†’ story â†’ focus).

---

### 6) Ingest into stores (as needed) ğŸ•¸ï¸ğŸ—„ï¸
Depending on dataset type:
- ğŸ—„ï¸ **PostGIS**: load geometries/rasters for spatial queries & serving
- ğŸ•¸ï¸ **Neo4j**: ingest catalog-derived nodes + relationships (keep it reference-based)
- ğŸ” **Search index**: full-text indexing for documents, story nodes, metadata
- ğŸ§  **Vector index** (optional): embeddings for retrieval (still provenance-linked)

---

### 7) Document the domain module ğŸ“š
Create / update:
- `docs/data/<domain>/<dataset_slug>/README.md`

Include:
- purpose & use cases
- exact source URLs + licensing
- pipeline steps (how to rebuild)
- validation steps
- known limitations / uncertainty

---

### 8) PR + CI gates âœ…
Open a PR that includes:
- raw snapshot + checksums
- source receipt
- pipeline code/config
- processed outputs
- STAC/DCAT/PROV artifacts
- docs updates

CI should fail if:
- metadata missing
- provenance incomplete
- broken links
- license absent
- governance rules violated
- tests failing

---

## âœ… Definition of done

Use this checklist before requesting review:

- [ ] Dataset slug + version chosen and documented
- [ ] Raw snapshot stored under `data/raw/**` (immutable)
- [ ] Checksums generated and committed
- [ ] Source receipt created (license + attribution + citation included)
- [ ] Pipeline added under `pipelines/**` (deterministic + idempotent)
- [ ] Processed outputs stored under `data/processed/**`
- [ ] STAC Collection + Items created and validated
- [ ] DCAT dataset entry created and validated
- [ ] PROV bundle created (raw â†’ work â†’ processed; agents; params; run/config/commit)
- [ ] Governance tags applied (classification propagates)
- [ ] (If needed) PostGIS/Neo4j/search ingest steps documented
- [ ] Domain dataset README updated under `docs/data/**`
- [ ] CI green âœ…

---

## ğŸ§© Templates

> [!NOTE]
> These are **starter shapes**. Align with your project profiles (e.g., `docs/standards/KFM_*_PROFILE.md`) once present.

<details>
<summary><strong>ğŸ§¾ Source receipt template (JSON)</strong></summary>

```json
{
  "id": "kansas-county-boundaries",
  "title": "Kansas County Boundaries",
  "source_url": "https://example.org/datasets/kansas_counties.zip",
  "retrieved_at": "2026-01-26T00:00:00Z",
  "license": "CC-BY-4.0",
  "attribution": "Example Agency (2024)",
  "citation": "Example Agency. Kansas County Boundaries (2024).",
  "checksums": {
    "kansas_counties.zip": {
      "sha256": "REPLACE_ME"
    }
  },
  "spatial_extent": {
    "bbox": [-102.05, 36.99, -94.59, 40.00],
    "crs": "EPSG:4326"
  },
  "temporal_extent": {
    "start": "2024-01-01",
    "end": "2024-12-31"
  },
  "governance": {
    "classification": "public",
    "faircare": {
      "collective_benefit": "TBD",
      "authority_to_control": "TBD",
      "responsibility": "TBD",
      "ethics": "TBD"
    }
  }
}
```

</details>

<details>
<summary><strong>ğŸ›°ï¸ STAC Item skeleton (JSON)</strong></summary>

```json
{
  "type": "Feature",
  "stac_version": "1.0.0",
  "id": "historical__kansas-county-boundaries__v2026-01-26",
  "collection": "historical__kansas-county-boundaries",
  "geometry": null,
  "bbox": [-102.05, 36.99, -94.59, 40.00],
  "properties": {
    "datetime": "2026-01-26T00:00:00Z",
    "license": "CC-BY-4.0"
  },
  "assets": {
    "data": {
      "href": "data/processed/historical/kansas-county-boundaries/v2026-01-26/kansas_counties.parquet",
      "type": "application/x-parquet",
      "roles": ["data"]
    },
    "provenance": {
      "href": "data/prov/historical__kansas-county-boundaries__v2026-01-26.prov.jsonld",
      "type": "application/ld+json",
      "roles": ["metadata"]
    }
  }
}
```

</details>

<details>
<summary><strong>ğŸ—‚ï¸ DCAT Dataset skeleton (JSON-LD)</strong></summary>

```json
{
  "@context": {
    "dcat": "http://www.w3.org/ns/dcat#",
    "dct": "http://purl.org/dc/terms/",
    "prov": "http://www.w3.org/ns/prov#"
  },
  "@type": "dcat:Dataset",
  "dct:identifier": "historical__kansas-county-boundaries",
  "dct:title": "Kansas County Boundaries",
  "dct:description": "County boundary polygons for Kansas.",
  "dct:license": "CC-BY-4.0",
  "dcat:keyword": ["kansas", "boundaries", "counties"],
  "dcat:distribution": [
    {
      "@type": "dcat:Distribution",
      "dcat:accessURL": "data/stac/items/historical__kansas-county-boundaries__v2026-01-26.item.json",
      "dct:format": "STAC Item JSON"
    }
  ],
  "prov:wasGeneratedBy": "data/prov/historical__kansas-county-boundaries__v2026-01-26.prov.jsonld"
}
```

</details>

<details>
<summary><strong>ğŸ§¾ PROV bundle skeleton (JSON-LD)</strong></summary>

```json
{
  "@context": {
    "prov": "http://www.w3.org/ns/prov#",
    "xsd": "http://www.w3.org/2001/XMLSchema#"
  },
  "@id": "prov:bundle/historical__kansas-county-boundaries__v2026-01-26",
  "@type": "prov:Bundle",
  "prov:entity": [
    {
      "@id": "prov:entity/raw_zip",
      "prov:atLocation": "data/raw/historical/kansas-county-boundaries/v2026-01-26/kansas_counties.zip",
      "prov:label": "Raw source archive"
    },
    {
      "@id": "prov:entity/processed_parquet",
      "prov:atLocation": "data/processed/historical/kansas-county-boundaries/v2026-01-26/kansas_counties.parquet",
      "prov:label": "Processed GeoParquet output"
    }
  ],
  "prov:activity": [
    {
      "@id": "prov:activity/etl_run",
      "@type": "prov:Activity",
      "prov:startedAtTime": {"@value": "2026-01-26T00:00:00Z", "@type": "xsd:dateTime"},
      "prov:used": ["prov:entity/raw_zip"],
      "prov:generated": ["prov:entity/processed_parquet"],
      "prov:label": "ETL + normalization",
      "prov:wasAssociatedWith": "prov:agent/pipeline"
    }
  ],
  "prov:agent": [
    {
      "@id": "prov:agent/pipeline",
      "@type": "prov:SoftwareAgent",
      "prov:label": "pipelines/historical/kansas-county-boundaries (commit: REPLACE_ME)"
    }
  ]
}
```

</details>

---

## ğŸ—ºï¸ Geospatial-specific guidance

### CRS & projection sanity âœ…
- Store the original CRS (raw), and clearly document any reprojection.
- Use consistent CRS conventions (many pipelines standardize on EPSG:4326 for metadata and web mapping).
- Validate:
  - geometry validity (self-intersections, nulls)
  - bbox correctness
  - â€œlooks rightâ€ quick map preview (spot-check)

### Scanned maps / georeferencing ğŸ§­
If ingesting historical maps:
- Record control points and method in the PROV bundle
- Track georeferencing error metrics (e.g., RMS error) in metadata
- Preserve raw scans untouched; derived georeferenced raster belongs in `data/processed/**`

---

## ğŸ“¡ Streaming / watcher intake

For data feeds that change over time:
- Prefer conditional HTTP requests (ETag / If-Modified-Since)
- Snapshot new versions as new dataset versions, donâ€™t overwrite
- Emit new STAC Item(s) per release/version and link revisions via DCAT + PROV

> [!TIP]
> Treat each watcher run as a PROV activity; treat each output as a versioned entity.

---

## ğŸ¤– AI & evidence artifacts

KFM treats AI/analysis outputs as **first-class datasets** (not â€œspecial casesâ€):
- stored in `data/processed/**`
- cataloged in STAC/DCAT (with â€œderived / AI-generatedâ€ flags)
- traced in PROV with:
  - inputs (datasets/docs)
  - model/algorithm identity
  - parameters
  - confidence/uncertainty
- integrated with graph **cautiously** (explicit provenance pointers)
- exposed only via governed APIs (redaction/classification enforced)

---

## ğŸ§ª Validation & CI gates

Minimum gates for intake PRs:
- âœ… schema validation for STAC/DCAT/PROV (project profiles)
- âœ… link integrity checks (assets & metadata references)
- âœ… license presence checks
- âœ… provenance completeness checks
- âœ… security scans (secrets, sensitive data leaks)
- âœ… pipeline tests (unit/integration) where applicable

> [!IMPORTANT]
> CI should reject contributions that violate pipeline ordering, provenance requirements, or governance rules.

---

## ğŸ§¯ Troubleshooting

**â€œCI says PROV missing / incompleteâ€**
- Confirm you generated a PROV bundle for the dataset version
- Ensure the PROV entity chain references raw â†’ processed
- Include pipeline run identifiers (run ID / config / commit hash)

**â€œSTAC asset links are brokenâ€**
- STAC Items must point to stable `data/processed/**` paths (or stable storage URLs)
- Avoid absolute local paths

**â€œOutput is wrong but raw is correctâ€**
- Fix pipeline/config and re-run
- Publish as a new dataset version; link via `prov:wasRevisionOf` in DCAT/PROV

**â€œSensitive data might be exposedâ€**
- Stop intake, classify inputs, and apply redaction/generalization
- Ensure outputs are not less restricted than inputs

---

## ğŸ“š Project file index

These project files inform this SOP (core design + architecture + AI + UI + tooling + learning library):

### ğŸ§­ Core KFM design & roadmap
- Kansas Frontier Matrix (KFM) â€“ Comprehensive Platform Overview and Roadmap.pdf
- Kansas Frontier Matrix (KFM) â€“ Comprehensive Architecture, Features, and Design.pdf
- Kansas Frontier Matrix (KFM) â€“ Comprehensive Technical Documentation.pdf
- Kansas Frontier Matrix (KFM) â€“ Comprehensive UI System Overview (Technical Architecture Guide).pdf

### ğŸ¤– AI & governance
- Kansas Frontier Matrix (KFM) â€“ AI System Overview ğŸ§­ğŸ¤–.pdf
- KFM AI Infrastructure â€“ Ollama Integration Overview.pdf

### ğŸ§° Expanded guide & research process
- ğŸ“š Kansas Frontier Matrix (KFM) â€“ Expanded Technical & Design Guide.pdf
- AI Concepts & more.pdf
- Data Managment-Theories-Architures-Data Science-Baysian Methods-Some Programming Ideas.pdf
- Data Mining Concepts & applictions.pdf
- Scientific Method _ Research _ Master Coder Protocol Documentation.pdf

### ğŸ—ºï¸ Mapping, GIS, and web stack references
- Maps-GoogleMaps-VirtualWorlds-Archaeological-Computer Graphics-Geospatial-webgl.pdf
- Mapping-Modeling-Python-Git-HTTP-CSS-Docker-GraphQL-Data Compression-Linux-Security.pdf
- Geographic Information-Security-Git-R coding-SciPy-MATLAB-ArcGIS-Apache Spark-Type Script-Web Applications.pdf
- Various programming langurages & resources 1.pdf

---

## ğŸ§© Suggested next additions (nice-to-have)

- ğŸ“ `mcp/intake/templates/` (source receipt + STAC/DCAT/PROV templates)
- ğŸ“ `mcp/intake/checklists/` (PR checklist, governance checklist)
- ğŸ“ `mcp/intake/examples/` (one worked example per domain)
- âœ… A CI rule: â€œno STAC/DCAT/PROV â†’ no mergeâ€ (hard gate)
- ğŸ” Optional: artifact signing / attestations for releases (cosign)

---