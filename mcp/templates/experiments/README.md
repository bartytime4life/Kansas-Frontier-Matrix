# ğŸ§ª MCP Experiment Templates (KFM)

![MCP](https://img.shields.io/badge/MCP-Experiment%20Templates-blue)
![Docs](https://img.shields.io/badge/docs-documentation--first-success)
![Evidence](https://img.shields.io/badge/ethos-provenance--first-informational)

> [!NOTE]
> This folder is the **canonical copy/paste kit** for documenting **reproducible** experiments in the Kansas Frontier Matrix (KFM) ecosystem â€” spanning **data intake**, **geospatial processing**, **AI/Focus Mode**, **UI/Story Nodes**, **simulations**, and **governance/policy**.

---

## ğŸ¯ Why this exists

KFM treats experiments as **first-class, reviewable artifacts**:
- **Evidence-first**: every claim can be traced to sources / inputs / provenance.
- **Reproducible**: same inputs + same config + same code = same outputs.
- **Governed**: policy gates & ethical constraints are part of the experiment, not â€œafter the factâ€.
- **Human-in-the-loop**: automation can propose, but humans approve (especially for publication).

These templates are aligned with:
- provenance-first intake & metadata standards (STAC/DCAT/PROV),
- KFMâ€™s Focus Mode requirements (citations + refusal when unsourced),
- policy-as-code & CI checks,
- Story Nodes + interactive narrative UX,
- longer-term ideas like OCI artifact publishing, federation, AR, and 4D digital twins.

---

## âš¡ Quick start (copy â†’ fill â†’ run â†’ publish)

1. **Create a new experiment folder** (usually under `mcp/experiments/`):
   - âœ… recommended naming: `EXP-YYYYMMDD--short-slug`  
   - examples: `EXP-20260121--pmtiles-vs-mvt-perf`, `EXP-20260121--focusmode-citations-gate`

2. **Copy the templates** from this folder into your experiment folder.

3. **Fill in the experiment docs** (especially the report + metadata + evidence manifest).

4. **Run the experiment** (pipeline, notebook, script, UI prototype, policy test, etc.).

5. **Record results**:
   - metrics (`metrics.json` or `metrics.csv`)
   - artifacts (plots, tiles, reports, trained weights)
   - provenance (PROV JSON-LD, run manifest)

6. **Open a PR** and link:
   - the experiment report
   - evidence manifest
   - outputs (or OCI references)

> [!TIP]
> If youâ€™re not sure which template to use: start with the **Core** set (Report + Metadata + Evidence + Metrics + Provenance). Everything else is optional add-ons.

---

## ğŸ—‚ï¸ Expected folder layout (templates vs actual runs)

```text
ğŸ“ mcp/
  ğŸ“ templates/
    ğŸ“ experiments/
      ğŸ“„ README.md                      ğŸ‘ˆ you are here
      ğŸ“„ TEMPLATE__EXPERIMENT_REPORT.md
      ğŸ“„ TEMPLATE__EXPERIMENT_META.yaml
      ğŸ“„ TEMPLATE__EVIDENCE_MANIFEST.yaml
      ğŸ“„ TEMPLATE__METRICS.json
      ğŸ“„ TEMPLATE__PROV.prov.jsonld
      ğŸ“„ TEMPLATE__RUN_MANIFEST.json
      ğŸ“„ TEMPLATE__MODEL_CARD.md
      ğŸ“„ TEMPLATE__DATASET_DATASHEET.md
      ğŸ“„ TEMPLATE__UI_TEST_PLAN.md
      ğŸ“„ TEMPLATE__SIM_RUN.md
      ğŸ“„ TEMPLATE__DECISION_RECORD.md

  ğŸ“ experiments/
    ğŸ“ EXP-20260121--example/
      ğŸ“„ README.md                      (filled experiment report)
      ğŸ“„ experiment.yaml
      ğŸ“ evidence/
      ğŸ“ results/
      ğŸ“ artifacts/
      ğŸ“ logs/
```

And remember: **data belongs in KFMâ€™s standard data areas** (not ad-hoc folders):
- `data/raw/` â†’ immutable â€œas receivedâ€ evidence
- `data/work/` â†’ deterministic transforms / staging
- `data/processed/` â†’ publish-ready derivatives

---

## ğŸ§© Template pack: what each file is for

### âœ… Core (use for almost every experiment)
- **`TEMPLATE__EXPERIMENT_REPORT.md`**  
  Narrative write-up: question â†’ hypothesis â†’ method â†’ results â†’ decision â†’ next steps.
- **`TEMPLATE__EXPERIMENT_META.yaml`**  
  Machine-readable metadata: scope, datasets, regions, timeframe, owners, risk tags.
- **`TEMPLATE__EVIDENCE_MANIFEST.yaml`**  
  â€œWhat evidence supports this?â€ inputs, citations, hashes, licenses, constraints.
- **`TEMPLATE__METRICS.json`**  
  Results in a consistent format (perf, quality, UX, accuracy, coverage).
- **`TEMPLATE__PROV.prov.jsonld`**  
  Formal lineage: used entities â†’ activity â†’ generated entities.

### â• Add-ons (pick what matches your experiment type)
- **`TEMPLATE__RUN_MANIFEST.json`**  
  Reproducibility contract (env, seeds, commit SHA, hardware, parameters).
- **`TEMPLATE__MODEL_CARD.md`**  
  Required for ML/GeoAI outputs (training data, eval, limits, bias/risks).
- **`TEMPLATE__DATASET_DATASHEET.md`**  
  Required when introducing a new dataset or materially changing one.
- **`TEMPLATE__UI_TEST_PLAN.md`**  
  For UI/UX changes (timeline, story nodes, map layers, accessibility, perf).
- **`TEMPLATE__SIM_RUN.md`**  
  For simulations + scenario runs (inputs, calibration, uncertainty, outputs).
- **`TEMPLATE__DECISION_RECORD.md`**  
  When an experiment drives a platform decision (adopt/reject/iterate).

---

## ğŸ§¬ Experiment types (and what â€œgoodâ€ looks like)

> [!IMPORTANT]
> Different experiments have different success criteria â€” but **all** must meet the provenance, reproducibility, and governance baseline.

### 1) ğŸ“¥ Data intake & metadata experiments
Use when testing:
- STAC/DCAT/PROV coverage & correctness
- deterministic ingestion patterns
- W-P-E agents in CI
- schema validation & link checks

**Key metrics**
- schema pass rate
- missing required fields count
- provenance completeness score
- policy gate violations (by rule)

### 2) ğŸ—ºï¸ Geospatial processing & tiling experiments
Use when testing:
- PMTiles vs vector tiles vs raster tiles
- GeoParquet vs GeoJSON vs Shapefile transforms
- PostGIS query performance & indexing strategy

**Key metrics**
- tile generation time
- artifact size (per zoom / per region)
- map FPS / render latency
- query latency (bbox/time/window)

### 3) ğŸ¤– AI / Focus Mode experiments
Use when testing:
- retrieval quality (graph + text + spatial)
- citation enforcement (â€œrefuse if unsourcedâ€)
- explainability surfaces (audit panels, factors, flags)
- entity extraction + graph linking

**Key metrics**
- citation coverage (% answers with valid sources)
- refusal correctness (refuse when should)
- hallucination rate (should be near-zero)
- latency (P50/P95)
- user task success (when paired with UI tests)

### 4) ğŸ§­ UI / Story Node / narrative experiments
Use when testing:
- Story Nodes + guided tours
- timeline navigation and temporal layering
- â€œmap behind the mapâ€ provenance UI
- offline packs & field usability
- AR prototypes (early stage)

**Key metrics**
- task completion time
- accessibility checks (keyboard nav, contrast, ARIA)
- performance (load time, frame rate)
- provenance visibility (can users find source + license?)

### 5) ğŸ§Š Simulation & scenario experiments
Use when testing:
- kfm-sim-run style workflows
- calibration/bias correction steps
- uncertainty capture
- scenario comparisons & visualization

**Key metrics**
- reproducibility (bitwise or tolerance bounds)
- calibration error vs reference
- sensitivity analysis outputs
- provenance completeness of model inputs/params/outputs

### 6) ğŸ›¡ï¸ Governance / policy / safety experiments
Use when testing:
- OPA / Conftest policy pack behavior
- sensitivity tagging + obfuscation methods
- cultural protocol enforcement
- licensing compliance automation

**Key metrics**
- policy failures by severity
- false positive / false negative rate for gates
- sensitivity leakage tests
- license coverage & conflicts

---

## âœ… â€œDefinition of Doneâ€ for an experiment (MCP baseline)

Before an experiment can be considered â€œdoneâ€ (and merged), it should have:

- âœ… clear question & hypothesis  
- âœ… explicit scope (region, timeframe, dataset IDs)  
- âœ… deterministic method (code/config, not manual edits)  
- âœ… **evidence manifest** with sources + licenses + hashes  
- âœ… provenance artifact (PROV) or equivalent run lineage  
- âœ… metrics captured in a consistent machine-readable way  
- âœ… policy checks run (or explicitly waived with rationale)  
- âœ… decision recorded (adopt / reject / iterate)  
- âœ… follow-up tasks filed (if needed)

> [!WARNING]
> Experiments that introduce data *without licenses*, *without provenance*, or *without sensitivity classification* should be treated as **fail-closed**.

---

## ğŸ¤– Agent-assisted experiments (Wâ€“Pâ€“E friendly)

KFMâ€™s Watcherâ€“Plannerâ€“Executor approach is designed so that automation can:
- detect issues (Watcher),
- propose a deterministic plan (Planner),
- open a PR (Executor),
- **never silently merge** unless policy explicitly allows it.

When your experiment uses W-P-E or similar automation, ensure your docs include:
- idempotency key / commit seed
- kill-switch behavior (how to disable safely)
- provenance links to watcher alerts & planner reasoning
- explicit statement of what was automated vs human-reviewed

---

## ğŸ“¦ Publishing outputs (files, registries, catalogs)

Depending on experiment type, outputs may be published as:
- **repo artifacts** (small, diffable results)
- **data area artifacts** (large datasets, tiles, rasters)
- **OCI artifacts** (for larger, signed, versioned outputs)

**Preferred pattern (KFM-style)**
1. generate output
2. attach provenance (PROV)
3. publish distribution pointers in metadata (DCAT distributions + STAC links)
4. optionally sign artifacts (supply-chain integrity)

---

## ğŸ§  Using the projectâ€™s â€œResearch Libraryâ€ PDFs in experiments

Some project files are **PDF portfolios** (collections). They are meant to be referenced in experiment docs for:
- background research & known-good methods
- statistical validation and evaluation design
- geospatial rendering, projections, and WebGL pipelines
- AI/ML best practices and model evaluation

> [!TIP]
> In your experiment report, add a â€œBackgroundâ€ section that explicitly links to the relevant portfolio(s) and the specific embedded docs you relied on.

---

## ğŸ“š Reference docs used to shape these templates

### ğŸ§± Core KFM architecture & rules
- ğŸ§  KFM â€“ AI System Overview ğŸ§­ğŸ¤– :contentReference[oaicite:0]{index=0}  
- ğŸ—ï¸ KFM â€“ Comprehensive Architecture, Features, and Design :contentReference[oaicite:1]{index=1}  
- ğŸ§© KFM â€“ Comprehensive Technical Documentation :contentReference[oaicite:2]{index=2}  
- ğŸ›ï¸ KFM â€“ Comprehensive UI System Overview :contentReference[oaicite:3]{index=3}  
- ğŸ“¥ KFM â€“ Data Intake (Technical & Design Guide) :contentReference[oaicite:4]{index=4}  
- ğŸŒŸ KFM â€“ Latest Ideas & Future Proposals :contentReference[oaicite:5]{index=5}  
- ğŸ’¡ Innovative Concepts to Evolve KFM :contentReference[oaicite:6]{index=6}  
- ğŸ§  Additional Project Ideas (incl. refinement proposals) :contentReference[oaicite:7]{index=7}  

### ğŸ§¾ MCP / documentation standards
- ğŸ§ª Scientific Method / Research / MCP Documentation :contentReference[oaicite:8]{index=8}  
- ğŸ“ MARKDOWN_GUIDE_v13 (repo structure, templates, DoD) :contentReference[oaicite:9]{index=9}  
- ğŸ—ºï¸ Open-Source Geospatial Historical Mapping Hub Design :contentReference[oaicite:10]{index=10}  

### ğŸ“¦ Research library portfolios (open in Acrobat for embedded docs)
- ğŸ¤– AI Concepts & more (portfolio) :contentReference[oaicite:11]{index=11}  
- ğŸ§­ Maps / Google Maps / Virtual Worlds / WebGL (portfolio) :contentReference[oaicite:12]{index=12}  
- ğŸ—„ï¸ Data Management / Data Science / Bayesian Methods (portfolio) :contentReference[oaicite:13]{index=13}  
- ğŸ§° Various programming languages & resources (portfolio placeholder) :contentReference[oaicite:14]{index=14}  

---

## ğŸ§· Notes for maintainers

- Keep templates **small + composable**: most experiments shouldnâ€™t need every file.
- If KFM adds a new gate (policy, schema, sensitivity), update templates first.
- Prefer **machine-readable** artifacts (`*.json`, `*.yaml`) alongside narrative (`*.md`).
- When new domains appear (e.g., archaeology, hydrology, education), add a template add-on rather than bloating the core.

---

## âœ… Next recommended additions (optional)

<details>
<summary>ğŸ“Œ Suggested template improvements</summary>

- `TEMPLATE__POLICY_REPORT.md` (auto-generated conftest/OPA summary)
- `TEMPLATE__PERF_BENCHMARK.md` (tile/query/UI perf harness notes)
- `TEMPLATE__DATA_QUALITY_CHECKLIST.md` (crowdsourcing + consensus workflows)
- `TEMPLATE__SENSITIVITY_REVIEW.md` (CARE/cultural protocol + obfuscation plan)

</details>

