<!--
ğŸ“Œ This README defines the repo-wide automation surface for KFM / Kansasâ€‘Matrixâ€‘System.
ğŸ—“ï¸ Last updated: 2026-01-19
ğŸ” Review cycle: 90 days (or anytime pipeline order / catalogs / policy pack / CI gates change)

Prime directive:
- `scripts/` = orchestration buttons & glue ğŸ§°
- `tools/`   = governed toolchain surface ğŸ› ï¸
- `src/`     = canonical implementation ğŸ—ï¸

Reminder:
- No catalogs â†’ no graph â†’ no API â†’ no UI â†’ no story â†’ no focus mode.
-->

<div align="center">

# ğŸ§° `scripts/` â€” KFM Automation Toolkit

**Repeatable commands for dev, data ops, GIS/remoteâ€‘sensing workflows, modeling/simulation orchestration, and deployment glue.**  
Safeâ€‘byâ€‘default âœ… â€¢ Idempotent â™»ï¸ â€¢ Contractâ€‘first ğŸ“œ â€¢ Provenanceâ€‘first ğŸ§¾ â€¢ Hostileâ€‘input aware ğŸ›¡ï¸ â€¢ â€œMetadata compilesâ€ ğŸ§¬

![Status](https://img.shields.io/badge/status-active-brightgreen)
![README](https://img.shields.io/badge/README-v1.5.0-8957e5)
![Shell](https://img.shields.io/badge/Shell-bash%20%7C%20pwsh-informational)
![Python](https://img.shields.io/badge/Python-3.11%2B-informational)
![Node](https://img.shields.io/badge/Node-18%2B-informational)
![Dry run](https://img.shields.io/badge/default-dry--run-success)
![Contract-first](https://img.shields.io/badge/contract--first-required-0aa3a3)
![Provenance](https://img.shields.io/badge/provenance-STAC%20%2B%20DCAT%20%2B%20PROV-informational)
![Policy Pack](https://img.shields.io/badge/policy%20pack-OPA%20%7C%20Conftest-111827)
![Sigstore](https://img.shields.io/badge/signing-Sigstore%20(optional)-1f6feb)
![Governance](https://img.shields.io/badge/governance-FAIR%20%2B%20CARE%20%2B%20Sovereignty-2ea043)
![Security](https://img.shields.io/badge/security-deny--by--default%20%2B%20hostile--inputs-critical)

</div>

> [!IMPORTANT]
> **`scripts/` is orchestration â€” not â€œthe truth.â€**  
> If behavior becomes core, move the implementation into **`src/`** (or `api/src/`) and let scripts call it.  
> If behavior becomes reusable + CIâ€‘gated, promote it into **`tools/`** and call it from scripts.

> [!IMPORTANT]
> **KFM pipeline ordering is absolute (governed):**  
> **ETL â†’ STAC/DCAT/PROV Catalogs â†’ Graph â†’ APIs â†’ UI â†’ Story Nodes â†’ Focus Mode**  
> Scripts must not create **mystery artifacts** that bypass catalogs/provenance.

---

## ğŸ”— Quick links
- ğŸ§­ Repo overview: **[`../README.md`](../README.md)**
- ğŸ§© Executable code boundary: **[`../src/README.md`](../src/README.md)** *(if present)*
- ğŸ› ï¸ Governed toolchain surface: **[`../tools/README.md`](../tools/README.md)** *(if present)*
- ğŸ§ª Repo-wide tests + QA gates: **[`../tests/README.md`](../tests/README.md)** *(if present)*
- ğŸ“¦ Data + metadata boundary: **[`../data/README.md`](../data/README.md)** *(recommended)*
- ğŸ§¾ Governance (policy pack, redaction rules, SOPs): **[`../docs/governance/`](../docs/governance/)** *(recommended)*
- ğŸ““ MCP (runs/receipts, experiments): **[`../mcp/README.md`](../mcp/README.md)** *(if present)*
- ğŸ“š Story Nodes (draft/published): **[`../docs/reports/story_nodes/`](../docs/reports/story_nodes/)** *(recommended)*
- ğŸŒ Web UI boundary: **[`../web/README.md`](../web/README.md)** *(if present)*

---

<details>
<summary><b>ğŸ§­ Table of contents</b></summary>

- [ğŸ§¾ Doc metadata](#-doc-metadata)
- [ğŸ§­ Where scripts live in the stack](#-where-scripts-live-in-the-stack)
- [ğŸ›‚ Governance review triggers](#-governance-review-triggers)
- [ğŸ›¡ï¸ Continuous compliance: Policy Pack](#ï¸-continuous-compliance-policy-pack)
- [ğŸ§¨ Script risk levels](#-script-risk-levels)
- [ğŸ¯ What belongs here (and what doesnâ€™t)](#-what-belongs-here-and-what-doesnt)
- [ğŸ§± The governed boundary scripts must respect](#-the-governed-boundary-scripts-must-respect)
- [ğŸ Quickstart](#-quickstart)
- [ğŸ—‚ï¸ Recommended folder map](#ï¸-recommended-folder-map)
- [ğŸ§± Standard script contract](#-standard-script-contract)
- [ğŸ“ Script templates (copy/paste)](#-script-templates-copypaste)
- [ğŸ§­ Data lifecycle + evidence artifacts](#-data-lifecycle--evidence-artifacts)
- [ğŸ“¥ Raw intake manifest standard](#-raw-intake-manifest-standard)
- [ğŸ” GitOps lanes: Detect â†’ Validate â†’ Promote](#-gitops-lanes-detect--validate--promote)
- [ğŸ§¾ Observability & provenance](#-observability--provenance)
- [ğŸ§¯ Rollbacks & incident playbooks](#-rollbacks--incident-playbooks)
- [ğŸ§¨ Safety guardrails](#-safety-guardrails)
- [âš¡ Performance, scaling, and concurrency](#-performance-scaling-and-concurrency)
- [ğŸ—ºï¸ GIS + PostGIS scripting tips](#ï¸-gis--postgis-scripting-tips)
- [ğŸ›°ï¸ Remote sensing scripting tips](#ï¸-remote-sensing-scripting-tips)
- [ğŸ§Š 3D / point cloud / photogrammetry scripting tips](#-3d--point-cloud--photogrammetry-scripting-tips)
- [ğŸ“š Story Nodes & Focus Mode automation](#-story-nodes--focus-mode-automation)
- [ğŸ§ª QA scripts (contracts & acceptance gates)](#-qa-scripts-contracts--acceptance-gates)
- [ğŸ¤– Automation roadmaps (optional, proposed)](#-automation-roadmaps-optional-proposed)
- [ğŸ§© Adding a new script (checklist)](#-adding-a-new-script-checklist)
- [ğŸ“‹ Script registry](#-script-registry)
- [ğŸ“š Project reference library influence map](#-project-reference-library-influence-map)
- [âœ… Definition of â€œdoneâ€ for a script](#-definition-of-done-for-a-script)
- [ğŸ•°ï¸ Version history](#ï¸-version-history)
- [ğŸ“ Evidence anchors](#-evidence-anchors)

</details>

---

## ğŸ§¾ Doc metadata

| Field | Value |
|---|---|
| Doc | `scripts/README.md` |
| Status | Active âœ… |
| Last updated | **2026-01-19** |
| Review cycle | 90 days ğŸ” |
| Audience | Contributors shipping automation, data ops, validators, safe wrappers around `tools/` + `src/` |
| Prime directive | **No script may bypass catalogs (STAC/DCAT/PROV) or weaken governance.** |
| Canon | Scripts are â€œbuttonsâ€; **policy pack + catalogs + provenance** are the trust surface ğŸ§¾ğŸ›¡ï¸ |

---

## ğŸ§­ Where scripts live in the stack

Think â€œbuttons â†’ instruments â†’ engineâ€:

```mermaid
flowchart LR
  S["ğŸ§° scripts/<...>\n(buttons & glue)"] --> T["ğŸ› ï¸ tools/<...>\n(governed CLI surface)"]
  T --> C["ğŸ—ï¸ src/<...>\n(canonical modules)"]
```

### âœ… How to decide â€œscripts vs tools vs srcâ€
- **If itâ€™s reusable and CIâ€‘gated** â†’ it probably belongs in `tools/`
- **If itâ€™s core logic** â†’ it belongs in `src/`
- **If itâ€™s environment glue, orchestration, or local convenience** â†’ `scripts/`

> [!TIP]
> If you find yourself duplicating validation/provenance logic in `scripts/`, stop and promote it into `tools/`. ğŸ› ï¸â¬†ï¸

---

## ğŸ›‚ Governance review triggers

Some changes are â€œjust automation,â€ but others are **policy changes wearing a script costume**.

> [!WARNING]
> Treat the following as **governance-significant** (require review, changelog, and usually an ADR):
> - ğŸ”“ **New outbound data exposure** (new export target, new API distribution, new â€œpublicâ€ artifact)
> - ğŸ§¾ **New external sources** or changed licensing terms (attribution/redistribution changes)
> - ğŸ§¬ **New catalog semantics** (STAC/DCAT/PROV schema changes; ID scheme changes)
> - ğŸ” **Classification / sensitivity changes** (including â€œdefault publicâ€ behavior)
> - ğŸ§  **AI-derived outputs** that could be interpreted as ground truth (models, classifications, narratives)
> - ğŸ§¹ **Destructive/irreversible operations** (delete/drop/purge/revoke) or anything touching prod
> - ğŸŒ **Network fetchers** that can reach arbitrary URLs (SSRF risk) or ingest untrusted archives
> - ğŸ§© **Graph sync rules** (ontology/mapping logic, entity resolution, authority ID strategies)
> - ğŸ›¡ï¸ **Policy pack rule changes** (OPA/Rego policies, waivers behavior, enforcement levels)

**Rule of thumb:** If the script changes what users can see, copy, or believe â†’ itâ€™s governance. ğŸ§­ğŸ›¡ï¸

---

## ğŸ›¡ï¸ Continuous compliance: Policy Pack

KFM treats automation as **policyâ€‘gated**. Scripts are expected to run (or be run under) a **Policy Pack** that enforces repo invariants.

### âœ… What the policy pack is
- A set of rules (OPA/Regoâ€‘style) evaluated in CI/locally that every pipeline/script must satisfy before publish.  
- Typical rules: â€œno publish without STAC/DCAT/PROVâ€, â€œno classification downgradeâ€, â€œdeny network unless allowlistedâ€, â€œAPI-only UI accessâ€, etc. ğŸ§¾ğŸ§¬

### ğŸ§ª How scripts should integrate it
Recommended pattern:
- `scripts/policy/run_policy_pack.*` â†’ runs OPA/Conftest rules locally
- `scripts/ci/policy_gate.*` â†’ CI entrypoint that blocks merge/publish on violations
- L2+ scripts should run policy evaluation **before** `--apply` (or as the final â€œcommit gateâ€)

> [!IMPORTANT]
> A script is not â€œsafeâ€ because it has `--dry-run`.  
> Itâ€™s safe when **policy + provenance + catalogs** say itâ€™s safe. âœ…ğŸ›¡ï¸

---

## ğŸ§¨ Script risk levels

Every non-trivial script should declare a risk level (in docs and its `*.script.yaml` manifest).

| Level | Name | Typical actions | Hard requirements |
|---:|---|---|---|
| L0 | Read-only | validate, lint, diff, report | deterministic output; no writes unless `--outdir` |
| L1 | Work-stage writes | write `data/work/**`, temp outputs | `--dry-run` default; atomic writes; cleanup |
| L2 | Publish-stage | write `data/processed/**` **and** emit catalogs/prov | must generate STAC/DCAT/PROV + pass gates + policy pack |
| L3 | System-impacting | DB migrations, graph sync, object store writes, CI promotion | explicit env + explicit approvals; audit logs; policy pack; receipts |
| L4 | Destructive | deletes/drops/purges/overwrites | multi-confirmation; backups; â€œprove you meant itâ€ flags; incident log |

> [!NOTE]
> Risk levels are about **impact**, not intent. A â€œsimple exportâ€ can be L2/L3 if it becomes public or feeds prod.

---

## ğŸ¯ What belongs here (and what doesnâ€™t)

### âœ… Good fits for `scripts/`
- ğŸ§± **Environment bootstrap**: install deps, initialize DB schema, load seed/reference data
- ğŸ§° **Dev helpers**: run local stack, health checks, smoke tests, â€œmake my laptop match CIâ€
- ğŸ·ï¸ **Policy pack runners**: local/CI enforcement wrappers (OPA/Conftest, rule reports, waiver checks)
- ğŸ—‚ï¸ **Catalog/provenance wrappers**: STAC/DCAT/PROV build + validate (usually calling `tools/`)
- ğŸ—ºï¸ **GIS wrappers**: convert formats, validate CRS, generate tiles, build COGs, reprojection helpers
- ğŸ›°ï¸ **Remote sensing orchestrators**: Earth Engine export triggers, download trackers, derived-product packagers
- ğŸ§® **Model/simulation orchestration**: run jobs with recorded configs, seeds, and output receipts
- ğŸ§ª **Acceptance gates**: schema validation, link checks, provenance completeness, contract checks
- ğŸ•’ **Scheduled jobs**: backups, cache cleanup, log rotation (cron/Kubernetes CronJob)
- ğŸ§¾ **MCP helpers**: generate experiment folders (`EXP-###`), receipts, and reproducibility manifests
- ğŸ“š **Story tooling**: scaffold Story Nodes, validate citations, build offline â€œstory packsâ€ for UI

### âŒ Not a good fit for `scripts/`
- ğŸš« **Core ETL logic** (belongs in `src/pipelines/` or equivalent)
- ğŸš« **Domain/business rules** (belongs in `src/` domain/application layers or `api/src/`)
- ğŸš« **Duplicate implementations** of pipeline steps (scripts should call canonical modules/tools)
- ğŸš« **One-off â€œforever scriptsâ€** that bypass provenance, approvals, or classification propagation
- ğŸš« **â€œPublished-lookingâ€ outputs** without STAC/DCAT/PROV boundary artifacts (orphans)

> [!TIP]
> Scripts are the â€œbuttons and levers.â€ If itâ€™s â€œthe engine,â€ it belongs in `src/`. ğŸ”§â¡ï¸ğŸ—ï¸

---

## ğŸ§± The governed boundary scripts must respect

KFM is designed so every user-facing claim traces back to evidence via a strict pipeline.  
(These stages are interfaces: each stage consumes only the prior stageâ€™s boundary artifacts.)

```mermaid
flowchart LR
  A["ğŸ“¥ ETL / Transforms"] --> B["ğŸ—‚ï¸ STAC/DCAT Catalogs"]
  B --> C["ğŸ§¾ PROV Lineage"]
  B --> D["ğŸ•¸ï¸ Graph (references catalogs)"]
  D --> E["ğŸ›¡ï¸ APIs (contracts + redaction)"]
  E --> F["ğŸ—ºï¸ UI"]
  F --> G["ğŸ“š Story Nodes (governed narratives)"]
  G --> H["ğŸ¯ Focus Mode (provenance-linked context bundle)"]
```

### âœ… What this means for automation
- Scripts can **run ETL**, but must ensure:
  - outputs land in `data/raw â†’ data/work â†’ data/processed` *(stage appropriately)*
  - boundary artifacts exist **before** downstream stages run:
    - STAC â†’ `data/stac/**`
    - DCAT â†’ `data/catalog/dcat/**`
    - PROV â†’ `data/prov/**`
- Scripts can **validate** contracts + metadata, and should fail fast in CI when:
  - schemas donâ€™t validate
  - provenance is missing/incomplete
  - links/assets donâ€™t resolve
  - governance rules fail (license, classification propagation, redaction expectations)
  - policy pack fails (unless a reviewed waiver exists)

### ğŸªª Dataset ID hygiene (recommended)
Use **two IDs** (donâ€™t overload one field):
- `dataset_id` = human meaningful (versioned) â†’ `kfm.ks.<domain>.<dataset>.v<major>`
- `artifact_id` = informationâ€‘free stable ID â†’ `ULID` / `UUID` / checksum-derived

> [!NOTE]
> **Donâ€™t change meaning without changing IDs.**  
> Link revisions in provenance (e.g., `prov:wasRevisionOf`) rather than overwriting history.

---

## ğŸ Quickstart

### 1) Discover available scripts
- Browse by category folder (see map below)
- Run help first:
  - `./scripts/<path>/my_script.sh --help`
  - `pwsh ./scripts/<path>/my_script.ps1 --help`
  - `python scripts/<path>/my_script.py --help`

> [!IMPORTANT]
> Every script **must** support `--help` and include **at least 2 runnable examples**.

### 2) Set environment (no secrets in git) ğŸ”
- Prefer repo-level `.env.example` â†’ `.env` *(if present)*
- Scripts should read config from:
  - environment variables âœ…
  - or a config file path passed as an argument/env âœ…

**Never hardcode credentials. Never print secrets.**

### 3) Default to safety âœ…
Preferred contract:
- `--dry-run` (default) â†’ prints actions
- `--apply` â†’ performs changes
- `--yes` â†’ skips prompts (dangerous)
- `--env {dev|staging|prod}` â†’ required when environment matters
- `--run-id <id>` â†’ strongly recommended for correlation + provenance
- `--no-network` default *(or explicit `--allow-network` for fetchers)*
- `--policy-pack` *(recommended)* â†’ run policy evaluation before mutating state

---

## ğŸ—‚ï¸ Recommended folder map

> Keep this list current as categories are introduced.

```text
ğŸ“ scripts/
â”œâ”€ ğŸ§° _lib/                  # shared helpers (logging, env validation, guardrails)
â”œâ”€ ğŸ§° dev/                   # local stack helpers, smoke tests, DX scripts
â”œâ”€ ğŸ§± bootstrap/             # first-run setup (deps, DB init, seed/reference loads)
â”œâ”€ ğŸ›¡ï¸ policy/                # policy pack runners (OPA/Conftest), waivers, reports
â”œâ”€ ğŸ§¾ lineage/               # PRâ†’PROV, OpenLineage emitters, run receipt helpers
â”œâ”€ ğŸ—„ï¸ db/                    # migrations, backups, restores, snapshots, sanity checks
â”œâ”€ ğŸ•¸ï¸ graph/                 # graph export/sync helpers (must reference catalog IDs)
â”œâ”€ ğŸ·ï¸ catalogs/               # STAC/DCAT/PROV build + validate wrappers (usually call tools/)
â”œâ”€ ğŸ§ª pipelines/              # pipeline runners (thin wrappers around src/pipelines)
â”œâ”€ ğŸ§© contracts/              # dataset contract generation/validation helpers (JSON schema, templates)
â”œâ”€ ğŸ“¥ ingest/                # raw intake helpers (downloaders, checksums, raw manifests)
â”œâ”€ ğŸ—ºï¸ gis/                   # vector/raster helpers (tiling, CRS checks, COG/PMTiles build)
â”œâ”€ ğŸ›°ï¸ remote_sensing/        # GEE wrappers, export tracking, indexing helpers
â”œâ”€ ğŸ§Š 3d/                    # point clouds / meshes / photogrammetry (LOD, glTF, tiles)
â”œâ”€ ğŸ§° workers/               # background job runners (tiles, indexing, nightly QA) ğŸ§µ
â”œâ”€ ğŸ“š story_nodes/           # scaffold/validate/publish story nodes + citations
â”œâ”€ ğŸ¯ focus_mode/            # focus bundles, RAG index rebuilds, evidence-bundle checks
â”œâ”€ ğŸ“¦ offline_packs/         # build â€œoffline bundlesâ€ (tiles + story + catalogs) for field/edu
â”œâ”€ ğŸ§® simulation/            # scenario runners (must record seeds/configs + provenance)
â”œâ”€ ğŸ¤– ml/                    # train/eval runners (must record datasets + metrics + provenance)
â”œâ”€ ğŸ“ˆ stats/                 # analysis helpers (EDA, regression diagnostics, uncertainty summaries)
â”œâ”€ ğŸ§ª qa/                    # validators, contract checks, dataset acceptance gates
â”œâ”€ ğŸ” security/              # secrets scans, sensitive-data scans, hostile-input checks
â”œâ”€ ğŸ§¹ housekeeping/          # rotate logs, purge caches, cleanup artifacts
â”œâ”€ ğŸ§ª ci/                    # stable entrypoints used by CI (deterministic, non-interactive)
â””â”€ ğŸ§¾ release/               # snapshot/release helpers (hash manifests, attestations, packaging)
```

> [!NOTE]
> If you add a new category folder, also add it to the Table of contents and script registry. ğŸ§©

---

## ğŸ§± Standard script contract

To keep `scripts/` predictable (and safe), every script must follow the same behavioral contract.

### âœ… CLI interface requirements
All scripts must support:

- `--help` prints:
  - purpose (1â€“2 lines)
  - inputs/outputs (paths or tables)
  - side effects (DB writes? file writes? network calls?)
  - required env vars
  - examples (at least 2)

- `--version` prints:
  - semver (preferred) or git SHA
  - build timestamp (optional)

- Modes:
  - `--dry-run` is the default (or clearly supported)
  - `--apply` performs writes/changes

- Safety:
  - `--yes` skips interactive prompts
  - `--env {dev|staging|prod}` when environment matters
  - if `--env prod` + `--apply` â†’ require an additional explicit prod acknowledgement flag  
    e.g. `--i-acknowledge-production`

- Governance & policy:
  - `--classification <label>` **or** `--classification-file <path>` (recommended for L2/L3)
  - `--license <SPDX|text>` (required when creating new distributions)
  - scripts must **fail closed** if classification/license cannot be determined
  - `--policy-pack` (recommended) runs policy evaluation prior to publish
  - policy waivers must be explicit:
    - `--waiver-id <ID>` (recommended) â†’ must exist in `docs/governance/waivers.yml` *(or repo equivalent)*

- Provenance ergonomics (recommended):
  - `--run-id <id>` (or env `KFM_RUN_ID`) to correlate logs + PROV
  - `--dataset-id <id>` (when acting on a dataset)
  - `--outdir <path>` for artifact destinations
  - `--log-json` for machine-readable logs (JSONL)
  - `--no-network` default *(or explicit `--allow-network` for fetchers)*

- Resource budgets (strongly recommended for hostile inputs):
  - `--timeout-sec <n>`
  - `--max-bytes <n>` (download limit / input limit)
  - `--max-features <n>` / `--max-pixels <n>`
  - `--max-workers <n>` (avoid laptop-melters and prod stampedes)

**Exit codes (standard):**
- `0` success
- `2` usage/CLI error (bad args)
- `3` validation failure (inputs invalid; catalogs missing; schema mismatch)
- `>=10` runtime failures (I/O, network, DB, permissions, unexpected exceptions)

> [!NOTE]
> Itâ€™s okay to add flags, but donâ€™t break the standard ones (`--help`, `--version`, `--dry-run`, `--apply`, `--yes`, `--env`).  
> Consistency beats cleverness. ğŸ§ âœ…

### ğŸ§¾ â€œScript headerâ€ (recommended)
At the top of each script, include:
- Name + purpose
- Risk level (`L0â€¦L4`)
- Inputs / outputs
- Side effects
- Owner/team (or â€œunownedâ€)
- Safety defaults (dry-run default, confirmation behavior)
- Provenance expectations (what IDs/receipts are written)
- Policy pack integration (which rule set applies; waiver mechanism)

### ğŸ§¾ Script manifest (recommended for discoverability)
Keep a tiny machine-readable manifest next to scripts that matter:

```yaml
# scripts/<category>/<name>.script.yaml
name: "export_county_tiles"
risk_level: "L2"
entrypoint: "scripts/gis/export_county_tiles.py"
owner: "@kfm-team"
inputs:
  - "data/processed/<domain>/**"
outputs:
  - "data/processed/<domain>/tiles/**"
  - "data/stac/**"
  - "data/catalog/dcat/**"
  - "data/prov/**"
default_mode: "dry_run"
policy_pack:
  enabled: true
  ruleset: "kfm-default"
  waiver_required_for_bypass: true
network:
  default: "deny"
  allowlist: []
budgets:
  timeout_sec: 3600
  max_bytes: 1073741824 # 1GiB
  max_workers: 4
determinism:
  stable_sorting: true
  seeded: true
gates:
  - "policy_pack_pass"
  - "stac_schema"
  - "dcat_schema"
  - "prov_schema"
  - "license_required"
  - "classification_no_downgrade"
```

---

## ğŸ“ Script templates (copy/paste)

> [!TIP]
> Prefer **Python** for cross-platform automation. Use Bash/Pwsh when itâ€™s truly OS-specific.  
> Treat CLI parsing + structured logging + resource limits as â€œtable stakes.â€

<details>
<summary><b>ğŸš Bash (safe defaults + traps + exit codes)</b></summary>

```bash
#!/usr/bin/env bash
set -euo pipefail
IFS=$'\n\t'

# -----------------------------------------------------------------------------
# Name:        example.sh
# Risk level:  L1
# Purpose:     Example safe-by-default script template
# Inputs:      flags + env vars
# Outputs:     files in outdir (when --apply)
# Side effects: none by default (dry-run)
# Owner:       @kfm-team
# -----------------------------------------------------------------------------

DRY_RUN=1
APPLY=0
YES=0
ENVIRONMENT="dev"
RUN_ID="${KFM_RUN_ID:-}"
LOG_JSON=0

cleanup() { :; }
trap cleanup EXIT INT TERM

usage() {
  cat <<'EOF'
Usage:
  ./scripts/example.sh [--dry-run|--apply] [--env dev|staging|prod] [--run-id ID] [--log-json] [--yes]

Examples:
  ./scripts/example.sh --dry-run
  ./scripts/example.sh --apply --env staging --run-id RUN-20260119-demo --yes
EOF
}

now_iso() { date -u +"%Y-%m-%dT%H:%M:%SZ"; }

log() {
  local level="$1"; shift
  local msg="$*"
  if [[ "$LOG_JSON" -eq 1 ]]; then
    printf '{"ts":"%s","level":"%s","run_id":"%s","msg":%q}\n' "$(now_iso)" "$level" "$RUN_ID" "$msg"
  else
    printf "[%s] %s (run_id=%s) %s\n" "$level" "$(now_iso)" "$RUN_ID" "$msg"
  fi
}

die_usage() { log "ERROR" "$*"; exit 2; }
die_runtime() { log "ERROR" "$*"; exit 10; }

while [[ $# -gt 0 ]]; do
  case "$1" in
    --help) usage; exit 0 ;;
    --version) echo "v0.0.0+local"; exit 0 ;;
    --dry-run) DRY_RUN=1; APPLY=0; shift ;;
    --apply) DRY_RUN=0; APPLY=1; shift ;;
    --yes) YES=1; shift ;;
    --log-json) LOG_JSON=1; shift ;;
    --env) ENVIRONMENT="${2:-}"; shift 2 ;;
    --run-id) RUN_ID="${2:-}"; shift 2 ;;
    *) usage; exit 2 ;;
  esac
done

RUN_ID="${RUN_ID:-RUN-$(date -u +%Y%m%dT%H%M%S)}"

if [[ "$ENVIRONMENT" == "prod" && "$APPLY" -eq 1 && "$YES" -ne 1 ]]; then
  die_usage "Refusing prod apply without --yes (and ideally --i-acknowledge-production)."
fi

log "INFO" "Starting (env=$ENVIRONMENT, dry_run=$DRY_RUN)"
log "INFO" "Would do X"
if [[ "$APPLY" -eq 1 ]]; then
  log "INFO" "Doing X now"
fi
log "INFO" "Done"
exit 0
```

</details>

<details>
<summary><b>ğŸ Python (argparse + JSONL logs + dry-run default + budgets)</b></summary>

```python
#!/usr/bin/env python3
from __future__ import annotations

import argparse
import json
import os
import sys
import time
from dataclasses import dataclass
from datetime import datetime, timezone
from pathlib import Path

EXIT_OK = 0
EXIT_USAGE = 2
EXIT_VALIDATION = 3
EXIT_RUNTIME = 10


def now_iso() -> str:
    return datetime.now(timezone.utc).isoformat()


def log(level: str, msg: str, *, run_id: str, jsonl: bool) -> None:
    payload = {"ts": now_iso(), "level": level, "run_id": run_id, "msg": msg}
    if jsonl:
        print(json.dumps(payload, ensure_ascii=False))
    else:
        print(f"[{level}] {payload['ts']} (run_id={run_id}) {msg}")


@dataclass(frozen=True)
class Budgets:
    timeout_sec: int
    max_workers: int


def main(argv: list[str]) -> int:
    p = argparse.ArgumentParser(
        description="Example KFM script template (safe-by-default).",
        formatter_class=argparse.RawTextHelpFormatter,
    )
    p.add_argument("--version", action="store_true", help="Print version and exit.")
    p.add_argument("--dry-run", action="store_true", help="Print actions only (default).")
    p.add_argument("--apply", action="store_true", help="Perform changes.")
    p.add_argument("--yes", action="store_true", help="Skip prompts (dangerous).")
    p.add_argument("--env", choices=["dev", "staging", "prod"], default="dev")
    p.add_argument("--run-id", default=os.environ.get("KFM_RUN_ID", ""))
    p.add_argument("--dataset-id", default="")
    p.add_argument("--log-json", action="store_true", help="Emit JSONL logs.")
    p.add_argument("--outdir", default="reports/example")
    p.add_argument("--timeout-sec", type=int, default=900)
    p.add_argument("--max-workers", type=int, default=4)
    p.add_argument("--no-network", action="store_true", help="Deny network calls (recommended default).")
    p.add_argument("--allow-network", action="store_true", help="Allow network calls (fetchers only).")
    args = p.parse_args(argv)

    if args.version:
        print("v0.0.0+local")
        return EXIT_OK

    dry_run = not args.apply
    if args.dry_run:
        dry_run = True

    run_id = args.run_id or f"RUN-{datetime.now(timezone.utc).strftime('%Y%m%dT%H%M%S')}"

    if args.env == "prod" and not dry_run and not args.yes:
        log("ERROR", "Refusing prod apply without --yes (and ideally --i-acknowledge-production).",
            run_id=run_id, jsonl=args.log_json)
        return EXIT_USAGE

    budgets = Budgets(timeout_sec=args.timeout_sec, max_workers=args.max_workers)
    start = time.time()

    outdir = Path(args.outdir)
    log("INFO", f"Starting (env={args.env}, dry_run={dry_run}, outdir={outdir}, budgets={budgets})",
        run_id=run_id, jsonl=args.log_json)

    if args.allow_network and args.no_network:
        log("ERROR", "Cannot set both --allow-network and --no-network.", run_id=run_id, jsonl=args.log_json)
        return EXIT_USAGE

    log("INFO", "Would generate artifact manifest", run_id=run_id, jsonl=args.log_json)
    if not dry_run:
        outdir.mkdir(parents=True, exist_ok=True)
        (outdir / "MANIFEST.txt").write_text(
            f"run_id={run_id}\n"
            f"dataset_id={args.dataset_id}\n"
            f"env={args.env}\n"
            f"timestamp={now_iso()}\n",
            encoding="utf-8",
        )

    elapsed = time.time() - start
    if elapsed > budgets.timeout_sec:
        log("ERROR", f"Timeout budget exceeded ({elapsed:.1f}s > {budgets.timeout_sec}s).",
            run_id=run_id, jsonl=args.log_json)
        return EXIT_RUNTIME

    log("INFO", f"Done (elapsed={elapsed:.2f}s)", run_id=run_id, jsonl=args.log_json)
    return EXIT_OK


if __name__ == "__main__":
    raise SystemExit(main(sys.argv[1:]))
```

</details>

<details>
<summary><b>ğŸŸ© Node.js (when you must live near the web toolchain)</b></summary>

```js
#!/usr/bin/env node
/**
 * Name: example.mjs
 * Risk level: L0
 * Purpose: Example KFM script template for Node tooling
 * Safety: dry-run default, explicit --apply
 */
import process from "node:process";

function usage() {
  console.log(`
Usage:
  node scripts/example.mjs [--dry-run|--apply] [--env dev|staging|prod] [--run-id ID] [--log-json] [--yes]

Examples:
  node scripts/example.mjs --dry-run
  node scripts/example.mjs --apply --env staging --run-id RUN-20260119-demo --log-json --yes
`);
}

function nowIso() { return new Date().toISOString(); }

function log(level, msg, { runId, jsonl }) {
  if (jsonl) console.log(JSON.stringify({ ts: nowIso(), level, run_id: runId, msg }));
  else console.log(`[${level}] ${nowIso()} (run_id=${runId}) ${msg}`);
}

const argv = process.argv.slice(2);
let dryRun = true;
let env = "dev";
let yes = false;
let runId = process.env.KFM_RUN_ID || "";
let jsonl = false;

for (let i = 0; i < argv.length; i++) {
  const a = argv[i];
  if (a === "--help") { usage(); process.exit(0); }
  if (a === "--version") { console.log("v0.0.0+local"); process.exit(0); }
  if (a === "--dry-run") { dryRun = true; continue; }
  if (a === "--apply") { dryRun = false; continue; }
  if (a === "--env") { env = argv[++i] || "dev"; continue; }
  if (a === "--yes") { yes = true; continue; }
  if (a === "--log-json") { jsonl = true; continue; }
  if (a === "--run-id") { runId = argv[++i] || ""; continue; }
  usage(); process.exit(2);
}

runId = runId || `RUN-${nowIso().replace(/[-:.TZ]/g, "")}`;

if (env === "prod" && !dryRun && !yes) {
  log("ERROR", "Refusing prod apply without --yes (and ideally --i-acknowledge-production).", { runId, jsonl });
  process.exit(2);
}

log("INFO", `Starting (env=${env}, dry_run=${dryRun})`, { runId, jsonl });
log("INFO", "Done", { runId, jsonl });
process.exit(0);
```

</details>

---

## ğŸ§­ Data lifecycle + evidence artifacts

KFMâ€™s data work is staged and traceable. Scripts that ingest or transform data must follow staging and produce boundary artifacts.

**Canonical staging shape (governed):**
1) ğŸ“¥ Write raw inputs â†’ `data/raw/<domain>/**`  
2) ğŸ§± Write intermediates â†’ `data/work/<domain>/**`  
3) âœ… Write publishable outputs â†’ `data/processed/<domain>/**`  
4) ğŸ—ƒï¸ Emit boundary artifacts **before** anything is used downstream:
   - STAC â†’ `data/stac/**`
   - DCAT â†’ `data/catalog/dcat/**`
   - PROV â†’ `data/prov/**`

> [!IMPORTANT]
> KFM treats analysis outputs / AI-generated layers / simulation outputs as **firstâ€‘class datasets** (â€œevidence artifactsâ€):  
> store them in `data/processed/â€¦`, catalog them in STAC/DCAT, trace them in PROV, and expose them only through governed APIs.

### ğŸªª Classification + sovereignty propagation (hard rule)
- Outputs cannot be less restricted than their inputs unless a reviewed redaction step exists.
- If a script cannot determine classification, it must default to **restricted** (deny-by-default).
- Avoid side-channels: never export sensitive precise coordinates by accident (e.g., debug GeoJSON dumps).

### âœ… â€œThin wrapperâ€ pattern (required for anything important)
If youâ€™re tempted to put real transformation logic in a script:
- implement core logic in `src/â€¦` (pipelines/domain/services)
- keep the script as a thin CLI wrapper that:
  - validates inputs + budgets
  - calls the canonical module/tool
  - writes run receipts/logs
  - triggers catalog/provenance generation and validation

---

## ğŸ“¥ Raw intake manifest standard

Raw intake is the **first trust boundary**. Treat `data/raw/**` as â€œimmutable evidence input.â€

Recommended raw drop structure (per dataset drop/version):

```text
data/raw/<domain>/<dataset_or_source>/<snapshot_or_date>/
â”œâ”€â”€ source.json         # who/what/when/where/how + license + classification
â”œâ”€â”€ checksums.sha256    # manifest for every file in this snapshot
â”œâ”€â”€ raw_index.csv       # optional: file listing + types + sizes (nice for review)
â”œâ”€â”€ telemetry.ndjson    # optional: machine logs (download events, retries, timings)
â””â”€â”€ <payload files...>  # archives / CSVs / PDFs / imagery, etc.
```

> [!TIP]
> A good raw intake script (`scripts/ingest/*`) does two things well:
> 1) **makes it hard to lie** (checksums + source manifest), and  
> 2) **makes it easy to review** (indexes + deterministic file layout). ğŸ§¾âœ…

---

## ğŸ” GitOps lanes: Detect â†’ Validate â†’ Promote

KFM is designed to support **GitOps-friendly ingestion**:
- data is added/updated via PR  
- CI runs validation + policy pack  
- merge triggers the publish lane (or a controlled â€œpromoteâ€ action)  
- catalogs/prov become the interface for graph/API/UI

### ğŸ§­ Recommended lane structure (conceptual)

```mermaid
flowchart LR
  D["ğŸ” Detect change\n(PR / schedule / watcher)"] --> V["âœ… Validate\n(schema+policy+gates)"]
  V -->|pass| P["ğŸŸ¢ Promote\n(publish + catalogs + prov)"]
  V -->|fail| F["ğŸ›‘ Fail closed\n(report + receipts)"]
```

### ğŸ§  Automation agents (proposed, PR-only) ğŸ¤–
If you adopt a Watcher â†’ Planner â†’ Executor loop:
- **Watcher** detects drift or missing data
- **Planner** drafts a patch (or run config)
- **Executor** opens a PR (never auto-merges), attaching receipts, SBOM, and attestations

> [!WARNING]
> Any â€œagentâ€ automation must be:
> - kill-switchable âœ…
> - idempotent âœ…
> - PR-only âœ…
> - policy-gated âœ…
> - provenance-emitting âœ…

---

## ğŸ§¾ Observability & provenance

Every script should:
- ğŸªµ Use structured logging (`timestamp`, `level`, `component`, `run_id`)
- ğŸ§· Print where outputs were written + what changed (counts, bytes, features)
- ğŸ§¾ Capture provenance inputs/outputs:
  - input file list + checksums (when feasible)
  - key parameters (bbox, time window, CRS, resolution, seeds)
  - IDs/paths to produced STAC/DCAT/PROV artifacts
  - config hash + code version (git SHA)

### ğŸ§¾ â€œRun receiptâ€ (ğŸ”¥ strongly recommended for `--apply`)
When a script changes state, write a receipt another person can replay:

- human-readable: `mcp/runs/<RUN-ID>/MANIFEST.md`
- machine-readable: `data/prov/<RUN-ID>.jsonld`

Receipt SHOULD include:
- git SHA, environment, operator identity (if available)
- inputs + checksums
- outputs + checksums
- produced catalog IDs (collection/item/dataset IDs)
- warnings (redactions applied, schema deviations, missing optional evidence)

### ğŸ”— DevOps provenance (recommended)
If a script runs in CI or promotes data:
- record PR/commit metadata into PROV (PR â†’ PROV)
- optionally emit OpenLineage events for pipeline tracking (if adopted)

---

## ğŸ§¯ Rollbacks & incident playbooks

KFM automation should assume mistakes happen and make rollbacks **boring**.

### âœ… Rollback principles
- Prefer **append-only** + â€œnew versionâ€ over in-place mutation
- Treat publishes as **atomic swaps** (run-scoped directory â†’ validate â†’ promote pointer)
- Keep **run receipts** so rollback is explainable (not just reversible)

### ğŸ§¯ Suggested playbooks (scriptable)
- **Dataset rollback**
  - revert the commit/PR that introduced the publish
  - mark distribution as deprecated in DCAT (donâ€™t pretend it never existed)
  - preserve PROV (audit trail stays)

- **Graph rollback**
  - graphs ingest from catalogs/exports â†’ rollback is re-import of prior export snapshot
  - keep graph exports versioned (e.g., `data/graph/csv/<RUN-ID>/...`)

- **Sensitive data incident**
  - immediately restrict classification tags + remove public distributions
  - purge caches / invalidate tiles / rotate any leaked keys (if any)
  - write incident record (who/what/when/impact/remediation)

> [!IMPORTANT]
> If a rollback canâ€™t be done with receipts + a documented sequence of steps, itâ€™s not a rollback plan â€” itâ€™s hope. ğŸ§¾ğŸ§¯

---

## ğŸ§¨ Safety guardrails

### âœ… Safe-by-default behavior
- ğŸ›‘ No destructive actions by default
- ğŸ§ª Default mode should be `--dry-run`
- ğŸ§¯ Destructive actions require explicit confirmation flags

### ğŸ­ Production protection
Scripts that can write to prod must:
- require explicit `--env prod`
- require `--i-acknowledge-production` *(recommended stronger-than-yes flag)*
- log who/what/when:
  - user (if detectable), host, timestamp, run_id
  - git SHA (if available)
  - container digest (if available)

### ğŸ§Š Atomic writes (strongly recommended)
For file outputs:
- write to `*.tmp` then rename (atomic on most filesystems)
- never leave half-written â€œpublishedâ€ outputs behind
- prefer content-addressed paths (hash-in-path) for immutable artifacts

### ğŸ›¡ï¸ Hostile input posture (always assume untrusted)
Assume inputs are hostile (files from the world, archives, rasters, JSON, PDFs, 3D assets).
- âœ… allowlist file types and content-types
- âœ… enforce size and decompression limits (zip bombs, huge rasters, nested archives)
- âœ… path traversal protections (never trust archive paths)
- âœ… treat URL fetching as high-risk (SSRF; private IP blocks; allowlists)
- âœ… isolate complex parsing when possible (containers / subprocess limits / timeouts)

### ğŸ” â€œDerived outputs can leakâ€ reminder
Even *model outputs* and *analytics exports* can leak sensitive information through inference.  
Scripts that expose aggregates, embeddings, or model responses should consider:
- query auditing / inference control
- rate limits + sampling
- suppressing small counts / sensitive attribute disclosure

> [!CAUTION]
> If a script can delete, drop, truncate, overwrite, revoke, or publish:  
> **dry-run default + explicit apply + explicit confirmation** is mandatory. ğŸš«ğŸ§¨âœ…

---

## âš¡ Performance, scaling, and concurrency

Automation should scale from â€œlaptop demoâ€ to â€œstatewide workloadsâ€ without rewrites.

### âœ… Scaling patterns (preferred)
- ğŸ“¦ Chunking/partitioning: process by tile, county, watershed, time window
- ğŸ§± Pipeline breakers: materialize only when needed (then resume streaming)
- â™»ï¸ Idempotency keys: `(dataset_id, version, tile_id, src_checksum)` patterns
- ğŸ—ƒï¸ Metadata-driven access: navigate via catalogs/IDs, not brittle paths
- ğŸ§µ Concurrency safety:
  - avoid shared mutable state without locks
  - serialize destructive ops
  - prefer job queues / worker pools over ad-hoc threading for heavy runs

### âœ… â€œBoring performance winsâ€
- cache downloads (ETag/Last-Modified when possible)
- avoid reprocessing unchanged inputs (checksum-based short-circuit)
- push expensive spatial operations into PostGIS when appropriate
- keep UI-facing assets web-friendly (tiles, COGs, simplified vectors)

### ğŸï¸ DB-aware scripts (when PostGIS/Neo4j are involved)
- prefer fewer, well-indexed queries over chatty loops
- measure tail latency, not just averages
- use staging tables + transactional swaps for imports
- batch writes and use explicit transactions

---

## ğŸ—ºï¸ GIS + PostGIS scripting tips

### âœ… Make CRS and units explicit
- refuse â€œunknown SRIDâ€ geometries by default
- log CRS for inputs/outputs
- document axis-order and unit conversion
- record reprojection and resampling choices (method, resolution, nodata)

### âœ… Prefer database-side spatial operations when appropriate
- buffers, intersects, within, distance joins: PostGIS is often safer/faster than Python loops
- use staging tables and transactional swaps:
  1) load â†’ 2) validate counts/geometry â†’ 3) swap/rename in a transaction

### âœ… Web-serving friendliness
When scripts generate assets meant for the UI:
- vectors: simplify or tile (avoid multiâ€‘MB GeoJSON blobs)
- rasters: prefer COG (with overviews)
- tiles: verify CRS (commonly EPSG:3857 for web tiles) and metadata

### ğŸ” Privacy reminder
GeoJSON is easy to copy. Treat â€œcommitted vectorsâ€ as a disclosure boundary:
- donâ€™t export restricted geometries without explicit governance approval
- prefer catalog pointers to governed stores for sensitive layers

---

## ğŸ›°ï¸ Remote sensing scripting tips

Remote sensing scripts are usually orchestrators for:
- exporting derived indices (NDVI, moisture proxies, composites)
- producing COGs + thumbnails
- emitting STAC Items + linking distributions via DCAT
- capturing PROV runs (inputs, AOI, time window, method, parameters)

### âœ… Prefer derived products + provenance over raw archives
- avoid committing raw satellite archives into the repo
- store raw externally when needed; keep catalog pointers in-repo
- ensure every derived product is traceable (PROV) and discoverable (STAC/DCAT)

### âœ… Record â€œhow it was madeâ€
For any export, record:
- AOI (bbox/geometry), time window
- dataset/source IDs
- compositing method (median/mean/mosaic, cloud mask logic)
- resolution/CRS
- model/algorithm version if AI-assisted

### ğŸ—ºï¸ Georeferencing note
If a workflow involves manual georeferencing (e.g., QGIS control points), record:
- number of control points
- residual/RMS if available
- transformation method
- who performed it + when  
â€¦and attach that to provenance metadata (PROV) and dataset docs.

---

## ğŸ§Š 3D / point cloud / photogrammetry scripting tips

KFM increasingly touches â€œ3D GISâ€ realities (LiDAR, meshes, photogrammetry, archaeology, built infrastructure).  
Treat 3D assets like any other evidence: stage â†’ catalog â†’ provenance â†’ QA.

### âœ… Recommended outputs (web + archival)
- **Web preview**: glTF/GLB + lightweight thumbnails
- **Point clouds**: LAS/LAZ (plus tiled/LOD derivatives if needed)
- **Meshes**: OBJ/PLY (archive) + GLB (serve)
- **Spatial tiling**: 3D Tiles / LOD pyramids *(if adopted)*
- **Metadata**: explicit CRS + vertical datum (and units!) + scale

### âœ… 3D must-haves
- coordinate reference + axis conventions (Y-up/Z-up) clearly stated
- decimation/simplification documented (what was removed and why)
- QA quicklooks (screenshots) and bounding box sanity checks
- provenance links to source surveys and processing parameters

> [!CAUTION]
> 3D assets can leak sensitive location detail fast.  
> Default to restricted until classification is explicit.

---

## ğŸ“š Story Nodes & Focus Mode automation

KFMâ€™s UI is **API-only** and Focus Mode is **evidenceâ€‘bound**:
- Story Nodes should cite cataloged evidence
- AI-assisted narrative must preserve provenance and citations
- Scripts can scaffold and validate story content, but may not bypass governance

### âœ… Recommended script jobs
- `scripts/story_nodes/new_*` â†’ scaffold a Story Node (Markdown + JSON config)
- `scripts/story_nodes/validate_*` â†’ enforce citation rules + required evidence anchors
- `scripts/focus_mode/build_bundle_*` â†’ build a â€œfocus bundleâ€ of catalogs + excerpts
- `scripts/offline_packs/build_*` â†’ bundle tiles + story + catalogs for offline/field use (if adopted)

> [!TIP]
> Treat story assets like data: validate, version, and ship with receipts. ğŸ“šğŸ§¾

---

## ğŸ§ª QA scripts (contracts & acceptance gates)

`scripts/qa/` is for â€œtrust checksâ€ â€” scripts that keep the system honest:

- âœ… policy pack evaluation (OPA/Conftest or equivalent)
- âœ… schema validation for STAC/DCAT/PROV
- âœ… dataset contract validation (JSON schema + required fields)
- âœ… catalog link checks (assets exist; hrefs resolve)
- âœ… definition-of-done checks (data present, metadata present, provenance present)
- âœ… security scans (secrets + sensitive patterns)
- âœ… governance checks (classification propagation; â€œno downgradeâ€ rules)
- âœ… â€œlooks-rightâ€ checks (quicklook renders, bbox sanity, sample stats/EDA)

**Starter examples (conceptual)**
```bash
# JSON sanity (fast fail)
find data/stac data/catalog/dcat data/prov -name "*.json*" -print0 | xargs -0 -n 1 jq empty

# Catalog asset/link integrity
python scripts/qa/validate_stac_links.py data/stac/items

# Provenance completeness
python scripts/qa/validate_prov_bundle.py data/prov

# Policy pack (conceptual)
./scripts/policy/run_policy_pack.sh --ruleset kfm-default

# Secrets scan (repo-wide)
python scripts/security/scan_secrets.py .
```

> [!TIP]
> Keep PR checks fast. Put heavy raster QA into nightly jobs unless it blocks correctness. âš¡

---

## ğŸ¤– Automation roadmaps (optional, proposed)

These are forward-looking patterns captured in KFM planning docs. Optional until implemented â€” but they shape folder naming and interfaces.

### ğŸ” Detect â†’ Validate â†’ Promote (promotion pipeline)
A pipeline watches for changes, runs validation lanes, and promotes via PR (not direct merges), with supplyâ€‘chain attestation and lineage events proposed.

**Suggested homes (if/when implemented):**
- `scripts/ci/detect_changes.*`
- `scripts/ci/validate_lanes.*`
- `scripts/ci/promote_via_pr.*`
- `scripts/ci/emit_lineage.*`
- `scripts/release/sign_*` *(Sigstore/attestations, if adopted)*

### ğŸ§  Watcher â†’ Planner â†’ Executor (Wâ€“Pâ€“E) agent loop
Proposed agent model:
- Watcher detects drift/breakage
- Planner drafts a patch
- Executor opens a PR (never auto-merges), with kill-switch + idempotency keys

> [!WARNING]
> If you add â€œagentâ€ automation, it must be:
> - kill-switchable âœ…
> - idempotent âœ…
> - PR-only âœ…
> - policy-gated âœ…
> - provenance-emitting âœ…

### ğŸ§¾ PR â†’ PROV (DevOps provenance)
Proposed integration maps GitHub PR activity into PROV JSONâ€‘LD for audit queries (â€œwhich code change produced dataset X?â€).

### ğŸ§ª (Optional) Mini-DSLs for pipelines â€” but safely
If you introduce a DSL/config language:
- treat it as data (YAML/JSON first)
- parse safely (no eval)
- validate via schemas
- version it explicitly  
*(If a real grammar/DSL emerges, consider compiler-grade parsing practices rather than ad-hoc string parsing.)*

---

## ğŸ§© Adding a new script (checklist)

1) ğŸ“ Put it in the right subfolder (`db/ gis/ ingest/ policy/ qa/ â€¦`)
2) ğŸ·ï¸ Name it as a verb: `import_*`, `export_*`, `generate_*`, `validate_*`, `backup_*`
3) ğŸ§ª Add `--help` + 2 examples
4) ğŸ›¡ï¸ Add `--dry-run` default and explicit confirmations for writes
5) ğŸ§¾ Write outputs to the correct `data/` stage + generate provenance/metadata when relevant
6) ğŸªµ Log clearly (what, where, record counts, elapsed time)
7) â™»ï¸ Make it idempotent (re-runs should not duplicate or corrupt)
8) ğŸ§ª Make it CI-friendly (non-interactive; stable exit codes)
9) ğŸ§¾ Add a `*.script.yaml` manifest (recommended) including risk level + budgets + policy pack
10) ğŸ›¡ï¸ Ensure policy pack passes (or document waiver with `waiver_id`)
11) ğŸ“ Update this README and the script registry below

---

## ğŸ“‹ Script registry

> âœï¸ Add rows as scripts are introduced. Keep this current.

| Category | Script pattern | Purpose | Safety posture |
|---|---|---|---|
| ğŸ§° dev | `dev/up.*` | Start local stack (compose) | read-only-ish |
| ğŸ§° dev | `dev/smoke.*` | Quick sanity checks | L0 |
| ğŸ›¡ï¸ policy | `policy/run_*` | Run policy pack locally/CI | L0 |
| ğŸ§¾ lineage | `lineage/pr_to_prov.*` | PR â†’ PROV receipts | L1/L2 |
| ğŸ—„ï¸ db | `db/migrate.*` | Apply DB migrations | L3 |
| ğŸ—„ï¸ db | `db/backup_*` | Create encrypted DB backups | L2/L3 |
| ğŸ—„ï¸ db | `db/restore_*` | Restore backups | L4 |
| ğŸ“¥ ingest | `ingest/fetch_*` | Fetch raw inputs + checksums + source.json | L1 |
| ğŸ—ºï¸ gis | `gis/import_*` | Load vectors/rasters into staging | L1/L2 |
| ğŸ—ºï¸ gis | `gis/export_*` | Export layers to tiles/COGs/PMTiles | L2 |
| ğŸ§Š 3d | `3d/build_*` | Build LOD/tiles for 3D assets | L2 |
| ğŸ·ï¸ catalogs | `catalogs/build_*` | Build STAC/DCAT/PROV artifacts | L2 |
| ğŸ·ï¸ catalogs | `catalogs/validate_*` | Validate schemas + links | L0 |
| ğŸ•¸ï¸ graph | `graph/export_*` | Export graph ingest files (CSV/Cypher/JSON) | L2 |
| ğŸ•¸ï¸ graph | `graph/sync_*` | Sync catalog refs into graph | L3 |
| ğŸ›°ï¸ remote_sensing | `remote_sensing/export_*` | Trigger/track derived EO exports | L2 |
| ğŸ§° workers | `workers/run_*` | Run background tasks (tiles, indexing) | L2/L3 |
| ğŸ“š story_nodes | `story_nodes/new_*` | Scaffold story nodes | L1 |
| ğŸ“š story_nodes | `story_nodes/validate_*` | Validate citations/evidence anchors | L0/L1 |
| ğŸ¯ focus_mode | `focus_mode/build_*` | Build focus bundles + evidence checks | L1/L2 |
| ğŸ“¦ offline_packs | `offline_packs/build_*` | Build offline bundles (if adopted) | L2 |
| ğŸ§® simulation | `simulation/run_*` | Run scenarios/jobs | L2 |
| ğŸ¤– ml | `ml/train_*` / `ml/eval_*` | Train/evaluate models | L2 |
| ğŸ“ˆ stats | `stats/eda_*` | Quick EDA + diagnostics | L0/L1 |
| ğŸ§ª qa | `qa/validate_*` | Acceptance gates | L0 |
| ğŸ” security | `security/scan_*` | Secrets/sensitive patterns | L0 |
| ğŸ§¹ housekeeping | `housekeeping/purge_*` | Cleanup caches/logs | L4 |
| ğŸ§ª ci | `ci/check.*` | CI entrypoint | deterministic + non-interactive |

---

## ğŸ“š Project reference library influence map

> These project files inform how we design, review, and harden automation in `scripts/`.

<details>
<summary><b>ğŸ“¦ Expand: project files â†’ how they shape scripts</b></summary>

### ğŸ§­ Core KFM specs (what scripts must serve)
- **KFM comprehensive technical blueprint** â†’ architecture boundaries, governance posture, evidence-first ethos  
   [oai_citation:0â€¡Kansas Frontier Matrix (KFM) â€“ Comprehensive Technical Documentation.pdf](file-service://file-AkqwUuYPp5zePf7pv5SMxi)
- **KFM architecture/features/design** â†’ clean boundaries + domain-first thinking + integration surface  
   [oai_citation:1â€¡ğŸŒŸ Kansas Frontier Matrix â€“ Latest Ideas & Future Proposals.docx.pdf](file-service://file-SQ3f7ve8SGiusT6ThZEuCe)
- **KFM AI system overview** â†’ policy pack gating, PR-based automation constraints, evidence-bound AI behavior  
   [oai_citation:2â€¡ğŸ“š Kansas Frontier Matrix (KFM) Data Intake â€“ Technical & Design Guide.pdf](file-service://file-EbUCdsJMbu5KwpoKMrLrgj)
- **KFM UI system overview** â†’ API-only boundary, story/focus workflows, evidence-linked narratives  
   [oai_citation:3â€¡Kansas Frontier Matrix â€“ Comprehensive UI System Overview.pdf](file-service://file-KcBQruYcoFVDEixzzRHTwt)
- **KFM data intake guide** â†’ raw trust boundary, manifest/checksum expectations, streaming watcher patterns  
   [oai_citation:4â€¡ğŸ“š Kansas Frontier Matrix (KFM) Data Intake â€“ Technical & Design Guide.pdf](file-service://file-EbUCdsJMbu5KwpoKMrLrgj)
- **Latest ideas & future proposals** â†’ agent loops, supply chain attestations, PR-only automation direction  
   [oai_citation:5â€¡ğŸ“š Kansas Frontier Matrix (KFM) Data Intake â€“ Technical & Design Guide.pdf](file-service://file-EbUCdsJMbu5KwpoKMrLrgj)
- **Innovative concepts (evolution roadmap)** â†’ advanced workflows (4D, AR storytelling, crowdsourced validation)  
   [oai_citation:6â€¡Innovative Concepts to Evolve the Kansas Frontier Matrix (KFM).pdf](file-service://file-G71zNoWKxsoSW44iwZaaCC)

### ğŸ“š Reference libraries (PDF portfolios ğŸ—‚ï¸)
These are curated â€œbookshelfâ€ portfolios that influence **how** we write scripts (safety, performance, GIS, WebGL, ML discipline).  
They may require a PDF portfolio-capable viewer (or local extraction via `pdfdetach`).

- **AI/ML concepts library** (evaluation discipline, resource constraints, ML ops instincts)  
   [oai_citation:7â€¡AI Concepts & more.pdf](file-service://file-K6BctJjeUwvyCahLf9qdwr)
- **Data management + Bayesian methods library** (data architectures, performance, reproducibility, uncertainty)  
   [oai_citation:8â€¡Data Managment-Theories-Architures-Data Science-Baysian Methods-Some Programming Ideas.pdf](file-service://file-RrXMFY7cP925exsQYermf2)
- **Maps + WebGL + virtual worlds library** (cartography, 3D GIS, rendering constraints, Cesium/WebGL patterns)  
   [oai_citation:9â€¡Maps-GoogleMaps-VirtualWorlds-Archaeological-Computer Graphics-Geospatial-webgl.pdf](file-service://file-RshcX5sNY2wpiNjRfoP6z6)
- **Programming languages & resources library** (polyglot references: bash/python/js/java/etc; scripting ergonomics)  
   [oai_citation:10â€¡Various programming langurages & resources 1.pdf](file-service://file-4wp3wSSZs7gk5qHWaJVudi)

</details>

---

## âœ… Definition of â€œdoneâ€ for a script

A script is considered complete when:
- âœ… Safe by default (`--dry-run` default or clearly supported)
- âœ… Risk level declared (docs + manifest)
- âœ… Repeatable/idempotent (re-run doesnâ€™t duplicate or corrupt)
- âœ… Documented (`--help` + 2 examples + `--version`)
- âœ… Logs what it did (counts, paths, elapsed time) + optional JSONL (`--log-json`)
- âœ… Outputs land in the correct stage (`data/raw/ â†’ data/work/ â†’ data/processed/`)
- âœ… (When applicable) emits/updates boundary artifacts (STAC/DCAT/PROV)
- âœ… Policy pack pass (or documented waiver) ğŸ›¡ï¸
- âœ… Registered in the script registry + has a `*.script.yaml` manifest (recommended)
- âœ… CI-friendly (non-interactive; stable exit codes)
- â­ Resource budgets supported for hostile inputs (timeouts, size caps, max workers)
- â­ Lintable: shellcheck/shfmt for shell, ruff/black for python, eslint/prettier for JS/TS *(when toolchains exist)*

---

## ğŸ•°ï¸ Version history

| Version | Date | Summary | Author |
|---:|---|---|---|
| v1.5.0 | 2026-01-19 | ğŸ›¡ï¸ Added **Policy Pack** integration expectations; aligned canonical staging to `data/raw â†’ data/work â†’ data/processed` and catalog roots (`data/stac`, `data/catalog/dcat`, `data/prov`); added raw intake manifest standard; added GitOps lane language (Detectâ†’Validateâ†’Promote) + PR-only automation posture; added rollback/incident playbooks; expanded folder map + registry for policy/lineage/story/focus/offline/workers; refreshed influence map to include **all project files** (incl. PDF portfolios). | KFM Engineering |
| v1.4.0 | 2026-01-13 | Introduced governance review triggers + risk levels (L0â€“L4); expanded contracts/budgets expectations; added 3D/point-cloud category and guidance; cleaned up evidence anchors. | KFM Engineering |
| v1.3.0 | 2026-01-11 | Aligned `scripts/` with Master Guide v13 principles (contract-first + deterministic pipeline + evidence artifacts); added templates; clarified boundary artifacts + dataset versioning hygiene; captured roadmaps (Detectâ†’Validateâ†’Promote, Wâ€“Pâ€“E). | KFM Engineering |
| v1.2.0 | 2026-01-09 | Aligned scripts with contract-first + evidence-first rules; added sovereignty/noâ€‘downgrade guardrails, performance/concurrency notes, tightened acceptance gates. | KFM Engineering |
| v1.1.0 | 2026-01-07 | Established repo-wide automation surface, safety defaults, folder map, standard script contract, and registry. | KFM Engineering |

---

## ğŸ“ Evidence anchors

> These are the â€œload-bearingâ€ references used to keep `scripts/` aligned with the repo doctrine.  
> If you move/rename these, update the influence map.

### ğŸ§­ KFM system documents
- KFM Comprehensive Technical Documentation  [oai_citation:11â€¡Kansas Frontier Matrix (KFM) â€“ Comprehensive Technical Documentation.pdf](file-service://file-AkqwUuYPp5zePf7pv5SMxi)  
- KFM Comprehensive Architecture, Features, and Design  [oai_citation:12â€¡ğŸŒŸ Kansas Frontier Matrix â€“ Latest Ideas & Future Proposals.docx.pdf](file-service://file-SQ3f7ve8SGiusT6ThZEuCe)  
- KFM AI System Overview ğŸ§­ğŸ¤–  [oai_citation:13â€¡ğŸ“š Kansas Frontier Matrix (KFM) Data Intake â€“ Technical & Design Guide.pdf](file-service://file-EbUCdsJMbu5KwpoKMrLrgj)  
- KFM Comprehensive UI System Overview  [oai_citation:14â€¡Kansas Frontier Matrix â€“ Comprehensive UI System Overview.pdf](file-service://file-KcBQruYcoFVDEixzzRHTwt)  
- KFM Data Intake â€” Technical & Design Guide  [oai_citation:15â€¡ğŸ“š Kansas Frontier Matrix (KFM) Data Intake â€“ Technical & Design Guide.pdf](file-service://file-EbUCdsJMbu5KwpoKMrLrgj)  
- KFM Latest Ideas & Future Proposals ğŸŒŸ  [oai_citation:16â€¡ğŸ“š Kansas Frontier Matrix (KFM) Data Intake â€“ Technical & Design Guide.pdf](file-service://file-EbUCdsJMbu5KwpoKMrLrgj)  
- Innovative Concepts to Evolve KFM  [oai_citation:17â€¡Innovative Concepts to Evolve the Kansas Frontier Matrix (KFM).pdf](file-service://file-G71zNoWKxsoSW44iwZaaCC)  

### ğŸ“š Reference library portfolios (bookshelf bundles)
- AI Concepts & more (PDF portfolio)  [oai_citation:18â€¡AI Concepts & more.pdf](file-service://file-K6BctJjeUwvyCahLf9qdwr)  
- Data Management + Bayesian Methods bundle (PDF portfolio)  [oai_citation:19â€¡Data Managment-Theories-Architures-Data Science-Baysian Methods-Some Programming Ideas.pdf](file-service://file-RrXMFY7cP925exsQYermf2)  
- Maps / GoogleMaps / VirtualWorlds / WebGL bundle (PDF portfolio)  [oai_citation:20â€¡Maps-GoogleMaps-VirtualWorlds-Archaeological-Computer Graphics-Geospatial-webgl.pdf](file-service://file-RshcX5sNY2wpiNjRfoP6z6)  
- Various programming languages & resources 1 (PDF portfolio)  [oai_citation:21â€¡Various programming langurages & resources 1.pdf](file-service://file-4wp3wSSZs7gk5qHWaJVudi)  

---

<div align="center">

**Â© 2026 Kansas Frontier Matrix** Â· CCâ€‘BY 4.0 (project docs)  
ğŸ§¬ FAIR+CARE Â· ğŸª¶ Sovereignty-aware Â· ğŸ›¡ï¸ Policy-gated builds Â· ğŸ§¾ Evidence-first

</div>