# ğŸ§° `scripts/` â€” KFM Automation Toolkit

> Repeatable commands for dev, data ops, GIS/remote-sensing workflows, and deployment â€œglueâ€.
>
> **Safe-by-default** âœ… â€¢ **Idempotent** â™»ï¸ â€¢ **Provenance-first** ğŸ§¾ â€¢ **Documented** ğŸ““

---

## ğŸ¯ What belongs here (and what doesnâ€™t)

### âœ… Good fits for `scripts/`
- ğŸ§± **Environment bootstrap**: install deps, initialize DB schema, load seed/reference data
- ğŸ—ºï¸ **GIS tooling wrappers**: convert shapefiles/GeoJSON, generate tiles, reproject rasters, etc.
- ğŸ§ª **Data import/export**: one-off admin imports (e.g., boundaries) and exports (snapshots, extracts)
- ğŸ•’ **Scheduled jobs**: backups, cache cleanup, log rotation (cron / Kubernetes CronJob)
- ğŸ§° **Dev helpers**: run local stack, health checks, smoke tests, â€œmake my laptop match CIâ€

### âŒ Not a good fit for `scripts/`
- ğŸš« **Core ETL logic** (belongs in `src/pipelines/` / the canonical pipeline subsystem)
- ğŸš« **Domain/business rules** (belongs in the core `src/` modules)
- ğŸš« **Duplicate implementations** of pipeline steps (scripts should *call into* canonical modules)

---

## ğŸ Quickstart

### 1) Discover available scripts
- Look for subfolders (e.g., `scripts/db/`, `scripts/gis/`, `scripts/data/`)
- Run help first:
  - `./scripts/<path>/my_script.sh --help`
  - `python scripts/<path>/my_script.py --help`

> ğŸ’¡ Tip: Every script should support `--help` and print clear examples.

### 2) Set environment (no secrets in git)
- Copy env template:
  - `cp .env.example .env`
- Load env (shell-specific), then run scripts.

> ğŸ” Scripts must read configuration from environment variables (or a config file referenced by env),
> and **never** hardcode credentials.

---

## ğŸ—‚ï¸ Recommended folder map

> This repo may evolve â€” keep this README updated when adding new categories.

```text
ğŸ“ scripts/
â”œâ”€ ğŸ§± bootstrap/        # first-run setup (DB init, seed/reference loads)
â”œâ”€ ğŸ—„ï¸ db/               # migrations, backups, restores, snapshots, sanity checks
â”œâ”€ ğŸ—ºï¸ gis/              # geoprocessing helpers (vector/raster, PostGIS utilities)
â”œâ”€ ğŸ›°ï¸ remote_sensing/   # imagery ingest helpers / GEE wrappers / indexing
â”œâ”€ ğŸ¤– ml/               # training/eval runners (should call src/ modules)
â”œâ”€ ğŸ§ª qa/               # validators, contract checks, dataset acceptance gates
â”œâ”€ ğŸ§¹ housekeeping/     # rotate logs, purge caches, cleanup artifacts
â””â”€ ğŸ§° dev/              # local stack helpers, smoke tests, DX scripts
```

---

## ğŸ§­ Data lifecycle rules scripts must respect

KFMâ€™s data work is **staged** and **traceable**. Scripts that ingest or transform data must:

1) ğŸ“¥ **Write raw inputs** to `data/raw/<domain>/`
2) ğŸ§± **Write intermediates** to `data/work/<domain>/`
3) âœ… **Write publishable outputs** to `data/processed/<domain>/`
4) ğŸ—ƒï¸ **Emit metadata + lineage artifacts** (STAC/DCAT/PROV) *before* anything is used downstream

> ğŸ§¾ If a script produces â€œevidence artifactsâ€ (derived analyses, model outputs, generated map layers),
> treat them like first-class datasets: store them properly, catalog them, and capture provenance.

---

## ğŸ§¨ Safety guardrails (nonâ€‘negotiable)

### âœ… Safe-by-default behavior
- ğŸ›‘ **No destructive actions by default**
- ğŸ§ª Default mode should be `--dry-run` (or at minimum support it)
- ğŸ§¯ Destructive actions require explicit confirmation flags (example patterns below)

**Recommended confirmation pattern**
- `--dry-run` prints what would happen
- `--apply` performs changes
- `--yes` skips interactive prompts
- `--env {dev|staging|prod}` and **refuse** dangerous combos without extra confirmation

### ğŸ­ Production protection
- Scripts that can write to prod must:
  - require explicit `--env prod`
  - require an additional â€œI really mean itâ€ confirmation flag (e.g., `--i-acknowledge-production`)
  - log who/what/when (user, git SHA if available, host, timestamp)

---

## ğŸ§¾ Observability & provenance

Every script should:
- ğŸªµ Use structured logging (timestamp, level, component, run_id)
- ğŸ§· Print where outputs were written (paths) and what changed
- ğŸ§¾ Capture provenance inputs/outputs:
  - input file list + checksums (when feasible)
  - key parameters (bounding box, time window, model version, CRS, etc.)
  - links/IDs to produced metadata artifacts (STAC/DCAT/PROV)

> ğŸ›ï¸ If itâ€™s not reproducible, itâ€™s not done.

---

## ğŸ§± Script templates

### ğŸš Bash template (portable + strict)
```bash
#!/usr/bin/env bash
set -euo pipefail

usage() {
  cat <<'EOF'
Usage:
  ./scripts/example.sh [--dry-run] [--apply] [--yes]

Examples:
  ./scripts/example.sh --dry-run
  ./scripts/example.sh --apply --yes
EOF
}

DRY_RUN=1
APPLY=0
YES=0

while [[ $# -gt 0 ]]; do
  case "$1" in
    --dry-run) DRY_RUN=1; APPLY=0; shift ;;
    --apply)   APPLY=1; DRY_RUN=0; shift ;;
    --yes)     YES=1; shift ;;
    -h|--help) usage; exit 0 ;;
    *) echo "Unknown arg: $1" >&2; usage; exit 2 ;;
  esac
done

echo "[INFO] dry_run=$DRY_RUN apply=$APPLY yes=$YES"

if [[ "$APPLY" -eq 1 && "$YES" -ne 1 ]]; then
  read -r -p "This will modify state. Type 'apply' to continue: " confirm
  [[ "$confirm" == "apply" ]] || { echo "Aborted."; exit 1; }
fi

# âœ… Put your logic here
echo "[OK] Done."
```

### ğŸ Python template (CLI + logging + exit codes)
```python
#!/usr/bin/env python3
from __future__ import annotations

import argparse
import logging
import sys
from dataclasses import dataclass

log = logging.getLogger("kfm.scripts")

@dataclass(frozen=True)
class Args:
    dry_run: bool
    apply: bool
    verbose: bool

def parse_args(argv: list[str]) -> Args:
    p = argparse.ArgumentParser(
        prog="python scripts/example.py",
        description="Example KFM script (safe-by-default, idempotent).",
    )
    mode = p.add_mutually_exclusive_group()
    mode.add_argument("--dry-run", action="store_true", help="Print actions without changing state.")
    mode.add_argument("--apply", action="store_true", help="Perform actions (writes/changes).")
    p.add_argument("-v", "--verbose", action="store_true", help="Verbose logs.")
    ns = p.parse_args(argv)

    dry_run = ns.dry_run or not ns.apply  # default safe
    apply = ns.apply
    return Args(dry_run=dry_run, apply=apply, verbose=ns.verbose)

def main(argv: list[str]) -> int:
    args = parse_args(argv)
    logging.basicConfig(
        level=logging.DEBUG if args.verbose else logging.INFO,
        format="%(asctime)s %(levelname)s %(name)s - %(message)s",
    )

    log.info("start dry_run=%s apply=%s", args.dry_run, args.apply)

    # âœ… Put your logic here
    if args.dry_run:
        log.info("DRY RUN: would do work hereâ€¦")
    else:
        log.info("APPLY: doing workâ€¦")

    log.info("done")
    return 0

if __name__ == "__main__":
    raise SystemExit(main(sys.argv[1:]))
```

---

## ğŸ—ºï¸ GIS + PostGIS scripting tips

- Prefer **PostGIS for heavy lifting** when appropriate (buffers, intersections, within queries)
- Use scripts to:
  - validate CRS
  - load data into PostGIS safely (staging tables, transactions)
  - export to GeoJSON for UI consumption
  - generate derived layers that can be tiled

> ğŸ§  Rule of thumb: big spatial ops belong close to the data; scripts orchestrate and validate.

---

## ğŸ§ª QA scripts (contracts & acceptance gates)

`scripts/qa/` is for â€œtrust checksâ€ â€” scripts that keep the system honest:
- âœ… schema validation for metadata records
- âœ… linting / formatting
- âœ… â€œdefinition of doneâ€ checks (data present, metadata present, provenance present)

> ğŸ“¦ Any dataset/evidence artifact should be verifiable via a repeatable command.

---

## ğŸ§© Adding a new script (checklist)

1) ğŸ“ Put it in the right subfolder (bootstrap/db/gis/remote_sensing/ml/qa/â€¦)
2) ğŸ·ï¸ Name it as a verb: `import_*`, `export_*`, `generate_*`, `validate_*`, `backup_*`
3) ğŸ§ª Add `--help` + examples
4) ğŸ›¡ï¸ Add `--dry-run` (or safe default) and explicit confirmations for writes
5) ğŸ§¾ Write outputs to the correct `data/` stage + generate provenance/metadata when relevant
6) ğŸªµ Log clearly (what, where, how many records, elapsed time)
7) ğŸ§¼ Make it idempotent (re-runs should not duplicate)
8) ğŸ“ Update this README (and any script registry table below)

---

## ğŸ“‹ Script registry (keep this current)

| Category | Script | Purpose | Safe mode |
|---|---|---|---|
| ğŸ—„ï¸ db | `backup_*` | Create encrypted DB backups | `--dry-run` default |
| ğŸ—ºï¸ gis | `import_*` | Load boundaries/shape data into PostGIS | `--dry-run` + `--apply` |
| ğŸ§ª qa | `validate_*` | Validate schemas/contracts/metadata | read-only |
| ğŸ§¹ housekeeping | `purge_*` | Cleanup caches/logs | confirm required |

> âœï¸ Add rows as scripts are introduced.

---

## ğŸ§¯ Troubleshooting (CLI â€œkung fuâ€)

A few battle-tested patterns:
- ğŸ” Inspect logs quickly: `grep`, `tail -f`, `less`
- ğŸ§® Quick stats: `cut`, `awk`, `sort | uniq -c`
- ğŸ§¹ Cleanup old logs: `find â€¦ -mtime +N -delete` *(be careful â€” pair with `--dry-run` style previews)*

---

## ğŸ¤ Related docs (inside this repo)

- ğŸ“¦ Data staging & dataset conventions â†’ `data/README.md`
- ğŸŒ Web app tooling & layer registry â†’ `web/README.md`
- ğŸ§© MCP services & tool interfaces â†’ `mcp/README.md`
- ğŸ›ï¸ Governance, contracts, and standards â†’ `.github/*` and `docs/*`

---

## âœ… Definition of â€œdoneâ€ for a script

A script is considered complete when:
- âœ… Itâ€™s safe by default
- âœ… Itâ€™s repeatable/idempotent
- âœ… It logs what it did
- âœ… Outputs land in the correct stage
- âœ… (When applicable) it emits/updates metadata + provenance artifacts
- âœ… It has a minimal usage example and is referenced in this README