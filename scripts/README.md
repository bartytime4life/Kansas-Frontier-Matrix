<!--
ğŸ“Œ This README defines the repo-wide automation surface for KFM.
ğŸ—“ï¸ Last updated: 2026-01-07
-->

# ğŸ§° `scripts/` â€” KFM Automation Toolkit

![Safe by default](https://img.shields.io/badge/safe--by--default-yes-success)
![Idempotent](https://img.shields.io/badge/idempotent-expected-blue)
![Provenance first](https://img.shields.io/badge/provenance--first-required-informational)
![Documented](https://img.shields.io/badge/--help-required-brightgreen)
![Shell](https://img.shields.io/badge/shell-bash%20%7C%20pwsh-lightgrey)
![Python](https://img.shields.io/badge/python-cli%20scripts-3776AB)
![GIS](https://img.shields.io/badge/gis-GDAL%20%7C%20PostGIS-2b9348)
![Contracts](https://img.shields.io/badge/contracts-OpenAPI%20%7C%20JSON%20Schema-0aa3a3)
![Security](https://img.shields.io/badge/security-hostile--inputs%20%2B%20deny--by--default-red)

> Repeatable commands for dev, data ops, GIS/remoteâ€‘sensing workflows, modeling/simulation orchestration, and deployment â€œglueâ€.  
> **Safe-by-default** âœ… â€¢ **Idempotent** â™»ï¸ â€¢ **Provenance-first** ğŸ§¾ â€¢ **Documented** ğŸ““

> [!IMPORTANT]
> `scripts/` is **orchestration**, not â€œthe truth.â€  
> If something becomes **core behavior**, move the implementation into `src/` (or `api/src/`) and let scripts call it.

> [!IMPORTANT]
> KFM pipeline ordering is absolute:  
> **ETL â†’ STAC/DCAT/PROV Catalogs â†’ Graph â†’ APIs â†’ UI â†’ Story Nodes â†’ Focus Mode**  
> Scripts must not create â€œmystery artifactsâ€ that bypass catalogs/provenance.

---

<details>
<summary><b>ğŸ§­ Table of contents</b></summary>

- [ğŸ¯ What belongs here (and what doesnâ€™t)](#what-belongs-here)
- [ğŸ Quickstart](#quickstart)
- [ğŸ—‚ï¸ Recommended folder map](#folder-map)
- [ğŸ§± Standard script contract](#script-contract)
- [ğŸ§­ Data lifecycle rules scripts must respect](#data-lifecycle)
- [ğŸ§¨ Safety guardrails (non-negotiable)](#safety-guardrails)
- [ğŸ§¾ Observability & provenance](#observability)
- [ğŸ§± Script templates](#script-templates)
- [ğŸ—ºï¸ GIS + PostGIS scripting tips](#gis-postgis)
- [ğŸ›°ï¸ Remote sensing scripting tips](#remote-sensing)
- [ğŸ§ª QA scripts (contracts & acceptance gates)](#qa-scripts)
- [ğŸ§© Adding a new script (checklist)](#adding-a-script)
- [ğŸ“‹ Script registry](#script-registry)
- [ğŸ§¯ Troubleshooting (CLI â€œkung fuâ€)](#troubleshooting)
- [ğŸ¤ Related docs (inside this repo)](#related-docs)
- [ğŸ“š Project reference library influence map](#reference-library-influence-map)
- [âœ… Definition of â€œdoneâ€ for a script](#definition-of-done)

</details>

---

<a id="what-belongs-here"></a>

## ğŸ¯ What belongs here (and what doesnâ€™t)

### âœ… Good fits for `scripts/`
- ğŸ§± **Environment bootstrap**: install deps, initialize DB schema, load seed/reference data
- ğŸ§° **Dev helpers**: run local stack, health checks, smoke tests, â€œmake my laptop match CIâ€
- ğŸ—ºï¸ **GIS tooling wrappers**: convert formats, validate CRS, generate tiles, build COGs, raster reprojection
- ğŸ›°ï¸ **Remote sensing helpers**: Earth Engine export triggers, index builders, downloaders for *derived* products
- ğŸ¤– **Model/simulation orchestration**: run pipelines/jobs with recorded configs, seeds, and output receipts
- ğŸ§ª **Acceptance gates**: schema validation, link checks, provenance completeness, contract checks (OpenAPI/JSON Schema)
- ğŸ•’ **Scheduled jobs**: backups, cache cleanup, log rotation (cron/Kubernetes CronJob)

### âŒ Not a good fit for `scripts/`
- ğŸš« **Core ETL logic** (belongs in `src/pipelines/`)
- ğŸš« **Domain/business rules** (belongs in `src/` domain/application layers or `api/src/`)
- ğŸš« **Duplicate implementations** of pipeline steps (scripts should *call into* canonical modules)
- ğŸš« **One-off â€œforever scriptsâ€** that bypass provenance, approvals, or classification propagation
- ğŸš« **Unreviewed publishing** (any path that creates â€œpublished-lookingâ€ outputs without STAC/DCAT/PROV)

> [!TIP]
> Scripts are the â€œbuttons and levers.â€ If itâ€™s â€œthe engine,â€ it belongs in `src/`. ğŸ”§â¡ï¸ğŸ—ï¸

---

<a id="quickstart"></a>

## ğŸ Quickstart

### 1) Discover available scripts
- Browse by category (e.g., `scripts/dev/`, `scripts/db/`, `scripts/gis/`, `scripts/qa/`)
- Run help first:
  - `./scripts/<path>/my_script.sh --help`
  - `pwsh ./scripts/<path>/my_script.ps1 --help`
  - `python scripts/<path>/my_script.py --help`

> [!IMPORTANT]
> Every script **must** support `--help` and include **at least 2 runnable examples**.

### 2) Set environment (no secrets in git) ğŸ”
- Copy env template (repo-level):
  - `cp .env.example .env`
- Load env in your shell (or pass vars inline).

Scripts should read config from:
- environment variables âœ…
- or a config file *path* provided via env âœ…

**Never hardcode credentials. Never print secrets.**

### 3) Default to safety âœ…
Preferred contract:
- `--dry-run` (default) â†’ prints actions
- `--apply` â†’ performs changes
- `--yes` â†’ skips prompts  
- `--env {dev|staging|prod}` â†’ required when environment matters

---

<a id="folder-map"></a>

## ğŸ—‚ï¸ Recommended folder map

> This repo may evolve â€” keep this README updated when adding new categories.

```text
ğŸ“ scripts/
â”œâ”€ ğŸ§° _lib/               # shared helpers (logging, env validation, guardrails)
â”œâ”€ ğŸ§° dev/                # local stack helpers, smoke tests, DX scripts
â”œâ”€ ğŸ§± bootstrap/          # first-run setup (deps, DB init, seed/reference loads)
â”œâ”€ ğŸ—„ï¸ db/                 # migrations, backups, restores, snapshots, sanity checks
â”œâ”€ ğŸ•¸ï¸ graph/              # graph sync/load helpers (must reference catalog IDs)
â”œâ”€ ğŸ·ï¸ catalogs/            # STAC/DCAT/PROV build + validate wrappers (usually call src/)
â”œâ”€ ğŸ§ª pipelines/           # pipeline runners (thin wrappers around src/pipelines)
â”œâ”€ ğŸ—ºï¸ gis/                # geoprocessing helpers (vector/raster, tiling, CRS checks)
â”œâ”€ ğŸ›°ï¸ remote_sensing/     # GEE wrappers, export tracking, indexing helpers
â”œâ”€ ğŸ§® simulation/         # scenario runners (must record seeds/configs + provenance)
â”œâ”€ ğŸ¤– ml/                 # train/eval runners (must record datasets + metrics + provenance)
â”œâ”€ ğŸ§ª qa/                 # validators, contract checks, dataset acceptance gates
â”œâ”€ ğŸ” security/           # secrets scans, sensitive-data scans, hostile-input checks
â”œâ”€ ğŸ§¹ housekeeping/       # rotate logs, purge caches, cleanup artifacts
â””â”€ ğŸ§ª ci/                 # stable entrypoints used by CI (deterministic, non-interactive)
```

> [!NOTE]
> If you add a new category folder, also add it to the Table of Contents and script registry. ğŸ§©

---

<a id="script-contract"></a>

## ğŸ§± Standard script contract

To keep `scripts/` predictable (and safe), every script **must** follow the same behavioral contract.

### âœ… CLI interface requirements
All scripts must support:

- `--help` prints:
  - purpose (1â€“2 lines)
  - inputs/outputs (paths or tables)
  - side effects (DB writes? file writes? network calls?)
  - required env vars
  - examples (at least 2)

- Modes:
  - `--dry-run` is the default (or clearly supported)
  - `--apply` performs writes/changes

- Safety:
  - `--yes` skips interactive prompts
  - `--env {dev|staging|prod}` when environment matters
  - if `--env prod` + `--apply` â†’ require an additional explicit prod acknowledgement flag
    - e.g., `--i-acknowledge-production`

- Output hygiene (recommended):
  - `--run-id <id>` (or env `KFM_RUN_ID`) to correlate logs + provenance
  - `--log-json` for machine-readable logs (JSONL)
  - `--outdir <path>` for artifact destinations
  - `--no-network` default (or at least an explicit `--allow-network` for scripts that fetch remote content)

**Exit codes (standard):**
- `0` success
- `2` usage/CLI error (bad args)
- `3` validation failure (inputs invalid; catalogs missing; schema mismatch) *(recommended)*
- `>=10` runtime failures (I/O, network, DB, permissions, unexpected exceptions)

> [!NOTE]
> Itâ€™s okay to add flags, but donâ€™t break the standard ones (`--help`, `--dry-run`, `--apply`, `--yes`, `--env`).  
> Consistency beats cleverness. ğŸ§ âœ…

### ğŸ§¾ â€œScript headerâ€ (recommended)
At the top of each script, include:
- Name
- Purpose
- Inputs / Outputs
- Side effects
- Owner/team (or â€œunownedâ€)
- Safety defaults (`dry-run` default, confirmation behavior)
- Provenance expectations (what IDs/receipts are written)

---

<a id="data-lifecycle"></a>

## ğŸ§­ Data lifecycle rules scripts must respect

KFMâ€™s data work is **staged** and **traceable**. Scripts that ingest or transform data must:

1) ğŸ“¥ Write raw inputs â†’ `data/raw/<domain>/`  
2) ğŸ§± Write intermediates â†’ `data/work/<domain>/`  
3) âœ… Write publishable outputs â†’ `data/processed/<domain>/`  
4) ğŸ—ƒï¸ Emit metadata + lineage artifacts (STAC/DCAT/PROV) **before** anything is used downstream

> [!IMPORTANT]
> If a script produces â€œevidence artifactsâ€ (derived analyses, model outputs, generated layers),  
> treat them like first-class datasets: store them correctly, catalog them, and capture provenance. ğŸ§¾ğŸ§¬

### âœ… â€œThin wrapperâ€ pattern (required for anything important)
If youâ€™re tempted to put real transformation logic in a script, do this instead:
- implement core logic in `src/â€¦` (pipelines/domain/services)
- keep the script as a thin CLI wrapper that:
  - validates inputs
  - calls the canonical module
  - writes run receipts/logs
  - triggers catalog/provenance generation

---

<a id="safety-guardrails"></a>

## ğŸ§¨ Safety guardrails (non-negotiable)

### âœ… Safe-by-default behavior
- ğŸ›‘ **No destructive actions by default**
- ğŸ§ª Default mode should be `--dry-run`
- ğŸ§¯ Destructive actions require explicit confirmation flags

**Recommended confirmation pattern**
- `--dry-run` prints what would happen  
- `--apply` performs changes  
- `--yes` skips interactive prompts  
- `--env {dev|staging|prod}` and **refuse** dangerous combos without extra confirmation  

### ğŸ­ Production protection
Scripts that can write to prod must:
- require explicit `--env prod`
- require an additional â€œI really mean itâ€ flag (`--i-acknowledge-production`)
- log who/what/when:
  - user (if detectable), host, timestamp, run_id
  - git SHA (if available)
  - container digest (if available)

### ğŸ§Š Atomic writes (strongly recommended)
For file outputs:
- write to `*.tmp` then rename to final output (atomic on most OS/filesystems)
- never leave half-written â€œpublishedâ€ outputs behind
- prefer content-addressed paths (hash-in-path) for immutable artifacts

### ğŸ§¯ Hostile input posture
Assume inputs are hostile (files from the world, archives, rasters, JSON, OBJ models, PDFs).
- validate file types (allowlists)
- enforce size limits and decompression limits
- treat URL fetching as high-risk (SSRF; private IP blocks; allowlists)
- isolate complex parsing when possible (containers / sandboxing / subprocess limits)

> [!CAUTION]
> If a script can delete, drop, truncate, overwrite, revoke, or publish:  
> **dry-run default + explicit apply + explicit confirmation** is mandatory. ğŸš«ğŸ§¨âœ…

---

<a id="observability"></a>

## ğŸ§¾ Observability & provenance

Every script should:
- ğŸªµ Use structured logging (`timestamp`, `level`, `component`, `run_id`)
- ğŸ§· Print where outputs were written (paths) + what changed (counts, bytes, features)
- ğŸ§¾ Capture provenance inputs/outputs:
  - input file list + checksums (when feasible)
  - key parameters (bbox, time window, model version, CRS, resolution, seeds)
  - IDs/paths to produced STAC/DCAT/PROV artifacts

### ğŸ§¾ Recommended â€œrun receiptâ€ (optional but ğŸ”¥)
When `--apply` is used, write a small receipt:
- human-readable: `mcp/runs/<RUN-ID>/MANIFEST.md`
- machine-readable: `data/prov/<RUN-ID>.jsonld`

Receipt should include:
- git SHA, environment, operator identity (if available)
- inputs + checksums
- outputs + checksums
- produced catalog IDs (collection/item/dataset IDs)
- warnings (redactions applied, schema deviations, missing optional evidence)

> ğŸ›ï¸ If itâ€™s not reproducible, itâ€™s not done.

---

<a id="script-templates"></a>

## ğŸ§± Script templates

<details>
<summary><b>ğŸš Bash template (portable + strict)</b></summary>

```bash
#!/usr/bin/env bash
set -euo pipefail
IFS=$'\n\t'

usage() {
  cat <<'EOF'
Usage:
  ./scripts/example.sh [--dry-run] [--apply] [--yes] [--env dev|staging|prod] [--run-id ID]

Purpose:
  Example KFM script (safe-by-default, idempotent).

Examples:
  ./scripts/example.sh --dry-run
  ./scripts/example.sh --apply --yes --env dev --run-id etl_20260107_120000_abcd123
EOF
}

DRY_RUN=1
APPLY=0
YES=0
ENVIRONMENT="${KFM_ENV:-dev}"
RUN_ID="${KFM_RUN_ID:-run-unknown}"

while [[ $# -gt 0 ]]; do
  case "$1" in
    --dry-run) DRY_RUN=1; APPLY=0; shift ;;
    --apply)   APPLY=1; DRY_RUN=0; shift ;;
    --yes)     YES=1; shift ;;
    --env)     ENVIRONMENT="${2:-}"; shift 2 ;;
    --run-id)  RUN_ID="${2:-}"; shift 2 ;;
    -h|--help) usage; exit 0 ;;
    *) echo "Unknown arg: $1" >&2; usage; exit 2 ;;
  esac
done

echo "[INFO] run_id=$RUN_ID env=$ENVIRONMENT dry_run=$DRY_RUN apply=$APPLY yes=$YES"

if [[ "$ENVIRONMENT" == "prod" && "$APPLY" -eq 1 ]]; then
  echo "[ERROR] Refusing to apply to prod without an explicit prod-ack flag." >&2
  echo "        Add a flag like: --i-acknowledge-production" >&2
  exit 2
fi

if [[ "$APPLY" -eq 1 && "$YES" -ne 1 ]]; then
  read -r -p "This will modify state. Type 'apply' to continue: " confirm
  [[ "$confirm" == "apply" ]] || { echo "Aborted."; exit 1; }
fi

# âœ… Put orchestration here (call into src/ modules, validate, write receipts)
echo "[OK] Done."
```

</details>

<details>
<summary><b>ğŸ Python template (CLI + logging + exit codes)</b></summary>

```python
#!/usr/bin/env python3
from __future__ import annotations

import argparse
import json
import logging
import os
import sys
from dataclasses import dataclass

log = logging.getLogger("kfm.scripts")

@dataclass(frozen=True)
class Args:
    dry_run: bool
    apply: bool
    yes: bool
    env: str
    run_id: str
    log_json: bool

def parse_args(argv: list[str]) -> Args:
    p = argparse.ArgumentParser(
        prog="python scripts/example.py",
        description="Example KFM script (safe-by-default, idempotent).",
    )
    mode = p.add_mutually_exclusive_group()
    mode.add_argument("--dry-run", action="store_true", help="Print actions without changing state.")
    mode.add_argument("--apply", action="store_true", help="Perform actions (writes/changes).")
    p.add_argument("--yes", action="store_true", help="Skip interactive prompts.")
    p.add_argument("--env", default=os.getenv("KFM_ENV", "dev"), choices=["dev", "staging", "prod"])
    p.add_argument("--run-id", default=os.getenv("KFM_RUN_ID", "run-unknown"))
    p.add_argument("--log-json", action="store_true", help="Emit JSON logs (JSONL friendly).")
    ns = p.parse_args(argv)

    dry_run = ns.dry_run or not ns.apply  # safe default
    return Args(
        dry_run=dry_run,
        apply=ns.apply,
        yes=ns.yes,
        env=ns.env,
        run_id=ns.run_id,
        log_json=ns.log_json,
    )

def emit(event: dict, *, log_json: bool) -> None:
    if log_json:
        print(json.dumps(event, ensure_ascii=False))
    else:
        log.info("%s", event)

def main(argv: list[str]) -> int:
    args = parse_args(argv)
    logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s %(name)s %(message)s")

    if args.env == "prod" and args.apply:
        emit({"level": "error", "msg": "refusing_prod_apply_without_ack", "run_id": args.run_id}, log_json=args.log_json)
        return 2

    emit({"level": "info", "msg": "start", "run_id": args.run_id, "env": args.env, "dry_run": args.dry_run}, log_json=args.log_json)

    # âœ… Put orchestration here (call into src/ modules; write receipts; emit STAC/DCAT/PROV when relevant)
    if args.dry_run:
        emit({"level": "info", "msg": "dry_run_no_changes", "run_id": args.run_id}, log_json=args.log_json)
    else:
        if not args.yes and sys.stdin.isatty():
            confirm = input("This will modify state. Type 'apply' to continue: ").strip()
            if confirm != "apply":
                emit({"level": "info", "msg": "aborted", "run_id": args.run_id}, log_json=args.log_json)
                return 1
        elif not args.yes:
            emit({"level": "error", "msg": "no_tty_and_no_yes", "run_id": args.run_id}, log_json=args.log_json)
            return 2

        emit({"level": "info", "msg": "apply_doing_work", "run_id": args.run_id}, log_json=args.log_json)

    emit({"level": "info", "msg": "done", "run_id": args.run_id}, log_json=args.log_json)
    return 0

if __name__ == "__main__":
    raise SystemExit(main(sys.argv[1:]))
```

</details>

---

<a id="gis-postgis"></a>

## ğŸ—ºï¸ GIS + PostGIS scripting tips

### âœ… Make CRS and units explicit
- refuse â€œunknown SRIDâ€ geometries by default
- log CRS for inputs/outputs
- document any axis-order or unit conversion

### âœ… Prefer database-side spatial operations when appropriate
- buffers, intersects, within, distance joins: PostGIS is usually safer and faster than â€œloop in Pythonâ€
- use staging tables and transactional swaps:
  1) load â†’ 2) validate counts/geometry â†’ 3) swap/rename in a transaction

### âœ… Web-serving friendliness
When scripts generate assets meant for the UI:
- vectors: simplify or tile (avoid megabyte GeoJSON blobs)
- rasters: prefer COG (with overviews)
- tiles: verify CRS (EPSG:3857 for web tiles) and metadata

### ğŸ” Privacy reminder
GeoJSON is easy to copy. Treat â€œcommitted vectorsâ€ as a disclosure boundary:
- donâ€™t export restricted geometries without explicit governance approval
- prefer catalog pointers to governed stores for sensitive layers

---

<a id="remote-sensing"></a>

## ğŸ›°ï¸ Remote sensing scripting tips

Remote sensing scripts are usually *orchestrators* for:
- exporting derived indices (NDVI, moisture proxies, composites)
- producing COGs and thumbnails
- emitting STAC Items and linking distributions via DCAT
- capturing PROV runs (inputs, AOI, time window, method, parameters)

### âœ… Prefer â€œderived products + provenanceâ€ over raw archives
- avoid committing raw satellite archives into the repo
- store raw externally when needed; keep catalog pointers in-repo
- ensure every derived product is traceable (PROV) and discoverable (STAC/DCAT)

### âœ… Record â€œhow it was madeâ€
For any export, record:
- AOI (bbox/geometry), time window
- dataset/source IDs
- compositing method (median/mean/mosaic, cloud mask logic)
- resolution/CRS
- model/algorithm version if AI-assisted

---

<a id="qa-scripts"></a>

## ğŸ§ª QA scripts (contracts & acceptance gates)

`scripts/qa/` is for â€œtrust checksâ€ â€” scripts that keep the system honest:
- âœ… schema validation for metadata records (STAC/DCAT/PROV)
- âœ… catalog link checks (assets exist; hrefs resolve)
- âœ… definition-of-done checks (data present, metadata present, provenance present)
- âœ… contract checks (OpenAPI snapshots, schema diffs)
- âœ… security scans (secrets + sensitive patterns)
- âœ… governance checks (classification propagation; â€œno downgradeâ€ rules)

**Starter examples (conceptual)**
```bash
# JSON sanity (fast fail)
find data/stac data/catalog/dcat data/prov -name "*.json*" -print0 | xargs -0 -n 1 jq empty

# Catalog asset/link integrity
python scripts/qa/validate_stac_links.py data/stac/items

# Provenance completeness
python scripts/qa/validate_prov_bundle.py data/prov

# OpenAPI/contract checks (if applicable)
python scripts/qa/validate_openapi.py src/server/contracts/openapi.yaml

# Secrets scan (repo-wide)
python scripts/security/scan_secrets.py .
```

> [!TIP]
> Keep PR checks fast. Put heavy raster QA into nightly jobs unless it blocks correctness. âš¡

---

<a id="adding-a-script"></a>

## ğŸ§© Adding a new script (checklist)

1) ğŸ“ Put it in the right subfolder (`db/ gis/ remote_sensing/ qa/ â€¦`)
2) ğŸ·ï¸ Name it as a **verb**: `import_*`, `export_*`, `generate_*`, `validate_*`, `backup_*`
3) ğŸ§ª Add `--help` + **2 examples**
4) ğŸ›¡ï¸ Add `--dry-run` default and explicit confirmations for writes
5) ğŸ§¾ Write outputs to the correct `data/` stage + generate provenance/metadata when relevant
6) ğŸªµ Log clearly (what, where, record counts, elapsed time)
7) â™»ï¸ Make it idempotent (re-runs should not duplicate or corrupt)
8) ğŸ§ª Make it CI-friendly (non-interactive; stable exit codes)
9) ğŸ“ Update this README **and** the script registry below

---

<a id="script-registry"></a>

## ğŸ“‹ Script registry

> âœï¸ Add rows as scripts are introduced. Keep this current.

| Category | Script pattern | Purpose | Safety posture |
|---|---|---|---|
| ğŸ§° dev | `dev/up.*` | Start local stack (compose) | read-only-ish |
| ğŸ§° dev | `dev/smoke.*` | Quick sanity checks | read-only |
| ğŸ—„ï¸ db | `db/migrate.*` | Apply DB migrations | `--apply` gated |
| ğŸ—„ï¸ db | `db/backup_*` | Create encrypted DB backups | `--dry-run` default |
| ğŸ—„ï¸ db | `db/restore_*` | Restore backups | multi-confirm required |
| ğŸ—ºï¸ gis | `gis/import_*` | Load vectors/rasters into staging | `--dry-run` + `--apply` |
| ğŸ—ºï¸ gis | `gis/export_*` | Export layers to GeoJSON/tiles/COGs | safe defaults |
| ğŸ·ï¸ catalogs | `catalogs/build_*` | Build STAC/DCAT/PROV artifacts | read-only â†’ writes artifacts |
| ğŸ·ï¸ catalogs | `catalogs/validate_*` | Validate schemas + links | read-only |
| ğŸ•¸ï¸ graph | `graph/sync_*` | Sync catalog references into graph | `--apply` gated |
| ğŸ›°ï¸ remote_sensing | `remote_sensing/export_*` | Trigger/track exports of derived EO products | record provenance always |
| ğŸ§® simulation | `simulation/run_*` | Run scenarios/jobs | seed + provenance required |
| ğŸ¤– ml | `ml/train_*` / `ml/eval_*` | Train/evaluate models | dataset IDs + metrics required |
| ğŸ§ª qa | `qa/validate_*` | Acceptance gates | read-only |
| ğŸ” security | `security/scan_*` | Secrets/sensitive patterns | read-only |
| ğŸ§¹ housekeeping | `housekeeping/purge_*` | Cleanup caches/logs | confirmations required |
| ğŸ§ª ci | `ci/check.*` | CI entrypoint | deterministic + non-interactive |

---

<a id="troubleshooting"></a>

## ğŸ§¯ Troubleshooting (CLI â€œkung fuâ€)

Useful patterns:
- log triage: `tail -f`, `less`, `grep`, `rg` (ripgrep)
- JSON sanity: `jq`
- YAML sanity: `yq` *(if used)*
- quick DB truth checks: `psql "$DATABASE_URL" -c "<query>"`

GIS sanity helpers:
- `gdalinfo <file.tif>`
- `ogrinfo <file.geojson> -so`
- `ogr2ogr -f GeoJSON /tmp/out.json in.shp -t_srs EPSG:4326` *(example transform)*

> [!CAUTION]
> Avoid â€œcreative one-linersâ€ on production data.  
> If it matters, turn it into a script with dry-run + logs + provenance. âœ…

---

<a id="related-docs"></a>

## ğŸ¤ Related docs (inside this repo)

- ğŸ“¦ Data staging & dataset conventions â†’ `data/README.md`
- ğŸ§ª Canonical executable boundary â†’ `src/README.md`
- ğŸ›°ï¸ API boundary scripts (service-local) â†’ `api/scripts/README.md` *(if present)*
- ğŸ§ª API tests (boundary test suite) â†’ `api/tests/README.md` *(if present)*
- ğŸ““ MCP runs, receipts, and research workflow â†’ `mcp/MCP-README.md`
- ğŸ““ Notebooks (explore â†’ graduate to src) â†’ `notebooks/README.md` *(if present)*
- ğŸŒ Web app conventions â†’ `web/README.md` *(if present)*

---

<a id="reference-library-influence-map"></a>

## ğŸ“š Project reference library influence map

> This is an explicit map from the projectâ€™s included reference files to the conventions enforced (or expected) in `scripts/`.

<details>
<summary><b>ğŸ“¦ Expand: project files â†’ how they shape scripts</b></summary>

| Project file | What it contributes to `scripts/` conventions |
|---|---|
| `Kansas Frontier Matrix (KFM) â€“ Comprehensive Engineering Design.docx` | Pipeline ordering, governance boundary posture, â€œno mystery artifacts,â€ and why scripts must be orchestration (not business logic). |
| `Latest Ideas.docx` | A place for experimental automation ideas that should graduate into canonical modules + contracts before becoming â€œprod scripts.â€ |
| `Kansas Frontier Matrix (KFM) â€“ Master Technical Specification.pdf` | System invariants, API boundary rule, provenance-first publishing, performance constraints, and â€œdeny-by-defaultâ€ thinking. |
| `clean-architectures-in-python.pdf` | Thin-wrapper pattern: scripts call canonical services/modules; keep logic importable and testable. |
| `implementing-programming-languages-an-introduction-to-compilers-and-interpreters.pdf` | Interface/contract mindset: validate inputs early; fail fast; standardize exit codes and parsing. |
| `Node.js Notes for Professionals - NodeJSNotesForProfessionals.pdf` | Practical CLI/tooling patterns in JS ecosystems (useful for build glue, web tooling, generators). |
| `Introduction-to-Docker.pdf` | Container parity for scripts; reproducible environments; safe handling of secrets via env. |
| `PostgreSQL Notes for Professionals - PostgreSQLNotesForProfessionals.pdf` | DB scripting discipline: migrations, backups/restores, roles, transactional safety, repeatable ops. |
| `MySQL Notes for Professionals - MySQLNotesForProfessionals.pdf` | General relational ops patterns; import/export caution; consistent admin workflows. |
| `python-geospatial-analysis-cookbook.pdf` | CRS hygiene; geospatial IO; PostGIS integration; â€œfix data at boundaries, not ad-hoc.â€ |
| `geoprocessing-with-python.pdf` | Pipeline-style geoprocessing orchestration patterns and repeatable spatial processing workflows. |
| `Geographic Information System Basics - geographic-information-system-basics.pdf` | Map â€œtruthâ€ discipline: symbology implications; disclosure boundaries; QA checks for geometry and CRS. |
| `making-maps-a-visual-guide-to-map-design-for-gis.pdf` | Why â€œexport defaultsâ€ matter; avoid misleading ramps; ensure legends/outputs are honest. |
| `Mobile Mapping_ Space, Cartography and the Digital - 9789048535217.pdf` | Offline/mobile constraints that scripts should respect when generating tiles/derivatives (size budgets, caching). |
| `Cloud-Based Remote Sensing with Google Earth Engine-Fundamentals and Applications.pdf` | Remote sensing export orchestration patterns; how to record AOI/time/method for provenance. |
| `Google Earth Engine Applications.pdf` | Real-world GEE workflow variety; reinforces that scripts should track parameters and outputs systematically. |
| `responsive-web-design-with-html5-and-css3.pdf` | â€œWeb realityâ€ constraints: payload size budgets, responsive asset generation, and why scripts should optimize outputs. |
| `webgl-programming-guide-interactive-3d-graphics-programming-with-webgl.pdf` | 3D/GL tooling awareness; coordinate conventions; caution with parsing complex model formats. |
| `compressed-image-file-formats-jpeg-png-gif-xbm-bmp.pdf` | Practical compression guidance for screenshots, thumbnails, QA artifacts, and web-facing outputs. |
| `Scalable Data Management for Future Hardware.pdf` | Performance thinking: partitions, locality, concurrency safety, and metadata-driven access patterns. |
| `Data Spaces.pdf` | Federation mindset: scripts should prefer dataset IDs + catalogs over local-only file assumptions. |
| `Scientific Modeling and Simulation_ A Comprehensive NASA-Grade Guide.pdf` | Simulation hygiene: verification/validation posture, parameter recording, seeds, and run receipts. |
| `Understanding Statistics & Experimental Design.pdf` | QA scripts should include sanity checks; avoid false confidence; validate assumptions. |
| `regression-analysis-with-python.pdf` | Baseline modeling scripts: reproducible training/eval; diagnostics and stability checks. |
| `slides-linear-regression.pdf` | Quick reminders for evaluation conventions and assumptions; supports â€œminimum viable evalâ€ scripts. |
| `graphical-data-analysis-with-r.pdf` | EDA sanity outputs; encourages â€œplot and verifyâ€ acceptance gates for derived products. |
| `think-bayes-bayesian-statistics-in-python.pdf` | Uncertainty reporting expectations; scripts should record priors/assumptions when Bayesian workflows are used. |
| `Spectral Geometry of Graphs.pdf` | Graph-related QA scripts: be explicit about what metrics mean and validate graph integrity. |
| `Generalized Topology Optimization for Structural Design.pdf` | Optimization workflows: record constraints/objectives; deterministic run IDs; repeatable results packaging. |
| `Principles of Biological Autonomy - book_9780262381833.pdf` | Systems thinking: feedback loops, stability, and why automation must be observable and safe. |
| `Introduction to Digital Humanism.pdf` | Human-centered governance: transparency, accountability, ethical defaults, and â€œdonâ€™t automate harm.â€ |
| `On the path to AI Lawâ€™s prophecies and the conceptual foundations of the machine learning age.pdf` | AI governance: label AI-assisted outputs; record model/version/config where permissible; treat derived outputs carefully. |
| `Gray Hat Python - Python Programming for Hackers and Reverse Engineers (2009).pdf` | Hostile-input mindset: treat parsers as attack surfaces; sandbox and validate. |
| `ethical-hacking-and-countermeasures-secure-network-infrastructures.pdf` | Threat modeling posture for scripts touching networks and privileged infra. |
| `concurrent-real-time-and-distributed-programming-in-java-threads-rtsj-and-rmi.pdf` | Concurrency warnings: avoid races; lock/serialize destructive ops; design for determinism. |
| `Deep Learning for Coders with fastai and PyTorch - Deep.Learning.for.Coders.with.fastai.and.PyTorchpdf` | Practical ML automation: prioritize baselines, data-centric iteration, and clear evaluation artifacts. |
| `applied-data-science-with-python-and-jupyter.pdf` | Reproducible research habits: record environment, parameters, outputs; avoid â€œmystery notebooksâ€ powering prod. |
| `MATLAB Programming for Engineers Stephen J. Chapman.pdf` | Engineering workflow discipline: repeatable scripts, explicit parameters, careful I/O handling. |
| `A programming Books.pdf` | General CLI/automation craftsmanship across languages; reinforces â€œboring, reliable scripts.â€ |
| `B-C programming Books.pdf` | Same: foundational engineering patterns and interoperability basics. |
| `D-E programming Books.pdf` | Same: tooling, interfaces, and maintainability patterns. |
| `F-H programming Books.pdf` | Same: pragmatic engineering discipline and robust automation culture. |
| `I-L programming Books.pdf` | Same: standardization and long-term maintainability. |
| `M-N programming Books.pdf` | Same: systems/network awareness relevant to ops scripts. |
| `O-R programming Books.pdf` | Same: reliability patterns and practical automation approaches. |
| `S-T programming Books.pdf` | Same: testing/tooling patterns that should influence script QA. |
| `U-X programming Books.pdf` | Same: breadth reference that supports consistent automation practices. |

</details>

---

<a id="definition-of-done"></a>

## âœ… Definition of â€œdoneâ€ for a script

A script is considered complete when:
- âœ… Safe by default (`--dry-run` default or clearly supported)
- âœ… Repeatable/idempotent (re-run doesnâ€™t duplicate or corrupt)
- âœ… Documented (`--help` + 2 examples)
- âœ… Logs what it did (counts, paths, elapsed time)
- âœ… Outputs land in the correct stage (`raw/ â†’ work/ â†’ processed/`)
- âœ… (When applicable) emits/updates metadata + provenance artifacts (STAC/DCAT/PROV)
- âœ… Registered in the script registry (table above)
- â­ (Recommended) passes basic linters (`shellcheck` / `ruff`) and is CI-friendly (non-interactive prompts require `--yes`)