<!--
ğŸ“Œ This README defines the repo-wide automation surface for KFM.
ğŸ—“ï¸ Last updated: 2026-01-09
-->

<div align="center">

# ğŸ§° `scripts/` â€” KFM Automation Toolkit

**Repeatable commands for dev, data ops, GIS/remoteâ€‘sensing workflows, modeling/simulation orchestration, and deployment â€œglueâ€.**  
Safe-by-default âœ… â€¢ Idempotent â™»ï¸ â€¢ Provenance-first ğŸ§¾ â€¢ Hostile-input aware ğŸ›¡ï¸

![Status](https://img.shields.io/badge/status-active-brightgreen)
![README](https://img.shields.io/badge/README-v1.2.0-8957e5)
![Safe by default](https://img.shields.io/badge/safe--by--default-required-success)
![Idempotent](https://img.shields.io/badge/idempotent-expected-blue)
![Contract-first](https://img.shields.io/badge/contract--first-required-0aa3a3)
![Provenance](https://img.shields.io/badge/provenance-STAC%2FDCAT%2FPROV-informational)
![Governance](https://img.shields.io/badge/governance-FAIR%20%2B%20CARE%20%2B%20Sovereignty-2ea043)
![Security](https://img.shields.io/badge/security-hostile--inputs%20%2B%20deny--by--default-red)

</div>

> [!IMPORTANT]
> **`scripts/` is orchestration â€” not â€œthe truth.â€**  
> If behavior becomes **core**, move the implementation into **`src/`** (or `api/src/`) and let scripts call it.

> [!IMPORTANT]
> **KFM pipeline ordering is absolute (governed):**  
> **ETL â†’ STAC/DCAT/PROV Catalogs â†’ Graph â†’ APIs â†’ UI â†’ Story Nodes â†’ Focus Mode**  
> Scripts must not create **mystery artifacts** that bypass catalogs/provenance.

---

## ğŸ”— Quick links
- ğŸ§­ Repo overview: **[`../README.md`](../README.md)**
- ğŸ§ª Executable code boundary: **[`../src/README.md`](../src/README.md)** *(if present)*
- ğŸ“¦ Data + metadata boundary: **[`../data/README.md`](../data/README.md)** *(if present)*
- ğŸ““ MCP (runs/receipts, experiments): **[`../mcp/MCP-README.md`](../mcp/MCP-README.md)** *(recommended, if present)*
- ğŸŒ Web UI boundary: **[`../web/README.md`](../web/README.md)** *(if present)*

---

<details>
<summary><b>ğŸ§­ Table of contents</b></summary>

- [ğŸ§¾ Doc metadata](#-doc-metadata)
- [ğŸ¯ What belongs here (and what doesnâ€™t)](#-what-belongs-here-and-what-doesnt)
- [ğŸ§± The governed boundary scripts must respect](#-the-governed-boundary-scripts-must-respect)
- [ğŸ Quickstart](#-quickstart)
- [ğŸ—‚ï¸ Recommended folder map](#ï¸-recommended-folder-map)
- [ğŸ§± Standard script contract](#-standard-script-contract)
- [ğŸ§­ Data lifecycle + evidence artifacts](#-data-lifecycle--evidence-artifacts)
- [ğŸ§¾ Observability & provenance](#-observability--provenance)
- [ğŸ§¨ Safety guardrails (non-negotiable)](#-safety-guardrails-non-negotiable)
- [âš¡ Performance, scaling, and concurrency](#-performance-scaling-and-concurrency)
- [ğŸ—ºï¸ GIS + PostGIS scripting tips](#ï¸-gis--postgis-scripting-tips)
- [ğŸ›°ï¸ Remote sensing scripting tips](#ï¸-remote-sensing-scripting-tips)
- [ğŸ§ª QA scripts (contracts & acceptance gates)](#-qa-scripts-contracts--acceptance-gates)
- [ğŸ§© Adding a new script (checklist)](#-adding-a-new-script-checklist)
- [ğŸ“‹ Script registry](#-script-registry)
- [ğŸ“š Project reference library influence map](#-project-reference-library-influence-map)
- [âœ… Definition of â€œdoneâ€ for a script](#-definition-of-done-for-a-script)
- [ğŸ•°ï¸ Version history](#ï¸-version-history)

</details>

---

## ğŸ§¾ Doc metadata

| Field | Value |
|---|---|
| Doc | `scripts/README.md` |
| Status | Active âœ… |
| Last updated | **2026-01-09** |
| Audience | Contributors shipping automation, data ops, validators, and safe wrappers around `src/` |
| Prime directive | **No script may bypass catalogs (STAC/DCAT/PROV) or weaken governance.** |

---

## ğŸ¯ What belongs here (and what doesnâ€™t)

### âœ… Good fits for `scripts/`
- ğŸ§± **Environment bootstrap**: install deps, initialize DB schema, load seed/reference data
- ğŸ§° **Dev helpers**: run local stack, health checks, smoke tests, â€œmake my laptop match CIâ€
- ğŸ—ºï¸ **GIS tooling wrappers**: convert formats, validate CRS, generate tiles, build COGs, reprojection helpers
- ğŸ›°ï¸ **Remote sensing orchestrators**: Earth Engine export triggers, download trackers, derived-product packagers
- ğŸ§® **Model/simulation orchestration**: run pipelines/jobs with recorded configs, seeds, and output receipts
- ğŸ§ª **Acceptance gates**: schema validation, link checks, provenance completeness, contract checks
- ğŸ•’ **Scheduled jobs**: backups, cache cleanup, log rotation (cron/Kubernetes CronJob)

### âŒ Not a good fit for `scripts/`
- ğŸš« **Core ETL logic** (belongs in `src/pipelines/`)
- ğŸš« **Domain/business rules** (belongs in `src/` domain/application layers or `api/src/`)
- ğŸš« **Duplicate implementations** of pipeline steps (scripts should call canonical modules)
- ğŸš« **One-off â€œforever scriptsâ€** that bypass provenance, approvals, or classification propagation
- ğŸš« **Unreviewed publishing** that creates â€œpublished-lookingâ€ outputs without STAC/DCAT/PROV

> [!TIP]
> Scripts are the â€œbuttons and levers.â€ If itâ€™s â€œthe engine,â€ it belongs in `src/`. ğŸ”§â¡ï¸ğŸ—ï¸

---

## ğŸ§± The governed boundary scripts must respect

KFM is designed so every user-facing claim traces back to evidence via a strict pipeline:

```mermaid
flowchart LR
  A["ğŸ“¥ ETL / Transforms"] --> B["ğŸ—‚ï¸ STAC/DCAT Catalogs"]
  B --> C["ğŸ§¾ PROV Lineage"]
  B --> D["ğŸ•¸ï¸ Graph (references catalogs)"]
  D --> E["ğŸ›¡ï¸ APIs (contracts + redaction)"]
  E --> F["ğŸ—ºï¸ UI"]
  F --> G["ğŸ“š Story Nodes"]
  G --> H["ğŸ¯ Focus Mode"]
```

### âœ… What this means for automation
- Scripts can **run** ETL, but must ensure:
  - outputs land in `data/raw â†’ data/work â†’ data/processed`
  - catalogs are generated/updated (`data/stac`, `data/catalog/dcat`, `data/prov`)
  - graph loads reference **catalog IDs**, not ad-hoc file paths
- Scripts can **validate** contracts + metadata, and should fail fast in CI when:
  - schemas donâ€™t validate
  - provenance is missing/incomplete
  - links/assets donâ€™t resolve
  - secrets/sensitive patterns appear in logs/configs

### ğŸ§© Dataset ID hygiene (recommended)
When a script needs a dataset identifier, prefer a stable, versioned ID (example pattern):
- `kfm.<state|region>.<theme>.<year_range>.v<version>`

(Exact naming is governed by `docs/standards/` when present.)

---

## ğŸ Quickstart

### 1) Discover available scripts
- Browse by category folder (see map below)
- Run help first:
  - `./scripts/<path>/my_script.sh --help`
  - `pwsh ./scripts/<path>/my_script.ps1 --help`
  - `python scripts/<path>/my_script.py --help`

> [!IMPORTANT]
> Every script **must** support `--help` and include **at least 2 runnable examples**.

### 2) Set environment (no secrets in git) ğŸ”
- Prefer repo-level `.env.example` â†’ `.env` *(if present)*
- Scripts should read config from:
  - environment variables âœ…
  - or a config file *path* passed as an argument/env âœ…

**Never hardcode credentials. Never print secrets.**

### 3) Default to safety âœ…
Preferred contract:
- `--dry-run` (default) â†’ prints actions
- `--apply` â†’ performs changes
- `--yes` â†’ skips prompts
- `--env {dev|staging|prod}` â†’ required when environment matters
- `--run-id <id>` â†’ strongly recommended for correlation + provenance

---

## ğŸ—‚ï¸ Recommended folder map

> Keep this list current as categories are introduced.

```text
ğŸ“ scripts/
â”œâ”€ ğŸ§° _lib/               # shared helpers (logging, env validation, guardrails)
â”œâ”€ ğŸ§° dev/                # local stack helpers, smoke tests, DX scripts
â”œâ”€ ğŸ§± bootstrap/          # first-run setup (deps, DB init, seed/reference loads)
â”œâ”€ ğŸ—„ï¸ db/                 # migrations, backups, restores, snapshots, sanity checks
â”œâ”€ ğŸ•¸ï¸ graph/              # graph sync/load helpers (must reference catalog IDs)
â”œâ”€ ğŸ·ï¸ catalogs/            # STAC/DCAT/PROV build + validate wrappers (usually call src/)
â”œâ”€ ğŸ§ª pipelines/           # pipeline runners (thin wrappers around src/pipelines)
â”œâ”€ ğŸ—ºï¸ gis/                # geoprocessing helpers (vector/raster, tiling, CRS checks)
â”œâ”€ ğŸ›°ï¸ remote_sensing/     # GEE wrappers, export tracking, indexing helpers
â”œâ”€ ğŸ§® simulation/         # scenario runners (must record seeds/configs + provenance)
â”œâ”€ ğŸ¤– ml/                 # train/eval runners (must record datasets + metrics + provenance)
â”œâ”€ ğŸ§ª qa/                 # validators, contract checks, dataset acceptance gates
â”œâ”€ ğŸ” security/           # secrets scans, sensitive-data scans, hostile-input checks
â”œâ”€ ğŸ§¹ housekeeping/       # rotate logs, purge caches, cleanup artifacts
â””â”€ ğŸ§ª ci/                 # stable entrypoints used by CI (deterministic, non-interactive)
```

> [!NOTE]
> If you add a new category folder, also add it to the Table of contents and script registry. ğŸ§©

---

## ğŸ§± Standard script contract

To keep `scripts/` predictable (and safe), every script **must** follow the same behavioral contract.

### âœ… CLI interface requirements
All scripts must support:

- `--help` prints:
  - purpose (1â€“2 lines)
  - inputs/outputs (paths or tables)
  - side effects (DB writes? file writes? network calls?)
  - required env vars
  - examples (at least 2)

- Modes:
  - `--dry-run` is the default (or clearly supported)
  - `--apply` performs writes/changes

- Safety:
  - `--yes` skips interactive prompts
  - `--env {dev|staging|prod}` when environment matters
  - if `--env prod` + `--apply` â†’ require an additional explicit prod acknowledgement flag  
    e.g. `--i-acknowledge-production`

- Provenance ergonomics (recommended):
  - `--run-id <id>` (or env `KFM_RUN_ID`) to correlate logs + PROV
  - `--dataset-id <id>` (when acting on a dataset)
  - `--outdir <path>` for artifact destinations
  - `--log-json` for machine-readable logs (JSONL)
  - `--no-network` default *(or explicit `--allow-network` for fetchers)*

**Exit codes (standard):**
- `0` success
- `2` usage/CLI error (bad args)
- `3` validation failure (inputs invalid; catalogs missing; schema mismatch) *(recommended)*
- `>=10` runtime failures (I/O, network, DB, permissions, unexpected exceptions)

> [!NOTE]
> Itâ€™s okay to add flags, but donâ€™t break the standard ones (`--help`, `--dry-run`, `--apply`, `--yes`, `--env`).  
> Consistency beats cleverness. ğŸ§ âœ…

### ğŸ§¾ â€œScript headerâ€ (recommended)
At the top of each script, include:
- Name + purpose
- Inputs / outputs
- Side effects
- Owner/team (or â€œunownedâ€)
- Safety defaults (`dry-run` default, confirmation behavior)
- Provenance expectations (what IDs/receipts are written)

---

## ğŸ§­ Data lifecycle + evidence artifacts

KFMâ€™s data work is **staged** and **traceable**. Scripts that ingest or transform data must:

1) ğŸ“¥ Write raw inputs â†’ `data/raw/<domain>/`  
2) ğŸ§± Write intermediates â†’ `data/work/<domain>/`  
3) âœ… Write publishable outputs â†’ `data/processed/<domain>/`  
4) ğŸ—ƒï¸ Emit metadata + lineage artifacts **before** anything is used downstream:
   - STAC â†’ `data/stac/**`
   - DCAT â†’ `data/catalog/dcat/**`
   - PROV â†’ `data/prov/**`

> [!IMPORTANT]
> If a script produces **evidence artifacts** (derived analysis, ML outputs, simulation outputs, OCR corpora),  
> treat them like first-class datasets: store them correctly, catalog them, and capture provenance. ğŸ§¾ğŸ§¬

### ğŸªª Classification + sovereignty propagation (hard rule)
- Outputs cannot be **less restricted** than their inputs unless a reviewed redaction step exists.
- If a script cannot determine classification, it must default to **restricted** (deny-by-default).

### âœ… â€œThin wrapperâ€ pattern (required for anything important)
If youâ€™re tempted to put real transformation logic in a script, do this instead:
- implement core logic in `src/â€¦` (pipelines/domain/services)
- keep the script as a thin CLI wrapper that:
  - validates inputs
  - calls the canonical module
  - writes run receipts/logs
  - triggers catalog/provenance generation and validation

---

## ğŸ§¾ Observability & provenance

Every script should:
- ğŸªµ Use structured logging (`timestamp`, `level`, `component`, `run_id`)
- ğŸ§· Print where outputs were written + what changed (counts, bytes, features)
- ğŸ§¾ Capture provenance inputs/outputs:
  - input file list + checksums (when feasible)
  - key parameters (bbox, time window, CRS, resolution, seeds)
  - IDs/paths to produced STAC/DCAT/PROV artifacts

### ğŸ§¾ Recommended â€œrun receiptâ€ (ğŸ”¥ strongly recommended for `--apply`)
When a script changes state, write a receipt that another person can replay:

- human-readable: `mcp/runs/<RUN-ID>/MANIFEST.md`
- machine-readable: `data/prov/<RUN-ID>.jsonld`

Receipt SHOULD include:
- git SHA, environment, operator identity (if available)
- inputs + checksums
- outputs + checksums
- produced catalog IDs (collection/item/dataset IDs)
- warnings (redactions applied, schema deviations, missing optional evidence)

> ğŸ›ï¸ If itâ€™s not reproducible, itâ€™s not done.

---

## ğŸ§¨ Safety guardrails (non-negotiable)

### âœ… Safe-by-default behavior
- ğŸ›‘ **No destructive actions by default**
- ğŸ§ª Default mode should be `--dry-run`
- ğŸ§¯ Destructive actions require explicit confirmation flags

### ğŸ­ Production protection
Scripts that can write to prod must:
- require explicit `--env prod`
- require `--i-acknowledge-production`
- log who/what/when:
  - user (if detectable), host, timestamp, run_id
  - git SHA (if available)
  - container digest (if available)

### ğŸ§Š Atomic writes (strongly recommended)
For file outputs:
- write to `*.tmp` then rename (atomic on most filesystems)
- never leave half-written â€œpublishedâ€ outputs behind
- prefer content-addressed paths (hash-in-path) for immutable artifacts

### ğŸ§¯ Hostile input posture
Assume inputs are hostile (files from the world, archives, rasters, JSON, PDFs, 3D assets).
- validate file types (allowlists)
- enforce size limits and decompression limits
- treat URL fetching as high-risk (SSRF; private IP blocks; allowlists)
- isolate complex parsing when possible (containers / sandboxing / subprocess limits)

> [!CAUTION]
> If a script can delete, drop, truncate, overwrite, revoke, or publish:  
> **dry-run default + explicit apply + explicit confirmation** is mandatory. ğŸš«ğŸ§¨âœ…

---

## âš¡ Performance, scaling, and concurrency

Automation should scale from â€œlaptop demoâ€ to â€œstatewide workloadsâ€ without rewrites.

### âœ… Scaling patterns (preferred)
- ğŸ“¦ **Chunking/partitioning**: process by tile, county, watershed, time window
- ğŸ§± **Pipeline breakers**: materialize only when needed (then resume streaming)
- â™»ï¸ **Idempotency keys**: `(dataset_id, version, tile_id, src_checksum)` patterns
- ğŸ—ƒï¸ **Metadata-driven access**: scripts should navigate via catalogs/IDs, not brittle paths
- ğŸ§µ **Concurrency safety**:
  - avoid shared mutable state without locks
  - serialize destructive ops
  - prefer job queues / worker pools over ad-hoc threading for heavy runs

### âœ… â€œBoring performance winsâ€
- cache downloads (ETag/Last-Modified when possible)
- avoid reprocessing unchanged inputs
- push expensive spatial operations into PostGIS when appropriate
- keep UI-facing assets web-friendly (tiles, COGs, simplified vectors)

---

## ğŸ—ºï¸ GIS + PostGIS scripting tips

### âœ… Make CRS and units explicit
- refuse â€œunknown SRIDâ€ geometries by default
- log CRS for inputs/outputs
- document axis-order and unit conversion
- record reprojection and resampling choices (method, resolution, nodata)

### âœ… Prefer database-side spatial operations when appropriate
- buffers, intersects, within, distance joins: PostGIS is often safer/faster than Python loops
- use staging tables and transactional swaps:
  1) load â†’ 2) validate counts/geometry â†’ 3) swap/rename in a transaction

### âœ… Web-serving friendliness
When scripts generate assets meant for the UI:
- vectors: simplify or tile (avoid multiâ€‘MB GeoJSON blobs)
- rasters: prefer COG (with overviews)
- tiles: verify CRS (commonly EPSG:3857 for web tiles) and metadata

### ğŸ” Privacy reminder
GeoJSON is easy to copy. Treat â€œcommitted vectorsâ€ as a disclosure boundary:
- donâ€™t export restricted geometries without explicit governance approval
- prefer catalog pointers to governed stores for sensitive layers

---

## ğŸ›°ï¸ Remote sensing scripting tips

Remote sensing scripts are usually orchestrators for:
- exporting derived indices (NDVI, moisture proxies, composites)
- producing COGs + thumbnails
- emitting STAC Items + linking distributions via DCAT
- capturing PROV runs (inputs, AOI, time window, method, parameters)

### âœ… Prefer derived products + provenance over raw archives
- avoid committing raw satellite archives into the repo
- store raw externally when needed; keep catalog pointers in-repo
- ensure every derived product is traceable (PROV) and discoverable (STAC/DCAT)

### âœ… Record â€œhow it was madeâ€
For any export, record:
- AOI (bbox/geometry), time window
- dataset/source IDs
- compositing method (median/mean/mosaic, cloud mask logic)
- resolution/CRS
- model/algorithm version if AI-assisted

### ğŸ—ºï¸ Georeferencing note
If a workflow involves manual georeferencing (e.g., QGIS control points), record:
- number of control points
- residual/RMS if available
- transformation method
- who performed it + when  
â€¦and attach that to provenance metadata (PROV) and dataset docs.

---

## ğŸ§ª QA scripts (contracts & acceptance gates)

`scripts/qa/` is for â€œtrust checksâ€ â€” scripts that keep the system honest:
- âœ… schema validation for STAC/DCAT/PROV
- âœ… catalog link checks (assets exist; hrefs resolve)
- âœ… definition-of-done checks (data present, metadata present, provenance present)
- âœ… contract checks (OpenAPI snapshots, schema diffs) *(if present)*
- âœ… security scans (secrets + sensitive patterns)
- âœ… governance checks (classification propagation; â€œno downgradeâ€ rules)

**Starter examples (conceptual)**
```bash
# JSON sanity (fast fail)
find data/stac data/catalog/dcat data/prov -name "*.json*" -print0 | xargs -0 -n 1 jq empty

# Catalog asset/link integrity
python scripts/qa/validate_stac_links.py data/stac/items

# Provenance completeness
python scripts/qa/validate_prov_bundle.py data/prov

# Secrets scan (repo-wide)
python scripts/security/scan_secrets.py .
```

> [!TIP]
> Keep PR checks fast. Put heavy raster QA into nightly jobs unless it blocks correctness. âš¡

---

## ğŸ§© Adding a new script (checklist)

1) ğŸ“ Put it in the right subfolder (`db/ gis/ remote_sensing/ qa/ â€¦`)
2) ğŸ·ï¸ Name it as a **verb**: `import_*`, `export_*`, `generate_*`, `validate_*`, `backup_*`
3) ğŸ§ª Add `--help` + **2 examples**
4) ğŸ›¡ï¸ Add `--dry-run` default and explicit confirmations for writes
5) ğŸ§¾ Write outputs to the correct `data/` stage + generate provenance/metadata when relevant
6) ğŸªµ Log clearly (what, where, record counts, elapsed time)
7) â™»ï¸ Make it idempotent (re-runs should not duplicate or corrupt)
8) ğŸ§ª Make it CI-friendly (non-interactive; stable exit codes)
9) ğŸ“ Update this README **and** the script registry below

---

## ğŸ“‹ Script registry

> âœï¸ Add rows as scripts are introduced. Keep this current.

| Category | Script pattern | Purpose | Safety posture |
|---|---|---|---|
| ğŸ§° dev | `dev/up.*` | Start local stack (compose) | read-only-ish |
| ğŸ§° dev | `dev/smoke.*` | Quick sanity checks | read-only |
| ğŸ—„ï¸ db | `db/migrate.*` | Apply DB migrations | `--apply` gated |
| ğŸ—„ï¸ db | `db/backup_*` | Create encrypted DB backups | `--dry-run` default |
| ğŸ—„ï¸ db | `db/restore_*` | Restore backups | multi-confirm required |
| ğŸ—ºï¸ gis | `gis/import_*` | Load vectors/rasters into staging | `--dry-run` + `--apply` |
| ğŸ—ºï¸ gis | `gis/export_*` | Export layers to tiles/COGs | safe defaults |
| ğŸ·ï¸ catalogs | `catalogs/build_*` | Build STAC/DCAT/PROV artifacts | writes artifacts |
| ğŸ·ï¸ catalogs | `catalogs/validate_*` | Validate schemas + links | read-only |
| ğŸ•¸ï¸ graph | `graph/sync_*` | Sync catalog references into graph | `--apply` gated |
| ğŸ›°ï¸ remote_sensing | `remote_sensing/export_*` | Trigger/track derived EO exports | provenance required |
| ğŸ§® simulation | `simulation/run_*` | Run scenarios/jobs | seed + provenance required |
| ğŸ¤– ml | `ml/train_*` / `ml/eval_*` | Train/evaluate models | dataset IDs + metrics required |
| ğŸ§ª qa | `qa/validate_*` | Acceptance gates | read-only |
| ğŸ” security | `security/scan_*` | Secrets/sensitive patterns | read-only |
| ğŸ§¹ housekeeping | `housekeeping/purge_*` | Cleanup caches/logs | confirmations required |
| ğŸ§ª ci | `ci/check.*` | CI entrypoint | deterministic + non-interactive |

---

## ğŸ“š Project reference library influence map

> These project files inform how we design, review, and harden automation in `scripts/`.

<details>
<summary><b>ğŸ“¦ Expand: project files â†’ how they shape scripts</b></summary>

### ğŸ§­ System governance & repo discipline
- `Kansas Frontier Matrix (KFM) â€“ Comprehensive Technical Documentation.docx` â€” pipeline ordering, provenance expectations, naming conventions, onboarding clarity
- `MARKDOWN_GUIDE_v13.md.gdoc` â€” contract-first + evidence-first repo rules; catalog/provenance as boundary artifacts; CI gates; â€œno downgradeâ€ governance

### ğŸ›°ï¸ Geospatial & mapping
- `python-geospatial-analysis-cookbook.pdf` â€” PostGIS + GIS scripting patterns (with a reminder: **do not hardcode secrets**)
- `making-maps-a-visual-guide-to-map-design-for-gis.pdf` â€” export defaults matter (legends, ramps, aggregation can mislead)
- `Mobile Mapping_ Space, Cartography and the Digital - 9789048535217.pdf` â€” offline/bandwidth constraints inform tiling + simplification scripts
- `Cloud-Based Remote Sensing with Google Earth Engine-Fundamentals and Applications.pdf` â€” export orchestration patterns; recording AOI/time/method
- `compressed-image-file-formats-jpeg-png-gif-xbm-bmp.pdf` â€” thumbnail/QA artifact compression choices

### ğŸŒ Web & 3D constraints
- `responsive-web-design-with-html5-and-css3.pdf` â€” payload budgets + responsive asset generation
- `webgl-programming-guide-interactive-3d-graphics-programming-with-webgl.pdf` â€” coordinate conventions + LOD/tiling expectations for 3D assets

### ğŸ—„ï¸ Data systems & scaling
- `PostgreSQL Notes for Professionals - PostgreSQLNotesForProfessionals.pdf` â€” migrations/backups/transactional safety for DB scripts
- `Data Spaces.pdf` â€” prefer dataset IDs + catalogs over local-only assumptions
- `Scalable Data Management for Future Hardware.pdf` â€” partitions, locality, concurrency safety, pipeline breakers

### ğŸ§ª Modeling, simulation, and rigor
- `Scientific Modeling and Simulation_ A Comprehensive NASA-Grade Guide.pdf` â€” V&V posture; seeds/params; run receipts
- `Understanding Statistics & Experimental Design.pdf` â€” QA scripts should challenge assumptions, avoid false confidence
- `regression-analysis-with-python.pdf` + `Regression analysis using Python - slides-linear-regression.pdf` â€” baseline eval scripts, diagnostics, assumptions
- `think-bayes-bayesian-statistics-in-python.pdf` â€” uncertainty reporting; priors/assumptions should be recorded
- `graphical-data-analysis-with-r.pdf` â€” EDA sanity plots as acceptance gates

### ğŸ•¸ï¸ Graph analytics & optimization
- `Spectral Geometry of Graphs.pdf` â€” interpret graph metrics carefully; validate integrity/meaning
- `Generalized Topology Optimization for Structural Design.pdf` â€” optimization runs should record constraints/objectives + determinism

### â¤ï¸ Human systems & ethics
- `Introduction to Digital Humanism.pdf` â€” transparency + accountability defaults for automation
- `Principles of Biological Autonomy - book_9780262381833.pdf` â€” systems thinking: feedback loops, resilience, observability

### âš–ï¸ AI governance & security posture
- `On the path to AI Lawâ€™s prophecies and the conceptual foundations of the machine learning age.pdf` â€” label AI involvement; traceability; risk framing
- `ethical-hacking-and-countermeasures-secure-network-infrastructures.pdf` â€” threat modeling mindset for automation touching networks/infra
- `Gray Hat Python - Python Programming for Hackers and Reverse Engineers (2009).pdf` â€” hostile-input awareness; parser hardening
- `concurrent-real-time-and-distributed-programming-in-java-threads-rtsj-and-rmi.pdf` â€” concurrency is hard: avoid races; design for determinism

### ğŸ§· Polyglot reference collections
- `A programming Books.pdf`
- `B-C programming Books.pdf`
- `D-E programming Books.pdf`
- `F-H programming Books.pdf`
- `I-L programming Books.pdf`
- `M-N programming Books.pdf`
- `O-R programming Books.pdf`
- `S-T programming Books.pdf`
- `U-X programming Books.pdf`

</details>

---

## âœ… Definition of â€œdoneâ€ for a script

A script is considered complete when:
- âœ… Safe by default (`--dry-run` default or clearly supported)
- âœ… Repeatable/idempotent (re-run doesnâ€™t duplicate or corrupt)
- âœ… Documented (`--help` + 2 examples)
- âœ… Logs what it did (counts, paths, elapsed time)
- âœ… Outputs land in the correct stage (`raw/ â†’ work/ â†’ processed/`)
- âœ… (When applicable) emits/updates metadata + provenance artifacts (STAC/DCAT/PROV)
- âœ… Registered in the script registry (table above)
- â­ (Recommended) CI-friendly (non-interactive prompts require `--yes`; stable exit codes)

---

## ğŸ•°ï¸ Version history

| Version | Date | Summary | Author |
|---:|---|---|---|
| v1.2.0 | 2026-01-09 | Aligned scripts with v13 contract-first + evidence-first rules; added dataset-id hygiene, sovereignty/noâ€‘downgrade guardrails, performance/concurrency notes, and tightened acceptance-gate expectations. | KFM Engineering |
| v1.1.0 | 2026-01-07 | Established repo-wide automation surface, safety defaults, folder map, standard script contract, and registry. | KFM Engineering |

