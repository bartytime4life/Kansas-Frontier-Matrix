```markdown
---
title: "ğŸ¯ Focus Mode AI â€” Context vs. Environment Heuristic Evaluation (v1)"
path: "src/ai/focus/evaluation/context-vs-environment/README.md"
version: "v9.7.0"
last_updated: "2025-11-09"
review_cycle: "Continuous / Autonomous"
commit_sha: "<latest-commit-hash>"
sbom_ref: "../../../../releases/v9.7.0/sbom.spdx.json"
manifest_ref: "../../../../releases/v9.7.0/manifest.zip"
telemetry_ref: "../../../../releases/v9.7.0/focus-telemetry.json"
telemetry_schema: "../../../../schemas/telemetry/focus-eval-v1.json"
governance_ref: "../../../../docs/standards/governance/ROOT-GOVERNANCE.md"
license: "MIT"
mcp_version: "MCP-DL v6.3"
---

<div align="center">

# ğŸ¯ **Focus Mode AI â€” Context vs. Environment Heuristic Evaluation**
`src/ai/focus/evaluation/context-vs-environment/README.md`

**Purpose:** Define a repeatable session to measure how **Focus Mode AI** balances **historical context weighting** vs. **environmental variables** (weather, hydrology, landcover, ownership, fauna, etc.). We will tune heuristics so the modelâ€™s interpretive output is **faithful, useful, and auditable** under FAIR+CARE.

[![Docs Â· MCP](https://img.shields.io/badge/Docs-MCP-blue)](../../../../docs/)
[![License](https://img.shields.io/badge/License-MIT-green)](../../../../LICENSE)
[![FAIR+CARE](https://img.shields.io/badge/FAIR%2BCARE-Aligned-orange)](../../../../docs/standards/fair-care/README.md)
[![Status](https://img.shields.io/badge/Status-Active-brightgreen)](#)

</div>

---

## ğŸ“˜ Overview

**Historical context weighting** = how strongly the model prioritizes time-aware, provenance-linked facts (e.g., 1870s plat maps, 1930s drought reports, 1950s aerials).  
**Environmental variables** = contemporaneous or reconstructed signals (e.g., LiDAR hillshade, flood recurrence, soil, microclimate, species presence, ownership parcels).

This evaluation isolates and then blends both to find the **optimal weighting schedule** per task type (discovery, explanation, ranking, mapping).

---

## ğŸ—‚ï¸ Directory Layout

```

src/ai/focus/evaluation/context-vs-environment/
â”œâ”€â”€ configs/                               # Experiment configs (YAML/JSON)
â”‚   â”œâ”€â”€ context_only.yaml                  # Context=1.0, Environment=0.0
â”‚   â”œâ”€â”€ env_only.yaml                      # Context=0.0, Environment=1.0
â”‚   â”œâ”€â”€ blended_grid.yaml                  # Grid-search over weights
â”‚   â””â”€â”€ ablation.yaml                      # Feature drop/disable tests
â”œâ”€â”€ datasets/                              # Curated eval sets (links or small refs)
â”‚   â”œâ”€â”€ tasks/                             # Task definitions & gold labels
â”‚   â””â”€â”€ splits/                            # train/val/test manifests
â”œâ”€â”€ metrics/                               # Metric calculators & schemas
â”‚   â”œâ”€â”€ explainability/                    # SHAP/LIME export adapters
â”‚   â””â”€â”€ provenance/                        # Lineage completeness checks
â”œâ”€â”€ runs/                                  # Generated artifacts (gitignored)
â”‚   â”œâ”€â”€ logs/                              # Structured logs/telemetry
â”‚   â”œâ”€â”€ reports/                           # HTML/MD summaries
â”‚   â””â”€â”€ shards/                            # Intermediate caches
â”œâ”€â”€ scripts/                               # CLI entrypoints
â”‚   â”œâ”€â”€ run_grid_search.py                 # Weight grid search
â”‚   â”œâ”€â”€ run_ablation.py                    # Feature ablations
â”‚   â”œâ”€â”€ summarize_reports.py               # Compile eval dashboards
â”‚   â””â”€â”€ export_explanations.py             # SHAP/LIME packs
â””â”€â”€ README.md

````

---

## ğŸ§© Evaluation Design

### Tasks (representative KFM use-cases)

| Task ID | Description | Primary Signal | Gold/Label Source |
|---|---|---|---|
| T1 | Rank candidate â€œlost homesteadâ€ sites | Historical context | Cross-checked plats + registry |
| T2 | Explain why a site likely had human activity in year-range | Context â†’ Env | Curated expert rationales |
| T3 | Prioritize survey tiles for field validation | Environmental | Hydrology + access + risk |
| T4 | Suggest interpretive panel copy (public history) | Context (time-aware) | Editor-reviewed text set |

### Weighting Schedule

We define scalar weights **w_c** (context) and **w_e** (environment) such that **w_c + w_e = 1.0**. The fusion layer accepts normalized feature groups and computes:
- **score = w_c Â· f_context + w_e Â· f_environment**

We sweep **w_c âˆˆ {1.0, 0.8, 0.6, 0.4, 0.2, 0.0}** (complement for **w_e**).

---

## ğŸ§¾ Metrics

| Category | Metric | Why it matters | Notes |
|---|---|---|---|
| Discovery | nDCG@k, MAP | Ranking quality | On T1, T3 |
| Faithfulness | Attributed Fact Precision | Cites correct time/place | Requires provenance anchors |
| Explanation | Rationale Helpfulness (Likert), Token Evidence Overlap | Human eval + text-to-evidence | Blind annotators |
| Robustness | Performance Drop under Ablation | Sensitivity to signal loss | Compare to baseline |
| Governance | Provenance Completeness %, License Pass | FAIR+CARE alignment | From lineage graph |
| Energy | J/Inference, gCOâ‚‚e/Run | Sustainability | From telemetry_ref schema |

---

## âš™ï¸ Heuristics Under Test

| Heuristic | Description | Control Range | Default |
|---|---|---|---|
| H1: Time-Proximity Boost | Prefer sources closer to target year-range | 0.0â€“2.0 | 1.0 |
| H2: Provenance Depth Bonus | Reward multi-hop, corroborated sources | 0.0â€“1.5 | 0.8 |
| H3: Spatial Concordance | Penalize mismatched footprints | 0â€“100 m tolerance | 25 m |
| H4: Env Variability Gate | Downweight volatile env layers | 0.0â€“1.0 | 0.5 |
| H5: Narrative Coherence | Encourage consistent, non-contradictory chains | 0.0â€“1.0 | 0.6 |

---

## ğŸ§ª Procedure (One-Command Session)

**Goal:** run grid search over weights + ablations, export reports, and produce explainability packs.

```bash
# From repo root
python src/ai/focus/evaluation/context-vs-environment/scripts/run_grid_search.py \
  --config src/ai/focus/evaluation/context-vs-environment/configs/blended_grid.yaml \
  --tasks  T1 T2 T3 T4 \
  --outdir src/ai/focus/evaluation/context-vs-environment/runs

python src/ai/focus/evaluation/context-vs-environment/scripts/run_ablation.py \
  --config src/ai/focus/evaluation/context-vs-environment/configs/ablation.yaml \
  --outdir src/ai/focus/evaluation/context-vs-environment/runs

python src/ai/focus/evaluation/context-vs-environment/scripts/export_explanations.py \
  --runs  src/ai/focus/evaluation/context-vs-environment/runs \
  --outdir src/ai/focus/evaluation/context-vs-environment/runs/reports/explainability
````

---

## ğŸ§© Components Flow

```mermaid
flowchart TD
  A["Start Session"] --> B["Load Tasks & Splits"]
  B --> C["Load Context Features (plats, archives, reports)"]
  B --> D["Load Environment Features (LiDAR, hydro, soil, landcover)"]
  C --> E["Normalize Context Features"]
  D --> F["Normalize Env Features"]
  E --> G["Fusion: score = w_c * f_context + w_e * f_env"]
  F --> G
  G --> H["Apply Heuristics H1â€“H5"]
  H --> I["Rank / Generate / Explain"]
  I --> J["Compute Metrics + Telemetry"]
  J --> K["Reports + SHAP/LIME Exports"]
```

---

## ğŸ§¾ Reporting & Acceptance

**Minimum acceptance per task (default thresholds):**

| Task | nDCG@10 | Attributed Fact Precision | Provenance Completeness | Energy Î” vs. baseline |
| ---- | ------: | ------------------------: | ----------------------: | --------------------: |
| T1   |  â‰¥ 0.78 |                         â€” |                  â‰¥ 0.90 |                 â‰¤ +5% |
| T2   |       â€” |                    â‰¥ 0.85 |                  â‰¥ 0.90 |                 â‰¤ +7% |
| T3   |  â‰¥ 0.75 |                         â€” |                  â‰¥ 0.85 |                 â‰¤ +5% |
| T4   |       â€” |                    â‰¥ 0.80 |                  â‰¥ 0.90 |                 â‰¤ +5% |

A run is **accepted** if all task-specific bars are met; else **flagged** for retune.

---

## ğŸ§ª Quick Start Datasets (placeholders / link-outs)

* `datasets/tasks/`:

  * `lost_homestead_rank.jsonl` â€” T1 pairs with labels.
  * `site_explanations.jsonl` â€” T2 rationales with evidence IDs.
  * `survey_prioritization.jsonl` â€” T3 tiles with priorities.
  * `panel_copy_eval.jsonl` â€” T4 target blurbs + editor scores.

* `datasets/splits/`:

  * `train.json`, `val.json`, `test.json`

> Note: Store only small manifests here; large rasters and graphs referenced via URIs in the repoâ€™s data catalog.

---

## â™¿ Accessibility & FAIR+CARE Notes

* Cite sources with **time and place**; include community-sensitive tags where applicable.
* Avoid deterministic claims on culturally sensitive sites; prefer **probabilistic language** and **consent-aware** metadata.

---

## ğŸ•°ï¸ Version History

| Version | Date       | Author    | Summary                                                    |
| ------- | ---------- | --------- | ---------------------------------------------------------- |
| v9.7.0  | 2025-11-09 | Core Team | Initial evaluation design, metrics, CLI, reporting layout. |

---

<div align="center">

Â© Kansas Frontier Matrix â€” Master Coder Protocol v6.3 Â· FAIR+CARE Certified Â· Diamondâ¹ Î© / CrownâˆÎ© Ultimate Certified
[Back to docs/] Â· [Governance Charter]

</div>
```
