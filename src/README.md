---
title: "ğŸ§­ Kansas Frontier Matrix (KFM) â€” src/ Core Code Guide"
path: "src/README.md"
version: "v13+"
status: "active"
last_updated: "2026-01-26"
doc_kind: "core-code-guide"
audience:
  - "backend engineers"
  - "data engineers"
  - "graph/ontology maintainers"
  - "AI/Focus Mode engineers"
principles:
  - "contract-first"
  - "provenance-first"
  - "evidence-first"
  - "fail-closed"
fair: true
care: true
care_label: "CARE (Collective Benefit Â· Authority to Control Â· Responsibility Â· Ethics)"
---

# ğŸ§­ Kansas Frontier Matrix (KFM) â€” `src/` Core Code Guide

![KFM](https://img.shields.io/badge/KFM-Kansas%20Frontier%20Matrix-0b7285?style=for-the-badge)
![v13+](https://img.shields.io/badge/version-v13%2B-5c7cfa?style=for-the-badge)
![Contract-first](https://img.shields.io/badge/Contract--first-%E2%9C%85-2f9e44?style=for-the-badge)
![Provenance-first](https://img.shields.io/badge/Provenance--first-%E2%9C%85-2f9e44?style=for-the-badge)
![Evidence-first](https://img.shields.io/badge/Evidence--first-%E2%9C%85-2f9e44?style=for-the-badge)
![Fail-closed](https://img.shields.io/badge/Policy%20Gates-Fail%20Closed-d9480f?style=for-the-badge)

![Python](https://img.shields.io/badge/Python-Backend-blue?style=for-the-badge)
![FastAPI](https://img.shields.io/badge/FastAPI-Governed%20API-009688?style=for-the-badge)
![PostGIS](https://img.shields.io/badge/PostGIS-Spatial%20DB-336791?style=for-the-badge)
![Neo4j](https://img.shields.io/badge/Neo4j-Knowledge%20Graph-018bff?style=for-the-badge)
![Elasticsearch](https://img.shields.io/badge/Search-Elastic%2FWhoosh-1f2937?style=for-the-badge)
![GeoParquet](https://img.shields.io/badge/GeoParquet-Analytics-6741d9?style=for-the-badge)
![PMTiles](https://img.shields.io/badge/PMTiles-Fast%20Vector%20Tiles-6741d9?style=for-the-badge)
![COG](https://img.shields.io/badge/COG-Raster%20Standard-6741d9?style=for-the-badge)
![STAC](https://img.shields.io/badge/STAC-Metadata-6741d9?style=for-the-badge)
![DCAT](https://img.shields.io/badge/DCAT-Catalog-6741d9?style=for-the-badge)
![PROV](https://img.shields.io/badge/PROV-Lineage-6741d9?style=for-the-badge)
![OPA](https://img.shields.io/badge/OPA%20%2B%20Conftest-Policy%20Pack-4c6ef5?style=for-the-badge)
![Cosign](https://img.shields.io/badge/Sigstore%20Cosign-Signed%20Artifacts-111827?style=for-the-badge)
![DVC](https://img.shields.io/badge/DVC-Data%20Versioning-111827?style=for-the-badge)
![Ollama](https://img.shields.io/badge/Ollama-Local%20LLM-111827?style=for-the-badge)

> **`src/` is KFMâ€™s canonical home for core implementation**:  
> ğŸ§ª deterministic pipelines â€¢ ğŸ§¾ catalog triplet writers (STAC/DCAT/PROV) â€¢ ğŸ•¸ï¸ graph/ontology tooling â€¢ ğŸŒ governed API boundary â€¢ ğŸ¤– Focus Mode orchestration (citations + refusals).  
> If you change code here, youâ€™re changing **governed systems** â€” every change must produce **auditable evidence** (receipts, hashes, policy decisions, lineage).

---

## ğŸ§­ Quick Navigation

- ğŸ¯ [What `src/` owns](#-what-src-owns)
- ğŸš¦ [Non-negotiables](#-non-negotiables-v13-invariants)
- ğŸ—‚ï¸ [Repo context](#ï¸-repo-context-v13-layout)
- ğŸ§© [Core runtime stack](#-core-runtime-stack)
- ğŸ§¾ [Contracts & boundary artifacts](#-contracts--boundary-artifacts)
- ğŸ§ªğŸ•¸ï¸ğŸŒ [`src/` layout](#ï¸-src-layout-recommended)
- ğŸ” [Canonical pipeline](#-the-canonical-pipeline)
- ğŸ§ª [Pipelines playbook](#-pipelines-playbook-srcpipelines)
- ğŸ“¦ [Data products & formats](#-data-products--formats)
- ğŸ•¸ï¸ [Graph playbook](#ï¸-graph-playbook-srcgraph)
- ğŸŒ [Server playbook](#-server-playbook-srcserver)
- ğŸ¤– [Focus Mode contract](#-focus-mode-contract-srcserverai)
- ğŸ§¾ [Governance ledger & receipts](#-governance-ledger--receipts)
- âš–ï¸ [Policy pack & quality gates](#ï¸-policy-pack--quality-gates)
- ğŸš€ [Golden paths](#-golden-paths)
- ğŸ§ª [Testing & validation matrix](#-testing--validation-matrix-recommended)
- ğŸ” [Security, privacy, and inference control](#-security-privacy-and-inference-control-dont-skip)
- âœ… [Definition of Done](#-definition-of-done-for-src-prs)
- ğŸ“š [Reference library](#-reference-library)

---

## ğŸ¯ What `src/` owns

| Area | `src/` owns it | Boundary notes |
|---|---:|---|
| ğŸ§ª Deterministic ETL pipelines | âœ… | Replayable + idempotent; config-driven; no ad-hoc edits |
| ğŸ§¾ **Boundary artifacts** (STAC/DCAT/PROV) | âœ… | Must exist **before** graph/UI/story/focus use |
| ğŸ“¦ Artifact packaging (optional) | âœ… | OCI/ORAS bundles + signatures (Cosign) for releases/seed |
| ğŸ•¸ï¸ Knowledge graph integration + integrity | âœ… | Ontology stability + migrations; prevent orphan/drift |
| ğŸ” Search + indexing hooks | âœ… | Full-text index + embeddings index are downstream of catalogs |
| ğŸŒ Governed API + redaction layer | âœ… | UI must not talk to Neo4j directly |
| ğŸ¤– Focus Mode orchestration + citations | âœ… | Hybrid retrieval; citations required; refuse when missing |
| ğŸ§¾ Governance ledger / receipts emitters | âœ… | Append-only evidence for pipelines + AI answers |
| ğŸ›ï¸ UI (React/MapLibre/Cesium) | âŒ | Canonical home: `web/` |
| âœï¸ Story Nodes (narratives) | âŒ | Canonical home: `docs/reports/story_nodes/` |
| ğŸ—ƒï¸ Data artifacts & catalogs | âŒ | Canonical home: `data/` (raw/work/processed + catalogs + audits) |
| ğŸ§ª Notebooks / experiments | âŒ | Canonical home: `mcp/` (methods + computational experiments) |

---

## ğŸš¦ Non-negotiables (v13 invariants)

> [!IMPORTANT]
> These are **hard gates** (design + CI). If you violate them, youâ€™re not â€œalmost doneâ€ â€” youâ€™re blocked.

1) **Pipeline ordering is absolute** ğŸ§±  
`ETL â†’ Catalogs (STAC/DCAT/PROV) â†’ Graph â†’ API â†’ UI â†’ Story Nodes â†’ Focus Mode`

2) **API boundary rule** ğŸš§  
Frontend **must never query Neo4j directly**. All graph access goes through `src/server/` for contracts + policy + redaction.

3) **Provenance-first publishing** ğŸ§¾  
No dataset is â€œpublishedâ€ until it has STAC + DCAT + PROV **and passes validation**.

4) **Evidence-first narrative + AI** ğŸ“Œ  
Story Nodes and Focus Mode must never introduce unsourced claims. If evidence is missing: **refuse**.

5) **Fail-closed policy gates** ğŸ”’  
Missing license, missing CRS, missing provenance, schema drift, missing citations, secrets leakage â†’ **block merge/publish**.

6) **Sovereignty & classification propagation** ğŸª¶  
Outputs cannot be less restricted than inputs. Redaction + approvals propagate end-to-end (pipelines â†’ graph â†’ API â†’ AI).

7) **â€œPerformance artifactsâ€ are still governed** âš¡  
PMTiles, caches, embeddings, summaries, indexes, thumbnailsâ€¦ are **derived datasets** â†’ must be cataloged + traced (PROV) + policy-checked.

---

## ğŸ—‚ï¸ Repo context (v13 layout)

`src/` lives inside a larger â€œcontracts + data + narrativeâ€ repo:

```text
ğŸ“ schemas/                  # âœ… machine-validated contracts (data/api/story/ui/telemetry)
ğŸ“ src/                      # ğŸš€ code (pipelines/graph/server)
ğŸ“ tools/                    # ğŸ§° policy pack, validators, linters, helpers
ğŸ“ web/                      # ğŸ›ï¸ UI (React + MapLibre + optional Cesium)
ğŸ“ data/                     # ğŸ—ƒï¸ raw/work/processed + catalogs + audits
â”‚  â”œâ”€â”€ raw/                  # immutable snapshots (may be DVC/LFS-backed)
â”‚  â”œâ”€â”€ work/                 # intermediate work products
â”‚  â”œâ”€â”€ processed/            # published artifacts (stable references)
â”‚  â”œâ”€â”€ catalogs/             # âœ… boundary artifacts
â”‚  â”‚   â”œâ”€â”€ stac/
â”‚  â”‚   â”œâ”€â”€ dcat/
â”‚  â”‚   â””â”€â”€ prov/
â”‚  â””â”€â”€ audits/               # run receipts, answer receipts, policy decisions, hash chains
ğŸ“ docs/                     # ğŸ“š governance, standards, templates, story nodes, runbooks
ğŸ“ mcp/                      # ğŸ§ª methods + computational experiments (not runtime)
ğŸ“ releases/                 # ğŸ” immutable, signed build & artifact bundles (optional)
```

> [!TIP]
> **Contract-first** means: schemas/contracts change first, code follows.  
> If youâ€™re editing code without a matching contract update, youâ€™re likely drifting.

---

## ğŸ§© Core runtime stack

KFM is a **hybrid stack**: spatial DB + graph DB + search indices + policy engine + AI runtime â€” unified behind a governed API.

```mermaid
flowchart TB
  UI["ğŸ›ï¸ web/ UI (React + MapLibre/Cesium)"] -->|REST/GraphQL| API["ğŸŒ src/server (FastAPI)"]

  API -->|SQL/Spatial| PG["ğŸ—ºï¸ PostGIS"]
  API -->|Cypher| N4J["ğŸ•¸ï¸ Neo4j"]
  API -->|AuthZ decisions| OPA["âš–ï¸ OPA (Rego)"]
  API -->|Full-text| SEARCH["ğŸ” Search (Elastic/Whoosh)"]
  API -->|Embeddings/Vector| VEC["ğŸ§  Vector Index (pgvector/Qdrant/FAISS)"]
  API -->|LLM inference| OLLAMA["ğŸ¤– Ollama (local model runtime)"]

  PIPE["ğŸ§ª src/pipelines"] --> PG
  PIPE --> N4J
  PIPE --> SEARCH
  PIPE --> VEC
  PIPE --> CAT["ğŸ§¾ data/catalogs (STAC/DCAT/PROV)"]
  PIPE --> AUDIT["ğŸ§¾ data/audits (receipts, hash chain)"]

  N4J -->|references| CAT
  PG -->|assets in| DATA["ğŸ“¦ data/processed (GeoParquet/PMTiles/COG/â€¦)"]
  CAT --> DATA
```

> [!NOTE]
> KFMâ€™s philosophy: **modularity + traceability**. The API is the enforcement boundary; data stores are implementation details behind it.

---

## ğŸ§¾ Contracts & boundary artifacts

### ğŸ§¬ The â€œcatalog tripletâ€ is the boundary contract

KFM treats **STAC + DCAT + PROV** as the canonical handoff artifacts between pipeline â†” graph â†” API â†” UI â†” Story â†” Focus Mode.

âœ… Expectations:
- **STAC** describes spatial/temporal assets and links to actual files/endpoints.
- **DCAT** makes datasets discoverable and harvestable (external catalogs can index it).
- **PROV** captures lineage (raw â†’ work â†’ processed) including pipeline run IDs/config/agents.

> [!IMPORTANT]
> If a dataset exists without the triplet, it does **not** exist (for KFM).

### ğŸ”— Cross-layer linkage rules (must hold)

- STAC Items â†’ **assets** in `data/processed/**` (or stable object storage)  
- DCAT â†’ **distributions** that point to STAC and/or direct downloads  
- PROV â†’ full chain: **raw inputs â†’ transforms â†’ outputs**, with run ID + commit hash  
- Graph â†’ references catalog IDs (donâ€™t duplicate large payloads)

### ğŸ§· Stable IDs & hashes

To keep everything rebuildable + auditable:
- ğŸ§¾ Every artifact should have a **SHA-256 digest** recorded.
- ğŸ§¾ Every run should emit a **run receipt** with:
  - input URIs + hashes
  - config hash
  - output paths + hashes
  - validation results
  - policy decision summary
  - code version (commit/build)

> [!TIP]
> Prefer **content-addressable IDs** for releases (digest-based tags), and keep â€œlatestâ€ as a floating pointer only.

---

## ğŸ—‚ï¸ `src/` layout (recommended)

> [!NOTE]
> Top-level `src/` boundaries are stable. Subfolders may evolve, but keep contracts clean.

```text
src/
â”œâ”€â”€ ğŸ§ª pipelines/                          # watcher â†’ ingest â†’ validate â†’ transform â†’ publish
â”‚   â”œâ”€â”€ ğŸ—‚ï¸ <domain_or_product>/            # one pipeline per domain/product (config-driven)
â”‚   â”‚   â”œâ”€â”€ ğŸ‘€ watcher.py                  # optional: detect upstream change (ETag/Last-Modified)
â”‚   â”‚   â”œâ”€â”€ ğŸ“¥ ingest.py                   # acquire inputs (snapshots + receipts)
â”‚   â”‚   â”œâ”€â”€ ğŸ§ª transform.py                # raw â†’ work â†’ processed (deterministic)
â”‚   â”‚   â”œâ”€â”€ âœ… validate.py                 # schema + geo + policy checks (fail-closed)
â”‚   â”‚   â”œâ”€â”€ ğŸ§  index.py                    # optional: build search + embeddings indexes
â”‚   â”‚   â”œâ”€â”€ ğŸ•¸ï¸ load_graph.py               # optional: catalogs â†’ graph mappings
â”‚   â”‚   â”œâ”€â”€ ğŸ“¦ publish.py                  # emit catalogs + run receipts + (optional) OCI bundle
â”‚   â”‚   â””â”€â”€ âš™ï¸ configs/                    # configs only (NO secrets)
â”‚   â””â”€â”€ â™»ï¸ _shared/                        # pipeline utilities (single source of truth)
â”‚       â”œâ”€â”€ ğŸ§° io/                         # canonical paths + atomic writes + receipts
â”‚       â”œâ”€â”€ ğŸ” hashing/                    # SHA-256, stable IDs, content addressing
â”‚       â”œâ”€â”€ ğŸ§¾ catalogs/                   # STAC/DCAT/PROV writers + validators
â”‚       â”œâ”€â”€ âœ… validators/                 # schema/link/geo sanity + policy hooks
â”‚       â”œâ”€â”€ ğŸ§© artifacts/                  # ORAS/OCI packaging helpers (optional)
â”‚       â””â”€â”€ ğŸ§¾ audit/                      # run receipts + hash chain writer
â”‚
â”œâ”€â”€ ğŸ•¸ï¸ graph/
â”‚   â”œâ”€â”€ ğŸ§  ontology/                       # vocab + mappings + versioned ontology definitions
â”‚   â”œâ”€â”€ ğŸ” migrations/                     # explicit migrations (no silent breaking changes)
â”‚   â”œâ”€â”€ ğŸ›¡ï¸ integrity/                      # constraints + drift detection + QA reports
â”‚   â”œâ”€â”€ ğŸ“¥ loaders/                        # STAC/DCAT/PROV â†’ graph (mapping + normalization)
â”‚   â””â”€â”€ ğŸ“¤ exports/                        # graph â†’ digestable outputs (JSON-LD, UI bundles)
â”‚
â””â”€â”€ ğŸŒ server/
    â”œâ”€â”€ ğŸŒ api/                            # REST routers/controllers (thin)
    â”œâ”€â”€ ğŸ§¬ graphql/                        # GraphQL schema/resolvers (optional; governed)
    â”œâ”€â”€ ğŸ§© services/                       # use-cases (search, tiles, story, focus, redaction)
    â”œâ”€â”€ ğŸ§  domain/                         # pure logic: entities + rules (no DB/HTTP)
    â”œâ”€â”€ ğŸ§· adapters/                       # ports/adapters perimeter
    â”‚   â”œâ”€â”€ ğŸ“¤ outbound/                   # PostGIS/Neo4j/OPA/Search/Ollama clients
    â”‚   â””â”€â”€ ğŸ“¥ inbound/                    # queues/webhooks/streams (if used)
    â”œâ”€â”€ ğŸ¤– ai/                             # Focus Mode (RAG + citations + answer receipts)
    â”‚   â”œâ”€â”€ ğŸšª prompt_gate.py              # input sanitization + injection detection
    â”‚   â”œâ”€â”€ ğŸ” retrieval.py                # hybrid retrieval orchestration
    â”‚   â”œâ”€â”€ ğŸ§  embeddings.py               # embedding + vector store operations
    â”‚   â”œâ”€â”€ ğŸ¦™ ollama_client.py            # local model runtime client
    â”‚   â”œâ”€â”€ ğŸ§¾ citations.py                # citation validation + rendering
    â”‚   â”œâ”€â”€ âš–ï¸ policy_checks.py            # OPA checks on inputs/outputs
    â”‚   â””â”€â”€ ğŸ§¾ answer_receipts.py          # audit log + governance ledger emit
    â”œâ”€â”€ ğŸ” auth/                           # RBAC/ABAC scopes + token validation + policy scopes
    â”œâ”€â”€ âš–ï¸ policy/                         # OPA wiring, policy version/hash, decision logs
    â”œâ”€â”€ ğŸ“ˆ observability/                  # tracing, metrics, structured logs
    â””â”€â”€ ğŸ§± middleware/                     # request IDs, rate limits, error mapping, CORS
```

---

## ğŸ” The canonical pipeline

KFM is designed so **each stage consumes validated artifacts from the stage before it**.

```mermaid
flowchart LR
  subgraph Ingest["ğŸ§ª Pipelines"]
    W["ğŸ‘€ Watcher (optional)"] --> I["ğŸ“¥ Ingest (snapshot + receipts)"]
    I --> T["ğŸ§ª Transform (deterministic)"]
    T --> V["âœ… Validate (schema + geo + policy)"]
    V --> P["ğŸ“¦ Publish"]
    P --> CAT["ğŸ§¾ Catalog Triplet (STAC/DCAT/PROV)"]
    P --> AUD["ğŸ§¾ Run receipt + hash chain"]
    P --> ART["ğŸ“¦ (Optional) OCI/ORAS bundle + signature"]
  end

  CAT --> G["ğŸ•¸ï¸ Neo4j Graph (references catalogs)"]
  CAT --> IDX["ğŸ” Search + ğŸ§  Embeddings Index (derived datasets)"]

  G --> API["ğŸŒ Governed API (contracts + redaction + policy)"]
  IDX --> API

  API --> UI["ğŸ›ï¸ UI (React + MapLibre/Cesium)"]
  UI --> STORY["âœï¸ Story Nodes (Markdown + storyboard JSON)"]
  STORY --> FOCUS["ğŸ¤– Focus Mode (evidence bundle + citations)"]
```

> [!IMPORTANT]
> The **catalog triplet** is the â€œboundary artifactâ€ contract.  
> Graph, API, UI, Story Nodes, and Focus Mode must link back to cataloged sources.

---

## ğŸ§± Architecture mental model (how to code inside `src/`)

KFMâ€™s implementation style is **clean, contract-driven, and evidence-led**:

- ğŸ§  **Domain layer**: entities + rules (pure code; no DB/HTTP)
- ğŸ§© **Service layer**: use-cases/orchestration (calls ports, returns results)
- ğŸ§· **Ports (interfaces)**: abstractions for PostGIS/Neo4j/policy/search/caches
- ğŸ“¦ **Adapters**: actual implementations (Neo4j driver, SQLAlchemy, HTTP clients)

> [!TIP]
> If youâ€™re about to import a database client inside `src/server/domain/`â€¦ stop ğŸ›‘  
> Thatâ€™s a boundary violation. Move it to an adapter and depend on a port/interface.

---

## ğŸ§ª Pipelines playbook (`src/pipelines/`)

### âœ… Core guarantees

A pipeline is â€œKFM-validâ€ only if it is:

- ğŸ” **Deterministic**: same inputs + same config â†’ same outputs (or explainable diff)
- â™»ï¸ **Idempotent**: reruns donâ€™t duplicate or corrupt state
- ğŸ§¾ **Accountable**: emits receipts + hashes + run identifiers
- âœ… **Validated**: schema + geo checks + policy pack checks (**fail closed**)
- ğŸ§· **Cataloged**: produces STAC/DCAT/PROV at publish time
- ğŸ” **Verifiable (recommended)**: publish step can produce signed artifacts / attestations

### ğŸ‘€ Watcher â†’ Planner â†’ Executor pattern (automation-ready)

Many domains benefit from a â€œdetect â†’ plan â†’ executeâ€ loop:
- **Watcher** notices upstream changes (ETag, Last-Modified, checksum changes, schema drift)
- **Planner** proposes a controlled update plan (mapping changes, migrations, policy impacts)
- **Executor** runs the pipeline, generates receipts, and (optionally) opens a PR with changes

> [!NOTE]
> Automation is allowed **only** if it produces the same evidence as humans do â€” and remains reviewable.

### ğŸ“¦ Staging rules (do not improvise)

- `data/raw/<domain>/` â†’ immutable snapshots (store original receipts/metadata)
- `data/work/<domain>/` â†’ intermediate work products (throwaway but logged)
- `data/processed/<domain>/` â†’ final published artifacts (stable & referenced)
- `data/catalogs/stac/` + `data/catalogs/dcat/` + `data/catalogs/prov/` â†’ boundary artifacts
- `data/audits/` â†’ run receipts + policy decisions + hash chain

### ğŸ§¾ Run manifests (evidence ledger)

Every `publish.py` should emit a compact run record that answers:
- what inputs were used (URIs + hashes + timestamps)
- what configs/params were applied (config hash)
- what outputs were produced (paths + hashes + record counts)
- what validations passed/failed
- what policy gates were checked (policy hash + decision summary)
- what commit/build produced the run

> [!TIP]
> A run receipt is your â€œrebuild contract.â€ If a third party canâ€™t reproduce the run from the receipt, the pipeline isnâ€™t done.

---

## ğŸ“¦ Data products & formats

KFM explicitly supports **multiple representations** of the same dataset to serve different needs â€” as long as they remain traceable and governed.

### âš¡ Dual-format vector pattern (recommended)

For many vector layers, publish:
- ğŸ§  **GeoParquet** (analysis-friendly, columnar, fast scans)
- ğŸ—ºï¸ **PMTiles** (UI-friendly, pre-generated vector tiles)

âœ… Both must:
- share the same dataset identity + catalog metadata
- have explicit PROV lineage (PMTiles derived from GeoParquet or same upstream)
- be policy-checked and hash-addressable

> [!NOTE]
> Example pattern: publish a full-fidelity GeoParquet for analytics and a PMTiles archive for the map UI â€” both cataloged under the same dataset metadata and traceable via hashes.

### ğŸŒ„ Raster pattern (recommended)

- ğŸŒ„ Cloud-Optimized GeoTIFF (COG) + overviews/pyramids
- Optional: pre-generated tile caches (treated as derived datasets)

### ğŸ“„ Documents / OCR / transcripts

Treat text corpora as datasets:
- extracted text is a processed artifact
- OCR configs are part of provenance
- citations in AI must resolve back to the source doc and extraction run

### ğŸ“ˆ Derived analytics outputs

Simulations, models, summaries, indices, embeddings:
- are first-class datasets
- must have PROV linking to source datasets + code version
- must be rebuildable (or explicitly labeled as non-reproducible with rationale)

---

## ğŸ•¸ï¸ Graph playbook (`src/graph/`)

### ğŸ§  Ontology first (and versioned)

- ontology defines node labels, relationship types, and key properties
- changes must be explicit and **migration-backed**
- prefer alignment with standard ontologies where it helps interoperability:
  - ğŸ›ï¸ CIDOC-CRM (cultural heritage)
  - ğŸ—ºï¸ GeoSPARQL (geospatial semantics)
  - ğŸ§¾ PROV-O (lineage semantics)
  - â±ï¸ OWL-Time (temporal modeling)

> [!IMPORTANT]
> The graph models **relationships**, not bulk storage.  
> The â€œsource of truthâ€ for data assets is catalogs + processed artifacts.

### ğŸ” Migrations are mandatory for breaking changes

- no â€œsilentâ€ ontology drift
- prefer **forward-only** migrations
- keep a migration index + version history

### ğŸ›¡ï¸ Integrity checks are normal operation

Examples:
- prevent orphan nodes / dangling relationships
- ensure required properties exist
- ensure graph nodes reference catalog IDs (STAC/DCAT/PROV) rather than duplicating payloads
- ensure classification tags propagate (e.g., â€œsensitive_locationâ€)
- ensure graph relationships remain valid across updates

### ğŸª¶ Sensitive location handling (donâ€™t leak)

Certain nodes/edges may be restricted:
- protected site coordinates may be generalized (e.g., bounding box / grid cell)
- raw coordinates may be stored only in restricted layers and never returned to UI/AI without permission
- redaction must be enforced server-side

---

## ğŸŒ Server playbook (`src/server/`)

### ğŸ›ï¸ API is the governance boundary

The API layer is responsible for:
- âœ… contract validation (OpenAPI / GraphQL schema)
- ğŸ”’ policy enforcement (OPA decisions, redaction, classification)
- ğŸ§¾ provenance linkage (responses can point to catalog/source artifacts)
- ğŸ§­ consistent query semantics (stable behavior for UI + AI)
- ğŸ“ˆ auditability (policy decision logs + request IDs)

> [!IMPORTANT]
> UI â†’ API â†’ (PostGIS/Neo4j/catalogs/search).  
> **No direct UIâ†’Neo4j**.

### ğŸ§¬ REST + GraphQL (both are valid, both must be governed)

Typical split:
- REST: tiles, downloads, health, auth, simple lookups
- GraphQL: rich entity graphs, cross-domain discovery, composition

âœ… Governance requirements:
- depth/cost limits (GraphQL)
- query allowlists for expensive paths (optional)
- redaction & policy checks in every resolver/service path

### ğŸ—ºï¸ Geospatial endpoints (typical)

Examples of API responsibilities (details vary per deployment):
- vector tiles (`/tiles/{layer}/{z}/{x}/{y}.pbf`) and raster tiles
- spatial search (bbox, radius, polygon)
- temporal filters (timeline window / time slider)
- layer registry + metadata lookups (ties to STAC/DCAT)
- story node retrieval (Markdown + storyboard JSON)
- evidence panel queries (show provenance + source links)

### ğŸ§­ Time as a first-class dimension (UI contract dependency)

The UI can drive a timeline slider / 4D map:
- server must support time filters for time-enabled layers
- server can provide event markers (for story timelines) derived from graph/datasets
- avoid expensive re-queries by scoping time filters to time-aware layers only

---

## ğŸ¤– Focus Mode contract (`src/server/ai/`)

Focus Mode is a **hybrid retrieval** pipeline:
- ğŸ” pulls structured context (Neo4j + PostGIS)
- ğŸ” pulls text context (search index: Elastic/Whoosh)
- ğŸ§  pulls semantic context (vector similarity embeddings)
- ğŸ§¾ builds a **context bundle** with links + citations
- âœï¸ generates an answer that **must cite evidence**
- ğŸ›‘ refuses when evidence is missing or policy blocks access

### ğŸ”’ Prompt security & guardrails (least privilege)

Focus Mode must implement:
- ğŸšª **Prompt Gate**: input filtering + sanitization + injection detection
- ğŸ§° **Tool allow/deny lists**: default-deny; no internet/filesystem access
- âš–ï¸ **OPA output checks**: run policies on draft answers; block/redact/fallback

> [!IMPORTANT]
> Treat the AI subsystem as **potentially untrusted code**: isolate it, least-privilege it, log it.

### âœ… Hard requirements

- **No citations â†’ no answer** (fail closed)
- citations must resolve to cataloged sources / datasets / documents
- policy + redaction must apply **before** prompt assembly
- return **answer receipts** for auditability (inputs + sources + policy decisions)

### ğŸ§¾ Answer receipts (what gets logged)

At minimum:
- request ID, user scope, policy hash + decision summary
- retrieval sources list (dataset IDs, doc IDs, graph node IDs)
- content hashes of retrieved snippets (or stable pointers)
- model version + parameters (temperature, context length)
- final citations list (resolvable IDs)
- refusal reason (if refused)

### ğŸ¦™ Ollama integration (local model runtime)

Focus Mode may run fully local via Ollama:
- use a configurable generation model (LLM)
- use a configurable embeddings model
- keep model selection behind a port/interface so cloud or other runtimes can swap in

Recommended config surface:
- `OLLAMA_HOST`
- `OLLAMA_MODEL`
- `OLLAMA_EMBED_MODEL`
- `FOCUS_TOP_K`, `FOCUS_MAX_CONTEXT_TOKENS`, `FOCUS_MIN_CITATIONS`

> [!TIP]
> Enforce â€œcitations requiredâ€ at multiple layers: prompt template + citation validator + OPA output policy.

---

## ğŸ§¾ Governance ledger & receipts

KFM maintains auditability through **append-only receipts**:
- ğŸ§ª pipeline run receipts (data transformations)
- ğŸ¤– answer receipts (AI interactions)
- âš–ï¸ policy decisions (OPA results + policy version/hash)
- ğŸ” artifact integrity info (hashes + optional signatures)

Recommended properties:
- append-only storage (no edits)
- hash-chain entries (tamper-evident)
- stable IDs that link receipts â†” PROV â†” catalogs â†” graph nodes

> [!NOTE]
> If a user asks â€œHow do you know that?â€, KFM should be able to answer with **links + receipts**, not vibes.

---

## âš–ï¸ Policy pack & quality gates

KFM governance is enforced via:
- **OPA (Rego policies)** + **Conftest** (CI evaluation)
- schema validators for STAC/DCAT/PROV + internal contracts
- security scanning (secrets, dependency health, artifacts integrity)
- geo validation (CRS, geometry validity, bounds sanity)

Example policy classes:
- ğŸ“œ **Data**: license required; CRS required; sensitivity restrictions; link integrity
- ğŸ¤– **AI**: â€œanswers must include citationsâ€; â€œno disallowed outputsâ€; â€œtool use must be allowlistedâ€
- ğŸ§‘â€ğŸ’» **Dev**: no secrets; review required; lint/tests required; SBOM generated (recommended)

### ğŸ” Supply chain integrity (recommended for releases)

- build artifacts with SBOM
- sign artifacts (Cosign/Sigstore) and verify in release workflows
- attach provenance attestations (SLSA-style) stating: pipeline X + code version Y produced artifact Z

> [!IMPORTANT]
> CI follows **Detect â†’ Validate â†’ Promote**.  
> If Validate fails, Promote does not happen. No exceptions without governance approval.

---

## ğŸš€ Golden paths

> [!TIP]
> These are the â€œhappy pathâ€ workflows. If your change doesnâ€™t fit, stop and document why.

<details>
<summary><strong>ğŸ§ª Add a new data domain (Watcher â†’ ETL â†’ catalogs â†’ graph â†’ API)</strong></summary>

**1) Create the pipeline skeleton**
- `src/pipelines/<new-domain>/watcher.py` *(optional)*
- `src/pipelines/<new-domain>/ingest.py`
- `src/pipelines/<new-domain>/transform.py`
- `src/pipelines/<new-domain>/validate.py`
- `src/pipelines/<new-domain>/publish.py`
- `src/pipelines/<new-domain>/configs/`

**2) Add staging paths**
- `data/raw/<new-domain>/`
- `data/work/<new-domain>/`
- `data/processed/<new-domain>/`

**3) Publish boundary artifacts**
- `data/catalogs/stac/...`
- `data/catalogs/dcat/...`
- `data/catalogs/prov/...`
- `data/audits/...` (run receipts)

**4) Add runbook**
- `docs/data/<new-domain>/README.md` (sources, caveats, methods, ethics)

**5) Integrate graph**
- add loader mapping only after catalogs exist
- graph nodes point to catalog IDs

**6) Expose via API**
- add endpoint(s) in `src/server/api/`
- add service in `src/server/services/`
- add policy checks + redaction if needed

</details>

<details>
<summary><strong>ğŸ•¸ï¸ Change the graph schema safely</strong></summary>

- update ontology definition (version it)
- add a migration in `src/graph/migrations/`
- add/adjust integrity checks in `src/graph/integrity/`
- update API contract if shape changes
- update UI contracts (if applicable) â€” but keep UI work in `web/`

</details>

<details>
<summary><strong>ğŸŒ Add a new API endpoint (contract-first)</strong></summary>

- update the contract (OpenAPI/GraphQL) first
- implement handler (thin)
- implement service/use-case (business logic)
- add adapter changes as needed (Neo4j/PostGIS/OPA/Search)
- add contract tests + policy tests
- ensure response contains provenance links when relevant

</details>

<details>
<summary><strong>ğŸ¤– Extend Focus Mode retrieval</strong></summary>

- define what evidence counts (cataloged datasets/docs only)
- implement retriever behind a port (adapter pattern)
- enforce policy/redaction before prompt assembly
- require citations for every non-trivial claim
- log an answer receipt (sources + hashes + policy decisions)

</details>

<details>
<summary><strong>ğŸ“¦ Publish a signed dataset bundle (optional, release-grade)</strong></summary>

- publish GeoParquet + PMTiles (or COG + tiles) to `data/processed/`
- emit STAC/DCAT/PROV
- bundle artifacts with ORAS into an OCI registry tag (digest-based)
- sign with Cosign
- write distributions into DCAT pointing to OCI pull URLs and/or downloads

</details>

---

## ğŸ§ª Testing & validation matrix (recommended)

| Layer | What to test | Where |
|---|---|---|
| ğŸ§ª Pipelines | determinism, idempotency, schema/geo validation, policy checks | `pytest` + CI |
| ğŸ§¾ Catalogs | STAC/DCAT/PROV validity; link integrity; profile compliance | `tools/validation/` + CI |
| ğŸ•¸ï¸ Graph | migrations, constraints, integrity checks | `src/graph/integrity/` + CI |
| ğŸŒ API | contract tests, auth/policy checks, redaction correctness | `src/server/tests/` + CI |
| ğŸ¤– Focus Mode | citation presence, refusal behavior, prompt gate, policy gates | `src/server/ai/tests/` + CI |
| ğŸ” Releases | SBOM generation, signature verify, artifact integrity | `releases/` workflows |

---

## ğŸ” Security, privacy, and inference control (donâ€™t skip)

- ğŸ”‘ **No secrets in repo** (configs are non-secret; secrets come from env/secret manager)
- ğŸªª **AuthZ everywhere** (RBAC/ABAC + OPA decisions)
- ğŸ§¼ **Redaction is server-side** (UI should not â€œself-policeâ€)
- ğŸ§  **Inference risk exists** even if you hide raw values  
  Use privacy-preserving controls when needed:
  - k-anonymity / l-diversity / t-closeness
  - differential privacy (where appropriate)
  - query auditing + inference control for sensitive outputs
- ğŸ§­ **Sensitive locations**: never return precise coordinates unless permitted (generalize/aggregate)

> [!IMPORTANT]
> If a dataset is sensitive, its derivatives **inherit** restrictions unless governance approves release.

---

## âœ… Definition of Done for `src/` PRs

- [ ] Pipelines are deterministic + idempotent (re-run safe)
- [ ] STAC/DCAT/PROV emitted/updated **before** graph/UI/story use
- [ ] Run receipts emitted (inputs/outputs/hashes/config/policy decisions)
- [ ] Dual-format artifacts cataloged (e.g., GeoParquet + PMTiles) when applicable
- [ ] Graph changes include migrations + integrity checks (when needed)
- [ ] API changes are contract-first and versioned (no silent breakage)
- [ ] Policy gates pass (license, provenance, schema, citations, security)
- [ ] If Focus Mode changed: answers still cite sources or refuse (fail closed)
- [ ] Telemetry/logging added for new critical paths (audit-ready)
- [ ] (Release-grade) artifacts are signable/verifiable (SBOM + signature checks)

---

## ğŸ“š Reference library

### ğŸ§­ Canonical KFM docs (read these first)
- ğŸ“˜ `docs/MASTER_GUIDE_v13.md` (canonical pipeline + invariants + repo contracts)
- ğŸ—ï¸ `docs/architecture/` (platform architecture, roadmap, subsystem contracts)
- ğŸ¤– `docs/ai/` (Focus Mode rules, evaluation, telemetry expectations)
- ğŸ–¥ï¸ `docs/ui/` (UI integration constraints + layer registry expectations)
- âš–ï¸ `docs/governance/` (FAIR+CARE, ethics, sovereignty, review triggers)
- âœï¸ `docs/templates/` (Universal doc, Story Node, API contract extension templates)
- ğŸ“ `docs/standards/` (KFM_STAC_PROFILE / KFM_DCAT_PROFILE / KFM_PROV_PROFILE / Markdown work protocol)

### ğŸ§¾ Project PDF bundles (architecture + implementation guidance)
These PDFs are â€œsource-of-truth styleâ€ design references for v13+ patterns (contracts, governance, AI, UI, infra):
- ğŸ§­ Kansas Frontier Matrix â€” **Expanded Technical & Design Guide**
- ğŸ§­ Kansas Frontier Matrix â€” **Comprehensive Technical Documentation**
- ğŸ§­ Kansas Frontier Matrix â€” **Comprehensive Platform Overview and Roadmap**
- ğŸ§­ Kansas Frontier Matrix â€” **Comprehensive Architecture, Features, and Design**
- ğŸ§­ Kansas Frontier Matrix â€” **Comprehensive UI System Overview (Technical Architecture Guide)**
- ğŸ§­ Kansas Frontier Matrix â€” **AI System Overview ğŸ§­ğŸ¤–**
- ğŸ¦™ KFM AI Infrastructure â€” **Ollama Integration Overview**

### ğŸ“š Research portfolios (reference-only, not runtime)
These are **idea libraries** for implementation choices (GIS, WebGL, security, data science).  
They are not runtime dependencies, but they influence patterns and guardrails.

- ğŸ—ºï¸ Web mapping & virtual worlds portfolio (MapLibre/Cesium/WebGL ecosystem)
- ğŸ§  AI/ML concept portfolio (modeling patterns, evaluation ideas)
- ğŸ—„ï¸ Data management + architectures + Bayesian methods portfolio
- ğŸ§° Programming resources portfolio (Git/Docker/GraphQL/security/tooling)
- ğŸ“ Geospatial analysis cookbook (Python/PostGIS/GDAL recipes)
- ğŸ§ª Scientific method + engineering rigor protocol (reproducibility culture)
- ğŸ” Data mining & privacy references (inference control, auditing, DP concepts)

---

## ğŸ§­ If you only remember one thingâ€¦

> [!IMPORTANT]
> **In KFM, code is not â€œdoneâ€ until it produces evidence.**  
> If your change creates or transforms data, it must emit **STAC/DCAT/PROV**, pass policy gates, and remain replayable â€” before it can reach graph, API, UI, stories, or Focus Mode.