# ğŸ” Hashing Utilities (Shared Pipelines)

![Scope](https://img.shields.io/badge/scope-pipelines-informational)
![Integrity](https://img.shields.io/badge/integrity-SHA--256-success)
![Determinism](https://img.shields.io/badge/goal-deterministic%20runs-blueviolet)
![KFM](https://img.shields.io/badge/KFM-provenance--first-ff69b4)

> Hashes are the â€œreceipt printerâ€ of KFM ğŸ§¾ â€” we use them to prove **what bytes we ingested**, **what we produced**, and **what the UI/AI is allowed to trust**.

---

## ğŸ¯ Purpose

This folder contains **shared hashing primitives** used by KFM pipelines to:

- âœ… compute cryptographic digests (default: **SHA-256**) for bytes/files
- ğŸ§¾ generate & verify `checksums.sha256` receipts alongside raw inputs (tamper-evident)
- ğŸ§  create *canonical* digests for structured JSON (**RFC 8785 / JCS**) used for run manifests / idempotency keys
- ğŸ“¦ produce content-addressable identifiers for packaged artifacts (e.g., OCI digests for PMTiles/COG/GeoParquet)
- â™»ï¸ keep pipelines **deterministic**, **cache-friendly**, and **auditable**

### ğŸš« Non-goals

Hashing here is **not**:

- encryption ğŸ”’
- password hashing (bcrypt/argon2) ğŸ”‘
- signing / attestations (Cosign/SLSA handle that) âœï¸

---

## ğŸ—‚ï¸ Folder layout

```text
src/pipelines/_shared/hashing/
â”œâ”€â”€ ğŸ“„ README.md                   # ğŸ‘ˆ you are here ğŸ“Œ Hashing rules: canonicalization, file coverage, and digest formats
â”œâ”€â”€ ğŸ§© â€¦                           # Implementation files (sha256 helpers, checksum IO, canonical JSON/YAML helpers)
â””â”€â”€ ğŸ§ª â€¦                           # Tests (golden vectors + determinism checks; guards against hash drift)
```

---

## ğŸ—ºï¸ Where hashing fits in the KFM pipeline

```mermaid
flowchart LR
  A[data/raw ğŸ§± - immutable evidence] --> B[ğŸš§ Ingestion gate - hash + checksums sha256]
  B --> C[data/work ğŸ§° - ETL scratch]
  C --> D[data/processed âœ… - hash receipts]
  D --> E[ğŸ“š STAC DCAT PROV - checksums + metadata]
  E --> F[ğŸ•¸ï¸ Graph]
  F --> G[ğŸ”Œ API]
  G --> H[ğŸ§­ğŸ¤– UI Story Focus]
```

---

## ğŸ§© Contracts & conventions

### 1) Digest string format ğŸ”

**Canonical output** (preferred everywhere in KFM):

- `sha256:<lowercase-hex>`

Acceptable inputs (parsing convenience only):

- `<lowercase-hex>`
- `SHA256:<hex>` (normalize to lowercase output)

> âœ… Why this format? Itâ€™s explicit about the algorithm **and** matches OCI-style digests.

---

### 2) `checksums.sha256` file format ğŸ§¾

`checksums.sha256` MUST be `sha256sum`-compatible:

```text
<hex><two spaces><posix-relative-path>
```

Rules:

- Paths are **relative** to the dataset root
- Use **POSIX** separators (`/`) (even on Windows)
- File listing is **deterministic** (sorted)
- Exclude the checksum file itself + obvious non-evidence noise (`.DS_Store`, `Thumbs.db`, etc.)
- Always hash **bytes** (`rb`). Never newline-normalize.

---

### 3) Canonical JSON hashing (RFC 8785 / JCS) ğŸ§ 

When hashing JSON for idempotency keys / manifests:

- Canonicalize JSON using **RFC 8785 (JCS)** before hashing
- Ensure stable ordering + normalized values
- Never include volatile fields (timestamps, hostnames, random IDs) in the hashed payload unless explicitly intended

**Common KFM pattern:** *self-fingerprinting* manifests  
(digest field blank during hashing, then populated).

<details>
  <summary><strong>Why RFC 8785 matters (practical)</strong> ğŸ§©</summary>

If you hash raw `json.dumps()` output, youâ€™ll get different digests depending on:
- key order
- whitespace
- floating formats
- encoder differences

RFC 8785 makes the â€œJSON â†’ bytesâ€ step deterministic, so everyone gets the same digest for the same logical manifest.
</details>

---

## ğŸ³ Usage recipes

> Examples below are written in **Python** because most KFM pipelines are Pythonic, but the **rules are language-agnostic**.

### 1) Hash bytes (SHA-256)

```python
import hashlib

def sha256_bytes(data: bytes) -> str:
    return hashlib.sha256(data).hexdigest()
```

---

### 2) Hash a file (streaming / big-file friendly) ğŸ“¦

```python
from __future__ import annotations

import hashlib
from pathlib import Path

def sha256_file(path: Path, *, chunk_size: int = 8 * 1024 * 1024) -> str:
    h = hashlib.sha256()
    with path.open("rb") as f:
        for chunk in iter(lambda: f.read(chunk_size), b""):
            h.update(chunk)
    return h.hexdigest()
```

---

### 3) Generate `checksums.sha256` for a dataset folder ğŸ§¾

```python
from pathlib import Path

IGNORE_NAMES = {"checksums.sha256", ".DS_Store", "Thumbs.db"}

def write_checksums_sha256(root: Path, out_file: Path | None = None) -> Path:
    root = root.resolve()
    out_file = out_file or (root / "checksums.sha256")

    files = sorted(
        p for p in root.rglob("*")
        if p.is_file() and p.name not in IGNORE_NAMES
    )

    lines: list[str] = []
    for p in files:
        rel = p.relative_to(root).as_posix()
        digest = sha256_file(p)
        lines.append(f"{digest}  {rel}")

    out_file.write_text("\n".join(lines) + "\n", encoding="utf-8")
    return out_file
```

---

### 4) Verify `checksums.sha256` (fail closed âœ…)

```python
from pathlib import Path

class ChecksumMismatch(RuntimeError):
    pass

def verify_checksums_sha256(root: Path, checksums_file: Path | None = None) -> None:
    root = root.resolve()
    checksums_file = checksums_file or (root / "checksums.sha256")

    for i, line in enumerate(checksums_file.read_text(encoding="utf-8").splitlines(), start=1):
        if not line.strip():
            continue

        expected, rel = line.split(None, 1)  # tolerant of extra whitespace
        rel = rel.strip()
        expected = expected.lower()

        p = root / rel
        if not p.exists():
            raise ChecksumMismatch(f"[line {i}] Missing file: {rel}")

        actual = sha256_file(p)
        if actual != expected:
            raise ChecksumMismatch(
                f"[line {i}] Digest mismatch for {rel}: expected {expected}, got {actual}"
            )
```

---

### 5) Canonical digest for a run manifest (RFC 8785) ğŸ§ 

âš™ï¸ **Contract:** Everyone (CI, local runs, replays) must compute the *same digest* for the *same logical manifest*.

```python
import hashlib
import json

def canonicalize_json_rfc8785(obj) -> bytes:
    """
    Placeholder: use an RFC 8785 (JCS) canonicalizer:
    - keys sorted
    - UTF-8
    - normalized numbers
    - no insignificant whitespace

    NOTE: The fallback below is NOT full RFC 8785. Use a real JCS implementation for production.
    """
    return json.dumps(obj, sort_keys=True, separators=(",", ":"), ensure_ascii=False).encode("utf-8")

def self_hash_manifest(manifest: dict, *, digest_field: str = "canonical_digest") -> dict:
    m = dict(manifest)      # shallow copy
    m[digest_field] = ""    # blank during hashing

    canonical = canonicalize_json_rfc8785(m)
    digest = hashlib.sha256(canonical).hexdigest()

    m[digest_field] = f"sha256:{digest}"
    return m
```

âœ… Use this for:

- `run_id`
- `idempotency_key`
- `canonical_digest`
- evidence IDs
- any â€œwhat exactly did we run?â€ audit artifacts

---

### 6) Digest a *set of files* (dataset snapshot ID ğŸ“¦)

If you need a single ID for â€œthis exact directory treeâ€:

1. Generate `checksums.sha256` (sorted, deterministic)
2. Hash the `checksums.sha256` **file content** (bytes)

This yields a cheap â€œMerkle-ishâ€ content ID without implementing a full Merkle tree.

---

## ğŸ”— Integration points in KFM

| KFM area | How hashing is used | Typical artifact |
|---|---|---|
| ğŸ“¥ Ingestion Gate | Integrity checks + tamper-evidence receipts | `checksums.sha256` |
| ğŸ§¾ Audits | Run manifests + idempotency keys | `data/audits/<run_id>/run_manifest.json` |
| ğŸ“š Catalogs | STAC/DCAT entries carry checksums for assets | `*.stac.json`, `*.dcat.jsonld` |
| ğŸ—ƒï¸ Artifact Storage | Content-addressable digests for PMTiles/COG/GeoParquet (OCI/ORAS) | `sha256:<digest>` |
| ğŸ§  Story Evidence | Evidence manifests reference stable IDs / checksums | `EM-*.yaml` / `EM-*.json` |
| ğŸ›¡ï¸ CI / Policy Gates | Reject changes lacking required receipts / integrity fields | OPA/Conftest checks |

---

## ğŸ›¡ï¸ Security notes

- Prefer **SHA-256** (or stronger) for integrity.
- A digest proves **integrity**, not **authenticity**.
  - Authenticity requires **signing** (Cosign) + **policy gates**.
- Fail closed: checksum mismatches should **stop the pipeline**, not warn-and-continue.

---

## âš ï¸ Common pitfalls (and how to avoid them)

<details>
  <summary><strong>Gotchas</strong> ğŸ§¨</summary>

- **Text mode reads** (`"r"`) change bytes (newline normalization). Always `"rb"`.
- **Unsorted directory walks** produce nondeterministic `checksums.sha256`. Always sort.
- **Hashing JSON without canonicalization** is unstable. Use RFC 8785 for manifest IDs.
- **Including timestamps** in the hashed payload breaks idempotency. Keep â€œidentityâ€ separate from â€œwhen it ranâ€.
- **Absolute paths** leak machine details into digests. Always hash **relative POSIX paths**.
</details>

---

## ğŸ§ª Testing checklist

âœ… Add/keep tests that assert:

- empty bytes/file hash matches a known vector
- hashing is stable across OS + runtime versions (binary reads)
- directory hashing is stable (sorted order, path normalization)
- canonical JSON hashing is stable (RFC 8785 canonicalization)

---

## ğŸ§° Contributing

When adding new helpers:

1. Keep functions **pure** (no global state, no hidden randomness)
2. Choose safe defaults (`sha256`)
3. Document the output format (examples!)
4. Add tests + at least one â€œgoldenâ€ fixture
5. Prefer streaming IO for large geospatial artifacts (PMTiles/COGs)

---

## ğŸ“š Related docs (KFM)

- ğŸ“¥ Data intake philosophy & trust boundaries: **ğŸ“š KFM Data Intake â€“ Technical & Design Guide**
- ğŸ“¦ OCI artifact storage + signatures: **Additional Project Ideas** (OCI + ORAS + Cosign)
- ğŸ§¾ Evidence manifests for Story Nodes: **Additional Project Ideas** (Evidence-First Narratives)
- ğŸ›¡ï¸ Supply chain security & attestations: **ğŸŒŸ Latest Ideas & Future Proposals**
