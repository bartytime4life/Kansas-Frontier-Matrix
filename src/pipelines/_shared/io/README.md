# ğŸ§° `_shared/io` â€” Shared Pipeline I/O (Contracts + Utilities)

![Provenance](https://img.shields.io/badge/Provenance-first-brightgreen)
![Contract-first](https://img.shields.io/badge/Contract-first-blue)
![Policy](https://img.shields.io/badge/Policy%20Gates-fail--closed-orange)
![Metadata](https://img.shields.io/badge/STAC%20%7C%20DCAT%20%7C%20PROV-required-purple)
![Determinism](https://img.shields.io/badge/Deterministic%20Pipelines-idempotent-informational)

> **One place for pipeline I/O**: fetch âœ stage âœ hash âœ manifest âœ publish âœ prove.  
> If a pipeline touches bytes, it should do it through this layer (or follow its contracts).

---

## ğŸ¯ Purpose

This directory defines the **canonical** I/O patterns used across KFM pipelines so every artifact is:

- **Auditable** (checksums + run manifests + telemetry)
- **Reproducible** (deterministic outputs from inputs + config)
- **Governable** (policy gates can evaluate predictable artifacts)
- **Linkable** (STAC/DCAT/PROV cross-references power graph, UI, and Focus Mode)

In short: `_shared/io` is how KFM prevents â€œmystery layersâ€ and preserves chain-of-custody endâ€‘toâ€‘end.

---

## ğŸ§­ Scope

âœ… This folder **should** contain:
- Fetchers (HTTP/S3/etc) + cache helpers
- Path builders for KFMâ€™s canonical `data/*` layout
- Atomic writes, hashing, checksums, canonical JSON helpers
- Manifest writers (source + run + distribution)
- Catalog writers/validators hooks (STAC/DCAT/PROV)
- Telemetry / ledger appenders (NDJSON)
- Artifact publishing (filesystem + optional OCI registry)

ğŸš« This folder **should NOT** contain:
- Domain-specific transforms (that lives in each pipeline)
- Business logic for graph ingestion or API services
- UI logic

---

## âœ… Nonâ€‘Negotiable Invariants (aka â€œDonâ€™t Break Trustâ€)

- **Raw is immutable**: `data/raw/**` is **readâ€‘only evidence**. Never modify bytes in place.
- **Transform in staging**: any conversion/cleanup/reprojection happens in `data/work/**`.
- **Publish only from processed**: userâ€‘facing assets live in `data/processed/**`.
- **Evidence triplet is required**: STAC + DCAT + PROV must exist for anything â€œpublishedâ€.
- **Failâ€‘closed governance**: missing license/provenance/required fields must block publishing.
- **UI & Focus Mode only show proven outputs**: if itâ€™s not cataloged + traceable, itâ€™s not real (in KFM terms).

---

## ğŸ—ºï¸ Where `_shared/io` Sits in the Canonical Pipeline

```mermaid
flowchart LR
  A[ğŸŒ External Sources] --> B[ğŸ“¥ data/raw/**]
  B --> C[ğŸ›‚ Ingestion Gate<br/>checksums + sanity + telemetry]
  C --> D[ğŸ§ª data/work/**]
  D --> E[ğŸ“¦ data/processed/**]
  E --> F[ğŸ“š Catalogs<br/>data/stac + data/catalogs + data/prov]
  F --> G[ğŸ•¸ï¸ Graph Ingest<br/>Neo4j metadata + relationships]
  G --> H[ğŸ§  API Layer]
  H --> I[ğŸ—ºï¸ UI + Focus Mode + Story Nodes]
```

> ğŸ’¡ **Key idea:** `_shared/io` is responsible for **B â†’ F** (and producing the artifacts that make **G â†’ I** trustworthy).

---

## ğŸ—‚ï¸ Canonical Data Layout (I/O Responsibilities)

Typical outputs this module must support:

```text
repo/
â”œâ”€ ğŸ—‚ï¸ data/                           # ğŸ—‚ï¸ Data plane: governed artifacts + catalogs + run evidence
â”‚  â”œâ”€ ğŸ“¥ raw/                          # ğŸ“¥ Immutable evidence boundary (as-received source drops; never â€œclean rawâ€)
â”‚  â”œâ”€ ğŸ§ª work/                         # ğŸ§ª Staging + intermediate transforms (OK to wipe/rebuild; reproducible steps)
â”‚  â”œâ”€ âœ… processed/                     # âœ… Publishable standardized assets (what UI/API/graph should serve; versioned)
â”‚  â”œâ”€ ğŸ›°ï¸ stac/                          # ğŸ›°ï¸ STAC collections/items (asset index; time/run snapshots)
â”‚  â”œâ”€ ğŸ—‚ï¸ catalogs/                      # ğŸ—‚ï¸ DCAT datasets (discovery layer; license/access/distributions)
â”‚  â”œâ”€ ğŸ§¬ prov/                          # ğŸ§¬ PROV bundles (lineage + agents + params; links rawâ†’workâ†’processedâ†’catalog)
â”‚  â”œâ”€ ğŸ§¾ audits/                        # ğŸ§¾ Run evidence bundles (run_manifest, digests, policy decisions, receipts)
â”‚  â””â”€ ğŸ“ˆ telemetry/                     # ğŸ“ˆ Append-only NDJSON logs (audit-safe; redacted; correlation-friendly)
â””â”€ ğŸ§° src/pipelines/_shared/io/         # ğŸ§° Shared pipeline IO utilities (canonical paths, safe reads/writes, manifests)
```

---

## ğŸ“¦ Core Artifacts `_shared/io` Must Produce

| Artifact | Lives near | Why it exists |
|---|---:|---|
| `source.json` | `data/raw/**` | Who/where/when/license/sensitivity of the bytes you fetched |
| `checksums.sha256` | `data/raw/**` | Tamper evidence + integrity verification |
| `run_manifest.json` | `data/audits/<run_id>/` | Full audit of a run (inputs/outputs/tools/params) + idempotency |
| `telemetry.ndjson` | `data/telemetry/` | Append-only event stream for dashboards/audits/watchers |
| `STAC` JSON | `data/stac/**` | Spatial/temporal index of assets (including â€œwhereâ€™s the file?â€) |
| `DCAT` JSON | `data/catalogs/**` | Discovery metadata (license, keywords, distributions, etc.) |
| `PROV` JSON | `data/prov/**` | Lineage graph (raw âœ work âœ processed) + agents + config |
| `distribution.oci` (optional) | with artifacts | OCI/ORAS publishing metadata (digests, signatures, provenance refs) |
| `evidence.yaml` (optional) | story/artifacts | Structured evidence list for Story Nodes / AI outputs |

---

## ğŸ§© Suggested Module Layout (Implementation Guidance)

> Names are flexible, but responsibilities arenâ€™t. Keep `_shared/io` boring and predictable. âœ…

```text
src/pipelines/_shared/io/
â”œâ”€ ğŸ§­ paths.py                # Canonical path builders for raw/work/processed/stac/dcat/prov/audits (single source of truth)
â”œâ”€ ğŸ§± atomic.py               # Atomic file writes: temp files, fsync/rename patterns, partial-write avoidance
â”œâ”€ ğŸ” hashing.py              # sha256/multihash utilities + canonical JSON helpers (stable ordering/encoding)
â”œâ”€ ğŸŒ fetch/                  # Fetch adapters (acquisition layer) with retries + caching semantics
â”‚  â”œâ”€ ğŸŒ http.py              # fetch_http(...): retries, backoff, ETag/If-None-Match, cache headers, safe logging
â”‚  â”œâ”€ â˜ï¸ s3.py                # fetch_s3(...): optional S3 fetcher (signed URLs/creds handled outside logs)
â”‚  â””â”€ ğŸ§© __init__.py          # Package exports for fetch helpers
â”œâ”€ ğŸ§¾ manifests/              # Receipt/manifest writers (run/source/OCI) with digest helpers
â”‚  â”œâ”€ ğŸ§¾ source.py            # write_source_json(...): source record (origin, license, retrieval metadata, checksums)
â”‚  â”œâ”€ ğŸ§¾ğŸ” run.py              # write_run_manifest(...), canonical_digest(...): run ledger + deterministic hashing
â”‚  â”œâ”€ ğŸ“¦ oci.py               # write_distribution_oci(...): optional OCI/ORAS distribution manifest writer
â”‚  â””â”€ ğŸ§© __init__.py          # Package exports for manifest writers
â”œâ”€ ğŸ—‚ï¸ catalogs/               # Catalog writers (STAC/DCAT/PROV) for the evidence triplet
â”‚  â”œâ”€ ğŸ›°ï¸ stac.py              # write_stac_collection/items(...): collections + items + asset roles/hrefs
â”‚  â”œâ”€ ğŸ—‚ï¸ dcat.py              # write_dcat_dataset(...): dataset + distributions + license/access metadata
â”‚  â”œâ”€ ğŸ§¬ prov.py              # write_prov_bundle(...): PROV entities/activities/agents linking inputsâ†’outputs
â”‚  â””â”€ ğŸ§© __init__.py          # Package exports for catalog writers
â”œâ”€ ğŸ“ˆ telemetry.py            # append_ndjson_event(...): append-only NDJSON telemetry (audit-safe; redaction-aware)
â”œâ”€ ğŸ”’ classification.py       # Sensitivity tags + redaction helpers (label propagation, safe logging guards)
â””â”€ ğŸ§© __init__.py             # Public exports for shared IO (keep stable; avoid circular deps)
```

---

## ğŸ§ª Minimal Pipeline Pattern (How to Use This)

A pipeline should follow this shape (pseudo-code):

```python
from pipelines._shared.io import paths, fetch, hashing, manifests, catalogs, telemetry

def run(cfg):
    run_id = cfg.run_id()

    # 1) Resolve canonical directories
    raw_dir = paths.raw(cfg.domain, cfg.dataset_id)
    work_dir = paths.work(cfg.domain, cfg.dataset_id, run_id=run_id)
    out_dir = paths.processed(cfg.domain, cfg.dataset_id)

    # 2) Fetch to raw (immutable evidence)
    telemetry.event(run_id, "fetch.start", url=cfg.source_url)
    raw_file = fetch.http(cfg.source_url, dest_dir=raw_dir)
    hashing.write_checksums_sha256(raw_dir)

    manifests.write_source_json(
        raw_dir,
        source_url=cfg.source_url,
        license=cfg.license,
        provider=cfg.provider,
        sensitivity=cfg.sensitivity,
    )

    # 3) Gate (light validation)
    # - schema sanity, required metadata, forbid secrets/sensitive leakage, etc.
    # - fail closed

    # 4) Transform in work â†’ publish to processed
    processed_assets = transform(raw_file, work_dir=work_dir, out_dir=out_dir)

    # 5) Write evidence triplet (STAC/DCAT/PROV) linking everything
    catalogs.write_stac(cfg, processed_assets)
    catalogs.write_dcat(cfg, processed_assets)
    catalogs.write_prov(cfg, run_id=run_id, raw_inputs=[raw_file], outputs=processed_assets)

    # 6) Run manifest (audit trail + idempotency)
    manifests.write_run_manifest(cfg, run_id=run_id, inputs=[raw_file], outputs=processed_assets)

    telemetry.event(run_id, "run.complete", outputs=len(processed_assets))
```

---

## ğŸŒ Geospatial IO Requirements (So the Map Works)

Pipelines that output geospatial layers should ensure:

- **CRS is explicit** and conversions are **logged** (no silent reprojection).
- Standardize outputs into web-friendly formats:
  - Vector: GeoParquet / GeoJSON (as appropriate) + optional PMTiles for fast web rendering
  - Raster: Cloudâ€‘Optimized GeoTIFF (COG)
  - 3D: Cesium-friendly assets where applicable
- Assets are **discoverable by time** (time slider support needs temporal fields reflected in catalogs)

---

## â›“ï¸ Audit + Provenance: Run Manifests & Determinism

**Run manifests** are your â€œflight recorder.â€ They should include:
- `run_id`, timestamps, pipeline version
- input URIs + checksums
- output paths + checksums
- tool versions
- config snapshot / parameters
- summary stats + error counts
- canonical digest / idempotency key

**Determinism expectations**
- Stable ordering
- Canonical JSON serialization (for stable hashing)
- Atomic writes (avoid half-written artifacts)

---

## ğŸ§¾ Telemetry (Appendâ€‘Only NDJSON)

Telemetry is not â€œdebug logs.â€ Itâ€™s a **ledger**:
- append-only
- machine-readable
- policy-checked when needed
- useful for watcher agents / dashboards

Recommended event fields:
- `ts`, `run_id`, `dataset_id`, `pipeline`, `event`, `level`, `details`, `outcome`

---

## ğŸ” Governance Hooks (Policy Gates)

This folder exists to make policy easy to enforce. Pipelines should be able to prove:

- âœ… license present
- âœ… provenance complete
- âœ… STAC/DCAT/PROV complete
- âœ… sensitivity classification present & respected
- âœ… no secrets committed
- âœ… reproducible run context captured

> ğŸ§± Rule of thumb: **if CI canâ€™t validate it, itâ€™s not a real artifact**.

---

## ğŸ§¨ Common Pitfalls (Avoid These)

- âŒ Writing outputs straight to `data/processed/**` without a run manifest
- âŒ Modifying `data/raw/**` â€œjust to fix a fieldâ€
- âŒ Missing license/provider metadata (â€œweâ€™ll add laterâ€ â†’ governance failure)
- âŒ Silent CRS transforms (distorts reality + breaks trust)
- âŒ â€œOne-off scriptsâ€ that bypass the pipeline contract

---

## ğŸ”— Related KFM Docs (Design Inputs)

These documents define the rules `_shared/io` is enforcing:

- ğŸ“˜ **KFM Master Guide v13** (pipeline ordering, directory layout, evidence triplet)
- ğŸ“¥ **Data Intake Guide** (ingestion gate, checksums, telemetry, deterministic ETL)
- ğŸ§  **AI System Overview** (immutable governance ledger, citations, provenance UX)
- ğŸ—ºï¸ **UI System Overview** (API boundary, provenance panels, layer trust model)
- ğŸ§± **Comprehensive Architecture & Design** (policy gates, fail-closed, W-P-E agents)
- ğŸ§ª **Additional Project Ideas / Future Proposals** (OCI artifacts, offline packs, evidence manifests)
- ğŸŒ **Geospatial / WebGL references** (MapLibre/Cesium patterns, virtual worlds research portfolios)

---

## ğŸ§  Contributing to `_shared/io`

When adding a new I/O helper, ask:

1. **Does it preserve immutability boundaries?**
2. **Does it produce audit artifacts (hashes/manifests/telemetry)?**
3. **Can policy validate it deterministically?**
4. **Does it strengthen cross-layer linkage (catalogs â†” graph â†” UI)?**

If â€œnoâ€ to any: refactor until â€œyes.â€ âœ…
