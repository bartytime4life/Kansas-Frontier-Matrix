# ğŸ§° `_shared/io` â€” Shared Pipeline I/O (Contracts + Utilities)

![Provenance](https://img.shields.io/badge/Provenance-first-brightgreen)
![Contract-first](https://img.shields.io/badge/Contract-first-blue)
![Policy](https://img.shields.io/badge/Policy%20Gates-fail--closed-orange)
![Metadata](https://img.shields.io/badge/STAC%20%7C%20DCAT%20%7C%20PROV-required-purple)
![Determinism](https://img.shields.io/badge/Deterministic%20Pipelines-idempotent-informational)

> **One place for pipeline I/O**: fetch âœ stage âœ hash âœ manifest âœ publish âœ prove.  
> If a pipeline touches bytes, it should do it through this layer (or follow its contracts).

---

## ğŸ¯ Purpose

This directory defines the **canonical** I/O patterns used across KFM pipelines so every artifact is:

- **Auditable** (checksums + run manifests + telemetry)
- **Reproducible** (deterministic outputs from inputs + config)
- **Governable** (policy gates can evaluate predictable artifacts)
- **Linkable** (STAC/DCAT/PROV cross-references power graph, UI, and Focus Mode)

In short: `_shared/io` is how KFM prevents â€œmystery layersâ€ and preserves chain-of-custody endâ€‘toâ€‘end.

---

## ğŸ§­ Scope

âœ… This folder **should** contain:
- Fetchers (HTTP/S3/etc) + cache helpers
- Path builders for KFMâ€™s canonical `data/*` layout
- Atomic writes, hashing, checksums, canonical JSON helpers
- Manifest writers (source + run + distribution)
- Catalog writers/validators hooks (STAC/DCAT/PROV)
- Telemetry / ledger appenders (NDJSON)
- Artifact publishing (filesystem + optional OCI registry)

ğŸš« This folder **should NOT** contain:
- Domain-specific transforms (that lives in each pipeline)
- Business logic for graph ingestion or API services
- UI logic

---

## âœ… Nonâ€‘Negotiable Invariants (aka â€œDonâ€™t Break Trustâ€)

- **Raw is immutable**: `data/raw/**` is **readâ€‘only evidence**. Never modify bytes in place.
- **Transform in staging**: any conversion/cleanup/reprojection happens in `data/work/**`.
- **Publish only from processed**: userâ€‘facing assets live in `data/processed/**`.
- **Evidence triplet is required**: STAC + DCAT + PROV must exist for anything â€œpublishedâ€.
- **Failâ€‘closed governance**: missing license/provenance/required fields must block publishing.
- **UI & Focus Mode only show proven outputs**: if itâ€™s not cataloged + traceable, itâ€™s not real (in KFM terms).

---

## ğŸ—ºï¸ Where `_shared/io` Sits in the Canonical Pipeline

```mermaid
flowchart LR
  A[ğŸŒ External Sources] --> B[ğŸ“¥ data/raw/**]
  B --> C[ğŸ›‚ Ingestion Gate<br/>checksums + sanity + telemetry]
  C --> D[ğŸ§ª data/work/**]
  D --> E[ğŸ“¦ data/processed/**]
  E --> F[ğŸ“š Catalogs<br/>data/stac + data/catalogs + data/prov]
  F --> G[ğŸ•¸ï¸ Graph Ingest<br/>Neo4j metadata + relationships]
  G --> H[ğŸ§  API Layer]
  H --> I[ğŸ—ºï¸ UI + Focus Mode + Story Nodes]
```

> ğŸ’¡ **Key idea:** `_shared/io` is responsible for **B â†’ F** (and producing the artifacts that make **G â†’ I** trustworthy).

---

## ğŸ—‚ï¸ Canonical Data Layout (I/O Responsibilities)

Typical outputs this module must support:

```text
ğŸ“¦ repo/
  â”œâ”€ ğŸ“ data/
  â”‚  â”œâ”€ ğŸ“ raw/                # Immutable evidence (as received)
  â”‚  â”œâ”€ ğŸ“ work/               # Staging + intermediate transforms
  â”‚  â”œâ”€ ğŸ“ processed/          # Standardized + publishable assets
  â”‚  â”œâ”€ ğŸ“ stac/               # STAC collections/items (asset index)
  â”‚  â”œâ”€ ğŸ“ catalogs/           # DCAT datasets (discovery layer)
  â”‚  â”œâ”€ ğŸ“ prov/               # PROV bundles (lineage + agents + params)
  â”‚  â”œâ”€ ğŸ“ audits/             # Run manifests, digests, policy artifacts
  â”‚  â””â”€ ğŸ“ telemetry/          # Append-only NDJSON logs
  â””â”€ ğŸ“ src/pipelines/_shared/io/
```

---

## ğŸ“¦ Core Artifacts `_shared/io` Must Produce

| Artifact | Lives near | Why it exists |
|---|---:|---|
| `source.json` | `data/raw/**` | Who/where/when/license/sensitivity of the bytes you fetched |
| `checksums.sha256` | `data/raw/**` | Tamper evidence + integrity verification |
| `run_manifest.json` | `data/audits/<run_id>/` | Full audit of a run (inputs/outputs/tools/params) + idempotency |
| `telemetry.ndjson` | `data/telemetry/` | Append-only event stream for dashboards/audits/watchers |
| `STAC` JSON | `data/stac/**` | Spatial/temporal index of assets (including â€œwhereâ€™s the file?â€) |
| `DCAT` JSON | `data/catalogs/**` | Discovery metadata (license, keywords, distributions, etc.) |
| `PROV` JSON | `data/prov/**` | Lineage graph (raw âœ work âœ processed) + agents + config |
| `distribution.oci` (optional) | with artifacts | OCI/ORAS publishing metadata (digests, signatures, provenance refs) |
| `evidence.yaml` (optional) | story/artifacts | Structured evidence list for Story Nodes / AI outputs |

---

## ğŸ§© Suggested Module Layout (Implementation Guidance)

> Names are flexible, but responsibilities arenâ€™t. Keep `_shared/io` boring and predictable. âœ…

```text
ğŸ§° src/pipelines/_shared/io/
  â”œâ”€ paths.py            # Canonical path builders (raw/work/processed/stac/dcat/prov/audits)
  â”œâ”€ atomic.py           # Atomic file writes + temp file helpers
  â”œâ”€ hashing.py          # sha256/multihash + canonical JSON helpers
  â”œâ”€ fetch/
  â”‚   â”œâ”€ http.py         # fetch_http(...) with retries + ETag/cache headers
  â”‚   â”œâ”€ s3.py           # fetch_s3(...) (optional)
  â”‚   â””â”€ __init__.py
  â”œâ”€ manifests/
  â”‚   â”œâ”€ source.py       # write_source_json(...)
  â”‚   â”œâ”€ run.py          # write_run_manifest(...) + canonical_digest
  â”‚   â”œâ”€ oci.py          # write_distribution_oci(...) (optional)
  â”‚   â””â”€ __init__.py
  â”œâ”€ catalogs/
  â”‚   â”œâ”€ stac.py         # write_stac_collection/items(...)
  â”‚   â”œâ”€ dcat.py         # write_dcat_dataset(...)
  â”‚   â”œâ”€ prov.py         # write_prov_bundle(...)
  â”‚   â””â”€ __init__.py
  â”œâ”€ telemetry.py        # append_ndjson_event(...)
  â”œâ”€ classification.py   # sensitivity tags + redaction helpers
  â””â”€ __init__.py
```

---

## ğŸ§ª Minimal Pipeline Pattern (How to Use This)

A pipeline should follow this shape (pseudo-code):

```python
from pipelines._shared.io import paths, fetch, hashing, manifests, catalogs, telemetry

def run(cfg):
    run_id = cfg.run_id()

    # 1) Resolve canonical directories
    raw_dir = paths.raw(cfg.domain, cfg.dataset_id)
    work_dir = paths.work(cfg.domain, cfg.dataset_id, run_id=run_id)
    out_dir = paths.processed(cfg.domain, cfg.dataset_id)

    # 2) Fetch to raw (immutable evidence)
    telemetry.event(run_id, "fetch.start", url=cfg.source_url)
    raw_file = fetch.http(cfg.source_url, dest_dir=raw_dir)
    hashing.write_checksums_sha256(raw_dir)

    manifests.write_source_json(
        raw_dir,
        source_url=cfg.source_url,
        license=cfg.license,
        provider=cfg.provider,
        sensitivity=cfg.sensitivity,
    )

    # 3) Gate (light validation)
    # - schema sanity, required metadata, forbid secrets/sensitive leakage, etc.
    # - fail closed

    # 4) Transform in work â†’ publish to processed
    processed_assets = transform(raw_file, work_dir=work_dir, out_dir=out_dir)

    # 5) Write evidence triplet (STAC/DCAT/PROV) linking everything
    catalogs.write_stac(cfg, processed_assets)
    catalogs.write_dcat(cfg, processed_assets)
    catalogs.write_prov(cfg, run_id=run_id, raw_inputs=[raw_file], outputs=processed_assets)

    # 6) Run manifest (audit trail + idempotency)
    manifests.write_run_manifest(cfg, run_id=run_id, inputs=[raw_file], outputs=processed_assets)

    telemetry.event(run_id, "run.complete", outputs=len(processed_assets))
```

---

## ğŸŒ Geospatial IO Requirements (So the Map Works)

Pipelines that output geospatial layers should ensure:

- **CRS is explicit** and conversions are **logged** (no silent reprojection).
- Standardize outputs into web-friendly formats:
  - Vector: GeoParquet / GeoJSON (as appropriate) + optional PMTiles for fast web rendering
  - Raster: Cloudâ€‘Optimized GeoTIFF (COG)
  - 3D: Cesium-friendly assets where applicable
- Assets are **discoverable by time** (time slider support needs temporal fields reflected in catalogs)

---

## â›“ï¸ Audit + Provenance: Run Manifests & Determinism

**Run manifests** are your â€œflight recorder.â€ They should include:
- `run_id`, timestamps, pipeline version
- input URIs + checksums
- output paths + checksums
- tool versions
- config snapshot / parameters
- summary stats + error counts
- canonical digest / idempotency key

**Determinism expectations**
- Stable ordering
- Canonical JSON serialization (for stable hashing)
- Atomic writes (avoid half-written artifacts)

---

## ğŸ§¾ Telemetry (Appendâ€‘Only NDJSON)

Telemetry is not â€œdebug logs.â€ Itâ€™s a **ledger**:
- append-only
- machine-readable
- policy-checked when needed
- useful for watcher agents / dashboards

Recommended event fields:
- `ts`, `run_id`, `dataset_id`, `pipeline`, `event`, `level`, `details`, `outcome`

---

## ğŸ” Governance Hooks (Policy Gates)

This folder exists to make policy easy to enforce. Pipelines should be able to prove:

- âœ… license present
- âœ… provenance complete
- âœ… STAC/DCAT/PROV complete
- âœ… sensitivity classification present & respected
- âœ… no secrets committed
- âœ… reproducible run context captured

> ğŸ§± Rule of thumb: **if CI canâ€™t validate it, itâ€™s not a real artifact**.

---

## ğŸ§¨ Common Pitfalls (Avoid These)

- âŒ Writing outputs straight to `data/processed/**` without a run manifest
- âŒ Modifying `data/raw/**` â€œjust to fix a fieldâ€
- âŒ Missing license/provider metadata (â€œweâ€™ll add laterâ€ â†’ governance failure)
- âŒ Silent CRS transforms (distorts reality + breaks trust)
- âŒ â€œOne-off scriptsâ€ that bypass the pipeline contract

---

## ğŸ”— Related KFM Docs (Design Inputs)

These documents define the rules `_shared/io` is enforcing:

- ğŸ“˜ **KFM Master Guide v13** (pipeline ordering, directory layout, evidence triplet)
- ğŸ“¥ **Data Intake Guide** (ingestion gate, checksums, telemetry, deterministic ETL)
- ğŸ§  **AI System Overview** (immutable governance ledger, citations, provenance UX)
- ğŸ—ºï¸ **UI System Overview** (API boundary, provenance panels, layer trust model)
- ğŸ§± **Comprehensive Architecture & Design** (policy gates, fail-closed, W-P-E agents)
- ğŸ§ª **Additional Project Ideas / Future Proposals** (OCI artifacts, offline packs, evidence manifests)
- ğŸŒ **Geospatial / WebGL references** (MapLibre/Cesium patterns, virtual worlds research portfolios)

---

## ğŸ§  Contributing to `_shared/io`

When adding a new I/O helper, ask:

1. **Does it preserve immutability boundaries?**
2. **Does it produce audit artifacts (hashes/manifests/telemetry)?**
3. **Can policy validate it deterministically?**
4. **Does it strengthen cross-layer linkage (catalogs â†” graph â†” UI)?**

If â€œnoâ€ to any: refactor until â€œyes.â€ âœ…