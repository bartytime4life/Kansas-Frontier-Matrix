# ğŸ§ª `src/pipelines/` â€” ETL + Simulations (Provenance-First)

![Pipelines](https://img.shields.io/badge/KFM-pipelines-2b6cb0)
![Deterministic](https://img.shields.io/badge/runs-deterministic-success)
![Idempotent](https://img.shields.io/badge/runs-idempotent-success)
![Provenance](https://img.shields.io/badge/provenance-PROV%20%2B%20STAC%20%2B%20DCAT-important)

> [!NOTE]
> This folder is the **data refinery** for Kansas Frontier Matrix (KFM): it turns **raw evidence** into **curated datasets** + **metadata catalogs** + **lineage logs**, so downstream layers (DB â†’ API â†’ UI â†’ Story Nodes â†’ Focus Mode) can trust what theyâ€™re showing.

---

## ğŸ§­ What lives here?

`src/pipelines/` contains offline / batch-oriented code that:
- ğŸ§² **Ingests** source material (CSVs, shapefiles, rasters, PDFs, scans, etc.)
- ğŸ§¼ **Normalizes & transforms** into stable, reviewable outputs
- ğŸ§¾ Emits the **boundary artifacts** that â€œpublishâ€ a dataset into KFM:
  - ğŸ—ºï¸ **STAC** (items + collections)
  - ğŸ“š **DCAT** (dataset-level catalog record)
  - ğŸ§¬ **PROV** (lineage bundle)
- ğŸ§± Optionally **loads/indexes** into databases (PostGIS / Neo4j) *after* the artifacts exist
- ğŸš« **Never** relies on manual edits of processed outputs

---

## ğŸ”’ Core invariants (nonâ€‘negotiable)

### 1) ğŸ§± Pipeline ordering is absolute
KFM data moves in this order, always:

```
Raw â†’ Processed â†’ Catalogs/PROV â†’ Database â†’ API â†’ UI â†’ Story Nodes â†’ Focus Mode
```

If youâ€™re tempted to â€œskipâ€ a stage (e.g., push directly into UI), youâ€™re probably breaking the system contract.

### 2) ğŸ” Deterministic + idempotent
A pipeline must:
- produce the **same outputs** given the same inputs + config
- be **safe to re-run** without duplicating/creating new copies of the same dataset

### 3) ğŸ§¬ Provenance first
Nothing is â€œpublishedâ€ into KFM without:
- descriptive metadata (STAC/DCAT)
- lineage (PROV)

### 4) ğŸš« No adâ€‘hoc edits
If something is wrong in `data/processed/â€¦`, the fix is:
âœ… update pipeline and re-run  
âŒ do *not* hand-edit the output file

---

## ğŸ—ºï¸ System flow (high-level)

```mermaid
flowchart LR
  A["ğŸ“¥ Raw Sources"] --> B["ğŸ§ª ETL + Normalization (pipelines)"]
  B --> C["ğŸ—ºï¸ STAC Items + Collections"]
  C --> D["ğŸ“š DCAT Dataset Views"]
  C --> E["ğŸ§¬ PROV Lineage Bundles"]
  C --> G["ğŸ§  Neo4j Graph (references catalogs)"]
  G --> H["ğŸ§± API Layer (contracts + redaction)"]
  H --> I["ğŸ—ºï¸ Map UI (React Â· MapLibre Â· optional Cesium)"]
  I --> J["ğŸ“ Story Nodes (governed narratives)"]
  J --> K["ğŸ¯ Focus Mode (provenance-linked context bundle)"]
```

---

## ğŸ“¦ Data staging contract (where pipelines read/write)

> [!TIP]
> The **folder is the API**. Keep stages clean, predictable, and reviewable.

### Required staging layout

| Stage | Folder | Rules |
|------:|--------|-------|
| ğŸ“¥ Raw (immutable) | `data/raw/<domain>/â€¦` | *Write once, then read-only.* Pipelines **must not** modify raw files. |
| ğŸ§° Work (scratch) | `data/work/<domain>/â€¦` | Intermediate artifacts okay. Safe to delete/regenerate. |
| âœ… Processed (final) | `data/processed/<domain>/â€¦` | Only pipeline outputs. No manual edits. Reviewable diffs. |

### Required publication artifacts (boundary artifacts)

| Artifact | Folder | Why it exists |
|---------|--------|---------------|
| ğŸ—ºï¸ STAC Collections | `data/stac/collections/â€¦` | Collection-level metadata |
| ğŸ—ºï¸ STAC Items | `data/stac/items/â€¦` | Item-level spatial/temporal metadata + asset links |
| ğŸ“š DCAT | `data/catalog/dcat/â€¦` | Dataset discovery layer |
| ğŸ§¬ PROV | `data/prov/â€¦` *(or `data/provenance/â€¦` in some layouts)* | Full lineage + agents + parameters |

> [!IMPORTANT]
> A dataset isnâ€™t â€œpublishedâ€ until **processed data + STAC + DCAT + PROV** exist.

---

## ğŸ§© Plugin mindset (how pipelines should scale)

KFMâ€™s pipeline system is designed to grow via **drop-in modules**:
- Create a new pipeline file/module
- Follow conventions
- Register it (manifest/registry)
- Runner discovers + executes it

This keeps contributions isolated and reviewable.

---

## ğŸ—‚ï¸ Suggested folder layout (recommended)

> [!NOTE]
> This is a **recommended** structure to keep things consistent as the repo scales. Adjust to match the actual implementation, but keep the ideas.

```text
ğŸ“ src/
â””â”€ ğŸ“ pipelines/                                ğŸ—ï¸ ETL + publishing pipelines (raw â†’ processed â†’ STAC/DCAT/PROV)
   â”œâ”€ ğŸ“„ README.md                               ğŸ“˜ overview, conventions, and how to run pipelines
   â”œâ”€ ğŸ§­ runner.py                               ğŸš€ orchestrates one/all pipelines (CLI entry)
   â”œâ”€ ğŸ§¾ manifest.yaml                           âœ… pipeline registry (plugin discovery + enable/disable)
   â”‚
   â”œâ”€ ğŸ“ lib/                                    ğŸ§° shared pipeline library (reusable primitives)
   â”‚  â”œâ”€ ğŸ§© io.py                                 ğŸ“¥ğŸ“¤ common readers/writers (files, cloud, db adapters)
   â”‚  â”œâ”€ ğŸ” hashing.py                            ğŸ§¬ checksums + content addressing + manifests
   â”‚  â”œâ”€ ğŸªµ logging.py                            ğŸ§¾ run logs + structured logging helpers
   â”‚  â”œâ”€ ğŸ›°ï¸ stac.py                               ğŸ›°ï¸ STAC emit helpers (collections/items/assets)
   â”‚  â”œâ”€ ğŸ—‚ï¸ dcat.py                               ğŸ§¾ DCAT emit helpers (JSON-LD dataset/distributions)
   â”‚  â”œâ”€ ğŸ§¬ prov.py                               ğŸ§¬ PROV emit helpers (activities/agents/entities)
   â”‚  â””â”€ âœ… validate.py                            ğŸ›¡ï¸ validation gates (schemas/profiles/policy hooks)
   â”‚
   â”œâ”€ ğŸ“ domains/                                ğŸ§© domain pipelines (one responsibility each)
   â”‚  â””â”€ ğŸ“ <domain_name>/                        ğŸ§­ e.g., hydrology/, historical/, climate/, transportation/
   â”‚     â”œâ”€ ğŸ§© pipeline.py                        ğŸ—ï¸ pipeline entry (one dataset or dataset family)
   â”‚     â”œâ”€ ğŸ§¾ config.example.yaml                âš™ï¸ example config (safe defaults; no secrets)
   â”‚     â””â”€ ğŸ“ tests/                             ğŸ§ª domain-specific tests (unit/integration as needed)
   â”‚
   â””â”€ ğŸ“ simulations/                            ğŸ§ª deterministic sims (fixed seeds, reproducible runs)
      â””â”€ ğŸ§ª climate_scenario.py                   ğŸŒ¦ï¸ scenario generator (deterministic; versionable outputs)
    ...
```

---

## ğŸƒ Running pipelines (local/dev)

Because pipelines must be **non-interactive**, the usual execution pattern is:

1) âœ… Ensure raw inputs exist under `data/raw/<domain>/â€¦`  
2) ğŸ§ª Run pipeline(s)  
3) ğŸ§¾ Verify boundary artifacts exist (STAC/DCAT/PROV)  
4) ğŸ§± (Optional) Load/index to DB  
5) ğŸ§± Validate API/UI can consume it

### Example CLI patterns (adapt to the repoâ€™s runner)

```bash
# Run one pipeline by id/name
python -m src.pipelines.runner --pipeline census_1900 --config configs/dev.yaml

# Run all registered pipelines
python -m src.pipelines.runner --all --config configs/dev.yaml

# Dry-run (compute what would change)
python -m src.pipelines.runner --pipeline landsat --dry-run
```

> [!TIP]
> Pipelines should detect â€œno changesâ€ (e.g., via checksums/version checks) and exit cleanly without duplicating outputs.

---

## âœï¸ Adding a new pipeline (checklist)

### âœ… Minimum steps
- [ ] Create a raw staging folder: `data/raw/<new-domain>/`
- [ ] Implement pipeline module/script under `src/pipelines/domains/<new-domain>/`
- [ ] Ensure it writes:
  - [ ] intermediates to `data/work/<new-domain>/`
  - [ ] finals to `data/processed/<new-domain>/`
- [ ] Emit boundary artifacts:
  - [ ] STAC Collection + Item(s)
  - [ ] DCAT dataset entry
  - [ ] PROV lineage bundle
- [ ] Register the pipeline (manifest/registry)
- [ ] Add a small domain runbook: `docs/data/<new-domain>/README.md`
- [ ] Add tests (even small â€œsmoke testsâ€)
- [ ] Confirm re-runs are deterministic + idempotent

---

## ğŸ§¬ Evidence artifacts (AI / analysis outputs)

If a pipeline produces â€œderivedâ€ outputs (OCR text, model predictions, simulations, inferred layers), treat them as **first-class datasets**:

- âœ… store in `data/processed/â€¦`
- âœ… catalog in STAC/DCAT
- âœ… trace in PROV (include method + parameters + confidence)
- âœ… integrate with graph carefully (explicit provenance pointers)
- âœ… expose only through governed APIs (never hard-code into UI)

---

## ğŸ§¾ Metadata rules of thumb

### STAC
- Must link to the actual processed asset(s)
- Must include attribution + license info
- Must be spatial/temporal honest (bbox/date range)

### DCAT
- Must make dataset discoverable (title/desc/keywords/license)
- Should link to STAC or direct distributions

### PROV
- Must capture:
  - input entities (raw files + checksums/URLs)
  - activities (pipeline run + timestamps + params)
  - agents (script version + runner identity)
- Should support auditability (â€œwhat produced this?â€)

---

## ğŸ§¯ Troubleshooting quick hits

- ğŸ§± **Docker volumes & permissions**: if containers canâ€™t write to `data/`, fix mount permissions or container user mapping.
- ğŸ”Œ **Port conflicts**: if Postgres/Neo4j ports are already in use, change compose mappings.
- ğŸ˜ **Big datasets**: prefer LFS / external blobs + checksums where needed, but keep identities tracked in Git.

---

## ğŸ§  PR review rubric (for pipelines)

> [!IMPORTANT]
> If a PR adds/changes processed data, reviewers should demand:
- [ ] deterministic reruns
- [ ] no raw edits
- [ ] STAC/DCAT/PROV present + consistent
- [ ] versioning/identity strategy (hashes/ids)
- [ ] docs/runbook updated
- [ ] validation gates pass (schemas/profiles)

---

## ğŸ”— Helpful pointers
- `data/` is the single place to understand **what stage a file is in**
- `docs/standards/` should define profiles (STAC/DCAT/PROV)
- `docs/templates/` should contain templates for new datasets + Story Nodes
- Keep pipelines boring, repeatable, and auditable âœ…
