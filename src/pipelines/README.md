<div align="center">

# ğŸ§© Kansas Frontier Matrix â€” Data Pipelines  
`src/pipelines/README.md`

**Time Â· Terrain Â· History Â· Knowledge Graphs**

[![Build & Deploy](https://github.com/bartytime4life/Kansas-Frontier-Matrix/actions/workflows/site.yml/badge.svg)](../../.github/workflows/site.yml)
[![STAC Validate](https://github.com/bartytime4life/Kansas-Frontier-Matrix/actions/workflows/stac-validate.yml/badge.svg)](../../.github/workflows/stac-validate.yml)
[![CodeQL](https://github.com/bartytime4life/Kansas-Frontier-Matrix/actions/workflows/codeql.yml/badge.svg)](../../.github/workflows/codeql.yml)
[![Trivy Security](https://github.com/bartytime4life/Kansas-Frontier-Matrix/actions/workflows/trivy.yml/badge.svg)](../../.github/workflows/trivy.yml)
[![Docs Â· MCP](https://img.shields.io/badge/Docs-MCP-blue)](../../docs/)
[![License](https://img.shields.io/badge/License-MIT-green)](../../LICENSE)

</div>

---

## ğŸ¯ Purpose

The `src/pipelines/` module houses the **ETL (Extract-Transform-Load)** and **AI/ML enrichment** pipelines that power the Kansas Frontier Matrix (KFM).  
These scripts transform heterogeneous Kansas datasetsâ€”maps, climate series, land records, and historical textsâ€”into standardized, provenance-tracked layers ready for:

- ğŸ—º **Spatiotemporal mapping** (GeoJSON + COG GeoTIFF)  
- ğŸ§  **Knowledge-graph ingestion** (Neo4j / RDF)  
- âš™ï¸ **Semantic linking** via CIDOC CRM + OWL-Time  
- ğŸ” **Interactive discovery** through the web UI and API  

The directory implements MCPâ€™s *documentation-first* and *reproducibility* principles: every script logs inputs, outputs, parameters, and checksums.

---

## ğŸ— Pipeline Architecture

```mermaid
flowchart TD
    A["Raw Sources (scans Â· rasters Â· vectors Â· texts)"]
      --> B["Extract â†’ download / fetch / OCR"]
      --> C["Transform â†’ clean Â· normalize Â· geocode"]
      --> D["Load â†’ GeoJSON Â· COG Â· Parquet Â· Graph DB"]

    C --> I["AI / ML Enrichment (NER Â· summarization Â· linking)"]
    I --> H["Knowledge Graph (Neo4j Â· CIDOC CRM Â· OWL-Time)"]
    D --> H
    H --> J["API Layer â†’ FastAPI / GraphQL"]
    J --> F["Frontend â†’ React / MapLibre timeline map"]
````

<!-- END OF MERMAID -->

Each ETL stage is modular and reproducible:

| Stage                | Description                                                                                                        | Key Tools                                               |
| :------------------- | :----------------------------------------------------------------------------------------------------------------- | :------------------------------------------------------ |
| **Extract**          | Fetch raw data from external APIs or archives (USGS, NOAA, FEMA, KHS, etc.).  Handles OCR for scanned PDFs / maps. | `requests`, `aiohttp`, `pytesseract`, `pdfplumber`      |
| **Transform**        | Standardize formats â†’ GeoJSON, CSV, COG; clean fields; geocode places; normalize dates.                            | `pandas`, `geopandas`, `rasterio`, `rio-cogeo`, `geopy` |
| **Load**             | Upsert processed outputs to data/processed and Neo4j graph; generate STAC items + checksums.                       | `neo4j-driver`, `jsonschema`, `hashlib`                 |
| **AI/ML Enrichment** | Apply NLP (entity extraction, summarization, fuzzy linking) and cross-source correlation.                          | `spaCy`, `transformers`, `sentence-transformers`        |

---

## ğŸ“‚ Directory Layout

```
src/pipelines/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ fetch/                 # data-source connectors (USGS, NOAA, FEMA, KHS etc.)
â”‚   â”œâ”€â”€ noaa_ingest.py
â”‚   â”œâ”€â”€ usgs_ingest.py
â”‚   â”œâ”€â”€ fema_ingest.py
â”‚   â””â”€â”€ kansas_memory_ingest.py
â”œâ”€â”€ transform/             # normalization & conversion utilities
â”‚   â”œâ”€â”€ geocode_utils.py
â”‚   â”œâ”€â”€ raster_to_cog.py
â”‚   â”œâ”€â”€ vector_to_geojson.py
â”‚   â””â”€â”€ text_cleaner.py
â”œâ”€â”€ enrich/                # AI / ML processing modules
â”‚   â”œâ”€â”€ nlp_entities.py
â”‚   â”œâ”€â”€ summarizer.py
â”‚   â”œâ”€â”€ entity_linker.py
â”‚   â””â”€â”€ correlate_sources.py
â”œâ”€â”€ load/                  # loaders â†’ Neo4j & STAC
â”‚   â”œâ”€â”€ stac_writer.py
â”‚   â”œâ”€â”€ graph_loader.py
â”‚   â””â”€â”€ checksum_utils.py
â””â”€â”€ pipeline_runner.py     # orchestrator Â· Makefile entrypoint
```

Each submodule can be executed standalone for testing, or orchestrated via the project-level `Makefile`:

```bash
# Rebuild all pipelines
make pipelines

# Or run a single stage
python src/pipelines/fetch/noaa_ingest.py --year 2020
```

---

## ğŸ”¬ AI / ML Workflow Summary

1. **Named Entity Recognition (NER)** â€“ `spaCy` model fine-tuned on Kansas texts to extract `PERSON`, `PLACE`, `DATE`, `EVENT`.
2. **Geocoding** â€“ Resolve place names via USGS GNIS API; attach lat/long & county context.
3. **Summarization** â€“ `BART-large-CNN` / `T5` models create concise summaries for documents and site dossiers.
4. **Entity Linking & Scoring** â€“ Fuzzy matching + context windowing align mentions to canonical graph nodes with confidence scores.
5. **Cross-Modal Correlation** â€“ Compare text vs map vs sensor data to flag verified historical changes (e.g., floodplain shift).

---

## ğŸ§± Data Outputs & Standards

| Output          | Format                  | Destination               | Standard                  |
| :-------------- | :---------------------- | :------------------------ | :------------------------ |
| Raster Layers   | Cloud-Optimized GeoTIFF | `data/processed/rasters/` | STAC 1.0 Â· OGC COG        |
| Vector Layers   | GeoJSON / TopoJSON      | `data/processed/vectors/` | GeoJSON 1.0               |
| Tables          | CSV / Parquet           | `data/processed/tables/`  | CSVW Â· DCAT               |
| Knowledge Graph | Neo4j DB                | `graph/`                  | CIDOC CRM Â· OWL-Time      |
| Metadata        | JSON                    | `data/stac/`              | STAC Â· schema.org Dataset |

All assets include **SHA-256** checksums and provenance metadata (source URL, license, ETL timestamp).

---

## ğŸ§° Configuration & Logging

* **Config** files live in `config/` (YAML/JSON) defining paths, API keys, CRS codes, and STAC templates.
* **Logging** uses Pythonâ€™s `logging` module with rotating file handlers â†’ `logs/pipelines/*.log`.
* **Error Handling** wraps each task with try/except to ensure a failure in one source does not halt the entire ETL.

Example log line:

```
[2025-10-05 14:22:01] INFO | usgs_ingest | Fetched 12 new DEM tiles (2.3 GB) in 214 s âœ”
```

---

## ğŸ” Reproducibility Â· CI/CD Integration

* **Makefile** targets link directly into GitHub Actions: every commit runs `make validate` and `make stac-check`.
* **STAC Validation** ensures metadata compliance before merge.
* **Containerized runs** (Docker + Compose) guarantee identical environments across contributors.
* **DVC integration** (coming soon) adds data version hashes to each ETL artifact.

---

## ğŸ“˜ Developer Notes

* Follow PEP-8 style and Google-style docstrings for functions and modules.
* Each script begins with a `@MCP-LOG` header summarizing its purpose, inputs, outputs, and expected runtime.
* Add unit tests in `tests/pipelines/` whenever you extend a module.
* When introducing a new data source, create a matching manifest under `data/sources/` and cite its license and URL.

---

## ğŸ“ˆ Example Workflow

```bash
# 1 Â· Fetch raw data from registered sources
make fetch

# 2 Â· Process raster + vector data â†’ standard formats
make process

# 3 Â· Run AI enrichment (NER + summary)
make enrich

# 4 Â· Update STAC catalog and load graph database
make stac graph
```

Each step logs provenance â†’ `logs/pipelines/` and outputs reproducible artifacts in `data/processed/`.

---

## ğŸ“ References & Further Reading

* *Kansas Frontier Matrix â€” AI System Developer Docs*
* *File & Data Architecture Guide*
* *Scientific Modeling & Simulation (NASA-Grade Guide)*
* *Integrating Historical, Cartographic, and Geological Research (MCP Reference)*

---

<div align="center">

**Kansas Frontier Matrix Â© 2025 Â· Open Knowledge Â· MIT License (code) | CC-BY 4.0 (data)**
*â€œDocument the past so the future can reproduce it.â€*

</div>

