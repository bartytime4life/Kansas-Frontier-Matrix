# ğŸ§ª KFM Pipelines (`src/pipelines/`) â€” Evidence Engine âš™ï¸ğŸ—ºï¸

![KFM](https://img.shields.io/badge/KFM-Kansas%20Frontier%20Matrix-6e40c9)
![Contract-First](https://img.shields.io/badge/contract--first-schemas%20%2B%20profiles-blueviolet)
![Evidence](https://img.shields.io/badge/evidence-STAC%20%2B%20DCAT%20%2B%20PROV-success)
![Determinism](https://img.shields.io/badge/determinism-idempotent%20%2B%20manifest--driven-2ea44f)
![Orchestration](https://img.shields.io/badge/orchestration-WPE%20(Watcher%E2%86%92Planner%E2%86%92Executor)-orange)
![Policy](https://img.shields.io/badge/policy-OPA%20%2F%20Conftest-blue)
![Storage](https://img.shields.io/badge/storage-PostGIS%20%2B%20Neo4j%20%2B%20Search-informational)
![Packaging](https://img.shields.io/badge/packaging-GeoParquet%20%2B%20PMTiles%20%2B%20COG-yellowgreen)
![UI](https://img.shields.io/badge/ui-React%20%2B%20MapLibre%20%2B%20Cesium-lightgrey)

> Pipelines are KFMâ€™s **ingestion + ETL + enrichment + packaging + publishing** engine.  
> They turn messy sources into **versioned, queryable, map-ready evidence** â€” and keep every claim traceable back to its origin ğŸ”ğŸ§¾  
>
> **Rule:** *Nothing enters KFM without metadata.* âœ…

---

## ğŸš¦ Quick navigation

- [âœ¨ What belongs here](#-what-belongs-here)
- [ğŸ§­ Operating principles](#-operating-principles)
- [ğŸ—ºï¸ End-to-end flow](#ï¸-end-to-end-flow)
- [ğŸ§± Core contracts](#-core-contracts)
  - [Evidence triplet](#1-evidence-triplet-stac--dcat--prov-)
  - [Data lifecycle + canonical layout](#2-data-lifecycle--canonical-layout-)
  - [RunContext + Run Manifest](#3-runcontext--run-manifest-)
  - [Receipts + checksums](#4-receipts--checksums-)
  - [Determinism + idempotency](#5-determinism--idempotency-)
  - [Clean Architecture boundaries](#6-clean-architecture-boundaries-)
  - [Packaging patterns](#7-packaging-patterns-)
  - [Graph + search ingestion](#8-graph--search-ingestion-)
  - [Artifact registry + signing](#9-artifact-registry--signing-)
- [ğŸ“ Repo touchpoints](#-repo-touchpoints)
- [â–¶ï¸ Running pipelines](#ï¸-running-pipelines)
- [ğŸ§° Creating a new pipeline](#-creating-a-new-pipeline)
- [âœ… Validation & policy gates](#-validation--policy-gates)
- [ğŸ§© Pipeline types](#-pipeline-types)
- [ğŸ“ˆ Ops: observability, backfills, drift](#-ops-observability-backfills-drift)
- [ğŸ” Security, privacy, ethics](#-security-privacy-ethics)
- [âœ… Definition of Done](#-definition-of-done)
- [ğŸ“š References](#-references)
- [ğŸ“– Glossary](#-glossary)

---

## âœ¨ What belongs here

`src/pipelines/` is the home for KFMâ€™s **evidence-producing jobs** â€” from raw intake to published artifacts and indexes.

### ğŸ§± Pipeline families

- **Ingestion pipelines** ğŸ“¥  
  Fetch â†’ verify â†’ stage immutable raw drops â†’ prepare standardized â€œworkâ€ intermediates â†’ publish curated outputs.

- **Validation & contract enforcement** âœ…  
  Schema checks, metadata completeness, CRS + bounds sanity, licensing/attribution, sensitivity tags, FAIR+CARE fields.

- **Enrichment pipelines** ğŸ§   
  OCR/NLP parsing, georeferencing, entity extraction, linking, ontology alignment (e.g., cultural heritage event/place modeling).

- **Packaging pipelines** ğŸ“¦  
  Produce **map-ready** + **analysis-ready** artifacts together (e.g., **PMTiles + GeoParquet**, COGs, simplified GeoJSON, 3D-ready assets).

- **Graph + search pipelines** ğŸ•¸ï¸ğŸ”  
  Deterministically ingest catalogs + PROV into the knowledge graph; maintain constraints; update/search-index unstructured text.

- **Narrative pipelines** ğŸ§µ  
  Story Nodes & â€œPulseâ€ updates that are *still evidence-first* (narratives are versioned artifacts with provenance).

- **AI support pipelines** ğŸ¤–ğŸ§­  
  Build retrieval indexes, enforce citation requirements, store governance logs, run drift/bias checks, manage local model runtime hooks (e.g., Ollama).

> [!IMPORTANT]
> **Pipelines are contract-first.** If it canâ€™t be validated, cataloged, and cited â€” it doesnâ€™t ship. ğŸš«ğŸ“¦

---

## ğŸ§­ Operating principles

### âœ… Contract-first
Schemas, profiles, and API contracts are first-class repo artifacts. Any change triggers versioning + compatibility checks.

### âœ… Evidence-first
Every published dataset or derived asset must have its **evidence triplet**: **STAC + DCAT + PROV**.

### âœ… Deterministic-by-default
Same inputs + same config + same code âœ same outputs (same hashes). Pipelines are re-runnable and auditable.

### âœ… Governed by design
Governance is enforced with **policy-as-code** (OPA/Rego + Conftest) and explicit waiver workflows when needed.

### âœ… Clean Architecture separation
Pipeline logic stays testable and portable by depending on interfaces/contracts â€” not specific infrastructure details.

---

## ğŸ—ºï¸ End-to-end flow

```mermaid
flowchart LR
  S[ğŸ“¡ Sources<br/>(APIs, files, scans, sensors)] --> W[ğŸ‘€ Watcher<br/>detect change]
  W --> P[ğŸ§­ Planner<br/>decide runs + deltas]
  P --> X[âš™ï¸ Executor<br/>run steps + open PRs if needed]

  X --> R[ğŸ“¥ Raw Intake<br/>immutable + receipts]
  R --> V[âœ… Validate<br/>contracts + policy]
  V --> T[ğŸ§ª Transform<br/>standardize + normalize]
  T --> PUB[ğŸ“¤ Publish<br/>artifacts + checksums]

  PUB --> META[ğŸ§¾ Metadata Publish<br/>STAC + DCAT + PROV]
  META --> G[ğŸ•¸ï¸ Graph Ingest<br/>Neo4j]
  PUB --> PG[ğŸ—„ï¸ Load Spatial Store<br/>PostGIS]
  PUB --> SI[ğŸ” Search Index<br/>Elastic/alt]

  G --> API[ğŸ”Œ Governed API<br/>GraphQL + REST]
  PG --> API
  SI --> API

  API --> UI[ğŸ—ºï¸ UI Layer<br/>MapLibre + (opt) Cesium]
  API --> FM[ğŸ¤– Focus Mode<br/>retrieval + citations]
  PUB --> OFF[ğŸ“¦ Offline Packs<br/>PMTiles/MBTiles + bundles]
  OFF --> UI

  PUB --> REG[ğŸ“¦ Artifact Registry<br/>OCI/ORAS + signatures]
  META --> REG
```

**Mental model:** pipelines create *evidence artifacts* âœ catalogs + provenance âœ graph/spatial/search stores âœ governed API âœ UI + Focus Mode.

> [!NOTE]
> The UI is **read-only** relative to the pipeline layer: it cannot bypass the API or â€œwrite dataâ€ directly. Any new data, story, or model output enters through governed pipelines and version control.

---

## ğŸ§± Core contracts

### 1) Evidence triplet (STAC + DCAT + PROV) ğŸ§¾

Every â€œpublishedâ€ pipeline output should emit:

- **STAC** ğŸ—‚ï¸ â€” spatial/temporal assets (items + collections): vectors, rasters, tiles, time series  
- **DCAT** ğŸ·ï¸ â€” dataset discovery metadata: publisher, license, access links, keywords, distributions  
- **PROV** ğŸ§¬ â€” lineage bundle: inputs â†’ activities (runs/steps) â†’ outputs, with agents and parameters

> [!TIP]
> Treat STAC/DCAT/PROV as **boundary artifacts** between pipelines and downstream systems (graph, API, UI, Focus Mode).  
> They are the *interface*, not an afterthought.

âœ… **FAIR + CARE**  
Metadata should carry FAIR *and* CARE considerations (e.g., sovereignty, ethics, authority-to-control) as explicit fields â€” not buried in prose.

---

### 2) Data lifecycle + canonical layout ğŸ—ƒï¸

KFM follows a staged lifecycle:

| Stage | Folder | Rule | Typical contents |
|---|---|---|---|
| **Raw** | `data/raw/<domain>/` | Immutable | source drops, receipts, source manifests |
| **Work** | `data/work/<domain>/` | Replaceable | scratch intermediates, temp conversions |
| **Processed** | `data/processed/<domain>/` | Versioned | curated datasets, tiles, bundles |
| **STAC** | `data/stac/collections/` + `data/stac/items/` | Required at publish | STAC JSON artifacts |
| **DCAT** | `data/catalog/dcat/` | Required at publish | DCAT JSON-LD datasets |
| **PROV** | `data/prov/` | Required at publish | provenance bundles |
| **Audits** | `data/audits/<pipeline>/` | Required | run manifests, policy results, telemetry |

> [!IMPORTANT]
> A dataset is not â€œpublishedâ€ in KFM until it has **both** the data artifacts *and* the evidence triplet (STAC/DCAT/PROV).

---

### 3) RunContext + Run Manifest ğŸ”

**RunContext** is the in-memory â€œpassportâ€ (config + run metadata) that flows through steps.

**Run Manifest** is the persisted audit record:

- pipeline name + pipeline version (git SHA / release tag)
- run id + mode (incremental/full/backfill/streaming)
- inputs (URIs, receipts, checksums)
- transforms (parameters, CRS decisions, normalization rules)
- outputs (paths + checksums)
- metadata pointers (STAC/DCAT/PROV locations)
- policy decisions (pass/fail + waivers)
- telemetry pointers (NDJSON logs)

<details>
<summary><strong>ğŸ“„ Example Run Manifest (illustrative)</strong></summary>

```yaml
pipeline: hydro_usgs_waterwatch
pipeline_version: "git:abcd1234"
run:
  run_id: "2026-01-23T18:01:22Z__hydro_usgs_waterwatch__sha256:9f3c..."
  started_at: "2026-01-23T18:01:22Z"
  mode: "incremental"   # full | incremental | backfill | streaming
inputs:
  - uri: "https://example.gov/usgs/waterwatch.csv"
    receipt:
      fetched_at: "2026-01-23T18:01:30Z"
      etag: "\"a1b2c3\""
      last_modified: "Tue, 21 Jan 2026 11:00:00 GMT"
    sha256: "..."
params:
  canonical_crs: "EPSG:4326"
  projected_crs_for_metrics: "EPSG:26914" # example: meters for buffering
  spatial_join: true
outputs:
  processed:
    - path: "data/processed/hydro/usgs_waterwatch/2026-01-23/waterwatch.parquet"
      sha256: "..."
  artifacts:
    - path: "data/processed/hydro/usgs_waterwatch/2026-01-23/checksums.sha256"
    - path: "data/audits/hydro_usgs_waterwatch/2026-01-23/run_manifest.yaml"
catalogs:
  stac_collection: "data/stac/collections/hydro_usgs_waterwatch.json"
  stac_items: "data/stac/items/hydro_usgs_waterwatch/2026-01-23/*.json"
  dcat: "data/catalog/dcat/hydro_usgs_waterwatch.dataset.jsonld"
  prov: "data/prov/hydro_usgs_waterwatch/run_2026-01-23.prov.json"
policy:
  status: "pass"
  waivers: []
telemetry:
  ndjson: "data/audits/hydro_usgs_waterwatch/2026-01-23/telemetry.ndjson"
```

</details>

---

### 4) Receipts + checksums âœ…

**Fetch is receipt-based**:
- record URL, timestamp, status, headers (ETag/Last-Modified), and content hash
- support conditional fetch (`If-None-Match` / `If-Modified-Since`) when available

**Publish is checksum-based**:
- write `checksums.sha256` alongside outputs
- downstream steps must be able to verify integrity quickly

---

### 5) Determinism + idempotency ğŸ¯

Pipelines must be safe to re-run:

- **Deterministic outputs**: same inputs + config + code âœ same digests
- **Idempotent ingest**: avoid duplicates; use manifest digest as idempotency key
- **Canonicalization**: normalize structured outputs (e.g., JSON key order) before hashing

> [!NOTE]
> For geospatial metrics (buffers/distances), use a projected CRS (meters), then publish in the platformâ€™s canonical CRS (commonly EPSG:4326).

---

### 6) Clean Architecture boundaries ğŸ§©

Pipelines should follow the platformâ€™s layering:

- **Domain logic**: schemas, rules, transformations (pure, testable)
- **Ports**: repository interfaces for files, PostGIS, graph, registry, etc.
- **Adapters**: implementations (GDAL, PostGIS client, Neo4j driver, registry client)
- **Infrastructure**: Docker/K8s, storage backends, external services

âœ… Result: ingestion logic does **not** depend on specific infrastructure details â€” adapters can be swapped without rewriting the pipeline.

---

### 7) Packaging patterns ğŸ“¦

KFM favors **paired outputs** that serve both analytics and UI performance:

- **Analysis-ready**: GeoParquet / Parquet / Arrow (fast filtering + joins)
- **Map-ready**: PMTiles / MBTiles / vector tiles (fast rendering)
- **Raster**: COG (Cloud-Optimized GeoTIFF) + pyramids/overviews
- **3D** (optional): 3D Tiles / glTF-friendly assets for Cesium-like viewers

**Canonical pattern (example):** one dataset publishes both **GeoParquet + PMTiles** under the same metadata and provenance.

> [!TIP]
> If you add a performance artifact (tiles, simplified geometry, cached joins), it must remain **reproducible**: same manifest â†’ same tile archive hash.

---

### 8) Graph + search ingestion ğŸ•¸ï¸ğŸ”

KFMâ€™s hybrid data strategy intentionally uses multiple stores:

- **PostGIS** ğŸ—„ï¸ â€” geospatial + tabular, efficient geometry queries, tile serving inputs
- **Neo4j** ğŸ•¸ï¸ â€” semantic/context graph: people, places, events, datasets, stories, activities (PROV)
- **Search index** ğŸ” â€” full-text and (optionally) semantic retrieval over OCR, narratives, documents

**Graph ingest conventions**
- DCAT datasets become Dataset nodes
- PROV Activities become Run/Activity nodes
- PROV relations become edges (e.g., `wasDerivedFrom`, `wasGeneratedBy`, `wasAssociatedWith`)
- Cultural heritage / historical domains may align to established ontologies (e.g., CIDOC-CRM classes for Event/Place patterns)

> [!IMPORTANT]
> â€œGraph ingestâ€ is not optional glue â€” it is part of publishing. Catalog â†’ graph mapping must be deterministic and validated.

---

### 9) Artifact registry + signing ğŸ”ğŸ“¦

KFM treats data like software packages:

- **OCI registry** patterns for data artifacts (via ORAS-style multi-file manifests)
- **DVC** pointers (or equivalent) to keep Git lean while preserving data version references
- **Cosign/Sigstore** signatures + attestations for official outputs (SLSA-aligned)

âœ… Benefits:
- reproducibility (pull exact artifact versions)
- integrity (verify signatures)
- federation (reuse artifacts across regional Frontier Matrix instances)

---

## ğŸ“ Repo touchpoints

Pipelines donâ€™t live in isolation; they connect to contracts, policies, stores, API, and UI.

```text
ğŸ“¦ repo/
â”œâ”€ ğŸ§  src/
â”‚  â”œâ”€ ğŸ§ª pipelines/                      # â† YOU ARE HERE
â”‚  â”‚  â”œâ”€ _kit/                           # shared pipeline kit (context, steps, io, hashing)
â”‚  â”‚  â”œâ”€ ingestion/                      # watchers/fetchers/receipts/telemetry
â”‚  â”‚  â”œâ”€ packaging/                      # tiling + bundling utilities
â”‚  â”‚  â”œâ”€ graph/                          # graph ingest + health checks
â”‚  â”‚  â”œâ”€ ai/                             # retrieval indexes + governance logs
â”‚  â”‚  â””â”€ <domain>/                       # e.g., hydro/, climate/, history/, ecology/, treaties/ ...
â”‚  â””â”€ ğŸ¤– reasoning/                      # Focus Mode agents + retrieval adapters (consumes pipeline outputs)
â”‚
â”œâ”€ ğŸ—‚ï¸ data/
â”‚  â”œâ”€ raw/                               # immutable source drops + receipts
â”‚  â”œâ”€ work/                              # intermediate outputs (replaceable)
â”‚  â”œâ”€ processed/                         # published artifacts (versioned)
â”‚  â”œâ”€ stac/                              # STAC items/collections (canonical)
â”‚  â”œâ”€ catalog/
â”‚  â”‚  â””â”€ dcat/                           # DCAT JSON-LD datasets/distributions (canonical)
â”‚  â”œâ”€ prov/                              # PROV bundles (canonical)
â”‚  â””â”€ audits/                            # manifests, telemetry, policy results
â”‚
â”œâ”€ ğŸ“œ schemas/                            # schema contracts (domain + STAC/DCAT/PROV profiles)
â”œâ”€ ğŸ” policies/                           # OPA/Rego + conftest rules (+ waivers)
â”œâ”€ ğŸ”Œ api/                                # governed API (GraphQL + REST; redaction + auth)
â”œâ”€ ğŸ—ºï¸ ui/                                 # React + TypeScript UI (MapLibre + optional Cesium)
â””â”€ ğŸ“š docs/
   â”œâ”€ standards/                          # KFM profiles + doc protocols
   â”œâ”€ templates/                          # universal doc + story node templates
   â””â”€ data/<domain>/README.md             # domain runbooks + stewardship notes
```

---

## â–¶ï¸ Running pipelines

### ğŸ§‘â€ğŸ’» Local dev

Most pipelines should expose a thin CLI (commonly **Typer**):

```bash
# discover commands
python src/pipelines/<domain>/<pipeline>/cli.py --help

# run with a manifest/config
python src/pipelines/<domain>/<pipeline>/cli.py run --manifest data/raw/<domain>/<pipeline>/manifest.yaml

# validate only (no publish)
python src/pipelines/<domain>/<pipeline>/cli.py validate --manifest data/raw/<domain>/<pipeline>/manifest.yaml

# backfill a range (chunked + restartable)
python src/pipelines/<domain>/<pipeline>/cli.py backfill --start 1900-01-01 --end 1950-12-31

# dry-run (produce plan + manifests, skip publish)
python src/pipelines/<domain>/<pipeline>/cli.py run --manifest ... --dry-run
```

> [!TIP]
> Keep CLIs â€œthinâ€: CLI âœ `RunContext` âœ pure steps.  
> CI and WPE should call the *same* pipeline logic you run locally.

### ğŸ³ Containerized runs

For reproducibility and parity (GDAL versions, system deps), pipelines should run cleanly in containers.

**Target behavior:**
- `docker compose up` for local full-stack (stores + API + UI)
- `docker run ... pipeline-image:tag` for isolated pipeline runs

### ğŸ¤– Orchestrated runs (WPE: Watcher â†’ Planner â†’ Executor)

In production, pipelines are triggered by **Watcherâ€“Plannerâ€“Executor**:

- **Watcher** ğŸ‘€ detects upstream changes, anomalies, schedules
- **Planner** ğŸ§­ decides *what* to run (incremental/backfill/rebuild tiles/schema migrate)
- **Executor** âš™ï¸ runs steps in governed channels (containers/queues) and opens PRs when human review is required

âœ… Therefore pipelines must support:
- incremental runs
- restartability (resume from manifest checkpoints)
- structured telemetry
- contract + policy enforcement

---

## ğŸ§° Creating a new pipeline

### ğŸª Scaffolding

Preferred: scaffold from the pipeline template/cookiecutter (especially for AI-assisted and governed pipelines).

> [!NOTE]
> Templates should include: contracts, run manifest, receipts/checksums, STAC/DCAT/PROV emission, tests, policy hooks.

### ğŸ§± Suggested folder skeleton

```text
src/pipelines/<domain>/<pipeline_name>/
â”œâ”€ cli.py                      # Typer CLI entrypoint (thin)
â”œâ”€ pipeline.py                  # orchestrates step order + RunContext
â”œâ”€ config.py                    # typed config (pydantic/dataclass)
â”œâ”€ contracts/
â”‚  â”œâ”€ input_schema.json         # expected input shape (when applicable)
â”‚  â””â”€ output_schema.json        # output shape (plus metadata expectations)
â”œâ”€ steps/
â”‚  â”œâ”€ fetch.py                  # receipts + raw drop
â”‚  â”œâ”€ validate.py               # contract validation + policy pre-check
â”‚  â”œâ”€ transform.py              # normalization + enrichment
â”‚  â”œâ”€ package.py                # tiles/bundles + performance artifacts
â”‚  â”œâ”€ publish.py                # write processed outputs + checksums
â”‚  â”œâ”€ catalog.py                # STAC/DCAT
â”‚  â”œâ”€ prov.py                   # PROV bundle
â”‚  â””â”€ ingest.py                 # PostGIS/Neo4j/Search ingest (adapters)
â”œâ”€ tests/
â”‚  â”œâ”€ test_contracts.py
â”‚  â”œâ”€ test_determinism.py        # golden hashes / snapshot checks
â”‚  â””â”€ test_smoke.py
â””â”€ README.md                    # pipeline runbook (inputs/outputs/backfill/failures)
```

> [!IMPORTANT]
> **Canonical ordering:** Fetch â†’ Validate â†’ Transform â†’ Package â†’ Publish â†’ Catalog â†’ PROV â†’ Ingest  
> (â€œPublishâ€ = artifacts + checksums + versioning â€” not â€œpush to UI directlyâ€.)

### ğŸ§  Step design pattern

**Steps should be small, pure, and inspectable.**

```python
# pseudocode (illustrative)
def transform(ctx: RunContext) -> StepResult:
    raw = read_raw(ctx.raw_paths)
    clean = normalize(raw, rules=ctx.config.rules)
    outputs = write_work(clean, ctx.work_dir)

    return StepResult(
        outputs=outputs,
        metrics={"rows_in": len(raw), "rows_out": len(clean)},
        warnings=[],
    )
```

---

## âœ… Validation & policy gates

Pipelines enforce quality **before** anything becomes â€œofficial.â€

### ğŸ“œ Contract validation

- schema validation (domain + STAC/DCAT/PROV profiles)
- required metadata: license, attribution, sensitivity tags, provenance pointers
- CRS correctness + bounds sanity (bbox, geometry validity, units)
- deterministic naming/versioning rules (no silent overwrites)

### ğŸ” Policy validation (OPA/Conftest)

Common gate categories:

- **license & attribution** (compatible + complete)
- **sensitivity** (sacred sites, endangered species habitats, archaeological locations, etc.)
- **privacy** (PII, inference risk, aggregation requirements)
- **security** (secrets scanning, dependency policy, SBOM/attestations where required)
- **governance** (FAIR+CARE alignment; review requirements)

### ğŸ¤ CI / automation

Pipelines should be CI-friendly:
- lint/type-check/test
- validate catalogs + provenance bundles
- run policy packs against artifacts produced in PRs
- ensure manifest + checksum rules

> [!NOTE]
> If automation (WPE) runs a pipeline and a policy requires review, the system should generate a **review-ready PR** or change record â€” never a silent merge.

---

## ğŸ§© Pipeline types

Below is a practical taxonomy (you can mix types; output rules still apply).

| Type | Best for | Typical outputs | Stores updated |
|---|---|---|---|
| ğŸ“¥ Batch ingestion | periodic datasets | GeoParquet/COG + STAC/DCAT/PROV | PostGIS + Neo4j |
| ğŸ“¡ Streaming ingestion | sensors/alerts | incremental partitions + rollups + telemetry | PostGIS + search |
| ğŸ—ºï¸ğŸ“œ Historic docs/maps | scans, archives | COGs + OCR corpora + entity edges | Search + Neo4j |
| ğŸ§±ğŸŒ Asset builds | performance/offline | PMTiles/MBTiles + simplified geom + bundles | UI offline packs |
| ğŸ•¸ï¸ğŸ©º Graph maintenance | health checks | constraint reports + drift alerts | Neo4j |
| ğŸ§µâš¡ Narrative/Pulse | timely updates | Story markdown + storyboard JSON + evidence manifest | Neo4j + search |
| ğŸ¤–ğŸ§­ AI support | retrieval/governance | indexes + model/run records + policy logs | Search + audits |
| ğŸ•°ï¸ğŸ§ª Simulation/twins | scenario modeling | datasets + uncertainty + replay manifests | PostGIS + Neo4j |

---

## ğŸ“ˆ Ops: observability, backfills, drift

### ğŸ“Š Telemetry

Each run should emit machine-readable telemetry (often NDJSON):
- step timing
- row/feature counts
- anomaly counts
- cache hits/misses
- optional: resource usage

### ğŸ”„ Backfills

Backfills must be:
- manifest-driven (range + parameters)
- chunked and restartable
- idempotent across repeated runs

### ğŸ“‰ Drift monitoring

For AI and streaming pipelines:
- track data distribution shifts
- track model performance metrics and provenance
- open issues / generate review-ready PRs when thresholds are crossed

---

## ğŸ” Security, privacy, ethics

### ğŸ›¡ï¸ Sensitive locations & cultural data (CARE)

If data includes sensitive locations (archaeological sites, endangered species habitat, sacred sites, etc.):

- generalize/fuzz coordinates where required
- apply access control (public vs restricted)
- add sensitivity tags in metadata (machine-readable)
- document sovereignty expectations and restrictions

### ğŸ•µï¸ Privacy-preserving outputs

When outputs could leak private information:

- consider **k-anonymity / l-diversity / t-closeness**
- apply query auditing for inference control
- use differential privacy for public aggregates when needed

### ğŸ” Supply-chain integrity

For official releases:

- generate SBOMs (where applicable)
- sign artifacts (Sigstore/Cosign)
- attach attestations tying artifact â†’ pipeline version â†’ run manifest

### ğŸ¤– Focus Mode safety hooks (AI pipelines)

AI support pipelines should assume a **governed execution** environment:

- **Prompt Gate**: validate/shape prompt + context
- **Tool allow-lists**: only approved tools/actions
- **Sandboxing**: isolate execution; protect secrets
- **OPA output policy check**: enforce content + data-handling rules
- **Citations ledger**: every answer ties to datasets/documents/graph nodes

> [!IMPORTANT]
> â€œNo citation â†’ no claim.â€ Focus Mode must prefer abstaining over hallucinating. âœ…

---

## âœ… Definition of Done

A pipeline is â€œdoneâ€ when:

- [ ] Purpose + scope are clear (what it ingests, transforms, publishes)
- [ ] Contracts exist (inputs/outputs + schema + sensitivity classification)
- [ ] Deterministic + idempotent (manifest + hashes + reproducible environment)
- [ ] Emits receipts, checksums, telemetry, and run manifest
- [ ] Produces STAC + DCAT + PROV (or documents why not)
- [ ] Updates stores deterministically (PostGIS/Neo4j/search) through adapters
- [ ] Passes policy gates (or includes approved waivers)
- [ ] Has tests (unit + contract + smoke + determinism checks)
- [ ] Documents backfill strategy + known failure modes
- [ ] Is runnable by humans locally and by WPE automation

---

## ğŸ“š References

### ğŸ§­ Canonical KFM docs (design + architecture)
- Kansas Frontier Matrix (KFM) â€“ Comprehensive Platform Overview and Roadmap
- Kansas Frontier Matrix (KFM) â€“ Comprehensive Technical Documentation
- Kansas Frontier Matrix (KFM) â€“ Comprehensive Architecture, Features, and Design
- ğŸ“š Kansas Frontier Matrix (KFM) â€“ Expanded Technical & Design Guide
- Kansas Frontier Matrix (KFM) â€“ Comprehensive UI System Overview (Technical Architecture Guide)
- Kansas Frontier Matrix (KFM) â€“ AI System Overview ğŸ§­ğŸ¤–
- KFM AI Infrastructure â€“ Ollama Integration Overview

### ğŸ“¦ Resource portfolios (deep dives for pipeline authors)
- AI Concepts & more (agents, governance, ethics, constraints)
- Maps/GoogleMaps/Virtual Worlds/Archaeological GIS/WebGL (geospatial visualization + 3D)
- Data Management theories + architectures + Bayesian ideas (lakehouse/data quality/reproducibility thinking)
- Mapping/Modeling + Python/Git/HTTP/CSS/Docker/GraphQL + security (engineering toolchain)
- Geographic Information + R/SciPy/MATLAB/ArcGIS/Spark/TypeScript/Web Apps (analysis + app stack)
- Various programming languages & resources (polyglot reference)

### ğŸ§© Repo standards (recommended)
- KFM STAC/DCAT/PROV profiles (docs/standards/)
- Universal doc + Story Node templates (docs/templates/)
- Domain runbooks (docs/data/<domain>/README.md)

---

## ğŸ“– Glossary

- **STAC** ğŸ—‚ï¸: SpatioTemporal Asset Catalog (items/collections for geospatial assets)  
- **DCAT** ğŸ·ï¸: Data Catalog Vocabulary (dataset discovery metadata; often JSON-LD)  
- **PROV** ğŸ§¬: W3C provenance model (inputs â†’ activities â†’ outputs, with agents)  
- **WPE** ğŸ¤–: Watcher â†’ Planner â†’ Executor automation loop (governed DevOps agents)  
- **OCI/ORAS** ğŸ“¦: Container registry patterns for storing arbitrary artifacts (not just images)  
- **DVC** ğŸ”: Data Version Control (pointers + hashes for large artifacts)  
- **FAIR + CARE** ğŸ¤: Findable/Accessible/Interoperable/Reusable + Collective Benefit/Authority to Control/Responsibility/Ethics  
- **PMTiles/MBTiles** ğŸ§±: Offline-friendly tile archives (vector/raster packaging)  
- **COG** ğŸ›°ï¸: Cloud-Optimized GeoTIFF (efficient raster access)  
- **Idempotent** ğŸ¯: safe to rerun without duplicating outputs or drifting results  
- **Run Manifest** ğŸ§¾: audit record of inputs/params/outputs/hashes/policy decisions

---