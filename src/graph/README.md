# ğŸ•¸ï¸ KFM Graph Subsystem (Neo4j) â€” `src/graph`

![Neo4j](https://img.shields.io/badge/Neo4j-knowledge%20graph-008CC1?logo=neo4j&logoColor=white)
![STAC](https://img.shields.io/badge/STAC-catalogs-informational)
![DCAT](https://img.shields.io/badge/DCAT-metadata-informational)
![PROV](https://img.shields.io/badge/PROV-lineage-informational)
![PostGIS](https://img.shields.io/badge/PostGIS-spatial%20DB-informational)

> **What this folder is:** the **semantic backbone** of Kansas Frontier Matrix (KFM).  
> It builds and maintains the **Neo4j knowledge graph** used to connect **datasets â†” places â†” events â†” people â†” documents â†” stories**, power **Focus Mode** multi-hop reasoning, and keep **provenance-first** guarantees intact. ğŸ§­ğŸ¤–

---

## ğŸ§  Why a Graph?

KFM uses **two â€œtruthyâ€ storage backends** for different kinds of questions:

- ğŸ—ºï¸ **PostGIS**: the heavy geospatial lifting (bbox filters, distance, tiling, geometry ops).
- ğŸ•¸ï¸ **Neo4j (Graph)**: semantic relationships, lineage chains, narratives, entity linking, and multi-hop exploration.

The graph exists so the system can answer questions like:

- â€œWhat datasets are related to **event X** in **place Y**?â€
- â€œWhat happened **here** during **the 1930s**?â€
- â€œShow me the **story nodes** that reference these layers and this county.â€

---

## ğŸ”— Quick Links (Repo)

- `../../data/graph/csv/README.md` ğŸ“„ (import-ready CSV contracts)
- `../../docs/MASTER_GUIDE_v13.md` ğŸ§± (repo conventions + pipeline ordering)
- `../../docs/architecture/` ğŸ›ï¸ (system architecture)
- `../../api/` ğŸ”Œ (API layer that runs Cypher/SQL and enforces policy)

> **Golden rule:** âœ… UI talks to the graph **only via the API** (never direct Cypher).  
> This is how we enforce access control, provenance requirements, and redaction rules.

---

## ğŸ§­ Responsibilities (What `src/graph` owns)

âœ… This module owns:

- ğŸ§¬ **Ontology bindings & mapping rules** (e.g., CIDOC-CRM / OWL-Time / GeoSPARQL / PROV-O alignment)
- ğŸ§± **Graph schema contract** (labels, relationship types, key properties)
- ğŸ§° **Ingest tooling** from `data/graph/csv/` (bulk load, incremental sync if supported)
- ğŸ§· **Constraints & indexes** (uniqueness, required fields, search indexes)
- ğŸ©º **Graph Health Check** routines (integrity, drift, orphan detection, counts)
- ğŸ” **Query helpers** used by the API + Focus Mode (curated traversals, not â€œfree-formâ€)

ğŸš« This module does **not** own:

- Heavy geometry computations (belongs in PostGIS)
- UI-specific view state (belongs in web client)
- Raw data fetching (belongs in pipeline intake)

---

## ğŸ“¦ Folder Map (Recommended Layout)

> Your tree may vary, but `src/graph` should feel like a **mini product**: schema â†’ ingest â†’ constraints â†’ health â†’ queries.

```text
ğŸ“ src/graph/
â”œâ”€ ğŸ“„ README.md                     # you are here
â”œâ”€ ğŸ“ ontology/                     # ğŸ§¬ ontology bindings (CIDOC, PROV, OWL-Time, GeoSPARQLâ€¦)
â”‚  â”œâ”€ ğŸ“„ cidoc_crm.yaml|json|ttl
â”‚  â”œâ”€ ğŸ“„ prov_o.yaml|json|ttl
â”‚  â””â”€ ğŸ“„ mapping_rules.md
â”œâ”€ ğŸ“ schema/                       # ğŸ§± label + rel contract (and invariants)
â”‚  â”œâ”€ ğŸ“„ labels.md
â”‚  â”œâ”€ ğŸ“„ relationships.md
â”‚  â””â”€ ğŸ“„ property_keys.md
â”œâ”€ ğŸ“ constraints/                  # ğŸ§· Neo4j constraints + indexes (Cypher)
â”‚  â”œâ”€ ğŸ“„ 001_constraints.cypher
â”‚  â””â”€ ğŸ“„ 002_indexes.cypher
â”œâ”€ ğŸ“ ingest/                       # ğŸ§° CSV â†’ Neo4j (bulk import / sync)
â”‚  â”œâ”€ ğŸ“„ ingest_csv.py|ts
â”‚  â”œâ”€ ğŸ“„ upsert_strategies.md
â”‚  â””â”€ ğŸ“„ id_strategy.md
â”œâ”€ ğŸ“ queries/                      # ğŸ” curated Cypher templates (used by API)
â”‚  â”œâ”€ ğŸ“„ search_entities.cypher
â”‚  â”œâ”€ ğŸ“„ expand_neighbors.cypher
â”‚  â””â”€ ğŸ“„ provenance_chain.cypher
â””â”€ ğŸ“ health/                       # ğŸ©º integrity + drift detection
   â”œâ”€ ğŸ“„ graph_health_check.py|ts
   â””â”€ ğŸ“„ health_checks.cypher
```

---

## ğŸ” Data Flow & Ordering Invariants (KFM-wide)

KFMâ€™s pipeline order matters because it enforces reproducibility and provenance:

1. ğŸ§± `data/raw/` â€” source data (immutable inputs)
2. ğŸ§ª `data/processed/` â€” derived data products (versioned)
3. ğŸ—‚ï¸ `data/stac/`, `data/catalogs/`, `data/prov/` â€” catalogs + lineage
4. ğŸ•¸ï¸ `data/graph/csv/` â†’ **Neo4j import** (derived semantic layer)

### Mermaid: end-to-end flow

```mermaid
flowchart LR
  raw[ğŸ§± data/raw] --> processed[ğŸ§ª data/processed]
  processed --> catalogs[ğŸ—‚ï¸ STAC/DCAT/PROV]
  catalogs --> csv[ğŸ“„ data/graph/csv]
  csv --> neo4j[(ğŸ•¸ï¸ Neo4j)]
  processed --> postgis[(ğŸ—ºï¸ PostGIS)]
  neo4j --> api[ğŸ”Œ API (FastAPI/GraphQL)]
  postgis --> api
  api --> ui[ğŸ–¥ï¸ Web UI]
  api --> ai[ğŸ§  Focus Mode AI]
```

---

## ğŸš€ Quickstart (Local Dev)

### 1) Start Neo4j (Docker-first)

> If the repo has a Neo4j service, use it. Otherwise, Neo4j Desktop is fine.

```bash
docker compose up -d neo4j
```

### 2) Configure env

```bash
export NEO4J_URI="bolt://localhost:7687"
export NEO4J_USER="neo4j"
export NEO4J_PASSWORD="password"
# export NEO4J_DATABASE="neo4j"   # optional
```

### 3) Generate import CSVs

The **authoritative import contract** is `data/graph/csv/`.  
These CSVs are typically generated **after** STAC/DCAT/PROV exists.

```bash
# Example (adjust to actual pipeline tooling):
python -m src.pipelines.build_graph_csv --out data/graph/csv
```

### 4) Apply constraints + indexes

```bash
# Example (adjust to actual tooling):
python -m src.graph.constraints.apply
```

### 5) Ingest into Neo4j

```bash
# Example (adjust to actual tooling):
python -m src.graph.ingest.ingest_csv --csv-dir data/graph/csv
```

âœ… **Rebuild-friendly:** the graph is a **derived store**.  
If you get into a bad state, itâ€™s usually acceptable to **drop + reimport** (assuming the CSVs are correct and complete).

---

## ğŸ§¬ Modeling Rules (Schema Contract)

KFMâ€™s graph is intended to be **ontology-aware**, not just â€œrandom nodes and edgesâ€.

### ğŸ·ï¸ Labels (canonical examples)

- `Dataset` â€” a layer/data product (ties to DCAT + STAC)
- `StacCollection`, `StacItem`, `Asset` â€” catalog structure
- `Place` â€” gazetteer-backed locations (counties, towns, riversâ€¦)
- `Event` â€” historical events, observations, incidents
- `Person`, `Organization` â€” people/institutions
- `Document` â€” text sources (newspapers, reports, treaties)
- `StoryNode` â€” narrative object referencing datasets + places + docs
- `Concept` â€” topic/theme nodes (used for clustering + browsing)
- `Activity`, `Agent`, `Entity` â€” provenance (PROV-style)

### ğŸ”— Relationships (canonical examples)

Use **UPPER_SNAKE_CASE** for relationships:

- `LOCATED_IN`, `HAPPENED_AT`
- `MENTIONS`, `CITES`, `DERIVED_FROM`
- `HAS_ITEM`, `HAS_ASSET`
- `REFERENCES_DATASET`, `REFERENCES_PLACE`
- `WAS_GENERATED_BY`, `USED` (PROV-ish)
- `TAGGED_WITH` (concept/topic)

### ğŸ†” Identity & Keys

Every node thatâ€™s â€œrealâ€ must have:

- `id` â€” stable unique ID (prefer **ULID** or deterministic composite keys)
- `source` â€” where it came from (DCAT distribution, archive, pipeline)
- `version` / `valid_time` where applicable
- `classification` / `sensitivity` when needed (privacy + cultural protocols)

---

## ğŸ§° Ingestion Strategy (CSV â†’ Neo4j)

### âœ… Principle: â€œNo mystery nodesâ€

All graph content should be traceable to:

- STAC/DCAT/PROV metadata, or
- curated narrative content (Story Nodes) with citations, or
- controlled enrichments (NLP entity linking / embeddings) that remain **auditable**

### ğŸ“„ CSV Contract Expectations

Your CSV set should typically include:

- `nodes_*.csv` (e.g., `nodes_dataset.csv`, `nodes_place.csv`, `nodes_event.csv`)
- `rels_*.csv` (e.g., `rels_mentions.csv`, `rels_located_in.csv`, `rels_prov_used.csv`)
- optional `meta_import_manifest.json` (build id, timestamp, git sha)

> Tip: keep â€œCSV generationâ€ deterministic so graph rebuilds are repeatable.

---

## ğŸ©º Graph Health Check (Integrity + Drift)

KFM design docs propose a recurring graph QA routine. This folder should own it.

### What to check (minimum viable)

- âœ… Constraint presence (uniqueness / required keys)
- âœ… Index presence (common lookup keys)
- âœ… Orphan detection (nodes with zero meaningful relationships)
- âœ… Broken foreign keys (e.g., `county_id` exists in graph but not in PostGIS)
- âœ… Provenance completeness for published nodes
- âœ… Drift vs CSV export (graph contains nodes not present in current export)

### Example Cypher checks

```cypher
// Orphaned nodes (excluding allowed singletons)
MATCH (n)
WHERE size((n)--()) = 0 AND NOT n:ImportManifest
RETURN labels(n) AS labels, count(*) AS orphans
ORDER BY orphans DESC;
```

```cypher
// Duplicate IDs (should be prevented by constraint, but useful as a smoke test)
MATCH (n)
WITH n.id AS id, count(*) AS c
WHERE id IS NOT NULL AND c > 1
RETURN id, c
ORDER BY c DESC;
```

---

## ğŸ§  Focus Mode Integration (AI + Graph)

Focus Mode should be able to:

- translate a user question into **graph traversals**
- find the right **datasets / places / events / docs**
- merge structured graph context with unstructured document retrieval (RAG-style)
- always return **traceable citations** back to sources

> The graph is the â€œmulti-hop context engineâ€ that prevents the LLM from guessing. âœ…

---

## ğŸ–¥ï¸ UI Integration (Graph-backed features)

Even if the UI never hits Neo4j directly, the graph should enable:

- ğŸ§¾ â€œEntity panelsâ€ (what is this place/person/dataset?)
- ğŸ§­ â€œRelated itemsâ€ (neighbors by concept, time, location, citations)
- ğŸ§µ Story Nodes with links to datasets + map features
- ğŸ§  AI â€œwhy this result?â€ explainability (show traversal + provenance)

---

## ğŸ§© Extending the Graph (Developer Playbook)

When adding a new concept, node type, or relationship:

1. ğŸ§± **Define** label + relationship in `schema/`
2. ğŸ§¬ **Map** it to an ontology class/property when appropriate
3. ğŸ“„ **Update** CSV generation (in pipeline) â†’ `data/graph/csv/`
4. ğŸ§· **Add** constraints/indexes
5. ğŸ”Œ **Expose** via API (curated endpoints; no raw Cypher from UI)
6. ğŸ©º **Update** health checks + tests
7. ğŸ“ **Document** in this README + relevant design docs

---

## ğŸ›¡ï¸ Governance: Provenance-first + Safe-by-default

The graph must support:

- âœ… provenance chains (what produced what, using which inputs)
- âœ… auditability (who/what created nodes, when)
- âœ… access control & redaction (sensitive places, private stations, protected knowledge)

**Rule of thumb:**  
If something doesnâ€™t have a provenance record (even a stub), it shouldnâ€™t be promoted for UI/AI use.

---

## ğŸ§ª Testing Guidance

Minimum recommended coverage:

- âœ… unit tests for CSV parsing + type coercion
- âœ… golden-file tests for CSV exports (deterministic ordering)
- âœ… integration test: spin up Neo4j container â†’ ingest â†’ run smoke queries
- âœ… contract tests: API endpoints return same node IDs as graph export
- âœ… health-check tests: corrupted fixtures fail loudly

---

## ğŸ—ºï¸ Roadmap Ideas (From project proposals)

- ğŸ§µ **Pulse Threads**: lightweight â€œmicro-storyâ€ nodes capturing emerging signals & references
- ğŸ§  **Conceptual Attention Nodes**: curated theme nodes that help browsing + clustering
- ğŸ” **Narrative pattern detection** using graph analytics (hubs, bridges, communities)
- ğŸŒ **Federated graph** (multi-institution, policy-controlled sharing)
- ğŸ§¾ **Explainable traversals** (â€œwhy did we link these?â€)

---

## ğŸ“š Project Docs & Reference Library (What informed this module)

### Core KFM Docs (must-read)
- **Comprehensive Architecture, Features, and Design** â€” stack + boundaries
- **Comprehensive Technical Documentation** â€” deep system design + graph analytics ideas
- **AI System Overview** â€” Focus Mode graph usage and RAG orchestration
- **UI System Overview** â€” how graph-backed entity linking shows up in UI
- **Data Intake Guide** â€” catalogs â†’ CSV â†’ graph pipeline expectations

### Proposals & Future Work
- **Latest Ideas & Future Proposals** â€” OCR/document ingestion into graph + vectors
- **Additional Project Ideas** â€” graph health checks, pulse threads, attention nodes
- **Innovative Concepts to Evolve KFM** â€” governance, community, AI explainability

### Research Libraries (background)
- **AI Concepts & more** â€” AI/ML theory base for retrieval + reasoning
- **Data Management / Architecture / Bayesian Methods** â€” performance + reliability thinking
- **Maps / WebGL / Geospatial** â€” map rendering + spatial computation references
- **Programming Languages & Resources** â€” Python/TypeScript reference material

---

## âœ… â€œDoneâ€ Definition for `src/graph`

This folder is â€œhealthyâ€ when:

- [ ] Graph can be rebuilt **entirely** from `data/graph/csv/`
- [ ] Constraints + indexes are applied consistently
- [ ] Health checks run and fail on drift/corruption
- [ ] API exposes curated graph traversals
- [ ] Focus Mode can cite sources via graph traversal paths
- [ ] Sensitive content is classified + enforceable through the API

---

ğŸ§­ If youâ€™re unsure where to implement something:
- **Is it semantic relationships / provenance / narrative linking?** â†’ `src/graph/`
- **Is it geometry / spatial filtering / tiles?** â†’ PostGIS + spatial adapters
- **Is it presentation / interaction?** â†’ web UI
- **Is it policy / redaction / permissioning?** â†’ API + policy pack