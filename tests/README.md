# ğŸ§ª `tests/` â€” Kansas Frontier Matrix (KFM)

![CI](https://img.shields.io/badge/CI-gated%20by%20tests-2ea44f?logo=githubactions&logoColor=white)
![Policy](https://img.shields.io/badge/policy-OPA%20%2B%20Conftest-1f6feb?logo=openpolicyagent&logoColor=white)
![Provenance](https://img.shields.io/badge/provenance-PROV%20required-8b5cf6)
![API](https://img.shields.io/badge/API-FastAPI%20%2B%20GraphQL-009688?logo=fastapi&logoColor=white)
![GIS](https://img.shields.io/badge/GIS-PostGIS%20%2B%20Neo4j-0ea5e9)

> **Core promise:** â€œNo Source, No Answer.â€ ğŸ¤–ğŸ“š  
> **Core posture:** â€œFail closed.â€ If metadata/policy/provenance is missing â†’ the build blocks. ğŸ§¯ğŸš«

---

<details>
<summary><strong>ğŸ“š Table of contents</strong></summary>

- [ğŸ¯ What this test suite protects](#-what-this-test-suite-protects)
- [ğŸ§± Test pyramid](#-test-pyramid)
- [ğŸ“ Recommended folder layout](#-recommended-folder-layout)
- [ğŸš€ Quick start](#-quick-start)
- [ğŸ§© Suites](#-suites)
  - [ğŸ§ª Backend API](#-backend-api)
  - [ğŸ—ºï¸ Data pipeline & metadata artifacts](#ï¸-data-pipeline--metadata-artifacts)
  - [ğŸ”’ Policy-as-code (OPA)](#-policy-as-code-opa)
  - [ğŸ¤– Focus Mode AI regression](#-focus-mode-ai-regression)
  - [ğŸ§­ UI & E2E (optional but recommended)](#-ui--e2e-optional-but-recommended)
- [ğŸ·ï¸ Markers, tags, and conventions](#ï¸-markers-tags-and-conventions)
- [âœ… CI expectations](#-ci-expectations)
- [ğŸ§° Troubleshooting](#-troubleshooting)
- [ğŸ§¾ Add-a-test checklist](#-add-a-test-checklist)

</details>

---

## ğŸ¯ What this test suite protects

KFM is **not** â€œjust a map appâ€ or â€œjust a chatbot.â€ It is a governed system where:

- ğŸ§¾ **Provenance is mandatory** (PROV lineage for anything publishable).
- ğŸ”’ **Policies are enforcement gates** (OPA rules in CI and runtime).
- ğŸŒ **The API is the single entry point** (no direct DB access from UI).
- ğŸ¤– **Focus Mode must cite sources** (answers must carry citations or refuse).

This `tests/` directory is the **evidence layer** that guards these guarantees.

---

## ğŸ§± Test pyramid

```mermaid
flowchart TD
  U[Unit tests ğŸ§©\nfast, deterministic] --> I[Integration tests ğŸ§ª\ncontainers + real deps]
  I --> E[E2E tests ğŸ§­\nUI flows + tiles + Focus Mode]
  I --> P[Policy gates ğŸ”’\nOPA/Conftest]
  I --> V[Provenance checks ğŸ§¾\nSTAC/DCAT/PROV artifacts]
  I --> A[AI regression ğŸ¤–\ncitations + safety + governance]
```

**Rule of thumb** ğŸ§ :
- Unit tests: *logic correctness*
- Integration tests: *contracts + real dependencies*
- E2E tests: *user-critical flows*
- Policy/provenance/AI tests: *trust-critical invariants*

---

## ğŸ“ Recommended folder layout

> This layout is flexibleâ€”match your repo, but keep the **intent** consistent.

```text
tests/
â”œâ”€ README.md                  # ğŸ‘ˆ you are here
â”œâ”€ unit/                      # ğŸ§© pure logic (fast)
â”‚  â”œâ”€ api/
â”‚  â”œâ”€ services/
â”‚  â””â”€ utils/
â”œâ”€ integration/               # ğŸ§ª real services (containers)
â”‚  â”œâ”€ api_endpoints/
â”‚  â”œâ”€ db_postgis/
â”‚  â”œâ”€ db_neo4j/
â”‚  â””â”€ tiles/
â”œâ”€ policy/                    # ğŸ”’ rego + conftest + fixtures
â”‚  â”œâ”€ rego/
â”‚  â”œâ”€ testdata/
â”‚  â””â”€ conftest/
â”œâ”€ data/                      # ğŸ§¾ pipeline, catalog, artifacts
â”‚  â”œâ”€ fixtures/
â”‚  â”œâ”€ stac/
â”‚  â”œâ”€ dcat/
â”‚  â””â”€ prov/
â”œâ”€ ai/                        # ğŸ¤– Focus Mode regression tests
â”‚  â”œâ”€ prompts/
â”‚  â”œâ”€ fixtures/
â”‚  â””â”€ eval/
â””â”€ e2e/                       # ğŸ§­ Playwright/Cypress (optional)
   â”œâ”€ specs/
   â””â”€ fixtures/
```

---

## ğŸš€ Quick start

### âœ… Prereqs
- ğŸ³ Docker + Docker Compose (for PostGIS / Neo4j / OPA / optional Ollama)
- ğŸ Python (recommended for FastAPI backend tests; commonly `pytest`)
- ğŸ§‘â€ğŸ’» Node.js (recommended for UI/E2E tests; optional if you only test backend)

### ğŸƒ Run â€œfastâ€ tests (unit only)
```bash
pytest -q
```

### ğŸ§ª Run integration tests (requires services)
```bash
# Start only what you need (example service names)
docker compose up -d postgis neo4j opa

pytest -m integration -q
```

### ğŸ”’ Run policy tests (OPA / Conftest)
```bash
conftest test policy/ -p policy/rego
```

### ğŸ¤– Run Focus Mode AI regression (optional)
```bash
# start ollama if your AI tests need it
docker compose up -d ollama

pytest -m ai -q
```

> Tip ğŸ’¡: If your AI tests are run in CI, prefer a **small deterministic model** (or mock the LLM) and assert **structure** (citations + policy compliance) instead of exact prose.

---

## ğŸ§© Suites

## ğŸ§ª Backend API

### What we test
- âœ… **HTTP contracts**: status codes, error format, pagination, timeouts
- âœ… **Governance hooks**: access checks, redaction/sanitization, logging
- âœ… **Key endpoints** (examples):
  - `/api/v1/datasets/{id}` (metadata + asset links)
  - `/api/v1/catalog/search` (keyword/bbox/time filtering)
  - `/api/v1/query` (safe, constrained querying)
  - `/tiles/{layer}/{z}/{x}/{y}.*` (vector/raster tiles)
  - `/api/v1/ai/query` (+ optional `/ai/stream`) for Focus Mode

### Patterns we prefer
- ğŸ§ª Use the frameworkâ€™s test client (e.g., FastAPI TestClient) for endpoint tests.
- ğŸ§± Dependency injection overrides for unit tests (fake repo adapters).
- ğŸ³ For integration tests, use containers and **real** PostGIS/Neo4j/OPA.

### Example assertions (good âœ… vs brittle ğŸš«)
âœ… Assert:
- response schema keys
- policy denial is 403 (or sanitized shape)
- citations exist in AI output
- tiles are valid MVT/PNG and cache headers are sane

ğŸš« Avoid:
- exact full-text AI answers
- relying on live external APIs
- â€œsleep-basedâ€ race handling

---

## ğŸ—ºï¸ Data pipeline & metadata artifacts

KFMâ€™s data lifecycle is staged and traceable:

- `data/raw/<domain>/` âœ `data/work/<domain>/` âœ `data/processed/<domain>/`
- Publication requires boundary artifacts:
  - ğŸ§¾ STAC records
  - ğŸ§¾ DCAT dataset entry
  - ğŸ§¾ PROV lineage bundle

### What we test
- âœ… New/changed datasets produce **STAC + DCAT + PROV**
- âœ… Missing/invalid provenance fails (publish blocked)
- âœ… Determinism: same inputs â†’ same outputs (checksums/manifests)
- âœ… Geo sanity:
  - CRS correctness
  - geometry validity
  - non-empty extents
  - stable IDs / primary keys

### Suggested checks
- ğŸ“¦ â€œArtifact presenceâ€ test per dataset domain
- ğŸ§¬ Schema validation (JSON Schema for STAC/DCAT/PROV)
- ğŸ” â€œRe-run pipeline yields identical manifestâ€ test (where feasible)

---

## ğŸ”’ Policy-as-code (OPA)

KFM uses policy gates at **CI time** and **runtime**.

### What we test
- âœ… Rego rules compile
- âœ… Expected allow/deny cases for:
  - RBAC roles (Public Viewer / Contributor / Maintainer / Admin)
  - sensitivity labels (public/internal/sensitive)
  - AI output rules (citations required, restricted content blocked)
- âœ… â€œFail closedâ€ behavior:
  - missing license / missing sensitivity label / missing PROV â†’ CI fails
- âœ… Policy decision metadata is traceable (policy bundle/version tagging if implemented)

### Tooling recommendations
- **Conftest** for CI policy tests
- â€œGoldenâ€ decision fixtures (`input.json` â†’ `allow: false/true`) ğŸ§Š

---

## ğŸ¤– Focus Mode AI regression

Focus Mode is governed: it must cite sources or refuse.

### What we test
- âœ… Output includes **citations** (at least one) when answering factual questions
- âœ… Output refuses when evidence is insufficient (â€œNo Source, No Answerâ€)
- âœ… Output passes policy (no restricted leakage)
- âœ… Optional: ensure certain key terms appear for specific fixtures (lightweight checks)

### How to keep AI tests stable
- ğŸ”§ Prefer:
  - a small local model, or
  - a mock Ollama client returning deterministic responses, or
  - fixed retrieval context fixtures (freeze the evidence bundle)
- ğŸ§ª Assert **structure**, not style:
  - citations present
  - no forbidden phrases
  - output type + fields are correct (e.g., `answer`, `citations[]`, `audit_id`)

### Suggested AI fixture approach
- `tests/ai/fixtures/<case>/question.txt`
- `tests/ai/fixtures/<case>/sources.json` (retrieval payload)
- `tests/ai/fixtures/<case>/expected.json` (minimal required invariants)

---

## ğŸ§­ UI & E2E (optional but recommended)

E2E ensures users can actually do the â€œKansas time-travel mapâ€ workflows ğŸ—ºï¸ğŸ•°ï¸.

### What we test
- âœ… App boots
- âœ… Map loads + tiles render
- âœ… Layer toggles work
- âœ… Timeline changes update the view
- âœ… Focus Mode returns an answer with citations (or refusal)
- âœ… RBAC: restricted layers not visible to unauthorized roles

### Tools
- Playwright or Cypress
- Use seeded data fixtures + test accounts

---

## ğŸ·ï¸ Markers, tags, and conventions

### Pytest markers (suggested)
- `unit` ğŸ§©
- `integration` ğŸ§ª
- `policy` ğŸ”’
- `data` ğŸ§¾
- `ai` ğŸ¤–
- `e2e` ğŸ§­
- `slow` ğŸ¢

Example run:
```bash
pytest -m "not slow" -q
pytest -m "integration and not ai" -q
```

### Naming
- `test_<thing>_<behavior>.py`
- One **primary assertion** per test (multiple small asserts are fine; avoid mega-tests).

---

## âœ… CI expectations

### PR (fast + trust-critical)
- âœ… unit
- âœ… policy (rego + conftest)
- âœ… artifact validation for touched datasets
- âœ… lightweight integration (API smoke)

### Nightly / scheduled (heavier)
- âœ… full integration suite (DB + tiles)
- âœ… AI regression suite (if enabled)
- âœ… E2E flows

> CI should be treated as a **governance gate**â€”if it fails, we assume the system would violate provenance/policy promises. ğŸš§

---

## ğŸ§° Troubleshooting

### ğŸ³ Containers wonâ€™t start / ports collide
- Check local ports used by Postgres (5432), Neo4j (7474/7687), OPA (8181), Ollama (11434)
- Stop conflicting services or re-map ports in your test compose file.

### ğŸ§  Neo4j memory errors
- Reduce dataset fixture size
- Increase container memory limits

### ğŸ§¾ â€œMissing PROV / license / sensitivity labelâ€
- This is expected â€œfail closedâ€ behavior âœ…  
  Fix the datasetâ€™s boundary artifacts and re-run the checks.

### ğŸ¤– AI tests are flaky
- Switch to mock Ollama client
- Freeze retrieval payloads
- Assert citations + policy compliance only

---

## ğŸ§¾ Add-a-test checklist

When you add or change something, make sure you also add tests that prove the trust contract:

### If you add an API endpoint ğŸŒ
- [ ] Unit tests for service logic
- [ ] Integration test for endpoint contract
- [ ] Policy test (authorized vs unauthorized)
- [ ] Audit/provenance log assertion (if applicable)

### If you add a dataset / pipeline ğŸ—‚ï¸
- [ ] Pipeline tests produce `processed/` outputs
- [ ] STAC/DCAT/PROV generated and valid
- [ ] Determinism/manifest check (where feasible)
- [ ] Policy gate tests (license/sensitivity present)

### If you change Focus Mode ğŸ¤–
- [ ] Regression test ensures citations
- [ ] Refusal behavior for insufficient evidence
- [ ] Policy block for restricted info
- [ ] Optional: snapshot minimal response structure (not prose)

---

ğŸ§  **Design philosophy:** The tests donâ€™t just prevent bugs â€” they preserve *trust*.  
ğŸ§­ If something cannot be proven (policy, provenance, citation), itâ€™s not â€œdone.â€