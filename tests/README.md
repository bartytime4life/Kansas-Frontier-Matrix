---
title: "ğŸ§ª Tests â€” Kansas Frontier Matrix (KFM) / Kansasâ€‘Matrixâ€‘System"
path: "tests/README.md"
version: "v1.4.0"
last_updated: "2026-01-20"
review_cycle: "90 days"
status: "active"
doc_kind: "Directory README"
license: "CC-BY-4.0"
owners:
  - "KFM Engineering"
tags:
  - tests
  - ci
  - determinism
  - contracts
  - catalogs
  - stac
  - dcat
  - prov
  - geo
  - eo
  - graph
  - ontology
  - api
  - ui
  - performance
  - governance
  - security
  - opa
  - conftest
  - receipts
  - run-manifest
  - telemetry
  - agents
  - wpe
  - streaming
  - offline
  - ar
  - supply-chain
  - oras
  - cosign
  - sbom
  - pdf
---

<!--
ğŸ“Œ This README defines the repo-wide testing & verification surface for KFM / Kansasâ€‘Matrixâ€‘System.
ğŸ—“ï¸ Last updated: 2026-01-20
ğŸ” Review cycle: 90 days (or anytime pipeline order / catalogs / policy pack / CI lanes change)
-->

<div align="center">

# ğŸ§ª Tests â€” Kansas Frontier Matrix (KFM) / Kansasâ€‘Matrixâ€‘System

**Trust-first testing for a contractâ€‘first, catalogâ€‘first geospatial + knowledge + modeling stack** ğŸ§¾ğŸ—ºï¸ğŸ§¬  
Determinism â€¢ Contracts â€¢ Governance â€¢ Evidence receipts â€¢ â€œFail closedâ€ gates âœ…ğŸ”’

![CI](https://img.shields.io/badge/CI-GitHub%20Actions-2ea44f?logo=githubactions&logoColor=white)
![CodeQL](https://img.shields.io/badge/Security-CodeQL-0b7285?logo=github&logoColor=white)
![Pytest](https://img.shields.io/badge/Python-pytest-blue?logo=python&logoColor=white)
![Node](https://img.shields.io/badge/Node.js-tests-brightgreen?logo=node.js&logoColor=white)
![Playwright](https://img.shields.io/badge/E2E-Playwright-0b7285?logo=playwright&logoColor=white)
![Docker](https://img.shields.io/badge/Integration-Docker%20Compose-2496ed?logo=docker&logoColor=white)
![Contracts](https://img.shields.io/badge/Contracts-OpenAPI%20%7C%20GraphQL-ff6b6b)
![Catalogs](https://img.shields.io/badge/Catalogs-STAC%20%7C%20DCAT%20%7C%20PROV-6f42c1)
![Policy%20as%20Code](https://img.shields.io/badge/Policy-OPA%20%2B%20Conftest-1f6feb)
![Supply%20Chain](https://img.shields.io/badge/Supply%20Chain-Cosign%20%2B%20SBOM-8a2be2)
![Fail%20Closed](https://img.shields.io/badge/Quality%20Gates-Fail%20Closed-red)

</div>

> KFM tests donâ€™t just check â€œit runs.â€  
> They prove that our **pipelines**, **catalogs**, **graph**, **APIs**, **docs/story nodes**, **Focus Mode**, and **UI behaviors** are:
>
> âœ… **Correct** â€¢ âœ… **Reproducible** â€¢ âœ… **Governanceâ€‘compliant** â€¢ âœ… **Honest about uncertainty**  
>
> We test the **seams (boundaries + contracts)** and treat metadata/provenance as **firstâ€‘class artifacts** ğŸ—‚ï¸ğŸ§¬

> [!IMPORTANT]
> **tests/** is part of KFMâ€™s *governed surface*.  
> If a change can affect what users see, what the system asserts as â€œtruthâ€, or what users can infer, it must be **testable**, **traceable**, and **failâ€‘closed** when requirements arenâ€™t met.

---

<details>
<summary><b>ğŸ§­ Table of contents</b></summary>

- [ğŸ”— Quick links](#-quick-links)
- [ğŸš¦ Nonâ€‘negotiables](#-non-negotiables)
- [ğŸš€ Quickstart](#-quickstart)
- [ğŸ§© KFM test matrix](#-kfm-test-matrix-subsystems--what-to-assert)
- [ğŸ§  Core invariant: governed ordering](#-core-invariant-governed-ordering)
- [ğŸ§± Architecture boundary tests](#-architecture-boundary-tests-clean-architecture)
- [ğŸ¤– Agentic QA workflows](#-agentic-qa-workflows-watcherplannerexecutor)
- [ğŸ”º Test pyramid](#-test-pyramid-how-we-keep-velocity--confidence)
- [ğŸ·ï¸ Test categories & markers](#ï¸-test-categories--markers-suggested)
- [ğŸ§° Tool & CLI contract tests](#-tool--cli-contract-tests)
- [ğŸ“„ Docs, Story Nodes, & Focus Mode validation](#-docs-story-nodes--focus-mode-validation)
- [ğŸ“„ PDF & doc-portfolio hygiene](#-pdf--doc-portfolio-hygiene)
- [ğŸ§¾ Contract & metadata tests](#-contract--metadata-tests)
- [ğŸ§¾ Evidence manifests & run receipts](#-evidence-manifests--run-receipts-run_manifestjson)
- [ğŸ§· Stable IDs & versioning tests](#-stable-ids--versioning-tests-dont-break-links)
- [ğŸ“œ License, citation, & redistribution tests](#-license-citation--redistribution-tests)
- [âœ… Data validation gates](#-data-validation-gates-fail-fast)
- [ğŸ“¡ Streaming & real-time tests](#-streaming--real-time-tests-watchers-sensors-gtfs-rt)
- [ğŸ—ºï¸ Geospatial tests](#ï¸-geospatial-tests-gis-correctness)
- [ğŸ›°ï¸ Remote sensing tests](#ï¸-remote-sensing-tests-earth-engine--imagery)
- [ğŸ§Š 3D / WebGL / 3D GIS tests](#-3d--webgl--3d-gis-tests)
- [ğŸ§  Scientific & simulation validation](#-scientific--simulation-validation)
- [ğŸ“Š ML / stats tests](#-ml--stats-tests-dont-fool-yourself)
- [ğŸ§­ Ontology & semantic layer tests](#-ontology--semantic-layer-tests-cidoc-crm-geosparql-owl-time-prov-o)
- [ğŸ•¸ï¸ Graph tests](#ï¸-graph-tests-neo4j--algorithms)
- [ğŸ§µ Pulse Threads & narrative detectors](#-pulse-threads--narrative-detectors)
- [ğŸ›¡ï¸ API tests](#ï¸-api-tests-fastapi--graphql)
- [ğŸŒ Web / frontend tests](#-web--frontend-test-guidance)
- [ğŸ“¦ Offline packs & AR tests](#-offline-packs--ar-tests)
- [ğŸ“¦ Supply chain & artifact integrity](#-supply-chain--artifact-integrity-oras-cosign-sbom)
- [ğŸ“ˆ Performance & capacity tests](#-performance--capacity-tests-latency-throughput-cost)
- [ğŸ” Security, governance, & ethics tests](#-security-governance--ethics-tests-defensive)
- [ğŸ§¾ Test artifacts & receipts](#-test-artifacts--receipts)
- [ğŸ—‚ï¸ Suggested folder layout](#ï¸-suggested-folder-layout)
- [âœ… CI gates](#-ci-gates-non-negotiable)
- [âœ… PR checklist](#-pr-checklist-copypaste)
- [ğŸ§¯ Troubleshooting](#-troubleshooting)
- [ğŸ“š Reference pointers](#-reference-pointers-project--library-index)
- [ğŸ•°ï¸ Version history](#ï¸-version-history)

</details>

---

## ğŸ”— Quick links

> Paths are relative to `tests/`. If your repo differs, treat these as the **target map** and document any deltas.

- ğŸ§­ Repo overview: `../README.md`
- ğŸ“š Docs boundary (canonical): `../docs/README.md`
- ğŸ§± Master Guide (v13): `../docs/MASTER_GUIDE_v13.md` *(if present)*
- ğŸ“¦ Data lifecycle + governance: `../data/README.md`
- ğŸ§¬ Schemas registry: `../schemas/` *(STAC/DCAT/PROV/Story/UI contracts)*
- ğŸ““ MCP (methods + receipts + model cards): `../mcp/`
- ğŸ§° Tools/validators (governed command surface): `../tools/README.md` *(if present)*
- ğŸ§‘â€âš–ï¸ Policy pack (OPA/Conftest): `../tools/validation/policy/` *(if present)*
- ğŸ§¾ Run receipts / audits: `../data/audits/` *(run manifests, checksums, receipts â€” if present)*
- ğŸ©º Graph health reports: `../docs/reports/qa/graph_health/` *(if present)*
- ğŸ¤– Agents (Watcher/Planner/Executor): `../src/agents/` or `../tools/agents/` *(if present)*
- ğŸ›¡ï¸ API boundary:
  - `../api/` *(if present â€” many KFM layouts put FastAPI here)*
  - `../src/server/` *(if present â€” some layouts put API here)*
- ğŸ›¡ï¸ API contracts (OpenAPI/GraphQL): `../src/server/contracts/` *(if present)*
- ğŸ“š Story Nodes (draft/published): `../docs/reports/story_nodes/`
- ğŸŒ Web UI boundary: `../web/` *(if present)*
- ğŸ¤ CI/CD + policy + security: `../.github/` *(workflows, templates, CodeQL, SECURITY.md)*

---

## ğŸš¦ Nonâ€‘negotiables

These are KFMâ€™s â€œmust not regressâ€ invariants. If any of these become false, **CI must block merge** ğŸš«âœ…

1) **Contractâ€‘first:** schemas + API contracts are firstâ€‘class repo artifacts ğŸ§¾  
   - Breaking changes must be explicit + versioned + tested.

2) **Catalogâ€‘first:** nothing is â€œrealâ€ unless itâ€™s cataloged (STAC/DCAT) and traceable (PROV) ğŸ—‚ï¸ğŸ§¬  
   - Catalogs are **boundary artifacts** that downstream stages consume.

3) **Governed ordering is enforced in tests** ğŸ§±  
   **Detect â†’ Validate â†’ Promote** across:
   **ETL â†’ STAC/DCAT/PROV â†’ Graph â†’ API â†’ UI â†’ Story Nodes â†’ Focus Mode**

4) **API boundary rule:** UI must never query Neo4j/DB directly ğŸ”  
   - Everything user-facing must flow through the API boundary for redaction + policy enforcement.

5) **Determinism by default:** reruns should match unless inputs/configs change ğŸ”  
   - Stochastic code must be seeded and tested by **properties** (not exact values).

6) **Sovereignty + classification propagation:** outputs canâ€™t be *less restricted* than inputs ğŸ·ï¸ğŸ›¡ï¸  
   - â€œNo downgradeâ€ is a gate, not a guideline.

7) **Policy as code is a gate:** governance rules must execute automatically (OPA/Conftest or equivalent) ğŸ“œğŸ§±  
   - If policy evaluation is unavailable, the pipeline **fails closed**.

8) **Run receipts exist for publish-grade outputs:** publishable artifacts must have a run receipt (run manifest + hashes + PROV link) ğŸ§¾ğŸ“  
   - If you canâ€™t answer â€œwhat produced this?â€ you canâ€™t publish it.

9) **No network in unit tests** ğŸš«ğŸŒ  
   - Record/replay, mock adapters, or cached fixtures only.

10) **Evidence over vibes:** failures must produce inspectable artifacts (logs, diffs, screenshots, receipts) ğŸ“

11) **Ontologies compile:** semantic layers must remain coherent (PROVâ€‘O + domain ontologies like CIDOCâ€‘CRM / GeoSPARQL / OWLâ€‘Time if used) ğŸ§ ğŸ§©  
   - No â€œmystery labelsâ€ without schema/ontology definitions.

12) **Supply chain is verifiable (when shipping artifacts):** if you publish containers/data packs/releases, signatures and SBOMs must verify ğŸ”ğŸ“¦  
   - If verification tooling is unavailable â†’ **fail closed** (release lane).

13) **Docs are searchable and linkable:** governed docs must be link-valid and searchable (no â€œPDF portfoliosâ€ without extraction) ğŸ“„ğŸ”  
   - If the repo canâ€™t search it, the repo canâ€™t govern it.

> [!TIP]
> If your PR changes **spatial truth** or **what users can infer**, it must be **traceable + testable** ğŸ§¾âœ…

---

## ğŸš€ Quickstart

### 0) Preconditions (oneâ€‘time)
- ğŸ Python env ready (`venv`, `uv`, `conda`, etc.)
- ğŸŒ Node env ready (`npm`, `pnpm`, or `yarn`)
- ğŸ³ Docker installed *(recommended for integration parity)*
- ğŸ“œ Policy tooling *(optional but recommended)*: `conftest` / `opa` *(if policy pack is enabled)*
- ğŸ” Supply chain tooling *(release lane)*: `cosign` *(if signature verification is enabled)*

### 1) Fast checks (developer loop âš¡)
```bash
# Python (fast)
pytest -q -m "not integration and not e2e and not slow"

# Web (fast â€” adapt to your repo)
npm test
```

### 2) Full suite (recommended on feature branches âœ…)
```bash
pytest -q
pytest -q -m integration
npm run test:e2e
```

### 3) Integration tests with containers (preferred ğŸ³)
```bash
docker compose up -d --build
pytest -q -m integration
docker compose down -v
```

<details>
<summary>ğŸ§¾ Command cheat sheet (copy/paste)</summary>

```bash
# Contracts only (API + schemas + catalogs)
pytest -q -m contracts

# Docs/story lint + story-node schema checks
pytest -q -m docs

# Policy pack (if enabled)
pytest -q -m policy

# Evidence receipts / run manifests
pytest -q -m receipts

# Geo sanity
pytest -q -m geo

# Earth-observation sanity
pytest -q -m eo

# Scientific V&V
pytest -q -m validation

# Ontology / semantic layer
pytest -q -m ontology

# Graph slice
pytest -q -m graph

# Streaming / watchers
pytest -q -m streaming

# Offline packs / AR
pytest -q -m offline
pytest -q -m ar

# Supply chain verification (release-lane tests)
pytest -q -m supplychain

# Performance/capacity (usually scheduled)
pytest -q -m perf

# Defensive security checks
pytest -q -m security

# Governance/policy pack (broader)
pytest -q -m governance
```
</details>

---

## ğŸ§© KFM test matrix (subsystems + what to assert)

KFM is layered (clean boundaries). Tests should **pin the seams** ğŸ”©:

| ğŸ§± Subsystem | ğŸ¯ What must never break | ğŸ§ª Best test types | ğŸ§° Typical tools |
|---|---|---|---|
| ğŸ§° Tools/CLIs | governed command surface: `--help`, safe defaults, stable exit codes, structured logs | unit âœ… + smoke âœ… | pytest, subprocess, snapshot tests |
| ğŸ§ª ETL / pipelines | deterministic outputs, idempotent reruns, schema+CRS correctness | unit âœ… + integration ğŸ”Œ + data QA gates âœ… | pytest, GDAL, GeoPandas, validators |
| ğŸ—‚ï¸ Catalogs (STAC/DCAT/PROV) | boundary artifacts exist *before* graph/UI uses data; links resolve; provenance complete | contracts ğŸ§¾ + integration ğŸ”Œ | JSON Schema, jq, custom validators |
| ğŸ§¾ Receipts (run manifests) | every publish-grade output has a run manifest + hashes + PROV linkage | contracts ğŸ§¾ + integration ğŸ”Œ | pydantic, jsonschema, canonicalization |
| ğŸ“œ Policy pack | governance rules are executable + fail closed | unit âœ… + integration ğŸ”Œ | OPA, Conftest, Rego tests |
| ğŸ•¸ï¸ Graph (Neo4j) | graph ingests **from catalogs**, not adâ€‘hoc; constraints + ontology rules hold | integration ğŸ”Œ + property tests ğŸ§ª | Neo4j container, Cypher assertions |
| ğŸ§­ Semantic layer | ontology alignment: PROV-O + domain ontologies (if used) remain coherent | contracts ğŸ§¾ + integration ğŸ”Œ | schema validation, Cypher + SHACL-ish checks |
| ğŸ›¡ï¸ API (REST/GraphQL) | contract stability, authz, deterministic pagination | contracts ğŸ§¾ + integration ğŸ”Œ | OpenAPI/GraphQL validation, TestClient |
| ğŸŒ UI (SPA) | responsive + accessible, stable map behaviors, timeline correctness | unit ğŸ§© + component ğŸ§± + e2e ğŸ§­ | Jest/Vitest, Playwright/Cypress |
| ğŸ—ºï¸ Maps / 3D | symbology & overlays donâ€™t silently shift; perf budgets | visual ğŸ–¼ï¸ + e2e ğŸ§­ | screenshot diffs, WebGL harness |
| ğŸ“š Story Nodes | citations resolve; narrative ordering consistent; no unsourced claims | docs âœ… + contracts ğŸ§¾ | markdown/link validators, schema checks |
| ğŸ¯ Focus Mode (AI) | provenance-linked outputs; safe refusals; uncertainty honesty; no sensitive leakage | eval âœ… + contract-like ğŸ§¾ | golden prompts, retrieval tests |
| ğŸ¤– Agents (Wâ€‘Pâ€‘E) | agents never bypass policy; kill-switch works; PRs are traceable | integration ğŸ”Œ + security âœ… | sandboxed runners, policy gates |
| ğŸ“¦ Offline packs | pack integrity, included licenses, sensitivity enforcement | integration ğŸ”Œ + contracts ğŸ§¾ | manifest validation, hashing |
| ğŸ›°ï¸ Streaming | idempotent ingestion (ETag/Lastâ€‘Modified), immutable events, replayable timeline | integration ğŸ”Œ + property tests ğŸ§ª | fixtures, time-series validators |
| ğŸ” Supply chain | released artifacts are verifiable (signatures + SBOM) | integration ğŸ”Œ + release lane âœ… | cosign, SBOM tools, ORAS (optional) |
| ğŸ“ˆ Performance | latency/throughput/cost regressions visible & explainable | perf â±ï¸ + scheduled âœ… | pytest-benchmark, k6, Locust, EXPLAIN |
| ğŸ” Governance | licenses, access constraints, â€œno downgradeâ€ classification | gates âœ… + integration ğŸ”Œ | OPA/Conftest, CI checks |

---

## ğŸ§  Core invariant: governed ordering

> [!IMPORTANT]
> KFM enforces a **nonâ€‘negotiable** pipeline order with a â€œDetect â†’ Validate â†’ Promoteâ€ mentality:
>
> **ETL â†’ STAC/DCAT/PROV catalogs â†’ Graph â†’ APIs â†’ UI â†’ Story Nodes â†’ Focus Mode**

```mermaid
flowchart LR
  A[ğŸ§ª ETL] --> B[ğŸ—‚ï¸ STAC/DCAT/PROV]
  B --> C[ğŸ•¸ï¸ Graph]
  C --> D[ğŸ›¡ï¸ API]
  D --> E[ğŸŒ UI]
  E --> F[ğŸ“š Story Nodes]
  F --> G[ğŸ¯ Focus Mode]
  B --> R[ğŸ§¾ Run Receipts]
  R --> C
```

### âœ… What tests should enforce (practically)
- ğŸ§ª ETL determinism (stable IDs/hashes; idempotent reruns; explicit versions)
- ğŸ—‚ï¸ Catalog records exist **before** graph/UI uses them
- ğŸ§¾ Run receipts exist for publish-grade outputs (run manifests + hashes + PROV link)
- ğŸ•¸ï¸ Graph loads only from catalogs (no adâ€‘hoc inserts in prod paths)
- ğŸ›¡ï¸ API is the only client boundary (UI never queries graph/DB directly)
- ğŸ“œ Policy pack enforces governance (licenses, classification, access controls)
- ğŸ” Classification/sensitivity never downgrades silently (requires audited redaction)
- ğŸ§¾ Provenance is complete (inputs â†’ activities â†’ outputs with run IDs/configs)
- ğŸ·ï¸ License is explicit before publish (block publish if missing/ambiguous)

---

## ğŸ§± Architecture boundary tests (clean architecture)

KFMâ€™s modular structure only stays maintainable if boundaries are enforced ğŸ§±âœ¨

### âœ… What to test
- **Dependency direction rules** (e.g., domain â†’ service â†’ adapters; never reverse)
- **No cross-layer shortcuts** (UI never imports DB drivers; pipelines donâ€™t import UI)
- **API boundary is the redaction/policy chokeâ€‘point** (everything public flows through it)
- **â€œIndependently testable componentsâ€ stays true** (isolated unit tests remain possible)

### ğŸ”§ Suggested patterns
- ğŸ§­ *Import-lints* that fail if forbidden imports appear (Python)  
- ğŸ§± *Module boundary tests* (TS/JS) so UI doesnâ€™t reach server internals  
- ğŸ”Œ *Contract-only integration tests* so adapters can be swapped without rewriting logic  

> [!TIP]
> Boundary tests are cheap insurance. They prevent â€œjust this onceâ€ coupling that becomes permanent. ğŸ§¯

---

## ğŸ¤– Agentic QA workflows (Watcherâ€“Plannerâ€“Executor)

KFMâ€™s roadmap includes **agent-assisted maintenance** (especially for data QA, catalog hygiene, and doc/story validation).  
If your repo implements this, treat it as a **high-risk boundary** that must be fenced by tests.

### ğŸ§  Mental model
- ğŸ‘€ **Watcher** detects drift (missing metadata, failing links, policy violations, stale indexes, schema mismatches)
- ğŸ§© **Planner** proposes tasks (ranked + scoped + governed)
- ğŸ› ï¸ **Executor** makes changes **only through PRs** (never direct writes to protected branches)

### âœ… Nonâ€‘negotiable agent controls (testable)
- ğŸ§¯ **Kill-switch:** one config flag disables all agent actions
- ğŸ§¾ **Receipt-first:** every agent action emits a structured receipt (inputs + decision + outputs)
- ğŸ§± **Policy-first:** agent outputs run through the same contract + policy gates as humans
- ğŸ§‘â€âš–ï¸ **No autonomous merge:** agents can open PRs, but cannot approve/merge
- ğŸ§° **Scoped diff:** PRs are limited to a declared scope (e.g., metadata fixes only)

### ğŸ§ª Test patterns (suggested)
- `test_agent_kill_switch_blocks_actions()`
- `test_agent_only_opens_prs_never_pushes_main()`
- `test_agent_receipt_schema_valid()`
- `test_agent_changes_fail_when_policy_fails()`
- `test_agent_does_not_weaken_classification()` *(no downgrade)*

> [!CAUTION]
> Agent tooling must never become a â€œback doorâ€ around governance.  
> If the policy pack canâ€™t run, **agents must halt** (fail closed). ğŸ”’

---

## ğŸ”º Test pyramid (how we keep velocity + confidence)

Most tests should be cheap and deterministic, then fewer (higherâ€‘value) integration + E2E:

```text
          ğŸ”º E2E (few)          â†’ critical user journeys (UI + API + DB)
        ğŸ”ºğŸ”º Integration (some)  â†’ services together (DB, API, pipelines)
      ğŸ”ºğŸ”ºğŸ”º Unit (many)          â†’ pure logic, transforms, validators
```

---

## ğŸ·ï¸ Test categories & markers (suggested)

### Python (`pytest`) markers
Standardize markers so devs can run focused slices quickly:

```ini
# pytest.ini (example)
[pytest]
markers =
  unit: fast pure logic
  integration: hits db/services/filesystem
  e2e: end-to-end journeys (rare for python)
  slow: long-running tests (non-gating)
  perf: benchmarks/capacity (usually scheduled)

  contracts: schemas + catalogs + API contract validation
  docs: markdown/front-matter/story-node validation
  pdf: PDF hygiene (searchable text layer / portfolios)

  receipts: run_manifest + hashes + receipts validation
  policy: OPA/Conftest policy pack execution and tests

  validation: scientific/V&V tests (tolerance-based)
  geo: GIS correctness checks
  eo: earth-observation / remote-sensing checks
  streaming: watchers / real-time ingestion checks
  offline: offline pack packaging + integrity
  ar: AR/field overlay correctness checks (where applicable)

  webgl: WebGL context + render sanity checks
  graph: graph (Neo4j + algorithms) checks
  ontology: PROV-O + domain ontology alignment checks (if used)

  api: API behavior checks (beyond schema)
  security: defensive security checks (no offensive testing)
  governance: license/classification/redaction/policy enforcement checks
  focus: Focus Mode contract tests (provenance + safety + uncertainty)
  supplychain: artifact signature/SBOM verification (release lane)
```

### Web tags (examples)
- Jest/Vitest: `test`, `test:unit`, `test:component`
- Playwright/Cypress: `test:e2e`
- Visual regression: `test:visual`

---

## ğŸ§° Tool & CLI contract tests

KFMâ€™s **governed toolchain** (`tools/`) is part of the contract surface. Tools must behave predictably under automation.

### âœ… What to assert for every CLI tool
- `--help` exists + includes **â‰¥2 examples**
- `--version` returns a stable value (semver or git SHA)
- safe-by-default (no writes unless `--apply`, or `--dry-run` default)
- stable exit codes (usage vs validation failure vs runtime failure)
- structured logs available (human + JSONL mode)
- **idempotency for ingest/watchers:** reruns do not duplicate outputs *(especially streaming watchers)*

### ğŸ”§ Suggested tests (patterns)
- `test_tools_help_smoke()` â€” exit 0 and non-empty help
- `test_tools_version_smoke()` â€” exit 0 and prints version
- `test_tools_dry_run_does_not_mutate()` â€” run against temp dirs; confirm no writes
- `test_tools_exit_codes_are_stable()` â€” validation failure â†’ code `3` (or your chosen standard)
- `test_tools_json_logs_valid_jsonl()` â€” parse emitted JSONL lines
- `test_watchers_are_idempotent()` â€” re-run fixture inputs; identical outputs + no dup IDs

> [!TIP]
> If you implement core logic inside `tools/`, thatâ€™s a smell.  
> Put logic in `src/` (or `api/src/`) and keep `tools/` as a predictable CLI + validator layer ğŸ› ï¸

---

## ğŸ“„ Docs, Story Nodes, & Focus Mode validation

KFM treats documentation + narrative as governed artifacts (not â€œfreeform notesâ€).

### âœ… Docs validation should cover
- YAML frontâ€‘matter present + valid
- internal link checks (`docs/**`, `data/**`, `schemas/**`)
- image/assets exist (no broken embeds)
- required sections exist for governed doc types *(templates)*
- **license/citation fragments resolve** (story exports + layer attributions)

### âœ… Story Nodes validation should cover
- lives under `docs/reports/story_nodes/{draft|published}/...`
- uses the Story Node template (v3) fields *(or repoâ€™s current schema)*
- citations resolve to cataloged sources (STAC/DCAT/PROV)
- narrative claims do **not** introduce uncited â€œfactsâ€
- published stories meet stricter gates than drafts
- **timeline correctness:** story steps align with dataset temporal extents

### âœ… Focus Mode contract tests should cover
- context bundles only contain provenanceâ€‘linked content
- AIâ€‘generated text is **clearly labeled** (and includes model/version where permissible)
- refusal behavior works when evidence is missing
- uncertainty is surfaced (intervals, confidence notes, or â€œunknownâ€)
- AI stays **advisory**: it cannot take autonomous actions or bypass policy gates
- **explainability hooks:** if UI includes an â€œauditâ€ panel, it must show the evidence set used

> [!CAUTION]
> If a Story Node (or Focus Mode) could expose sensitive locations or culturally sensitive information:  
> **CI should flag it for governance review** and block publish until review completes ğŸ”’

---

## ğŸ“„ PDF & doc-portfolio hygiene

Some KFM project PDFs are **PDF portfolios** (container PDFs with embedded files). Portfolios are convenient for distribution, but they are **hostile to repo search/governance** unless extracted.

### âœ… What to enforce
- PDFs in governed docs must be:
  - searchable (text layer present)
  - link-stable (file name + path stable)
  - extractable (no â€œportfolio-onlyâ€ docs in governed paths unless accompanied by extracted parts)

### ğŸ§ª Suggested tests
- `test_pdf_has_text_layer()` *(sample pages contain text extract)*
- `test_no_pdf_portfolios_in_docs()` *(or: portfolios require extraction manifest)*
- `test_doc_assets_exist_and_are_linked()` *(no broken embeds)*

> [!TIP]
> If you store portfolios for â€œlibrary shelvesâ€, add a small `LIBRARY_INDEX.md` next to them listing embedded docs and why they matter. ğŸ“šğŸ—‚ï¸

---

## ğŸ§¾ Contract & metadata tests

KFM is **contractâ€‘first** and **catalogâ€‘first**. Tests must protect:
- ğŸ›¡ï¸ OpenAPI / GraphQL contracts (breaking changes explicit + versioned)
- ğŸ—‚ï¸ STAC (collections/items validity + required fields)
- ğŸ·ï¸ DCAT (distributions point to real assets/endpoints)
- ğŸ§¬ PROV (inputs â†’ activities â†’ outputs; run IDs/configs recorded)
- ğŸ§¬ Crossâ€‘layer linkage (Graph references catalogs; UI references API; Story references catalogs)
- ğŸ§¾ â€œMetadata like codeâ€: validation is a compile step, not a best-effort lint

### âœ… What to validate
- JSON parses + schema passes
- links resolve (STAC assets exist; DCAT distributions point somewhere real)
- provenance completeness (raw â†’ work â†’ processed trace exists)
- stable IDs/hashes present where required
- time metadata makes sense (windows applied; plausible ranges)
- **required governance fields** present (license, sensitivity/classification, access constraints)
- **no deprecations:** policy can reject deprecated endpoints/layers *(if your policy pack defines this)*

### Example checks (starter)
```bash
# JSON sanity
find data/stac data/catalog/dcat data/prov -name "*.json*" -print0 | xargs -0 -n 1 jq empty

# pytest contract suite
pytest -q -m contracts
```

> [!NOTE]
> Dataset PRs should run a **Catalog QA gate** (schema + links + license) before merge.  
> If metadata is incomplete (e.g., missing license, broken href), CI must fail. ğŸš«âœ…

---

## ğŸ§¾ Evidence manifests & run receipts (`run_manifest.json`)

KFMâ€™s architecture treats **runs** as first-class, auditable events.  
If an output can be published, it should have a machine-validated run receipt.

### âœ… What a run receipt should contain (minimum)
- `run_id` (stable unique ID)
- `pipeline_id` / `stage` *(e.g., rawâ†’processed, processedâ†’publish)*
- `git_sha` / build identifier
- `started_at` / `ended_at`
- `inputs[]` with stable IDs + checksums
- `outputs[]` with stable IDs + checksums
- `config` (or hash of config)
- `environment` (optional but recommended: versions, container digest)
- `canonical_digest` (recommended): digest of a canonicalized manifest JSON for determinism
- `idempotency_key` (recommended for watchers + streaming)

### ğŸ§ª Suggested tests
- `test_run_manifest_schema_valid()`
- `test_run_manifest_references_real_catalog_entities()`
- `test_run_manifest_outputs_have_hashes()`
- `test_run_manifest_canonical_digest_matches()` *(if you implement canonicalization)*
- `test_streaming_idempotency_key_prevents_duplicates()`

> [!TIP]
> Treat run receipts like â€œunit tests for trust.â€ If theyâ€™re missing, you lost the thread. ğŸ§µğŸ§¾

---

## ğŸ§· Stable IDs & versioning tests (donâ€™t break links)

Stable IDs are how KFM stays citable, reversible, and auditable ğŸ§·ğŸ§¾

### âœ… What to test
- IDs remain stable across refactors and non-semantic changes
- IDs do not depend on display names alone (renames must not create new identities)
- merges/dedup donâ€™t silently rewrite public identifiers (requires migration record)
- â€œpublishedâ€ artifacts remain fetchable by prior IDs (redirect or alias map)

### ğŸ”§ Suggested patterns
- **Golden ID fixtures**: small fixture catalogs with â€œexpected IDsâ€ that must not change
- **Migration tests**: schema bumps must include a migration + tests for backward compatibility (where required)
- **Round-trip tests**: catalog â†’ graph â†’ API â†’ UI uses the same stable identifiers everywhere

> [!TIP]
> If an ID changes, you owe the repo an ADR + migration note + rollback plan. ğŸ§¾ğŸ”

---

## ğŸ“œ License, citation, & redistribution tests

Licensing is not paperwork â€” itâ€™s a publish gate ğŸ“œâœ…

### âœ… What to test (gating)
- every dataset/distribution has a **license** field before publish
- license terms are not contradictory across catalog layers (STAC/DCAT vs local metadata)
- restricted/non-commercial datasets trigger UI warnings (and/or access controls)
- attribution/citation generation works (e.g., story export includes sources list)
- presence of `CITATION.cff` for the software release (recommended)

### ğŸ”§ Suggested patterns
- `test_license_required_before_publish()` â€” fail if missing/unknown license
- `test_noncommercial_blocks_public_download()` â€” ensure policy is enforced
- `test_story_export_includes_attributions()` â€” evidence pointers flow through
- `test_layer_provenance_panel_has_license()` *(UI/contract test if implemented)*

> [!IMPORTANT]
> If a license is unclear, treat it as **restricted** until governance resolves it. ğŸ§¯

---

## âœ… Data validation gates (fail fast)

These gates are your â€œnoâ€‘badâ€‘data firewallâ€ ğŸ§±ğŸ”¥ â€” especially for GeoParquet + COG pipelines and publish flows.

### Ring model (recommended)
**Ring 0: Structure**
- parses (JSON/GeoJSON/Parquet/TIFF)
- schema validation (STAC/DCAT/PROV + local schemas)
- required files exist

**Ring 1: Integrity**
- checksums/manifest inventory (if used)
- deterministic IDs present where required
- atomic publish (no half-state)

**Ring 2: Semantics**
- CRS correctness + axis order
- geometry validity (and any allowed repair policy)
- raster sanity (nodata, overviews, alignment)
- time/bounds sanity (domain-specific checks)

**Ring 3: Governance & safety**
- license required before publish
- classification propagation (no downgrade)
- sensitive fields redaction rules
- policy tests (OPA/Conftest) where used
- secrets scans + dependency hygiene checks

---

## ğŸ“¡ Streaming & real-time tests (watchers, sensors, GTFSâ€‘RT)

KFMâ€™s roadmap includes **real-time ingestion** via â€œWatcherâ€ patterns (polling feeds with ETag/Lastâ€‘Modified, producing immutable events and catalog entries).

### âœ… What to test
- **Idempotency:** re-running the watcher does not duplicate observations
- **Caching correctness:** ETag/Lastâ€‘Modified prevents redundant pulls (fixture-based)
- **Immutable event log:** each observation is a unique event with timestamp/source metadata
- **Catalog-first streaming:** streaming events still produce STAC Items (and DCAT datasets if applicable)
- **Timeline replay correctness:** last 24h playback is ordered and consistent
- **Governance propagation:** streaming outputs inherit classification + license constraints
- **Backpressure & rate limits:** watchers donâ€™t overload sources (fixture-based throttling tests)

### ğŸ§ª Suggested patterns
- fixture feed snapshots (JSON) with known ETag / Lastâ€‘Modified sequences
- property tests over:
  - time monotonicity
  - dedup keys
  - event windowing
- integration tests: watcher â†’ STAC â†’ graph ingest â†’ API list â†’ UI layer refresh

> [!CAUTION]
> Unit tests should not call live services.  
> Prefer recorded fixtures, mock adapters, or cached exports. âœ…

---

## ğŸ—ºï¸ Geospatial tests (GIS correctness)

Geospatial pipelines fail in predictable waysâ€”test them explicitly:

- ğŸŒ CRS sanity: EPSG correctness; meters vs degrees; axis order
- ğŸ§± topology: geometry validity; selfâ€‘intersections per policy
- ğŸ§© overlay correctness: clip/intersect/union behaviors
- ğŸ§­ buffer correctness: distance units + projection correctness
- ğŸ§Š raster alignment: resolution, nodata handling, resampling method
- ğŸ“¦ format IO: GeoJSON/GeoPackage/GeoParquet/COG round-trips
- ğŸ§­ coordinate range checks: latitude/longitude in valid ranges
- ğŸ§® area sanity (datasetâ€‘specific): within tolerance of mask boundary
- ğŸ«¥ sensitive geometry policy: location generalization (e.g., pointâ†’hex/area) is correct and enforced

> [!TIP]
> Put CRS + units in the **fixture metadata** and test names.  
> It prevents â€œsilent degrees vs metersâ€ disasters. ğŸ¥²

---

## ğŸ›°ï¸ Remote sensing tests (Earth Engine & imagery)

Remote sensing workflows fail quietly unless assumptions are tested:
- band availability & naming
- scale / resolution
- cloud masking logic (QA bits)
- temporal compositing rules
- index calculations (NDVI) & expected ranges
- export formats & metadata consistency

Truthiness checks that catch many bugs:
- output range sanity (e.g., NDVI âˆˆ [-1, 1])
- masked pixel counts within expected bounds
- timestamps monotonic; windows applied correctly

> [!CAUTION]
> Unit tests should not call live services.  
> Prefer recorded fixtures, mock adapters, or small cached exports. âœ…

---

## ğŸ§Š 3D / WebGL / 3D GIS tests

KFM supports meaning-making beyond flat maps: 3D scenes, meshes, archaeology-grade reconstructions ğŸ§ŠğŸ—ºï¸

### âœ… What to test
- **WebGL context sanity** (creates reliably; fails gracefully; debug mode not shipped to prod)
- **Coordinate conventions** (ECEF vs local ENU vs EPSG; axis order; units)
- **LOD/tiling rules** (no â€œinfinite detailâ€ payloads; progressive loading works)
- **Georeferenced mesh validation** (mesh â†” site CRS â†” metadata alignment)
- **Visual regressions** (symbology + overlays + 3D render snapshots)

### ğŸ”§ Suggested patterns
- screenshot diffs at canonical zoom/tilt angles
- tiny deterministic scenes in fixtures (`tests/fixtures/3d/`)
- tolerance-based image diffs (antialiasing-aware) rather than pixel-perfect

> [!TIP]
> 3D can leak sensitive locations faster than 2D. Treat 3D fixtures as high-risk and keep them coarse + safe. ğŸ«¥ğŸ”’

---

## ğŸ§  Scientific / simulation validation

Treat simulation/analysis code like a **scientific instrument** ğŸ”¬:
- **verification**: implementation matches intended math
- **validation**: model matches reality within uncertainty
- **regression baselines**: detect drift across refactors

### âœ… Recommended patterns
- analytical solution comparisons (tiny cases with known answers)
- convergence tests (refinement reduces error)
- invariant checks (symmetry, conservation, monotonicity)
- tolerance-based golden files (with metadata + tolerances)
- uncertainty reporting checks (intervals, credible bands, PPC)

> [!TIP]
> If results are stochastic, test **properties** (ranges, quantiles, calibration), not exact values. ğŸ²âœ…

---

## ğŸ“Š ML / stats tests (donâ€™t fool yourself)

Data science code needs tests beyond â€œit runsâ€:
- split is leakâ€‘free (train/val/test)
- metrics stable within tolerance
- baseline comparisons exist (simple model beats random)
- diagnostics exist (residuals, leverage/outliers, calibration)
- uncertainty reporting present when relevant
- multiple comparisons / pâ€‘hacking risks handled (where applicable)

ğŸ“ On failure, attach plots as CI artifacts:
- confusion matrix
- residual plots
- calibration curve
- drift dashboards (if applicable)

> [!NOTE]
> ML tests should also protect **meaning**: avoid reporting a metric without context, uncertainty, and known failure modes. ğŸ§ ğŸ§¾

---

## ğŸ§­ Ontology & semantic layer tests (CIDOCâ€‘CRM, GeoSPARQL, OWLâ€‘Time, PROVâ€‘O)

KFMâ€™s knowledge graph is not â€œjust nodesâ€â€”itâ€™s meant to be **semantically coherent**.

### âœ… What to test (if your repo uses these concepts)
- **PROV completeness**: every published dataset node links to a PROV Activity with `run_id` + config hash
- **Temporal modeling**: OWLâ€‘Time-like structures remain consistent (no impossible intervals)
- **Geospatial semantics**: GeoSPARQL-like geometry fields remain valid and consistently typed
- **Cultural heritage modeling**: CIDOCâ€‘CRM-aligned entities keep required fields and relationships

### ğŸ§ª Suggested patterns
- schema-like constraints expressed as:
  - Cypher assertions
  - JSON Schema for graph export slices
  - optional SHACL-like checks in CI (if implemented)
- tiny â€œontology toy graphsâ€ as fixtures (fast, deterministic)

> [!TIP]
> If you canâ€™t describe your semantic model, you canâ€™t test it. Write it down once, then compile it forever. ğŸ§ âœ…

---

## ğŸ•¸ï¸ Graph tests (Neo4j + algorithms)

KFM treats the graph as **derived truth** (built from catalogs + provenance), not a writeâ€‘anywhere scratchpad.

Test categories:
- ğŸ§¾ graph build contract: rebuild from catalogs is reproducible
- ğŸ”’ constraints: uniqueness, required properties, relationship rules
- ğŸ§­ query invariants: deterministic pagination; stable ordering; filters correct
- ğŸ§  algorithm sanity: tiny deterministic graphs for spectral/routing invariants
- ğŸ§· canonical nodes: glossary terms/entities resolve consistently (no duplicate â€œalmost-sameâ€ nodes)
- ğŸ©º health checks: scheduled checks produce a report (see below)

Example assertions:
- â€œGraph contains only entities referenced by STAC/DCAT/PROVâ€
- â€œEvery published dataset node links to a PROV Activity with run_id + config hashâ€
- â€œNo unbounded traversals in query layer (guardrails enforced)â€
- â€œOntology label set is from the allowed registry (no mystery labels)â€
- â€œSpectral metrics (where used) stay within known bounds on toy graphsâ€ ğŸ§ âš¡

---

## ğŸ§µ Pulse Threads & narrative detectors

KFMâ€™s roadmap includes â€œPulse Threadsâ€ (short, geotagged updates) and narrative pattern detection (finding temporal/geographic clusters).

### âœ… What to test
- Pulse Thread schema validity (author, timestamp, location bounds, linked evidence IDs)
- time ordering + pagination determinism
- permission gating (who can publish pulses; moderation workflows)
- linkage integrity:
  - pulses link to cataloged evidence (STAC/DCAT/PROV IDs)
  - story nodes can reference pulses but must still remain citable
- detector sanity (if implemented):
  - known fixtures produce known â€œpattern hitsâ€
  - thresholds are configurable and tested
  - false-positive rate is monitored on toy corpora

### ğŸ§ª Suggested patterns
- small fixture pulse feeds as JSONL
- detectors tested as pure functions (unit) + end-to-end (integration)
- report artifacts produced on scheduled CI (daily/weekly)

> [!NOTE]
> â€œPattern detectionâ€ can create strong inferences. Treat it like a scientific claim: it needs provenance, uncertainty, and guardrails. ğŸ§¾ğŸ§ 

---

## ğŸ›¡ï¸ API tests (FastAPI + GraphQL)

What to test:
- ğŸ§¾ OpenAPI schema validation (breaking changes explicit)
- ğŸ§¾ GraphQL schema validation + query shapes
- ğŸ” AuthN/AuthZ: role-based access, classification enforcement
- ğŸ§­ Pagination determinism: stable ordering, cursor correctness
- ğŸŒ Geo correctness: GeoJSON validity; bbox correctness; CRS behavior
- ğŸŒ CORS headers correct (UI shouldnâ€™t need workarounds)
- ğŸ§¾ â€œFail closedâ€ for missing provenance/license: endpoints should not serve ungoverned data
- ğŸ«¥ sensitive outputs: coordinate generalization/redaction is enforced for restricted layers

---

## ğŸŒ Web / frontend test guidance

### ğŸ§± Component tests (fast)
- render correctness given props/state
- event handling correctness
- accessibility checks (labels, keyboard nav, contrast)

### ğŸ“± Responsive tests (mobileâ€‘first)
- run tests across multiple viewport sizes
- verify map controls remain usable on small screens
- check touch targets + scroll locking behaviors

### ğŸ§­ E2E tests (few but powerful)
Focus on â€œmoney pathsâ€ ğŸ’¸:
- auth/login
- load a layer **from catalog**
- timeline navigation (time slider updates map + panels)
- select feature â†’ details panel updates
- open â€œLayer Provenanceâ€ panel â†’ sources/licenses visible *(if implemented)*
- export/report flow (metadata/provenance attached)

### ğŸ–¼ï¸ Visual regression (maps + WebGL)
Maps can regress visually while â€œlogic testsâ€ pass. Use screenshot diffs for:
- symbology stability
- overlay legibility at common zooms
- dark/light contrast
- WebGL rendering regressions (tolerance-based diffs)

> [!CAUTION]
> Mobile experiences can unintentionally enable tracking.  
> If you render live location or device IDs, add tests for optâ€‘in + minimization + clear user controls. ğŸ“ğŸ«¥

---

## ğŸ“¦ Offline packs & AR tests

KFMâ€™s UI roadmap includes **offline data packs** (field use) and **AR overlays**.

### âœ… Offline pack tests
- pack manifest schema is valid
- included datasets have licenses + provenance
- hashing/integrity checks verify (no tampering)
- sensitivity/classification enforcement:
  - restricted layers are excluded or generalized appropriately
- deterministic rebuilds:
  - same inputs produce same pack hash *(within expected build variance)*

### âœ… AR overlay tests (if implemented)
- coordinate alignment sanity (CRS/units)
- graceful degradation on poor GPS (UX shows uncertainty)
- restricted location policies apply (AR must not reveal sensitive points)
- offline mode behavior consistent with governance (no hidden restricted caches)

> [!TIP]
> Treat offline packs like â€œmini-releasesâ€ with the same governance expectations as prod. ğŸ“¦âœ…

---

## ğŸ“¦ Supply chain & artifact integrity (ORAS, Cosign, SBOM)

When KFM ships artifacts (containers, offline packs, published datasets), the pipeline should be verifiable.

### âœ… What to test (release lane / gated)
- signed artifacts verify (cosign or equivalent)
- SBOM exists for software artifacts (and optionally for data pack manifests)
- provenance/attestations exist for release builds (optional but recommended)
- release only proceeds if:
  - contracts + policy pack pass
  - receipts exist for outputs
  - signatures verify in a clean environment

### ğŸ§ª Suggested patterns
- â€œverify in CIâ€ tests that run *after build* but *before release publish*
- signature verification tests run in a clean job (no local keys)
- SBOM validation: file exists + is parseable

> [!IMPORTANT]
> Supply chain checks are not optional once you publish. If verification canâ€™t run, **do not ship**. ğŸ”’

---

## ğŸ“ˆ Performance & capacity tests (latency, throughput, cost)

Performance tests are how we keep KFM usable as it scales ğŸ“ˆâš™ï¸

### âœ… What to measure (min set)
- **Latency distributions** (p50/p95/p99), not just averages
- **Throughput** under realistic concurrency
- **Error rates** under load (including timeouts)
- **Resource cost** (CPU/RAM/IO) per request / per pipeline run
- **DB query stability** (query plans donâ€™t regress silently)

### ğŸ§ª What to test (examples)
- API list endpoints: stable pagination under load
- graph queries: bounded traversals with sane timeouts
- tile/layer loading: payload budgets respected (no â€œmegatileâ€ surprises)
- ETL steps: runtime bounds on representative fixtures
- DB: migration impacts + index effectiveness (EXPLAIN plan snapshots)

### ğŸ•› Where these run
- PRs: **tiny perf smoke** (fast + deterministic)
- Nightly: full benchmarks + trend checks (alert on regressions)
- Release: publish-grade load profile (documented + repeatable)

> [!TIP]
> Treat performance like correctness: if it regresses, you need a reason, a measurement, and a rollback path. ğŸ§¾ğŸ”

---

## ğŸ” Security, governance, & ethics tests (defensive)

KFMâ€™s security stance is defensive: prevent leaks, enforce policy, keep audit trails.

### âœ… What to test (defensive)
- ğŸ” classification boundaries & redaction rules enforced (endâ€‘toâ€‘end)
- ğŸ•µï¸ secrets scanning (prevent committed tokens/keys)
- ğŸ“¦ dependency scanning (vulnerable libs flagged)
- ğŸ³ container scanning (base image CVEs flagged)
- ğŸ§¾ FAIR/CARE gates (required metadata present; access constraints honored)
- ğŸ§¬ auditability (publish actions produce receipts: who/what/when)
- ğŸ–¼ï¸ media hygiene: image metadata stripping; decoder safety checks for hostile inputs
- ğŸ§­ â€œsensitive inferenceâ€ tests: confirm UI cannot leak restricted info via aggregations

### ğŸ§‘â€âš–ï¸ Governance review triggers (manual review beyond CI)
Some changes should automatically require human review:
- adding sensitive/culturally protected datasets or layers
- new AI-driven narrative features
- new external data sources (license + provenance scrutiny)
- new public-facing endpoints or downloads that could expose sensitive info
- classification/sensitivity changes (especially â€œless restrictedâ€)
- enabling/expanding agentic workflows (Wâ€‘Pâ€‘E)

> [!IMPORTANT]
> Do **not** add offensive security instructions here.  
> Security tests are for hardening, verification, and prevention. ğŸ›¡ï¸âœ…

---

## ğŸ§¾ Test artifacts & receipts

KFM work is evidenceâ€‘driven. When tests fail, make failures inspectable:

### âœ… CI artifacts to upload on failure
- structured logs (`.jsonl` or `.txt`)
- diff outputs (schema diffs, snapshot diffs)
- screenshots (UI E2E + map diffs)
- â€œminiâ€ STAC/DCAT/PROV bundles from fixtures
- performance traces (if relevant)
- **run receipts** (`run_manifest.json`, checksums, â€œwhat changedâ€ summary)
- **policy evaluation output** (conftest/OPA logs) for governance failures

### ğŸ§¾ â€œReceipt mindsetâ€
For integration tests that simulate real workflows (pipeline â†’ catalogs â†’ graph â†’ API):
- record the run id / config used
- capture hashes of produced artifacts
- include a minimal PROV-like trace for the test run (even if toy)

> [!TIP]
> If a test failure canâ€™t be diagnosed from artifacts alone, itâ€™s a documentation bug. ğŸ““ğŸ§¯

---

## ğŸ—‚ï¸ Suggested folder layout

Adapt as needed, but keep intent obvious:

```text
ğŸ“¦ repo-root/
â”œâ”€ ğŸ§ª tests/
â”‚  â”œâ”€ ğŸ“„ README.md                      # you are here ğŸ‘‹
â”‚  â”œâ”€ ğŸ§· fixtures/                      # tiny, deterministic test data only
â”‚  â”‚  â”œâ”€ ğŸ—ºï¸ geo/                        # small vectors/rasters (safe + tiny)
â”‚  â”‚  â”œâ”€ ğŸ›°ï¸ eo/                         # tiny EO chips / QA bit samples
â”‚  â”‚  â”œâ”€ ğŸ“¡ streaming/                  # recorded feeds (ETag/Last-Modified sequences)
â”‚  â”‚  â”œâ”€ ğŸ§Š 3d/                         # tiny meshes/scenes (coarse + safe)
â”‚  â”‚  â”œâ”€ ğŸ–¼ï¸ media/                      # tiny images + metadata
â”‚  â”‚  â”œâ”€ ğŸ§¬ ml/                         # toy datasets / tiny model artifacts
â”‚  â”‚  â”œâ”€ ğŸ§¾ catalogs/                   # STAC/DCAT/PROV fixtures
â”‚  â”‚  â”œâ”€ ğŸ§¾ receipts/                   # run_manifest + checksum fixtures
â”‚  â”‚  â””â”€ ğŸ“˜ FIXTURES.md                 # fixture rules + provenance notes
â”‚  â”œâ”€ ğŸ“„ docs/                          # markdown/front-matter/story-node checks
â”‚  â”œâ”€ ğŸ“„ pdf/                           # PDF hygiene tests (optional)
â”‚  â”œâ”€ ğŸ§± architecture/                  # boundary tests (imports/dep rules)
â”‚  â”œâ”€ ğŸ§° tools_contract/                # CLI contract tests for tools/
â”‚  â”œâ”€ ğŸ“œ policy/                        # policy pack tests (rego unit tests, etc.)
â”‚  â”œâ”€ ğŸ¤– agents/                        # W-P-E guardrail tests (if enabled)
â”‚  â”œâ”€ ğŸ§¾ receipts/                      # run receipt validation tests
â”‚  â”œâ”€ ğŸ python/
â”‚  â”‚  â”œâ”€ ğŸ§© unit/
â”‚  â”‚  â”œâ”€ ğŸ”Œ integration/
â”‚  â”‚  â”œâ”€ âœ… validation/                 # scientific V&V (tolerance-based)
â”‚  â”‚  â”œâ”€ â±ï¸ perf/                       # benchmarks (scheduled)
â”‚  â”‚  â”œâ”€ ğŸ” security/                   # defensive security checks
â”‚  â”‚  â””â”€ ğŸ§± conftest.py
â”‚  â”œâ”€ ğŸŒ web/
â”‚  â”‚  â”œâ”€ ğŸ§© unit/
â”‚  â”‚  â”œâ”€ ğŸ§± component/
â”‚  â”‚  â”œâ”€ ğŸ§­ e2e/
â”‚  â”‚  â””â”€ ğŸ–¼ï¸ visual/
â”‚  â”œâ”€ ğŸ—„ï¸ db/
â”‚  â”‚  â”œâ”€ ğŸ§¬ migrations/
â”‚  â”‚  â””â”€ ğŸ”Œ integration/
â”‚  â”œâ”€ ğŸ•¸ï¸ graph/
â”‚  â”‚  â”œâ”€ ğŸ”Œ integration/
â”‚  â”‚  â”œâ”€ ğŸ§© unit/
â”‚  â”‚  â””â”€ ğŸ§­ ontology/                   # semantic/ontology checks (if used)
â”‚  â”œâ”€ ğŸ“œ contracts/
â”‚  â”‚  â”œâ”€ ğŸ›¡ï¸ api/                        # OpenAPI/GraphQL fixtures
â”‚  â”‚  â”œâ”€ ğŸ—ºï¸ stac/
â”‚  â”‚  â”œâ”€ ğŸ·ï¸ dcat/
â”‚  â”‚  â””â”€ ğŸ§¬ prov/
â”‚  â”œâ”€ ğŸ“¦ offline/                       # offline-pack validation tests (if enabled)
â”‚  â”œâ”€ ğŸ§Š ar/                            # AR overlay tests (if enabled)
â”‚  â”œâ”€ ğŸ” supply_chain/                  # signature/SBOM verification (release lane)
â”‚  â””â”€ ğŸ“˜ TEST_POLICY.md                 # merge gates + definition of done
â””â”€ ğŸ§° scripts/                          # CI glue & wrappers (optional; prefer tools/)
```

> [!NOTE]
> If you keep tests alongside code (e.g., `src/**/tests`), thatâ€™s fineâ€”just keep naming + markers consistent.

---

## âœ… CI gates (non-negotiable)

**Policy:** the pipeline must be green before merge ğŸ¤–âœ…  
CI should mirror KFMâ€™s â€œbuild â†’ validate â†’ publishâ€ discipline and keep logs/artifacts for traceability.

### âœ… Minimum PR gates (recommended)
1) ğŸ§¹ format + lint (Python + JS/TS)
2) ğŸ§± build (frontend + backend; container build if applicable)
3) ğŸ§ª unit tests
4) ğŸ§¾ docs protocol checks (frontâ€‘matter + link validation)
5) ğŸ§¾ schema validation (STAC/DCAT/PROV + story/node schemas)
6) ğŸ§¾ receipts validation (run manifests required for publishable outputs)
7) ğŸ“œ policy pack checks (OPA/Conftest) *(if enabled)*
8) âœ… data validation gates (CRS + geometry + raster sanity + license required)
9) ğŸ”Œ integration tests (ephemeral DB/services via Compose)
10) ğŸ•¸ï¸ graph integrity tests (constraints + rebuild invariants + ontology checks if used)
11) ğŸ›¡ï¸ API contract tests (OpenAPI/GraphQL + deterministic pagination)
12) ğŸ” security & governance scans (secrets + PII + sensitive location + â€œno downgradeâ€)
13) ğŸ§‘â€âš–ï¸ CodeQL/static analysis lane (recommended)
14) ğŸ“ˆ coverage thresholds (target, not a religion)

### ğŸ•› Nightly / scheduled checks (recommended)
Keep PR CI fast; push expensive checks to nightly:
- â±ï¸ benchmarks (trend monitoring)
- ğŸ§  longer ML runs (PRs use toy models)
- ğŸ—ºï¸ large raster workloads (PRs use fixtures & sampling)
- ğŸ•¸ï¸ deeper graph consistency (full rebuild + diff)
- ğŸ©º graph health checks report (markdown/json)
- ğŸ“¡ streaming replay checks (24h timeline replay on fixtures)
- ğŸ” deeper security scanning (if it slows PR CI)
- ğŸ” supply-chain verification lane (release candidate builds)

> [!TIP]
> If a gate is â€œimportantâ€, it must be **automatable** and **repeatable**.  
> If itâ€™s not automatable, it must be a **documented manual review step** ğŸ§¾

---

## âœ… PR checklist (copy/paste)

- [ ] Unit tests added/updated
- [ ] Integration tests added (if behavior crosses boundaries)
- [ ] Boundary tests updated (if you touched architecture seams)
- [ ] Determinism confirmed (seeds + stable outputs) if ML/sim
- [ ] Tools/CLI contract checks updated (if adding/modifying tools/)
- [ ] Docs/story checks updated (frontâ€‘matter, links, story templates) if docs changed
- [ ] Contracts updated + verified (OpenAPI/GraphQL) if API changed
- [ ] Catalog/metadata tests updated (STAC/DCAT/PROV) if outputs changed
- [ ] Run receipts updated/validated (run_manifest + hashes) if publishable outputs changed
- [ ] Policy pack checks pass (OPA/Conftest) if enabled
- [ ] Stable IDs preserved (or migration + ADR added) if identifiers changed
- [ ] Data validation gates updated (schema/CRS/geometry) if ETL changed
- [ ] License + governance checks pass (block publish if missing license)
- [ ] Sensitive location / â€œno downgradeâ€ checks pass (or governance review requested)
- [ ] UI changes include component tests + (if visual) snapshot updates
- [ ] Offline/AR tests updated *(if affected)*
- [ ] CI is green (required)

---

## ğŸ§¯ Troubleshooting

### âŒ Tests fail only in CI?
- check lockfiles & pinned versions
- confirm containers match local versions
- eliminate reliance on local paths, locale, timezone, GPU availability

### ğŸ² Flaky tests?
- remove sleeps; wait on conditions
- fix randomness (seed)
- mock/record external services

### ğŸ³ Docker stack wonâ€™t start?
```bash
docker compose logs -f
docker compose config
docker compose up -d --build
```

---

## ğŸ“š Reference pointers (project + library index)

These project files (and embedded library shelves) inform KFMâ€™s test posture: contracts, governance, UI trust surfaces, AI guardrails, geospatial correctness, simulation V&V, and scalable CI/CD. ğŸ§ ğŸ§¾

<details>
<summary>ğŸ§­ Core KFM project docs (define what tests must protect)</summary>

- `Kansas Frontier Matrix (KFM) â€“ Comprehensive Technical Documentation.pdf` *(end-to-end architecture, governance, licensing posture, and trust surface)*
- `Kansas Frontier Matrix (KFM) â€“ Comprehensive Architecture, Features, and Design.pdf` *(system boundaries, catalogs-first/contract-first, federation mindset)*
- `ğŸ“š Kansas Frontier Matrix (KFM) Data Intake â€“ Technical & Design Guide.pdf` *(detectâ†’validateâ†’promote pipeline, policy pack, data QA posture)*
- `Kansas Frontier Matrix (KFM) â€“ AI System Overview ğŸ§­ğŸ¤–.pdf` *(Focus Mode contracts, safety, W-P-E patterns, provenance requirements)*
- `Kansas Frontier Matrix â€“ Comprehensive UI System Overview.pdf` *(UI boundaries: map/timeline/story/focus mode, offline packs, AR, community workflows)*
- `ğŸŒŸ Kansas Frontier Matrix â€“ Latest Ideas & Future Proposals.docx.pdf` *(roadmap: streaming watchers, DevOpsâ†’PROV integration, policy pack, reproducible research lanes)*
- `Innovative Concepts to Evolve the Kansas Frontier Matrix (KFM).pdf` *(4D/VoxelMaps, sensory layers, AR + field workflows; adds test needs for time+uncertainty)*
- `Additional Project Ideas.pdf` *(run manifests, JSON canonicalization, narrative detectors, graph health checks, ORAS/cosign supply-chain ideas)*

</details>

<details>
<summary>ğŸ“š â€œLibrary shelfâ€ portfolios (embedded references; influence test standards)</summary>

> These are PDF portfolios (containers with embedded docs). If you rely on them for governance, extract them and index them in-repo.

- `AI Concepts & more.pdf` *(AI law/humanism/ethics + safety framing)*
- `Maps-GoogleMaps-VirtualWorlds-Archaeological-Computer Graphics-Geospatial-webgl.pdf` *(WebGL, map projections, archaeology-grade 3D GIS and mapping UX)*
- `Data Managment-Theories-Architures-Data Science-Baysian Methods-Some Programming Ideas.pdf` *(data architecture + Bayesian rigor + CI/CD patterns + clean architecture)*
- `Various programming langurages & resources 1.pdf` *(language/tooling references; security awareness â€” keep tests defensive)*

</details>

---

## ğŸ•°ï¸ Version history

| Version | Date | Summary | Author |
|---:|---|---|---|
| v1.4.0 | 2026-01-20 | Added agentic QA guardrails (Watcherâ€“Plannerâ€“Executor), policy-pack lane (OPA/Conftest), run receipts (`run_manifest.json`) + canonical determinism guidance, streaming/watchers test lane, ontology/semantic layer checks, offline pack + AR test guidance, supply-chain verification lane (Cosign/SBOM), and PDF portfolio hygiene gates. Updated markers, folder layout, CI gates, and reference pointers to align with KFM project docs and roadmap. | KFM Engineering |
| v1.3.0 | 2026-01-13 | Added architecture boundary tests, stable ID/versioning lane, explicit license/citation gates, 3D/WebGL/3Dâ€‘GIS testing guidance, and performance/capacity test lane. Updated quick links and CI gates to include CodeQL/static analysis lane. | KFM Engineering |
| v1.2.0 | 2026-01-11 | Aligned tests with Master Guide v13: contract-first + catalog-first gates, docs/story-node validation lane, governance trigger guidance, and tool/CLI contract testing. Removed internal placeholder evidence anchors. | KFM Engineering |
| v1.1.0 | 2026-01-09 | Tightened â€œcatalog-firstâ€ & data QA gates; added receipts/artifacts section; clarified defensive security stance; aligned CI gates with KFM engineering/testing guidance. | KFM Engineering |
| v1.0.0 | 2026-01-07 | Initial repo-wide testing README: pyramid, markers, subsystem matrix, validation + governance posture. | KFM Engineering |