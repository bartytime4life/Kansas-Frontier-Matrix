<!--
ğŸ“Œ This README defines the repo-wide testing & verification surface for KFM / Kansasâ€‘Matrixâ€‘System.
ğŸ—“ï¸ Last updated: 2026-01-09
-->

<div align="center">

# ğŸ§ª Tests â€” Kansas Frontier Matrix (KFM) / Kansasâ€‘Matrixâ€‘System

**Trust-first testing for a catalogâ€‘first geospatial + knowledge + modeling stack** ğŸ§¾ğŸ—ºï¸ğŸ§¬  
Determinism â€¢ Contracts â€¢ Governance â€¢ Evidence receipts

![CI](https://img.shields.io/badge/CI-GitHub%20Actions-2ea44f?logo=githubactions&logoColor=white)
![Coverage](https://img.shields.io/badge/Coverage-target%20%F0%9F%9A%80-informational)
![Python](https://img.shields.io/badge/Python-pytest-blue?logo=python&logoColor=white)
![Node](https://img.shields.io/badge/Node.js-tests-brightgreen?logo=node.js&logoColor=white)
![Docker](https://img.shields.io/badge/Docker-Compose-2496ed?logo=docker&logoColor=white)
![Deterministic](https://img.shields.io/badge/Determinism-preferred-success)
![Catalogs](https://img.shields.io/badge/Catalogs-STAC%20%7C%20DCAT%20%7C%20PROV-6f42c1)
![DB](https://img.shields.io/badge/DB-PostGIS%20%7C%20Neo4j-0b7285)
![API](https://img.shields.io/badge/API-FastAPI%20%7C%20GraphQL-ff6b6b)
![UI](https://img.shields.io/badge/UI-MapLibre%20%7C%20Cesium%20%7C%20WebGL-fab005)
![Governance](https://img.shields.io/badge/Governance-FAIR%20%7C%20CARE-2f9e44)

</div>

> KFM tests donâ€™t just check â€œit runs.â€  
> They prove that our **pipelines**, **catalogs**, **graph**, **APIs**, and **UI behaviors** are **correct**, **reproducible**, and **honest about uncertainty** âœ…ğŸ”¬  
> We test the seams (boundaries + contracts) and treat metadata/provenance as firstâ€‘class artifacts ğŸ—‚ï¸ğŸ§¬

---

<details>
<summary><b>ğŸ§­ Table of contents</b></summary>

- [ğŸ”— Quick links](#-quick-links)
- [ğŸš¦ Nonâ€‘negotiables](#-non-negotiables)
- [ğŸš€ Quickstart](#-quickstart)
- [ğŸ§© KFM test matrix](#-kfm-test-matrix-subsystems--what-to-assert)
- [ğŸ§  Core invariant: governed ordering](#-core-invariant-governed-ordering)
- [ğŸ”º Test pyramid](#-test-pyramid-how-we-keep-velocity--confidence)
- [ğŸ·ï¸ Test categories & markers](#ï¸-test-categories--markers-suggested)
- [ğŸ§¾ Contract & metadata tests](#-contract--metadata-tests)
- [âœ… Data validation gates](#-data-validation-gates-fail-fast)
- [ğŸ—ºï¸ Geospatial tests](#ï¸-geospatial-tests-gis-correctness)
- [ğŸ›°ï¸ Remote sensing tests](#ï¸-remote-sensing-tests-earth-engine--imagery)
- [ğŸ§  Scientific & simulation validation](#-scientific--simulation-validation)
- [ğŸ“Š ML / stats tests](#-ml--stats-tests-dont-fool-yourself)
- [ğŸ•¸ï¸ Graph tests](#ï¸-graph-tests-neo4j--algorithms)
- [ğŸ›¡ï¸ API tests](#ï¸-api-tests-fastapi--graphql)
- [ğŸŒ Web / frontend tests](#-web--frontend-test-guidance)
- [ğŸ” Security, governance, & ethics tests](#-security-governance--ethics-tests-defensive)
- [ğŸ§¾ Test artifacts & receipts](#-test-artifacts--receipts)
- [ğŸ—‚ï¸ Suggested folder layout](#ï¸-suggested-folder-layout)
- [âœ… CI gates](#-ci-gates-non-negotiable)
- [âœ… PR checklist](#-pr-checklist-copypaste)
- [ğŸ§¯ Troubleshooting](#-troubleshooting)
- [ğŸ“š Reference pointers](#-reference-pointers-library-index)
- [ğŸ•°ï¸ Version history](#ï¸-version-history)

</details>

---

## ğŸ”— Quick links

- ğŸ§­ Repo overview: `../README.md`
- ğŸ§© Executable boundary (architecture): `../src/README.md`
- ğŸ“¦ Data lifecycle + governance: `../data/README.md`
- ğŸ““ MCP (experiments + run receipts): `../mcp/MCP-README.md`
- ğŸ§° Automation helpers: `../scripts/README.md`
- ğŸŒ Web UI boundary: `../web/README.md` *(if present)*

---

## ğŸš¦ Nonâ€‘negotiables

1) **Catalog-first:** nothing is â€œrealâ€ unless itâ€™s cataloged (STAC/DCAT) and traceable (PROV) ğŸ—‚ï¸ğŸ§¬  
2) **Determinism by default:** reruns should match unless inputs/configs change ğŸ”  
3) **Governed ordering is enforced in tests** (see below) ğŸ§±  
4) **No network in unit tests** (record/replay or mock) ğŸš«ğŸŒ  
5) **Defensive security only:** tests harden systems; they donâ€™t teach exploitation ğŸ›¡ï¸  
6) **Evidence over vibes:** failing tests should produce actionable artifacts (logs, diffs, screenshots) ğŸ“

> [!TIP]
> If your PR changes **spatial truth**, it must be **traceable + contractable + testable** ğŸ§¾âœ…

---

## ğŸš€ Quickstart

### 0) Preconditions (one-time)
- ğŸ Python env ready (`venv`, `conda`, `uv`, etc.)
- ğŸŒ Node env ready (`npm`, `pnpm`, or `yarn`)
- ğŸ³ Docker installed *(recommended for integration parity)*

> [!IMPORTANT]
> If your PR touches **DB / API / pipelines / contracts / catalogs / graph**  
> run at least one Docker-backed integration pass before requesting review ğŸ³âœ…

### 1) Fast checks (developer loop âš¡)
```bash
# Python (fast)
pytest -q -m "not integration and not e2e and not slow"

# Web (fast â€” adapt to your repo)
npm test
```

### 2) Full suite (recommended on feature branches âœ…)
```bash
make test        # preferred if available

# or:
pytest -q
pytest -q -m integration
npm run test:e2e
```

### 3) Integration tests with containers (preferred ğŸ³)
```bash
docker compose up -d --build
pytest -q -m integration
docker compose down -v
```

<details>
<summary>ğŸ§¾ Command cheat sheet (copy/paste)</summary>

```bash
# Contracts only
pytest -q -m contracts

# Geo sanity
pytest -q -m geo

# Earth-observation sanity
pytest -q -m eo

# Scientific V&V
pytest -q -m validation

# Graph slice
pytest -q -m graph

# Defensive security checks
pytest -q -m security
```
</details>

---

## ğŸ§© KFM test matrix (subsystems + what to assert)

KFM is layered (clean boundaries). Tests should **pin the seams** ğŸ”©:

| ğŸ§± Subsystem | ğŸ¯ What must never break | ğŸ§ª Best test types | ğŸ§° Typical tools |
|---|---|---|---|
| ğŸ§ª ETL / pipelines | deterministic outputs, idempotent reruns, schema+CRS correctness | unit âœ… + integration ğŸ”Œ + data QA gates âœ… | pytest, GDAL, GeoPandas, validators |
| ğŸ—‚ï¸ Catalogs (STAC/DCAT/PROV) | catalogs exist *before* graph/UI uses data; links resolve; provenance complete | contracts ğŸ§¾ + integration ğŸ”Œ | JSON Schema, jq, custom validators |
| ğŸ•¸ï¸ Graph (Neo4j) | graph ingests **from catalogs**, not ad-hoc; constraints hold | integration ğŸ”Œ + property tests ğŸ§ª | Neo4j container, Cypher assertions |
| ğŸ›¡ï¸ API (REST/GraphQL) | contract stability, authz, deterministic pagination | contracts ğŸ§¾ + integration ğŸ”Œ | OpenAPI/GraphQL validation, TestClient |
| ğŸŒ UI (SPA) | responsive + accessible, stable map behaviors, timeline correctness | unit ğŸ§© + component ğŸ§± + e2e ğŸ§­ | Jest/Vitest, Playwright/Cypress |
| ğŸ—ºï¸ Maps / 3D | symbology & overlays donâ€™t silently shift; perf budgets | visual ğŸ–¼ï¸ + e2e ğŸ§­ | screenshot diffs, WebGL harness |
| ğŸ“š Story Nodes | citations resolve; narrative ordering consistent | unit ğŸ§© + contracts ğŸ§¾ | markdown/link validators |
| ğŸ¯ Focus Mode (AI) | citation discipline, refusal on missing evidence, uncertainty honesty | eval âœ… + contract-like tests ğŸ§¾ | golden prompts, retrieval tests |
| ğŸ” Governance | licenses, access constraints, auditability | gates âœ… + integration ğŸ”Œ | policy validators, CI checks |

---

## ğŸ§  Core invariant: governed ordering

> [!IMPORTANT]
> KFM enforces a **nonâ€‘negotiable** pipeline order:
>
> **ETL â†’ Catalogs (STAC/DCAT/PROV) â†’ Graph â†’ APIs â†’ UI â†’ Story Nodes â†’ Focus Mode**

```mermaid
flowchart LR
  A[ğŸ§ª ETL] --> B[ğŸ—‚ï¸ Catalogs<br/>STAC/DCAT/PROV]
  B --> C[ğŸ•¸ï¸ Graph]
  C --> D[ğŸ›¡ï¸ API]
  D --> E[ğŸŒ UI]
  E --> F[ğŸ“š Story Nodes]
  F --> G[ğŸ¯ Focus Mode]
```

### âœ… What tests should enforce (practically)
- ğŸ§ª ETL determinism (stable IDs/hashes; idempotent reruns; explicit versions)
- ğŸ—‚ï¸ Catalog records exist (STAC/DCAT/PROV) **before** graph/UI uses them
- ğŸ•¸ï¸ Graph loads only from catalogs (no adâ€‘hoc inserts in prod paths)
- ğŸ›¡ï¸ API is the only client boundary (UI never queries graph/DB directly)
- ğŸ” Classification/sensitivity never downgrades silently (requires audited redaction)
- ğŸ§¾ Provenance is complete (inputs â†’ activities â†’ outputs with run IDs/configs)
- ğŸ·ï¸ License is explicit before publish (block publish if missing/ambiguous)

---

## ğŸ”º Test pyramid (how we keep velocity + confidence)

Most tests should be cheap and deterministic, then fewer (higher-value) integration + E2E:

```text
          ğŸ”º E2E (few)          â†’ critical user journeys (UI + API + DB)
        ğŸ”ºğŸ”º Integration (some)  â†’ services together (DB, API, pipelines)
      ğŸ”ºğŸ”ºğŸ”º Unit (many)          â†’ pure logic, transforms, validators
```

---

## ğŸ·ï¸ Test categories & markers (suggested)

### Python (`pytest`) markers
Standardize markers so devs can run focused slices quickly:

```ini
# pytest.ini (example)
[pytest]
markers =
  unit: fast pure logic
  integration: hits db/services/filesystem
  e2e: end-to-end journeys (rare for python)
  slow: long-running tests (non-gating)
  validation: scientific/V&V tests (tolerance-based)
  perf: benchmarks (nightly)
  contracts: OpenAPI + metadata contract validation
  geo: GIS correctness checks
  eo: earth-observation / remote-sensing checks
  graph: graph (Neo4j + algorithms) checks
  security: defensive security checks (no offensive testing)
```

### Web tags (examples)
- Jest/Vitest: `test`, `test:unit`, `test:component`
- Playwright/Cypress: `test:e2e`
- Visual regression: `test:visual`

---

## ğŸ§¾ Contract & metadata tests

KFM is **contract-first** and **catalog-first**. Tests must protect:
- ğŸ›¡ï¸ OpenAPI / GraphQL contracts (breaking changes are explicit + versioned)
- ğŸ—‚ï¸ STAC (collections/items validity + required fields)
- ğŸ·ï¸ DCAT (distributions point to real assets/endpoints)
- ğŸ§¬ PROV (inputs â†’ activities â†’ outputs; run IDs/configs recorded)

### âœ… What to validate
- JSON parses + schema passes
- links resolve (STAC assets exist; DCAT distributions point somewhere real)
- provenance completeness (raw â†’ work â†’ processed trace exists)
- stable IDs/hashes present where required
- time metadata makes sense (windows applied; monotonic when required)

### Example checks (starter)
```bash
# JSON sanity
find data/stac data/catalog/dcat data/prov -name "*.json*" -print0 | xargs -0 -n 1 jq empty

# pytest contract suite
pytest -q -m contracts
```

> [!NOTE]
> Dataset PRs should run a **Catalog QA gate** (schema + links + license) before merge.  
> If metadata is incomplete (e.g., missing license, broken href), CI must fail. ğŸš«âœ…

---

## âœ… Data validation gates (fail fast)

These gates are your â€œnoâ€‘badâ€‘data firewallâ€ ğŸ§±ğŸ”¥ â€” especially for GeoParquet + COG pipelines and publish flows.

**Typical gate set (recommended):**
- âœ… field non-empties: required columns present + non-null
- ğŸ§­ CRS required: input CRS present; output CRS standardized; original CRS recorded for lineage
- ğŸ§± geometry valid: non-empty; policy for `make_valid` explicit
- ğŸ§Š raster sanity: nodata/resolution consistent; alignment rules enforced
- ğŸ§¾ catalogs emitted: STAC/DCAT/PROV produced + validate
- ğŸ·ï¸ license present: block publish if missing/ambiguous
- ğŸ” privacy/classification: no downgrades; redaction/aggregation audited

**Example CI hooks (pattern):**
```bash
pytest -q -k "test_<dataset>_etl_"

python scripts/qa/validate_geoparquet_schema.py
python scripts/geo/validate_geom.py
python scripts/governance/validate_license.py
python scripts/qa/catalog_qa.py data/stac  # example name
```

---

## ğŸ—ºï¸ Geospatial tests (GIS correctness)

Geospatial pipelines fail in predictable waysâ€”test them explicitly:

- ğŸŒ CRS sanity: EPSG correctness; meters vs degrees; axis order
- ğŸ§± topology: geometry validity; self-intersections per policy
- ğŸ§© overlay correctness: clip/intersect/union behaviors
- ğŸ§Š raster alignment: resolution, nodata handling, resampling method
- ğŸ“¦ format IO: GeoJSON/GeoPackage/GeoParquet/COG round-trips
- ğŸ§­ coordinate range checks: latitude/longitude in valid ranges
- ğŸ§® area sanity (datasetâ€‘specific): within tolerance of mask boundary

> [!TIP]
> Put CRS + units in the **fixture metadata** and test names.  
> It prevents â€œsilent degrees vs metersâ€ disasters. ğŸ¥²

---

## ğŸ›°ï¸ Remote sensing tests (Earth Engine & imagery)

Remote sensing workflows fail quietly unless assumptions are tested:
- band availability & naming
- scale / resolution
- cloud masking logic (QA bits)
- temporal compositing rules
- index calculations (NDVI) & expected ranges
- export formats & metadata consistency

Truthiness checks that catch many bugs:
- output range sanity (e.g., NDVI âˆˆ [-1, 1])
- masked pixel counts within expected bounds
- timestamps monotonic; windows applied correctly

> [!CAUTION]
> Unit tests should not call live services.  
> Prefer recorded fixtures, mock adapters, or small cached exports. âœ…

---

## ğŸ§  Scientific / simulation validation

Treat simulation/analysis code like a **scientific instrument** ğŸ”¬:
- **verification**: implementation matches intended math
- **validation**: model matches reality within uncertainty
- **regression baselines**: detect drift across refactors

### âœ… Recommended patterns
- analytical solution comparisons (tiny cases with known answers)
- convergence tests (refinement reduces error)
- invariant checks (symmetry, conservation, monotonicity)
- tolerance-based golden files (with metadata + tolerances)
- uncertainty reporting checks (intervals, credible bands, PPC)

> [!TIP]
> If results are stochastic, test **properties** (ranges, quantiles, calibration), not exact values. ğŸ²âœ…

---

## ğŸ“Š ML / stats tests (donâ€™t fool yourself)

Data science code needs tests beyond â€œit runsâ€:
- split is leakâ€‘free (train/val/test)
- metrics stable within tolerance
- baseline comparisons exist (simple model beats random)
- diagnostics exist (residuals, leverage/outliers, calibration)
- uncertainty reporting present when relevant
- multiple comparisons / p-hacking risks handled (where applicable)

ğŸ“ On failure, attach plots as CI artifacts:
- confusion matrix
- residual plots
- calibration curve
- drift dashboards (if applicable)

---

## ğŸ•¸ï¸ Graph tests (Neo4j + algorithms)

KFM treats the graph as **derived truth** (built from catalogs + provenance), not a write-anywhere scratchpad.

Test categories:
- ğŸ§¾ graph build contract: rebuild from catalogs is reproducible
- ğŸ”’ constraints: uniqueness, required properties, relationship rules
- ğŸ§­ query invariants: deterministic pagination; stable ordering; filters correct
- ğŸ§  algorithm sanity: tiny deterministic graphs for spectral/routing invariants

Example assertions:
- â€œGraph contains only entities referenced by STAC/DCAT/PROVâ€
- â€œEvery published dataset node links to a PROV Activity with run_id + config hashâ€
- â€œNo unbounded traversals in query layer (guardrails enforced)â€

---

## ğŸ›¡ï¸ API tests (FastAPI + GraphQL)

What to test:
- ğŸ§¾ OpenAPI schema validation (breaking changes explicit)
- ğŸ§¾ GraphQL schema validation + query shapes
- ğŸ” AuthN/AuthZ: role-based access, classification enforcement
- ğŸ§­ Pagination determinism: stable ordering, cursor correctness
- ğŸŒ Geo correctness: GeoJSON validity; bbox correctness; CRS behavior
- ğŸŒ CORS headers correct (UI shouldnâ€™t need workarounds)

---

## ğŸŒ Web / frontend test guidance

### ğŸ§± Component tests (fast)
- render correctness given props/state
- event handling correctness
- accessibility checks (labels, keyboard nav, contrast)

### ğŸ“± Responsive tests (mobile-first)
- run tests across multiple viewport sizes
- verify map controls remain usable on small screens
- check touch targets + scroll locking behaviors

### ğŸ§­ E2E tests (few but powerful)
Focus on â€œmoney pathsâ€ ğŸ’¸:
- auth/login
- load a layer **from catalog**
- timeline navigation (time slider updates map + panels)
- select feature â†’ details panel updates
- export/report flow (metadata/provenance attached)

### ğŸ–¼ï¸ Visual regression (maps + WebGL)
Maps can regress visually while â€œlogic testsâ€ pass. Use screenshot diffs for:
- symbology stability
- overlay legibility at common zooms
- dark/light contrast
- WebGL rendering regressions (tolerance-based diffs)

---

## ğŸ” Security, governance, & ethics tests (defensive)

KFMâ€™s security stance is defensive: prevent leaks, enforce policy, keep audit trails.

Test themes:
- ğŸ” classification boundaries & redaction rules enforced
- ğŸ•µï¸ secrets scanning: prevent committed tokens/keys
- ğŸ“¦ dependency scanning: vulnerable libs flagged
- ğŸ³ container scanning: base image CVEs flagged
- ğŸ§¾ FAIR/CARE gates: required metadata present; access constraints honored
- ğŸ§¬ auditability: publish actions produce receipts (who/what/when)

> [!IMPORTANT]
> Do **not** add offensive security instructions here.  
> Security tests are for hardening, verification, and prevention. ğŸ›¡ï¸âœ…

---

## ğŸ§¾ Test artifacts & receipts

KFM work is evidence-driven. When tests fail, make failures inspectable:

### âœ… CI artifacts to upload on failure
- structured logs (`.jsonl` or `.txt`)
- diff outputs (schema diffs, snapshot diffs)
- screenshots (UI E2E + map diffs)
- â€œminiâ€ STAC/DCAT/PROV bundles from fixtures
- performance traces (if relevant)

### ğŸ§¾ â€œReceipt mindsetâ€
For integration tests that simulate real workflows (pipeline â†’ catalogs â†’ graph â†’ API):
- record the run id / config used
- capture hashes of produced artifacts
- include a minimal PROV-like trace for the test run (even if toy)

> [!TIP]
> If a test failure canâ€™t be diagnosed from artifacts alone, itâ€™s a documentation bug. ğŸ““ğŸ§¯

---

## ğŸ—‚ï¸ Suggested folder layout

Adapt as needed, but keep intent obvious:

```text
ğŸ“¦ repo-root/
â”œâ”€ ğŸ§ª tests/
â”‚  â”œâ”€ ğŸ“„ README.md                      # you are here ğŸ‘‹
â”‚  â”œâ”€ ğŸ§· fixtures/                      # tiny, deterministic test data only
â”‚  â”‚  â”œâ”€ ğŸ—ºï¸ geo/                        # small vectors/rasters (safe + tiny)
â”‚  â”‚  â”œâ”€ ğŸ›°ï¸ eo/                         # tiny EO chips / QA bit samples
â”‚  â”‚  â”œâ”€ ğŸ–¼ï¸ media/                      # tiny images + metadata
â”‚  â”‚  â”œâ”€ ğŸ§¬ ml/                         # toy datasets / tiny model artifacts
â”‚  â”‚  â”œâ”€ ğŸ§¾ catalogs/                   # STAC/DCAT/PROV fixtures
â”‚  â”‚  â””â”€ ğŸ“˜ FIXTURES.md                 # fixture rules + provenance notes
â”‚  â”œâ”€ ğŸ python/
â”‚  â”‚  â”œâ”€ ğŸ§© unit/
â”‚  â”‚  â”œâ”€ ğŸ”Œ integration/
â”‚  â”‚  â”œâ”€ âœ… validation/                 # scientific V&V (tolerance-based)
â”‚  â”‚  â”œâ”€ â±ï¸ perf/                       # benchmarks (nightly / non-gating)
â”‚  â”‚  â”œâ”€ ğŸ” security/                   # defensive security checks
â”‚  â”‚  â””â”€ ğŸ§± conftest.py
â”‚  â”œâ”€ ğŸŒ web/
â”‚  â”‚  â”œâ”€ ğŸ§© unit/
â”‚  â”‚  â”œâ”€ ğŸ§± component/
â”‚  â”‚  â”œâ”€ ğŸ§­ e2e/
â”‚  â”‚  â””â”€ ğŸ–¼ï¸ visual/
â”‚  â”œâ”€ ğŸ—„ï¸ db/
â”‚  â”‚  â”œâ”€ ğŸ§¬ migrations/
â”‚  â”‚  â””â”€ ğŸ”Œ integration/
â”‚  â”œâ”€ ğŸ•¸ï¸ graph/
â”‚  â”‚  â”œâ”€ ğŸ”Œ integration/
â”‚  â”‚  â””â”€ ğŸ§© unit/
â”‚  â”œâ”€ ğŸ“œ contracts/
â”‚  â”‚  â”œâ”€ ğŸ›¡ï¸ api/                        # OpenAPI/GraphQL fixtures
â”‚  â”‚  â”œâ”€ ğŸ—ºï¸ stac/
â”‚  â”‚  â”œâ”€ ğŸ·ï¸ dcat/
â”‚  â”‚  â””â”€ ğŸ§¬ prov/
â”‚  â””â”€ ğŸ“˜ TEST_POLICY.md                 # merge gates + definition of done
â””â”€ ğŸ§° scripts/                          # CI glue & validators (optional)
```

> [!NOTE]
> If you keep tests alongside code (e.g., `src/**/tests`), thatâ€™s fineâ€”just keep naming + markers consistent.

---

## âœ… CI gates (non-negotiable)

**Policy:** the pipeline must be green before merge ğŸ¤–âœ…  
CI should mirror KFMâ€™s â€œbuild â†’ test â†’ publishâ€ discipline and keep logs/artifacts for traceability.

Typical PR gates:
1) ğŸ§¹ format + lint (Python + JS/TS)
2) ğŸ§± build (frontend + backend; container build if applicable)
3) ğŸ§ª unit tests
4) ğŸ”Œ integration tests (ephemeral DB/services via Compose)
5) ğŸ§¾ contract/schema validation (OpenAPI/GraphQL + STAC/DCAT/PROV)
6) âœ… data validation gates (schema + CRS + geometry + license + provenance)
7) ğŸ” security scans (secrets, deps, containers; defensive posture)
8) ğŸ“ˆ coverage thresholds (target, not a religion)

### ğŸ•› Nightly / scheduled checks (recommended)
Keep PR CI fast; push expensive checks to nightly:
- â±ï¸ benchmarks (trend monitoring)
- ğŸ§  longer ML runs (PRs use toy models)
- ğŸ—ºï¸ large raster workloads (PRs use fixtures & sampling)
- ğŸ§ª deeper graph consistency (full rebuild + diff)
- ğŸ” deeper security scanning (if it slows PR CI)

---

## âœ… PR checklist (copy/paste)

- [ ] Unit tests added/updated
- [ ] Integration tests added (if behavior crosses boundaries)
- [ ] Determinism confirmed (seeds + stable outputs) if ML/sim
- [ ] Contracts updated + verified (OpenAPI/GraphQL) if API changed
- [ ] Catalog/metadata tests updated (STAC/DCAT/PROV) if outputs changed
- [ ] Data validation gates updated (schema/CRS/geometry) if ETL changed
- [ ] License + governance checks pass (block publish if missing license)
- [ ] UI changes include component tests + (if visual) snapshot updates
- [ ] CI is green (required)

---

## ğŸ§¯ Troubleshooting

### âŒ Tests fail only in CI?
- check lockfiles & pinned versions
- confirm containers match local versions
- eliminate reliance on local paths, locale, timezone, GPU availability

### ğŸ² Flaky tests?
- remove sleeps; wait on conditions
- fix randomness (seed)
- mock/record external services

### ğŸ³ Docker stack wonâ€™t start?
```bash
docker compose logs -f
docker compose config
docker compose up -d --build
```

---

## ğŸ“š Reference pointers (library index)

These repo library files directly inform KFMâ€™s test posture (V&V, stats rigor, GIS correctness, visualization stability, data governance, security hardening). ğŸ§ ğŸ§¾

<details>
<summary>ğŸ›ï¸ Architecture, system design, and â€œhow KFM is supposed to workâ€</summary>

- `Kansas Frontier Matrix (KFM) â€“ Comprehensive Technical Documentation.docx`
- `Scientific Method _ Research _ Master Coder Protocol Documentation.pdf`
- `Kansas-Frontier-Matrix Design Audit â€“ Gaps and Enhancement Opportunities.pdf`

</details>

<details>
<summary>ğŸ”¬ Scientific modeling, simulation V&amp;V, and uncertainty</summary>

- `Scientific Modeling and Simulation_ A Comprehensive NASA-Grade Guide.pdf`
- `Understanding Statistics & Experimental Design.pdf`
- `think-bayes-bayesian-statistics-in-python.pdf`

</details>

<details>
<summary>ğŸ“Š Statistics, regression, Bayesian, and analysis discipline</summary>

- `regression-analysis-with-python.pdf`
- `Regression analysis using Python - slides-linear-regression.pdf`
- `graphical-data-analysis-with-r.pdf`
- `Understanding Machine Learning: From Theory to Algorithms.pdf`

</details>

<details>
<summary>ğŸ—ºï¸ Geospatial, remote sensing, cartography, and mapping UX</summary>

- `python-geospatial-analysis-cookbook.pdf`
- `Cloud-Based Remote Sensing with Google Earth Engine-Fundamentals and Applications.pdf`
- `making-maps-a-visual-guide-to-map-design-for-gis.pdf`
- `Mobile Mapping_ Space, Cartography and the Digital - 9789048535217.pdf`

</details>

<details>
<summary>ğŸ—„ï¸ Data management, interoperability, and scale</summary>

- `PostgreSQL Notes for Professionals - PostgreSQLNotesForProfessionals.pdf`
- `Scalable Data Management for Future Hardware.pdf`
- `Data Spaces.pdf`
- `Spectral Geometry of Graphs.pdf`

</details>

<details>
<summary>ğŸŒ Web UI, WebGL visualization, and responsive design</summary>

- `responsive-web-design-with-html5-and-css3.pdf`
- `webgl-programming-guide-interactive-3d-graphics-programming-with-webgl.pdf`

</details>

<details>
<summary>ğŸ›¡ï¸ Security, concurrency, law, humanism, and governance</summary>

- `ethical-hacking-and-countermeasures-secure-network-infrastructures.pdf` *(defensive posture only)*
- `Gray Hat Python - Python Programming for Hackers and Reverse Engineers (2009).pdf` *(defensive awareness only)*
- `concurrent-real-time-and-distributed-programming-in-java-threads-rtsj-and-rmi.pdf`
- `On the path to AI Lawâ€™s prophecies and the conceptual foundations of the machine learning age.pdf`
- `Introduction to Digital Humanism.pdf`

</details>

<details>
<summary>ğŸ“š Language &amp; tooling reference shelves (programming books index)</summary>

- `A programming Books.pdf`
- `B-C programming Books.pdf`
- `D-E programming Books.pdf`
- `F-H programming Books.pdf`
- `I-L programming Books.pdf`
- `M-N programming Books.pdf`
- `O-R programming Books.pdf`
- `S-T programming Books.pdf`
- `U-X programming Books.pdf`

</details>

<details>
<summary>ğŸ–¼ï¸ Media formats (if you ingest scans/images)</summary>

- `compressed-image-file-formats-jpeg-png-gif-xbm-bmp.pdf`

</details>

---

## ğŸ•°ï¸ Version history

| Version | Date | Summary | Author |
|---:|---|---|---|
| v1.1.0 | 2026-01-09 | Tightened â€œcatalog-firstâ€ & data QA gates; added receipts/artifacts section; clarified defensive security stance; aligned CI gates with KFM engineering/testing guidance. | KFM Engineering |
| v1.0.0 | 2026-01-07 | Initial repo-wide testing README: pyramid, markers, subsystem matrix, validation + governance posture. | KFM Engineering |

---

<!--
ğŸ“ Evidence anchors (project docs used to update this README)
- KFM testing/CI + data QA gate rationale: :contentReference[oaicite:0]{index=0}
- MCP testing guidance (unit/integration/e2e + data pipeline distribution checks + CI rules): :contentReference[oaicite:1]{index=1}
- MCP determinism + reproducibility practices: :contentReference[oaicite:2]{index=2}
- Data validation patterns (schema, geospatial validity, coordinate ranges, regression diffs): :contentReference[oaicite:3]{index=3}:contentReference[oaicite:4]{index=4}
- Simulation V&V testing patterns (unit/regression/convergence/CI for scientific reproducibility): :contentReference[oaicite:5]{index=5}
- Design audit testing/reproducibility gaps to close (roadmap signal): :contentReference[oaicite:6]{index=6}

ğŸ”— File handles (ChatGPT workspace links; keep in HTML comment so GitHub stays clean)
:contentReference[oaicite:7]{index=7} :contentReference[oaicite:8]{index=8} :contentReference[oaicite:9]{index=9} :contentReference[oaicite:10]{index=10} :contentReference[oaicite:11]{index=11} :contentReference[oaicite:12]{index=12} :contentReference[oaicite:13]{index=13} :contentReference[oaicite:14]{index=14} :contentReference[oaicite:15]{index=15}
:contentReference[oaicite:16]{index=16} :contentReference[oaicite:17]{index=17} :contentReference[oaicite:18]{index=18} :contentReference[oaicite:19]{index=19} :contentReference[oaicite:20]{index=20} :contentReference[oaicite:21]{index=21}
-->
