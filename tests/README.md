# ğŸ§ª Tests â€” Kansas Frontier Matrix (KFM) / Kansasâ€‘Matrixâ€‘System

![CI](https://img.shields.io/badge/CI-GitHub%20Actions-2ea44f?logo=githubactions&logoColor=white)
![Coverage](https://img.shields.io/badge/Coverage-target%20%F0%9F%9A%80-informational)
![Python](https://img.shields.io/badge/Python-pytest-blue?logo=python&logoColor=white)
![Node](https://img.shields.io/badge/Node.js-tests-brightgreen?logo=node.js&logoColor=white)
![Docker](https://img.shields.io/badge/Docker-Compose-2496ed?logo=docker&logoColor=white)
![Deterministic](https://img.shields.io/badge/Deterministic-preferred-success)
![Contracts](https://img.shields.io/badge/Contracts-STAC%20%7C%20DCAT%20%7C%20PROV-6f42c1)
![DB](https://img.shields.io/badge/DB-PostGIS%20%7C%20Neo4j-0b7285)
![API](https://img.shields.io/badge/API-FastAPI%20%7C%20GraphQL-ff6b6b)
![UI](https://img.shields.io/badge/UI-Mapbox%20%7C%20Cesium%20%7C%20WebGL-fab005)
![Governance](https://img.shields.io/badge/Governance-FAIR%20%7C%20CARE-2f9e44)

> KFM is a **trust-first** geospatial + knowledge + modeling system.  
> This folder is where we continuously prove that our **code**, **pipelines**, **contracts**, and **UI behaviors** are **correct**, **reproducible**, and **honest about uncertainty**. âœ…ğŸ§¾  
> KFM tests are **architecture-aware** (layers + boundaries) and **catalog-first** (metadata + provenance are first-class artifacts). ğŸ—‚ï¸ğŸ§¬

---

## ğŸ§­ Quick navigation

- ğŸš€ Run tests now â†’ [Quickstart](#-quickstart)
- ğŸ§© Subsystem map â†’ [Test matrix](#-kfm-test-matrix-subsystems--what-to-assert)
- ğŸ”’ Merge requirements â†’ [CI gates](#-ci-gates-non-negotiable)
- ğŸ§ª What to test â†’ [Test pyramid](#-test-pyramid-how-we-keep-velocity--confidence)
- ğŸ§¾ Contracts (OpenAPI + STAC/DCAT/PROV) â†’ [Contract & metadata tests](#-contract--metadata-tests)
- âœ… Data QA & publish gates â†’ [Data validation gates](#-data-validation-gates-fail-fast)
- ğŸ—ºï¸ GIS correctness â†’ [Geospatial tests](#ï¸-geospatial-tests-gis-correctness)
- ğŸ›°ï¸ Remote sensing â†’ [Remote sensing tests](#ï¸-remote-sensing-tests-earth-engine--imagery)
- ğŸ§  Modeling/simulation validity â†’ [Scientific validation](#-scientific--simulation-validation)
- ğŸŒ Frontend & visualization â†’ [Web / frontend guidance](#-web--frontend-test-guidance)
- ğŸ” Security + governance â†’ [Security & ethics](#-security-governance--ethics-tests-defensive)
- ğŸ§± Suggested layout â†’ [Folder layout](#ï¸-suggested-folder-layout)
- ğŸ“š Project library index â†’ [Reference pointers](#-reference-pointers-library-index)

---

## ğŸš€ Quickstart

### 0) Preconditions (one-time)
- ğŸ Python env ready (`venv`, `conda`, `uv`, etc.)
- ğŸŒ Node env ready (`npm`, `pnpm`, or `yarn`)
- ğŸ³ Docker installed (recommended for integration parity)

> [!TIP]
> If your PR touches **DB/API/pipelines/contracts/catalogs**: run at least one Docker-backed integration pass before requesting review.  
> Container parity saves everyone time. ğŸ³âœ…

### 1) Fast checks (preâ€‘commit vibes âš¡)
```bash
# Python (fast)
pytest -q -m "not integration and not e2e and not slow"

# Web (fast â€” adjust to your repo)
npm test
```

### 2) Full suite (recommended on feature branches âœ…)
```bash
# If you have Make targets (recommended)
make test

# Or explicit slices
pytest -q
pytest -q -m integration
npm run test:e2e
```

### 3) Integration tests with containers (preferred ğŸ³)
```bash
docker compose up -d --build
pytest -q -m integration
docker compose down -v
```

<details>
<summary>ğŸ§¾ Command cheat sheet (copy/paste)</summary>

```bash
# Contracts only
pytest -q -m contracts

# Geo sanity
pytest -q -m geo

# Remote sensing sanity
pytest -q -m eo

# Validation (scientific V&V)
pytest -q -m validation

# â€œSurficial ETLâ€ (example of dataset-scoped suite)
pytest -q -k "test_surficial_etl_"
```
</details>

---

## ğŸ§© KFM test matrix (subsystems + what to assert)

KFM is layered (clean boundaries) and tests should **pin the seams**:

| ğŸ§± Subsystem | ğŸ¯ What must never break | ğŸ§ª Best test types | ğŸ§° Typical tools |
|---|---|---|---|
| ğŸ§ª ETL / pipelines | Deterministic outputs, idempotent reruns, schema + CRS correctness | unit âœ… + integration ğŸ”Œ + data QA gates âœ… | pytest, GDAL, GeoPandas, validators |
| ğŸ—‚ï¸ Catalogs (STAC/DCAT/PROV) | Artifacts exist *before* graph/UI uses them; links resolve; provenance complete | contracts ğŸ§¾ + integration ğŸ”Œ | JSON Schema, jq, custom validators |
| ğŸ•¸ï¸ Knowledge graph (Neo4j) | Graph loads only from catalogs; invariants & constraints hold | integration ğŸ”Œ + property tests ğŸ§ª | Neo4j test container, Cypher assertions |
| ğŸ›¡ï¸ API (FastAPI + GraphQL) | Contract stability, authz, CORS, deterministic pagination | contracts ğŸ§¾ + integration ğŸ”Œ | OpenAPI/GraphQL validation, TestClient |
| ğŸŒ UI (SPA) | Responsive + accessible, stable map behaviors, timeline correctness | unit ğŸ§© + component ğŸ§± + e2e ğŸ§­ | Jest/Vitest, Playwright/Cypress |
| ğŸ—ºï¸ Maps / 3D (Mapbox/Cesium/WebGL) | Visual correctness (symbology, overlays), shader stability, perf budgets | visual ğŸ–¼ï¸ + e2e ğŸ§­ | screenshot diffs, WebGL harness |
| ğŸ“š Story Nodes | Narrative links match sources; citations are non-broken; time ordering | unit ğŸ§© + contracts ğŸ§¾ | markdown/link validators |
| ğŸ¯ Focus Mode (AI) | Truth-first behavior: citations/provenance, uncertainty, refusal on missing data | eval âœ… + contract-like tests ğŸ§¾ | golden prompts, retrieval tests |
| ğŸ” Governance | License present, FAIR/CARE checks, auditability | gates âœ… + integration ğŸ”Œ | policy validators, CI checks |

---

## ğŸ§  Core invariant tests must protect

> [!IMPORTANT]
> KFM enforces a **non-negotiable** pipeline order:
>
> **ETL â†’ Catalogs (STAC/DCAT/PROV) â†’ Graph â†’ APIs â†’ UI â†’ Story Nodes â†’ Focus Mode**

```mermaid
flowchart LR
  A[ğŸ§ª ETL] --> B[ğŸ—‚ï¸ Catalogs<br/>STAC/DCAT/PROV]
  B --> C[ğŸ•¸ï¸ Graph]
  C --> D[ğŸ›¡ï¸ API]
  D --> E[ğŸŒ UI]
  E --> F[ğŸ“š Story Nodes]
  F --> G[ğŸ¯ Focus Mode]
```

### âœ… What tests should enforce (practically)
- ğŸ§ª **ETL is deterministic** (stable IDs/hashes; idempotent reruns; explicit versions)
- ğŸ—‚ï¸ **Catalog records exist** (STAC/DCAT/PROV) **before** graph/UI uses them
- ğŸ•¸ï¸ **Graph loads only from catalogs** (no ad-hoc inserts in prod paths)
- ğŸ›¡ï¸ **API is the only client boundary** (UI never queries graph/DB directly)
- ğŸ” **Sensitivity/classification never downgrades silently** (requires audited redaction)
- ğŸ§¾ **Provenance is complete** (inputs â†’ activities â†’ outputs with run IDs/configs)
- ğŸ·ï¸ **License is explicit before publish** (block â€œpublishâ€ if missing)

---

## ğŸ§± Test pyramid (how we keep velocity + confidence)

Most tests should be cheap and deterministic. Then we add fewer (but higher-value) integration + E2E checks.

```text
          ğŸ”º E2E (few)          â†’ critical user journeys (UI + API + DB)
        ğŸ”ºğŸ”º Integration (some)  â†’ services together (DB, API, pipelines)
      ğŸ”ºğŸ”ºğŸ”º Unit (many)          â†’ pure logic, transforms, validators
```

---

## ğŸ§· Core test principles (KFM style)

### âœ… Test public behavior, not private plumbing
Prefer testing **public entry points** (functions, use-cases, endpoints, contracts).  
This reduces refactor pain and encourages clean boundaries ğŸ§¼ğŸ›ï¸.

### ğŸ” Determinism is a feature
For research/AI/simulation code: set seeds, pin dependencies, eliminate hidden state.

**Determinism checklist:**
- [ ] seeds set (Python, NumPy, ML frameworks)
- [ ] stable sorting (donâ€™t rely on hash iteration order)
- [ ] time mocked/frozen where needed
- [ ] no network calls in unit tests (record/replay if unavoidable)
- [ ] fixtures are tiny & versioned (prefer synthetic/toy)
- [ ] floats use tolerances (`pytest.approx`, `np.testing.assert_allclose`)

### ğŸ§¾ Trust-first means we test uncertainty too
If outputs are probabilistic / estimated:
- test **ranges**, **invariants**, **calibration**, or **convergence** (not single-point exactness)
- attach uncertainty artifacts on failure (plots, traces, summaries)
- document tolerances + rationale (in code comments or `TEST_POLICY.md`)

### ğŸ“ When a test fails, attach receipts
- upload CI artifacts: logs, diffs, screenshots, JSON snapshots, calibration plots
- make failures actionable (clear â€œwhat changed?â€ + â€œwhat to inspect?â€)

---

## ğŸ—‚ï¸ Suggested folder layout

Adapt as needed, but keep intent obvious:

```text
ğŸ“¦ repo-root/
â”œâ”€ ğŸ§ª tests/
â”‚  â”œâ”€ ğŸ“„ README.md                      # you are here ğŸ‘‹
â”‚  â”œâ”€ ğŸ§· fixtures/                      # tiny, deterministic test data only
â”‚  â”‚  â”œâ”€ ğŸ—ºï¸ geo/                        # small vectors/rasters (safe + tiny)
â”‚  â”‚  â”œâ”€ ğŸ›°ï¸ eo/                         # tiny EO chips / QA bit samples
â”‚  â”‚  â”œâ”€ ğŸ–¼ï¸ media/                      # tiny images (JPEG/PNG/GIF) + metadata
â”‚  â”‚  â”œâ”€ ğŸ§¬ ml/                         # toy datasets / tiny model artifacts
â”‚  â”‚  â”œâ”€ ğŸ§¾ catalogs/                   # STAC/DCAT/PROV fixtures
â”‚  â”‚  â”œâ”€ ğŸ§¾ schemas/                    # JSON/YAML schemas used in tests
â”‚  â”‚  â””â”€ ğŸ“˜ FIXTURES.md                 # fixture rules + provenance notes
â”‚  â”œâ”€ ğŸ python/
â”‚  â”‚  â”œâ”€ ğŸ§© unit/
â”‚  â”‚  â”œâ”€ ğŸ”Œ integration/
â”‚  â”‚  â”œâ”€ âœ… validation/                 # scientific V&V (tolerance-based)
â”‚  â”‚  â”œâ”€ â±ï¸ perf/                       # benchmarks (nightly / non-gating)
â”‚  â”‚  â”œâ”€ ğŸ” security/                   # defensive security checks (SAST-ish)
â”‚  â”‚  â”œâ”€ ğŸ§· helpers/
â”‚  â”‚  â””â”€ ğŸ§± conftest.py
â”‚  â”œâ”€ ğŸŒ web/
â”‚  â”‚  â”œâ”€ ğŸ§© unit/
â”‚  â”‚  â”œâ”€ ğŸ§± component/
â”‚  â”‚  â”œâ”€ ğŸ§­ e2e/
â”‚  â”‚  â”œâ”€ ğŸ–¼ï¸ visual/
â”‚  â”‚  â””â”€ ğŸ§· helpers/
â”‚  â”œâ”€ ğŸ—„ï¸ db/
â”‚  â”‚  â”œâ”€ ğŸ§¬ migrations/
â”‚  â”‚  â”œâ”€ ğŸ”Œ integration/
â”‚  â”‚  â””â”€ ğŸ§ª seed/
â”‚  â”œâ”€ ğŸ•¸ï¸ graph/
â”‚  â”‚  â”œâ”€ ğŸ”Œ integration/                # Neo4j containers, constraints, loaders
â”‚  â”‚  â””â”€ ğŸ§© unit/                       # query builders, mappers
â”‚  â”œâ”€ ğŸ“œ contracts/
â”‚  â”‚  â”œâ”€ ğŸ›¡ï¸ api/                        # OpenAPI/GraphQL fixtures
â”‚  â”‚  â”œâ”€ ğŸ—ºï¸ stac/                       # STAC contract fixtures
â”‚  â”‚  â”œâ”€ ğŸ·ï¸ dcat/                       # DCAT contract fixtures
â”‚  â”‚  â””â”€ ğŸ§¬ prov/                       # PROV contract fixtures
â”‚  â”œâ”€ ğŸ§° tools/                         # helper scripts (validators, runners)
â”‚  â””â”€ ğŸ“˜ TEST_POLICY.md                 # merge gates + definition of done
â””â”€ ğŸ§° scripts/                          # optional: CI glue & utilities
```

> [!NOTE]
> If you keep tests alongside code (e.g., `src/**/tests`), thatâ€™s fineâ€”just keep **naming + markers** consistent.

---

## ğŸ·ï¸ Test categories & markers (suggested)

### Python (`pytest`) markers
If you use `pytest`, standardize markers so developers can run focused slices:

```ini
# pytest.ini (example)
[pytest]
markers =
  unit: fast pure logic
  integration: hits db/services/filesystem
  e2e: end-to-end journeys (rare for python)
  slow: long-running tests (non-gating)
  validation: scientific/V&V tests (tolerance-based)
  perf: benchmarks (nightly)
  contracts: OpenAPI + metadata contract validation
  geo: GIS correctness checks
  eo: earth-observation / remote-sensing checks
  graph: graph (Neo4j + algorithms) checks
  security: defensive security checks (no offensive testing)
```

### Web tags
Use your stackâ€™s convention (examples):
- Jest/Vitest: `test`, `test:unit`, `test:component`
- Playwright/Cypress: `test:e2e`
- Visual regression: `test:visual`

---

## ğŸ›¡ï¸ CI gates (non-negotiable)

**Policy:** the pipeline must be green before merge. ğŸ¤–âœ…  
CI should mirror the projectâ€™s â€œbuild â†’ test â†’ publishâ€ discipline and keep logs/artifacts for traceability.

Typical PR gates:
1) ğŸ§¹ format + lint (Python + JS/TS)
2) ğŸ§± build (frontend + backend; container build if applicable)
3) ğŸ§ª unit tests
4) ğŸ”Œ integration tests (ephemeral DB/services via Compose)
5) ğŸ§¾ contract/schema validation (OpenAPI/GraphQL + STAC/DCAT/PROV where applicable)
6) âœ… data validation gates (schema + CRS + geometry + license + provenance)
7) ğŸ” security scans (secrets, deps, container vulnerabilities; defensive posture)
8) ğŸ“ˆ coverage thresholds (target, not a religion)

### ğŸ•› Nightly / scheduled checks (recommended)
Keep PR CI fast. Push expensive checks to nightly:
- â±ï¸ benchmarks (trend monitoring)
- ğŸ§  longer ML training runs (PRs use toy models)
- ğŸ—ºï¸ large raster workloads (PRs use fixtures & sampling)
- ğŸ§ª deeper graph consistency checks (full graph rebuild + diff)
- ğŸ” deeper security scanning (if it slows PR CI)

---

## ğŸ§¾ Contract & metadata tests

KFM is **contract-first** and **catalog-first**. Tests must protect:
- ğŸ›¡ï¸ **OpenAPI / GraphQL** contracts (breaking changes are explicit + versioned)
- ğŸ—‚ï¸ **STAC** (collections/items link validity + required fields)
- ğŸ·ï¸ **DCAT** (distributions link to STAC/asset access points)
- ğŸ§¬ **PROV** (inputs â†’ activities â†’ outputs; run IDs/configs recorded)

### âœ… What to validate
- JSON parses + schema passes
- links resolve (STAC assets exist; DCAT distributions point somewhere real)
- provenance completeness (raw â†’ work â†’ processed trace exists)
- stable IDs/hashes present where required
- time metadata makes sense (monotonic where required; time windows applied correctly)

### Example checks (starter)
```bash
# JSON sanity
find data/stac data/catalog/dcat data/prov -name "*.json*" -print0 | xargs -0 -n 1 jq empty

# pytest contract suite (example)
pytest -q -m contracts
```

---

## âœ… Data validation gates (fail fast)

These gates are your â€œno-bad-dataâ€ firewall ğŸ§±ğŸ”¥ â€” especially for GeoParquet + COG pipelines and publish flows.

**Typical gate set (recommended):**
- âœ… Field non-empties: required columns present + non-null
- ğŸ§­ CRS required: input CRS present; output CRS standardized (often EPSG:4326); original CRS recorded for lineage
- ğŸ§± Geometry valid: non-empty, `make_valid` if allowed, reject post-fix self-intersections if policy demands
- ğŸ§Š Raster sanity: resolution/nodata consistent; alignment rules enforced
- ğŸ§¾ Catalog artifacts: STAC/DCAT/PROV emitted + validate
- ğŸ·ï¸ License present: block publish if missing/ambiguous
- ğŸ” Privacy/classification: no downgrades; sensitive outputs redacted/aggregated per policy

**Example CI hooks (pattern):**
```bash
# dataset-scoped tests (example)
pytest -q -k "test_surficial_etl_"

# schema checks (example helper)
python scripts/validate_geoparquet_schema.py

# CRS & geometry validators (example helper)
python scripts/geo/validate_geom.py

# license gate (example)
python scripts/governance/validate_license.py
```

> [!NOTE]
> Prefer deterministic identifiers for dataset rows/features (UUIDv5 or hash-based) so reruns match exactly unless inputs change. ğŸ”ğŸ§¬

---

## ğŸ Python test guidance

### ğŸ§© Unit tests
Best for:
- parsers/validators
- coordinate transforms & unit conversions
- domain rules & invariants
- pure math transforms
- query builders (SQL/Cypher/GraphQL) as pure functions

âœ… Tips:
- prefer tolerance-based asserts for floats
- encode invariants (monotonicity, conservation) instead of brittle constants
- include â€œsad pathsâ€ for invalid inputs

### ğŸ”Œ Integration tests
Best for:
- PostGIS interactions (spatial joins, buffers, reprojections)
- Neo4j interactions (catalog â†’ graph loaders; constraints)
- API routes (FastAPI) against a test DB
- filesystem/object store adapters
- queue/worker boundaries (smoke-level)

âœ… Tips:
- use Compose to create repeatable dependencies
- isolate state via transactions or per-test schemas
- avoid public internet in tests (mock or record/replay)

---

## ğŸ•¸ï¸ Graph tests (Neo4j + algorithms)

KFM treats the graph as **derived truth** (built from catalogs + provenance), not a write-anywhere scratchpad.

Test categories:
- ğŸ§¾ **Graph build contract**: graph rebuild from catalogs is reproducible
- ğŸ”’ **Constraints**: uniqueness, required properties, relationship rules
- ğŸ§­ **Query invariants**: pagination stable; filters correct; time/space constraints respected
- ğŸ§  **Algorithm sanity**: small deterministic graphs for spectral/routing/clustering invariants (see â€œSpectral Geometryâ€ reference)

Examples:
- â€œGraph contains only entities referenced by STAC/DCAT/PROVâ€
- â€œEvery published dataset node links to a PROV Activity with run_id + config hashâ€
- â€œSmall known graph has expected Laplacian spectrum / connectivity propertiesâ€

---

## ğŸ›¡ï¸ API tests (FastAPI + GraphQL)

What to test:
- ğŸ§¾ OpenAPI schema validation (breaking changes must be explicit)
- ğŸ§¾ GraphQL schema validation + query shapes
- ğŸ” AuthN/AuthZ: role-based access, classification enforcement
- ğŸ§­ Pagination determinism: stable ordering, cursor correctness
- ğŸŒ Geo correctness: GeoJSON validity; bounding boxes; CRS behavior
- ğŸŒ CORS headers correct (UI must not need workarounds)

---

## ğŸŒ Web / frontend test guidance

### ğŸ§± Component tests (fast)
- render correctness given props/state
- event handling correctness
- accessibility checks (labels, keyboard nav, contrast)

### ğŸ“± Responsive tests (mobile-first)
From CSS/layout perspective: breakpoints should be **tested**, not assumed.
- run component + E2E suites across multiple viewport sizes
- verify map controls remain usable on small screens
- check touch targets + scroll locking behaviors

### ğŸ§­ E2E tests (few but powerful)
Focus on â€œmoney pathsâ€ ğŸ’¸:
- auth/login
- load a layer (from catalog)
- timeline navigation (time slider updates map + panels)
- select a feature â†’ details panel updates
- export/report flow (and provenance/metadata attached)

### ğŸ–¼ï¸ Visual regression (maps + WebGL)
Maps can regress visually while â€œstill passing logic tests.â€ Use screenshot diffs where it matters:
- symbology doesnâ€™t silently change
- overlays remain legible at common zoom levels
- dark/light modes keep contrast
- WebGL rendering regressions are caught (tolerance-based diffs)

---

## ğŸ—ºï¸ Geospatial tests (GIS correctness)

Geospatial pipelines are fragile in predictable waysâ€”test these explicitly:

- ğŸŒ **CRS sanity**: EPSG correctness; meters vs degrees issues
- ğŸ§± **Topology**: geometry validity, no self-intersections when required
- ğŸ§© **Overlay correctness**: clip/intersect/union behaviors
- ğŸ§Š **Raster alignment**: resolution, nodata handling, resampling method
- ğŸ“¦ **Format IO**: GeoJSON/GeoPackage/GeoParquet/COG round-trips
- ğŸ§­ **Scale guardrails** (example): enforce expected map scales; warn on outliers
- ğŸ§® **Area sanity** (example): union area within tolerance of mask boundary (dataset-specific)

> [!NOTE]
> Always include CRS + units in test names or fixture metadata. It prevents â€œsilent degrees vs metersâ€ disasters. ğŸ¥²

---

## ğŸ›°ï¸ Remote sensing tests (Earth Engine & imagery)

Remote sensing workflows fail quietly unless you test assumptions:
- band availability & naming
- scale / resolution
- cloud masking logic (QA bits)
- temporal compositing rules
- index calculations (e.g., NDVI) & expected ranges
- export formats & metadata consistency

Truthiness checks that catch many bugs:
- output range sanity (e.g., NDVI âˆˆ [-1, 1])
- masked pixel counts within expected bounds
- timestamps monotonic; windows applied correctly

> [!TIP]
> Unit tests should not call live services. Prefer recorded fixtures, mock adapters, or small cached exports.

---

## ğŸ§  Scientific / simulation validation

Treat simulation/analysis code like **scientific instruments** ğŸ”¬:
- **verification**: implementation matches intended math
- **validation**: model matches reality within uncertainty
- **regression baselines**: detect drift across refactors

KFM-style simulation paradigms you may need to validate:
- ğŸ¤– Agent-Based Modeling (ABM) (emergent behavior, distributional properties)
- â±ï¸ Discrete-Event Simulation (DES) (queueing, events ordering, reproducibility)
- ğŸŒŠ System Dynamics (SD) (stocks/flows, stability, conservation)
- ğŸ—ºï¸ Continuous spatial / hybrid models (PDE-ish, coupled data â†’ model â†’ output)

### âœ… Recommended patterns
- analytical solution comparisons (tiny cases with known answers)
- convergence tests (timestep/resolution refinement reduces error)
- invariant checks (symmetry, conservation, monotonicity)
- tolerance-based golden files (with metadata + tolerances)
- uncertainty reporting checks (intervals, credible bands, PPC)

> [!TIP]
> If results are stochastic, test **properties** (ranges, quantiles, calibration) rather than exact values.

---

## ğŸ“Š ML / stats tests (donâ€™t fool yourself)

Data science code needs tests beyond â€œit runsâ€:
- split is leak-free (train/val/test)
- metrics stable within tolerance
- baseline comparisons exist (simple model beats random)
- regression diagnostics exist (residuals, leverage, outliers)
- uncertainty reporting present where relevant
- multiple comparisons / p-hacking risks handled (where applicable)

> [!TIP]
> Attach sanity plots as CI artifacts on failure (confusion matrix, residuals, calibration curve). ğŸ“

---

## ğŸ§® Optimization, topology, and â€œhard mathâ€ modules

If you include:
- generalized topology optimization
- constrained optimization loops
- spectral graph methods / clustering / routing
- high-performance data transforms

Add tests that check:
- invariants (symmetry, conservation, monotonicity)
- convergence behavior (within iteration limits)
- gradient checks (finite-difference sanity, if applicable)
- known micro-benchmarks (tiny, deterministic)
- constraint satisfaction (bounds respected; feasibility maintained)

---

## ğŸ—„ï¸ Database tests (Postgres/PostGIS)

Principles:
- use ephemeral DBs (containers)
- run migrations in CI
- isolate with transactions
- seed minimal fixtures (never production dumps)

Recommended:
- migration tests: upgrade/downgrade + schema assertions
- query tests: correctness + (where critical) index usage checks (`EXPLAIN`)
- contract tests: API responses match schema

---

## ğŸ§° Data management & interoperability tests

KFM targets interoperability and long-lived data products:
- validate data partitions & deterministic file layouts
- validate storage formats (GeoParquet, COG, Parquet) and metadata parity
- validate federation/export paths (catalog harvesting, API discovery)
- track versioning for datasets/models/configs (DVC or equivalent)

Performance posture (nightly, non-gating):
- regressions in IO throughput
- memory usage ceilings
- â€œhot pathâ€ query latency budgets (API + DB + graph)

---

## ğŸ” Security, governance, & ethics tests (defensive)

KFMâ€™s security stance is **defensive**: prevent leaks, enforce policy, keep audit trails.

Test themes:
- ğŸ” AuthZ checks: classification boundaries & redaction rules enforced
- ğŸ•µï¸ Secrets scanning: prevent committed tokens/keys
- ğŸ“¦ Dependency scanning: vulnerable libraries flagged
- ğŸ³ Container scanning: base image CVEs flagged
- ğŸ§¾ FAIR/CARE governance gates: required metadata, access constraints honored
- ğŸ§¬ Auditability: immutable-ish logs/ledgers for publish actions (who/what/when)

> [!IMPORTANT]
> Do **not** add â€œoffensiveâ€ testing instructions here. Security tests in this repo are for hardening, verification, and prevention.

---

## ğŸ–¼ï¸ Media + imagery tests (maps, scans, exports)

If KFM ingests scanned maps/photos or exports images:
- confirm JPEG/PNG/GIF decode paths are stable
- verify EXIF orientation behavior (if applicable)
- ensure color space/profile handling does not silently shift
- validate tile pyramids / downsampling / resampling correctness
- ensure thumbnails are deterministic (same input â†’ same output)

---

## ğŸ“± Mobile mapping tests (optional but valuable)

If KFM targets mobile:
- offline tile cache behavior
- GPS drift tolerance handling
- touch controls & accessibility
- performance & battery budgets
- low-bandwidth resilience (degraded mode)

---

## âœ… PR checklist (copy/paste)

- [ ] Unit tests added/updated
- [ ] Integration tests added (if behavior crosses boundaries)
- [ ] Determinism confirmed (seeds + stable outputs) if ML/sim
- [ ] Contracts updated + verified (OpenAPI/GraphQL) if API changed
- [ ] Catalog/metadata tests updated (STAC/DCAT/PROV) if data outputs changed
- [ ] Data validation gates updated (schema/CRS/geometry) if ETL changed
- [ ] License + governance checks pass (block publish if missing license)
- [ ] UI changes include component tests + (if visual) snapshot updates
- [ ] CI is green (required)

---

## ğŸ§¯ Troubleshooting

### âŒ Tests fail only in CI?
- check lockfiles & pinned versions
- confirm containers match local versions
- eliminate reliance on local paths, locale, timezone, GPU availability

### ğŸ² Flaky tests?
- remove sleeps; wait on conditions
- fix randomness (seed)
- mock/record external services

### ğŸ³ Docker stack wonâ€™t start?
```bash
docker compose logs -f
docker compose config
docker compose up -d --build
```

---

## ğŸ“š Reference pointers (library index)

> Keep these in `docs/library/` (or your chosen path) and link them in ADRs / `TEST_POLICY.md` when needed.  
> These project files directly inform KFMâ€™s test posture (V&V, stats rigor, GIS correctness, visualization stability, data governance, security hardening). ğŸ§ ğŸ§¾

<details>
<summary>ğŸ›ï¸ Architecture, system design, and â€œhow KFM is supposed to workâ€</summary>

- ğŸ“˜ `docs/library/Kansas Frontier Matrix (KFM) â€“ Comprehensive Engineering Design.docx`  
- ğŸ§  `docs/library/Latest Ideas.docx`

</details>

<details>
<summary>ğŸ”¬ Scientific modeling, simulation V&amp;V, and uncertainty</summary>

- ğŸ”¬ `docs/library/Scientific Modeling and Simulation_ A Comprehensive NASA-Grade Guide.pdf`  
- ğŸ§¬ `docs/library/Principles of Biological Autonomy - book_9780262381833.pdf`  
- ğŸ§® `docs/library/Generalized Topology Optimization for Structural Design.pdf`

</details>

<details>
<summary>ğŸ“Š Statistics, regression, Bayesian, and analysis discipline</summary>

- ğŸ“ˆ `docs/library/regression-analysis-with-python.pdf`  
- ğŸ§¾ `docs/library/Regression analysis using Python - slides-linear-regression.pdf`  
- ğŸ§ª `docs/library/Understanding Statistics & Experimental Design.pdf`  
- ğŸ§  `docs/library/think-bayes-bayesian-statistics-in-python.pdf`  
- ğŸ“‰ `docs/library/graphical-data-analysis-with-r.pdf`  
- ğŸ¤– `docs/library/Deep Learning for Coders with fastai and PyTorch - Deep.Learning.for.Coders.with.fastai.and.PyTorchpdf`

</details>

<details>
<summary>ğŸ—ºï¸ Geospatial, remote sensing, cartography, and mapping UX</summary>

- ğŸ›°ï¸ `docs/library/Cloud-Based Remote Sensing with Google Earth Engine-Fundamentals and Applications.pdf`  
- ğŸ§­ `docs/library/python-geospatial-analysis-cookbook.pdf`  
- ğŸ—ºï¸ `docs/library/making-maps-a-visual-guide-to-map-design-for-gis.pdf`  
- ğŸ“± `docs/library/Mobile Mapping_ Space, Cartography and the Digital - 9789048535217.pdf`

</details>

<details>
<summary>ğŸ—„ï¸ Data management, interoperability, and scale</summary>

- ğŸ—ƒï¸ `docs/library/PostgreSQL Notes for Professionals - PostgreSQLNotesForProfessionals.pdf`  
- ğŸ§± `docs/library/Scalable Data Management for Future Hardware.pdf`  
- ğŸŒ `docs/library/Data Spaces.pdf`  
- ğŸ•¸ï¸ `docs/library/Spectral Geometry of Graphs.pdf`

</details>

<details>
<summary>ğŸŒ Web UI, WebGL visualization, and responsive design</summary>

- ğŸ“± `docs/library/responsive-web-design-with-html5-and-css3.pdf`  
- ğŸ§Š `docs/library/webgl-programming-guide-interactive-3d-graphics-programming-with-webgl.pdf`

</details>

<details>
<summary>ğŸ›¡ï¸ Security, concurrency, law, humanism, and governance</summary>

- ğŸ§¯ `docs/library/ethical-hacking-and-countermeasures-secure-network-infrastructures.pdf` *(defensive posture only)*  
- ğŸ›¡ï¸ `docs/library/Gray Hat Python - Python Programming for Hackers and Reverse Engineers (2009).pdf` *(defensive awareness only)*  
- â±ï¸ `docs/library/concurrent-real-time-and-distributed-programming-in-java-threads-rtsj-and-rmi.pdf`  
- ğŸ§¾ `docs/library/On the path to AI Lawâ€™s prophecies and the conceptual foundations of the machine learning age.pdf`  
- ğŸ§‘â€ğŸ¤â€ğŸ§‘ `docs/library/Introduction to Digital Humanism.pdf`

</details>

<details>
<summary>ğŸ“š Language &amp; tooling reference shelves (programming books index)</summary>

- ğŸ“š `docs/library/A programming Books.pdf`  
- ğŸ“š `docs/library/B-C programming Books.pdf`  
- ğŸ“š `docs/library/D-E programming Books.pdf`  
- ğŸ“š `docs/library/F-H programming Books.pdf`  
- ğŸ“š `docs/library/I-L programming Books.pdf`  
- ğŸ“š `docs/library/M-N programming Books.pdf`  
- ğŸ“š `docs/library/O-R programming Books.pdf`  
- ğŸ“š `docs/library/S-T programming Books.pdf`  
- ğŸ“š `docs/library/U-X programming Books.pdf`

</details>

<details>
<summary>ğŸ–¼ï¸ Media formats (if you ingest scans/images)</summary>

- ğŸ–¼ï¸ `docs/library/compressed-image-file-formats-jpeg-png-gif-xbm-bmp.pdf`

</details>

---

## âœ¨ Small â€œnext improvementsâ€ (optional, high ROI)

- Add `make test`, `make test-unit`, `make test-integration`, `make test-e2e`
- Add `make validate-data` (schema + CRS + geom + license + catalogs)
- Upload CI artifacts on failure (coverage HTML, E2E screenshots, diff images, STAC validation logs)
- Add nightly benchmarks for geospatial ops + API latencies (separate from PR gates)
- Add graph rebuild + diff test (catalog â†’ graph) nightly