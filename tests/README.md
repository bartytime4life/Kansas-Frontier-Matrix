---
title: "ğŸ§ª Tests â€” Kansas Frontier Matrix (KFM) / Kansasâ€‘Matrixâ€‘System"
path: "tests/README.md"
version: "v1.2.0"
last_updated: "2026-01-11"
review_cycle: "90 days"
owners:
  - "KFM Engineering"
tags:
  - tests
  - ci
  - determinism
  - contracts
  - stac
  - dcat
  - prov
  - geo
  - graph
  - api
  - ui
  - governance
---

<!--
ğŸ“Œ This README defines the repo-wide testing & verification surface for KFM / Kansasâ€‘Matrixâ€‘System.
ğŸ—“ï¸ Last updated: 2026-01-11
ğŸ” Review cycle: 90 days (or anytime pipeline order / catalogs / policy pack / CI lanes change)
-->

<div align="center">

# ğŸ§ª Tests â€” Kansas Frontier Matrix (KFM) / Kansasâ€‘Matrixâ€‘System

**Trust-first testing for a contractâ€‘first, catalogâ€‘first geospatial + knowledge + modeling stack** ğŸ§¾ğŸ—ºï¸ğŸ§¬  
Determinism â€¢ Contracts â€¢ Governance â€¢ Evidence receipts â€¢ â€œFail closedâ€ gates âœ…ğŸ”’

![CI](https://img.shields.io/badge/CI-GitHub%20Actions-2ea44f?logo=githubactions&logoColor=white)
![Pytest](https://img.shields.io/badge/Python-pytest-blue?logo=python&logoColor=white)
![Node](https://img.shields.io/badge/Node.js-tests-brightgreen?logo=node.js&logoColor=white)
![Playwright](https://img.shields.io/badge/E2E-Playwright-0b7285?logo=playwright&logoColor=white)
![Docker](https://img.shields.io/badge/Integration-Docker%20Compose-2496ed?logo=docker&logoColor=white)
![Contracts](https://img.shields.io/badge/Contracts-OpenAPI%20%7C%20GraphQL-ff6b6b)
![Catalogs](https://img.shields.io/badge/Catalogs-STAC%20%7C%20DCAT%20%7C%20PROV-6f42c1)
![Governance](https://img.shields.io/badge/Governance-FAIR%20%2B%20CARE%20%2B%20Sovereignty-2f9e44)
![Fail%20Closed](https://img.shields.io/badge/Quality%20Gates-Fail%20Closed-red)

</div>

> KFM tests donâ€™t just check â€œit runs.â€  
> They prove that our **pipelines**, **catalogs**, **graph**, **APIs**, **docs/story nodes**, and **UI behaviors** are:
>
> âœ… **Correct** â€¢ âœ… **Reproducible** â€¢ âœ… **Governanceâ€‘compliant** â€¢ âœ… **Honest about uncertainty**  
>
> We test the **seams (boundaries + contracts)** and treat metadata/provenance as **firstâ€‘class artifacts** ğŸ—‚ï¸ğŸ§¬

> [!IMPORTANT]
> **tests/** is part of KFMâ€™s *governed surface*.  
> If a change can affect what users see or what the system asserts as â€œtruthâ€, it must be **testable**, **traceable**, and **failâ€‘closed** when requirements arenâ€™t met.

---

<details>
<summary><b>ğŸ§­ Table of contents</b></summary>

- [ğŸ”— Quick links](#-quick-links)
- [ğŸš¦ Nonâ€‘negotiables](#-non-negotiables)
- [ğŸš€ Quickstart](#-quickstart)
- [ğŸ§© KFM test matrix](#-kfm-test-matrix-subsystems--what-to-assert)
- [ğŸ§  Core invariant: governed ordering](#-core-invariant-governed-ordering)
- [ğŸ”º Test pyramid](#-test-pyramid-how-we-keep-velocity--confidence)
- [ğŸ·ï¸ Test categories & markers](#ï¸-test-categories--markers-suggested)
- [ğŸ§° Tool & CLI contract tests](#-tool--cli-contract-tests)
- [ğŸ“„ Docs, Story Nodes, & Focus Mode validation](#-docs-story-nodes--focus-mode-validation)
- [ğŸ§¾ Contract & metadata tests](#-contract--metadata-tests)
- [âœ… Data validation gates](#-data-validation-gates-fail-fast)
- [ğŸ—ºï¸ Geospatial tests](#ï¸-geospatial-tests-gis-correctness)
- [ğŸ›°ï¸ Remote sensing tests](#ï¸-remote-sensing-tests-earth-engine--imagery)
- [ğŸ§  Scientific & simulation validation](#-scientific--simulation-validation)
- [ğŸ“Š ML / stats tests](#-ml--stats-tests-dont-fool-yourself)
- [ğŸ•¸ï¸ Graph tests](#ï¸-graph-tests-neo4j--algorithms)
- [ğŸ›¡ï¸ API tests](#ï¸-api-tests-fastapi--graphql)
- [ğŸŒ Web / frontend tests](#-web--frontend-test-guidance)
- [ğŸ” Security, governance, & ethics tests](#-security-governance--ethics-tests-defensive)
- [ğŸ§¾ Test artifacts & receipts](#-test-artifacts--receipts)
- [ğŸ—‚ï¸ Suggested folder layout](#ï¸-suggested-folder-layout)
- [âœ… CI gates](#-ci-gates-non-negotiable)
- [âœ… PR checklist](#-pr-checklist-copypaste)
- [ğŸ§¯ Troubleshooting](#-troubleshooting)
- [ğŸ“š Reference pointers](#-reference-pointers-library-index)
- [ğŸ•°ï¸ Version history](#ï¸-version-history)

</details>

---

## ğŸ”— Quick links

> Paths are relative to `tests/`. If your repo differs, treat these as the **target map** and document any deltas.

- ğŸ§­ Repo overview: `../README.md`
- ğŸ§± Master Guide (v13): `../docs/MASTER_GUIDE_v13.md`
- ğŸ§© Executable boundary (architecture): `../src/README.md` *(if present)*
- ğŸ§° Governed toolchain surface (validators + packaging): `../tools/README.md`
- ğŸ“¦ Data lifecycle + governance: `../data/README.md`
- ğŸ§¬ Schemas (contracts for STAC/DCAT/PROV/Story/UI): `../schemas/`
- ğŸ““ MCP (experiments + run receipts + model cards): `../mcp/`
- ğŸ›¡ï¸ API contracts (OpenAPI/GraphQL): `../src/server/contracts/`
- ğŸ“š Story Nodes (draft/published): `../docs/reports/story_nodes/`
- ğŸŒ Web UI boundary: `../web/` *(if present)*

---

## ğŸš¦ Nonâ€‘negotiables

These are KFMâ€™s â€œmust not regressâ€ invariants. If any of these become false, **CI must block merge** ğŸš«âœ…

1) **Contractâ€‘first:** schemas + API contracts are firstâ€‘class repo artifacts ğŸ§¾  
   - Breaking changes must be explicit + versioned + tested.

2) **Catalogâ€‘first:** nothing is â€œrealâ€ unless itâ€™s cataloged (STAC/DCAT) and traceable (PROV) ğŸ—‚ï¸ğŸ§¬  
   - Catalogs are **boundary artifacts** that downstream stages consume.

3) **Governed ordering is enforced in tests** ğŸ§±  
   **ETL â†’ STAC/DCAT/PROV â†’ Graph â†’ API â†’ UI â†’ Story Nodes â†’ Focus Mode**

4) **API boundary rule:** UI must never query Neo4j/DB directly ğŸ”  
   - Everything user-facing must flow through `src/server/` for redaction + policy enforcement.

5) **Determinism by default:** reruns should match unless inputs/configs change ğŸ”  
   - Stochastic code must be seeded and tested by **properties** (not exact values).

6) **Sovereignty + classification propagation:** outputs canâ€™t be *less restricted* than inputs ğŸ·ï¸ğŸ›¡ï¸  
   - â€œNo downgradeâ€ is a gate, not a guideline.

7) **No network in unit tests** ğŸš«ğŸŒ  
   - Record/replay, mock adapters, or cached fixtures only.

8) **Evidence over vibes:** failures must produce inspectable artifacts (logs, diffs, screenshots) ğŸ“

> [!TIP]
> If your PR changes **spatial truth** or **what users can infer**, it must be **traceable + testable** ğŸ§¾âœ…

---

## ğŸš€ Quickstart

### 0) Preconditions (oneâ€‘time)
- ğŸ Python env ready (`venv`, `uv`, `conda`, etc.)
- ğŸŒ Node env ready (`npm`, `pnpm`, or `yarn`)
- ğŸ³ Docker installed *(recommended for integration parity)*

### 1) Fast checks (developer loop âš¡)
```bash
# Python (fast)
pytest -q -m "not integration and not e2e and not slow"

# Web (fast â€” adapt to your repo)
npm test
```

### 2) Full suite (recommended on feature branches âœ…)
```bash
pytest -q
pytest -q -m integration
npm run test:e2e
```

### 3) Integration tests with containers (preferred ğŸ³)
```bash
docker compose up -d --build
pytest -q -m integration
docker compose down -v
```

<details>
<summary>ğŸ§¾ Command cheat sheet (copy/paste)</summary>

```bash
# Contracts only (API + schemas + catalogs)
pytest -q -m contracts

# Docs/story lint + story-node schema checks
pytest -q -m docs

# Geo sanity
pytest -q -m geo

# Earth-observation sanity
pytest -q -m eo

# Scientific V&V
pytest -q -m validation

# Graph slice
pytest -q -m graph

# Defensive security checks
pytest -q -m security
```
</details>

---

## ğŸ§© KFM test matrix (subsystems + what to assert)

KFM is layered (clean boundaries). Tests should **pin the seams** ğŸ”©:

| ğŸ§± Subsystem | ğŸ¯ What must never break | ğŸ§ª Best test types | ğŸ§° Typical tools |
|---|---|---|---|
| ğŸ§° Tools/CLIs | governed command surface: `--help`, safe defaults, stable exit codes, structured logs | unit âœ… + smoke âœ… | pytest, subprocess, snapshot tests |
| ğŸ§ª ETL / pipelines | deterministic outputs, idempotent reruns, schema+CRS correctness | unit âœ… + integration ğŸ”Œ + data QA gates âœ… | pytest, GDAL, GeoPandas, validators |
| ğŸ—‚ï¸ Catalogs (STAC/DCAT/PROV) | boundary artifacts exist *before* graph/UI uses data; links resolve; provenance complete | contracts ğŸ§¾ + integration ğŸ”Œ | JSON Schema, jq, custom validators |
| ğŸ•¸ï¸ Graph (Neo4j) | graph ingests **from catalogs**, not adâ€‘hoc; constraints hold | integration ğŸ”Œ + property tests ğŸ§ª | Neo4j container, Cypher assertions |
| ğŸ›¡ï¸ API (REST/GraphQL) | contract stability, authz, deterministic pagination | contracts ğŸ§¾ + integration ğŸ”Œ | OpenAPI/GraphQL validation, TestClient |
| ğŸŒ UI (SPA) | responsive + accessible, stable map behaviors, timeline correctness | unit ğŸ§© + component ğŸ§± + e2e ğŸ§­ | Jest/Vitest, Playwright/Cypress |
| ğŸ—ºï¸ Maps / 3D | symbology & overlays donâ€™t silently shift; perf budgets | visual ğŸ–¼ï¸ + e2e ğŸ§­ | screenshot diffs, WebGL harness |
| ğŸ“š Story Nodes | citations resolve; narrative ordering consistent; no unsourced claims | docs âœ… + contracts ğŸ§¾ | markdown/link validators, schema checks |
| ğŸ¯ Focus Mode (AI) | provenance-linked outputs; safe refusals; uncertainty honesty; no sensitive leakage | eval âœ… + contract-like ğŸ§¾ | golden prompts, retrieval tests |
| ğŸ” Governance | licenses, access constraints, â€œno downgradeâ€ classification | gates âœ… + integration ğŸ”Œ | OPA/Conftest policies, CI checks |

---

## ğŸ§  Core invariant: governed ordering

> [!IMPORTANT]
> KFM enforces a **nonâ€‘negotiable** pipeline order:
>
> **ETL â†’ STAC/DCAT/PROV catalogs â†’ Graph â†’ APIs â†’ UI â†’ Story Nodes â†’ Focus Mode**

```mermaid
flowchart LR
  A[ğŸ§ª ETL] --> B[ğŸ—‚ï¸ STAC/DCAT/PROV]
  B --> C[ğŸ•¸ï¸ Graph]
  C --> D[ğŸ›¡ï¸ API]
  D --> E[ğŸŒ UI]
  E --> F[ğŸ“š Story Nodes]
  F --> G[ğŸ¯ Focus Mode]
```

### âœ… What tests should enforce (practically)
- ğŸ§ª ETL determinism (stable IDs/hashes; idempotent reruns; explicit versions)
- ğŸ—‚ï¸ Catalog records exist **before** graph/UI uses them
- ğŸ•¸ï¸ Graph loads only from catalogs (no adâ€‘hoc inserts in prod paths)
- ğŸ›¡ï¸ API is the only client boundary (UI never queries graph/DB directly)
- ğŸ” Classification/sensitivity never downgrades silently (requires audited redaction)
- ğŸ§¾ Provenance is complete (inputs â†’ activities â†’ outputs with run IDs/configs)
- ğŸ·ï¸ License is explicit before publish (block publish if missing/ambiguous)

---

## ğŸ”º Test pyramid (how we keep velocity + confidence)

Most tests should be cheap and deterministic, then fewer (higherâ€‘value) integration + E2E:

```text
          ğŸ”º E2E (few)          â†’ critical user journeys (UI + API + DB)
        ğŸ”ºğŸ”º Integration (some)  â†’ services together (DB, API, pipelines)
      ğŸ”ºğŸ”ºğŸ”º Unit (many)          â†’ pure logic, transforms, validators
```

---

## ğŸ·ï¸ Test categories & markers (suggested)

### Python (`pytest`) markers
Standardize markers so devs can run focused slices quickly:

```ini
# pytest.ini (example)
[pytest]
markers =
  unit: fast pure logic
  integration: hits db/services/filesystem
  e2e: end-to-end journeys (rare for python)
  slow: long-running tests (non-gating)
  perf: benchmarks (nightly)
  contracts: schemas + catalogs + API contract validation
  docs: markdown/front-matter/story-node validation
  validation: scientific/V&V tests (tolerance-based)
  geo: GIS correctness checks
  eo: earth-observation / remote-sensing checks
  graph: graph (Neo4j + algorithms) checks
  api: API behavior checks (beyond schema)
  security: defensive security checks (no offensive testing)
  governance: license/classification/redaction/policy pack checks
  focus: Focus Mode contract tests (provenance + safety + uncertainty)
```

### Web tags (examples)
- Jest/Vitest: `test`, `test:unit`, `test:component`
- Playwright/Cypress: `test:e2e`
- Visual regression: `test:visual`

---

## ğŸ§° Tool & CLI contract tests

KFMâ€™s **governed toolchain** (`tools/`) is part of the contract surface. Tools must behave predictably under automation.

### âœ… What to assert for every CLI tool
- `--help` exists + includes **â‰¥2 examples**
- `--version` returns a stable value (semver or git SHA)
- safe-by-default (no writes unless `--apply`, or `--dry-run` default)
- stable exit codes (usage vs validation failure vs runtime failure)
- structured logs available (human + JSONL mode)

### ğŸ”§ Suggested tests (patterns)
- `test_tools_help_smoke()` â€” exit 0 and non-empty help
- `test_tools_version_smoke()` â€” exit 0 and prints version
- `test_tools_dry_run_does_not_mutate()` â€” run against temp dirs; confirm no writes
- `test_tools_exit_codes_are_stable()` â€” validation failure â†’ code `3` (or your chosen standard)
- `test_tools_json_logs_valid_jsonl()` â€” parse emitted JSONL lines

> [!TIP]
> If you implement core logic inside `tools/`, thatâ€™s a smell.  
> Put logic in `src/` and keep `tools/` as a predictable CLI + validator layer ğŸ› ï¸

---

## ğŸ“„ Docs, Story Nodes, & Focus Mode validation

KFM treats documentation + narrative as governed artifacts (not â€œfreeform notesâ€).

### âœ… Docs validation should cover
- YAML frontâ€‘matter present + valid
- internal link checks (`docs/**`, `data/**`, `schemas/**`)
- image/assets exist (no broken embeds)
- required sections exist for governed doc types *(templates)*

### âœ… Story Nodes validation should cover
- lives under `docs/reports/story_nodes/{draft|published}/...`
- uses the Story Node template (v3) fields
- citations resolve to cataloged sources (STAC/DCAT/PROV)
- narrative claims do **not** introduce uncited â€œfactsâ€
- published stories meet stricter gates than drafts

### âœ… Focus Mode contract tests should cover
- context bundles only contain provenanceâ€‘linked content
- AIâ€‘generated text is **clearly labeled** (and includes model/version where permissible)
- refusal behavior works when evidence is missing
- uncertainty is surfaced (intervals, confidence notes, or â€œunknownâ€)

> [!CAUTION]
> If a Story Node (or Focus Mode) could expose sensitive locations or culturally sensitive information:  
> **CI should flag it for governance review** and block publish until review completes ğŸ”’

---

## ğŸ§¾ Contract & metadata tests

KFM is **contractâ€‘first** and **catalogâ€‘first**. Tests must protect:
- ğŸ›¡ï¸ OpenAPI / GraphQL contracts (breaking changes are explicit + versioned)
- ğŸ—‚ï¸ STAC (collections/items validity + required fields)
- ğŸ·ï¸ DCAT (distributions point to real assets/endpoints)
- ğŸ§¬ PROV (inputs â†’ activities â†’ outputs; run IDs/configs recorded)
- ğŸ§¬ Crossâ€‘layer linkage (Graph references catalogs; UI references API; Story references catalogs)

### âœ… What to validate
- JSON parses + schema passes
- links resolve (STAC assets exist; DCAT distributions point somewhere real)
- provenance completeness (raw â†’ work â†’ processed trace exists)
- stable IDs/hashes present where required
- time metadata makes sense (windows applied; plausible ranges)

### Example checks (starter)
```bash
# JSON sanity
find data/stac data/catalog/dcat data/prov -name "*.json*" -print0 | xargs -0 -n 1 jq empty

# pytest contract suite
pytest -q -m contracts
```

> [!NOTE]
> Dataset PRs should run a **Catalog QA gate** (schema + links + license) before merge.  
> If metadata is incomplete (e.g., missing license, broken href), CI must fail. ğŸš«âœ…

---

## âœ… Data validation gates (fail fast)

These gates are your â€œnoâ€‘badâ€‘data firewallâ€ ğŸ§±ğŸ”¥ â€” especially for GeoParquet + COG pipelines and publish flows.

### Ring model (recommended)
**Ring 0: Structure**
- parses (JSON/GeoJSON/Parquet/TIFF)
- schema validation (STAC/DCAT/PROV + local schemas)
- required files exist

**Ring 1: Integrity**
- checksums/manifest inventory (if used)
- deterministic IDs present where required
- atomic publish (no half-state)

**Ring 2: Semantics**
- CRS correctness + axis order
- geometry validity (and any allowed repair policy)
- raster sanity (nodata, overviews, alignment)
- time/bounds sanity (domain-specific checks)

**Ring 3: Governance & safety**
- license required before publish
- classification propagation (no downgrade)
- sensitive fields redaction rules
- policy tests (OPA/Conftest) where used
- secrets scans + dependency hygiene checks

---

## ğŸ—ºï¸ Geospatial tests (GIS correctness)

Geospatial pipelines fail in predictable waysâ€”test them explicitly:

- ğŸŒ CRS sanity: EPSG correctness; meters vs degrees; axis order
- ğŸ§± topology: geometry validity; selfâ€‘intersections per policy
- ğŸ§© overlay correctness: clip/intersect/union behaviors
- ğŸ§Š raster alignment: resolution, nodata handling, resampling method
- ğŸ“¦ format IO: GeoJSON/GeoPackage/GeoParquet/COG round-trips
- ğŸ§­ coordinate range checks: latitude/longitude in valid ranges
- ğŸ§® area sanity (datasetâ€‘specific): within tolerance of mask boundary

> [!TIP]
> Put CRS + units in the **fixture metadata** and test names.  
> It prevents â€œsilent degrees vs metersâ€ disasters. ğŸ¥²

---

## ğŸ›°ï¸ Remote sensing tests (Earth Engine & imagery)

Remote sensing workflows fail quietly unless assumptions are tested:
- band availability & naming
- scale / resolution
- cloud masking logic (QA bits)
- temporal compositing rules
- index calculations (NDVI) & expected ranges
- export formats & metadata consistency

Truthiness checks that catch many bugs:
- output range sanity (e.g., NDVI âˆˆ [-1, 1])
- masked pixel counts within expected bounds
- timestamps monotonic; windows applied correctly

> [!CAUTION]
> Unit tests should not call live services.  
> Prefer recorded fixtures, mock adapters, or small cached exports. âœ…

---

## ğŸ§  Scientific / simulation validation

Treat simulation/analysis code like a **scientific instrument** ğŸ”¬:
- **verification**: implementation matches intended math
- **validation**: model matches reality within uncertainty
- **regression baselines**: detect drift across refactors

### âœ… Recommended patterns
- analytical solution comparisons (tiny cases with known answers)
- convergence tests (refinement reduces error)
- invariant checks (symmetry, conservation, monotonicity)
- tolerance-based golden files (with metadata + tolerances)
- uncertainty reporting checks (intervals, credible bands, PPC)

> [!TIP]
> If results are stochastic, test **properties** (ranges, quantiles, calibration), not exact values. ğŸ²âœ…

---

## ğŸ“Š ML / stats tests (donâ€™t fool yourself)

Data science code needs tests beyond â€œit runsâ€:
- split is leakâ€‘free (train/val/test)
- metrics stable within tolerance
- baseline comparisons exist (simple model beats random)
- diagnostics exist (residuals, leverage/outliers, calibration)
- uncertainty reporting present when relevant
- multiple comparisons / pâ€‘hacking risks handled (where applicable)

ğŸ“ On failure, attach plots as CI artifacts:
- confusion matrix
- residual plots
- calibration curve
- drift dashboards (if applicable)

---

## ğŸ•¸ï¸ Graph tests (Neo4j + algorithms)

KFM treats the graph as **derived truth** (built from catalogs + provenance), not a writeâ€‘anywhere scratchpad.

Test categories:
- ğŸ§¾ graph build contract: rebuild from catalogs is reproducible
- ğŸ”’ constraints: uniqueness, required properties, relationship rules
- ğŸ§­ query invariants: deterministic pagination; stable ordering; filters correct
- ğŸ§  algorithm sanity: tiny deterministic graphs for spectral/routing invariants

Example assertions:
- â€œGraph contains only entities referenced by STAC/DCAT/PROVâ€
- â€œEvery published dataset node links to a PROV Activity with run_id + config hashâ€
- â€œNo unbounded traversals in query layer (guardrails enforced)â€

---

## ğŸ›¡ï¸ API tests (FastAPI + GraphQL)

What to test:
- ğŸ§¾ OpenAPI schema validation (breaking changes explicit)
- ğŸ§¾ GraphQL schema validation + query shapes
- ğŸ” AuthN/AuthZ: role-based access, classification enforcement
- ğŸ§­ Pagination determinism: stable ordering, cursor correctness
- ğŸŒ Geo correctness: GeoJSON validity; bbox correctness; CRS behavior
- ğŸŒ CORS headers correct (UI shouldnâ€™t need workarounds)

---

## ğŸŒ Web / frontend test guidance

### ğŸ§± Component tests (fast)
- render correctness given props/state
- event handling correctness
- accessibility checks (labels, keyboard nav, contrast)

### ğŸ“± Responsive tests (mobileâ€‘first)
- run tests across multiple viewport sizes
- verify map controls remain usable on small screens
- check touch targets + scroll locking behaviors

### ğŸ§­ E2E tests (few but powerful)
Focus on â€œmoney pathsâ€ ğŸ’¸:
- auth/login
- load a layer **from catalog**
- timeline navigation (time slider updates map + panels)
- select feature â†’ details panel updates
- export/report flow (metadata/provenance attached)

### ğŸ–¼ï¸ Visual regression (maps + WebGL)
Maps can regress visually while â€œlogic testsâ€ pass. Use screenshot diffs for:
- symbology stability
- overlay legibility at common zooms
- dark/light contrast
- WebGL rendering regressions (tolerance-based diffs)

---

## ğŸ” Security, governance, & ethics tests (defensive)

KFMâ€™s security stance is defensive: prevent leaks, enforce policy, keep audit trails.

### âœ… What to test (defensive)
- ğŸ” classification boundaries & redaction rules enforced (endâ€‘toâ€‘end)
- ğŸ•µï¸ secrets scanning (prevent committed tokens/keys)
- ğŸ“¦ dependency scanning (vulnerable libs flagged)
- ğŸ³ container scanning (base image CVEs flagged)
- ğŸ§¾ FAIR/CARE gates (required metadata present; access constraints honored)
- ğŸ§¬ auditability (publish actions produce receipts: who/what/when)

### ğŸ§‘â€âš–ï¸ Governance review triggers (manual review beyond CI)
Some changes should automatically require human review:
- adding sensitive/culturally protected datasets or layers
- new AI-driven narrative features
- new external data sources (license + provenance scrutiny)
- new public-facing endpoints or downloads that could expose sensitive info
- classification/sensitivity changes (especially â€œless restrictedâ€)

> [!IMPORTANT]
> Do **not** add offensive security instructions here.  
> Security tests are for hardening, verification, and prevention. ğŸ›¡ï¸âœ…

---

## ğŸ§¾ Test artifacts & receipts

KFM work is evidenceâ€‘driven. When tests fail, make failures inspectable:

### âœ… CI artifacts to upload on failure
- structured logs (`.jsonl` or `.txt`)
- diff outputs (schema diffs, snapshot diffs)
- screenshots (UI E2E + map diffs)
- â€œminiâ€ STAC/DCAT/PROV bundles from fixtures
- performance traces (if relevant)

### ğŸ§¾ â€œReceipt mindsetâ€
For integration tests that simulate real workflows (pipeline â†’ catalogs â†’ graph â†’ API):
- record the run id / config used
- capture hashes of produced artifacts
- include a minimal PROV-like trace for the test run (even if toy)

> [!TIP]
> If a test failure canâ€™t be diagnosed from artifacts alone, itâ€™s a documentation bug. ğŸ““ğŸ§¯

---

## ğŸ—‚ï¸ Suggested folder layout

Adapt as needed, but keep intent obvious:

```text
ğŸ“¦ repo-root/
â”œâ”€ ğŸ§ª tests/
â”‚  â”œâ”€ ğŸ“„ README.md                      # you are here ğŸ‘‹
â”‚  â”œâ”€ ğŸ§· fixtures/                      # tiny, deterministic test data only
â”‚  â”‚  â”œâ”€ ğŸ—ºï¸ geo/                        # small vectors/rasters (safe + tiny)
â”‚  â”‚  â”œâ”€ ğŸ›°ï¸ eo/                         # tiny EO chips / QA bit samples
â”‚  â”‚  â”œâ”€ ğŸ–¼ï¸ media/                      # tiny images + metadata
â”‚  â”‚  â”œâ”€ ğŸ§¬ ml/                         # toy datasets / tiny model artifacts
â”‚  â”‚  â”œâ”€ ğŸ§¾ catalogs/                   # STAC/DCAT/PROV fixtures
â”‚  â”‚  â””â”€ ğŸ“˜ FIXTURES.md                 # fixture rules + provenance notes
â”‚  â”œâ”€ ğŸ“„ docs/                          # markdown/front-matter/story-node checks
â”‚  â”œâ”€ ğŸ§° tools_contract/                # CLI contract tests for tools/
â”‚  â”œâ”€ ğŸ python/
â”‚  â”‚  â”œâ”€ ğŸ§© unit/
â”‚  â”‚  â”œâ”€ ğŸ”Œ integration/
â”‚  â”‚  â”œâ”€ âœ… validation/                 # scientific V&V (tolerance-based)
â”‚  â”‚  â”œâ”€ â±ï¸ perf/                       # benchmarks (nightly / non-gating)
â”‚  â”‚  â”œâ”€ ğŸ” security/                   # defensive security checks
â”‚  â”‚  â””â”€ ğŸ§± conftest.py
â”‚  â”œâ”€ ğŸŒ web/
â”‚  â”‚  â”œâ”€ ğŸ§© unit/
â”‚  â”‚  â”œâ”€ ğŸ§± component/
â”‚  â”‚  â”œâ”€ ğŸ§­ e2e/
â”‚  â”‚  â””â”€ ğŸ–¼ï¸ visual/
â”‚  â”œâ”€ ğŸ—„ï¸ db/
â”‚  â”‚  â”œâ”€ ğŸ§¬ migrations/
â”‚  â”‚  â””â”€ ğŸ”Œ integration/
â”‚  â”œâ”€ ğŸ•¸ï¸ graph/
â”‚  â”‚  â”œâ”€ ğŸ”Œ integration/
â”‚  â”‚  â””â”€ ğŸ§© unit/
â”‚  â”œâ”€ ğŸ“œ contracts/
â”‚  â”‚  â”œâ”€ ğŸ›¡ï¸ api/                        # OpenAPI/GraphQL fixtures
â”‚  â”‚  â”œâ”€ ğŸ—ºï¸ stac/
â”‚  â”‚  â”œâ”€ ğŸ·ï¸ dcat/
â”‚  â”‚  â””â”€ ğŸ§¬ prov/
â”‚  â””â”€ ğŸ“˜ TEST_POLICY.md                 # merge gates + definition of done
â””â”€ ğŸ§° scripts/                          # CI glue & wrappers (optional; prefer tools/)
```

> [!NOTE]
> If you keep tests alongside code (e.g., `src/**/tests`), thatâ€™s fineâ€”just keep naming + markers consistent.

---

## âœ… CI gates (non-negotiable)

**Policy:** the pipeline must be green before merge ğŸ¤–âœ…  
CI should mirror KFMâ€™s â€œbuild â†’ validate â†’ publishâ€ discipline and keep logs/artifacts for traceability.

### âœ… Minimum PR gates (recommended)
1) ğŸ§¹ format + lint (Python + JS/TS)
2) ğŸ§± build (frontend + backend; container build if applicable)
3) ğŸ§ª unit tests
4) ğŸ§¾ docs protocol checks (frontâ€‘matter + link validation)
5) ğŸ§¾ schema validation (STAC/DCAT/PROV + story/node schemas)
6) âœ… data validation gates (CRS + geometry + raster sanity)
7) ğŸ”Œ integration tests (ephemeral DB/services via Compose)
8) ğŸ•¸ï¸ graph integrity tests (constraints + rebuild invariants)
9) ğŸ›¡ï¸ API contract tests (OpenAPI/GraphQL + deterministic pagination)
10) ğŸ” security & governance scans (secrets + PII + sensitive location + classification â€œno downgradeâ€)
11) ğŸ“ˆ coverage thresholds (target, not a religion)

### ğŸ•› Nightly / scheduled checks (recommended)
Keep PR CI fast; push expensive checks to nightly:
- â±ï¸ benchmarks (trend monitoring)
- ğŸ§  longer ML runs (PRs use toy models)
- ğŸ—ºï¸ large raster workloads (PRs use fixtures & sampling)
- ğŸ•¸ï¸ deeper graph consistency (full rebuild + diff)
- ğŸ” deeper security scanning (if it slows PR CI)

> [!TIP]
> If a gate is â€œimportantâ€, it must be **automatable** and **repeatable**.  
> If itâ€™s not automatable, it must be a **documented manual review step** ğŸ§¾

---

## âœ… PR checklist (copy/paste)

- [ ] Unit tests added/updated
- [ ] Integration tests added (if behavior crosses boundaries)
- [ ] Determinism confirmed (seeds + stable outputs) if ML/sim
- [ ] Tools/CLI contract checks updated (if adding/modifying tools/)
- [ ] Docs/story checks updated (frontâ€‘matter, links, story templates) if docs changed
- [ ] Contracts updated + verified (OpenAPI/GraphQL) if API changed
- [ ] Catalog/metadata tests updated (STAC/DCAT/PROV) if outputs changed
- [ ] Data validation gates updated (schema/CRS/geometry) if ETL changed
- [ ] License + governance checks pass (block publish if missing license)
- [ ] Sensitive location / â€œno downgradeâ€ checks pass (or governance review requested)
- [ ] UI changes include component tests + (if visual) snapshot updates
- [ ] CI is green (required)

---

## ğŸ§¯ Troubleshooting

### âŒ Tests fail only in CI?
- check lockfiles & pinned versions
- confirm containers match local versions
- eliminate reliance on local paths, locale, timezone, GPU availability

### ğŸ² Flaky tests?
- remove sleeps; wait on conditions
- fix randomness (seed)
- mock/record external services

### ğŸ³ Docker stack wonâ€™t start?
```bash
docker compose logs -f
docker compose config
docker compose up -d --build
```

---

## ğŸ“š Reference pointers (library index)

These repo library files inform KFMâ€™s test posture (V&V, stats rigor, GIS correctness, visualization stability, data governance, security hardening). ğŸ§ ğŸ§¾

<details>
<summary>ğŸ›ï¸ System design, contracts, governance, and â€œhow KFM is supposed to workâ€</summary>

- `docs/MASTER_GUIDE_v13.md` *(contract-first + evidence-first + canonical layout)*
- `Kansas Frontier Matrix (KFM) â€“ Comprehensive Technical Documentation.docx`
- `Kansas-Frontier-Matrix Design Audit â€“ Gaps and Enhancement Opportunities.pdf`
- `ğŸŒŸ Kansas Frontier Matrix â€“ Latest Ideas & Future Proposals.docx` *(policy pack + CI promotion lanes)*

</details>

<details>
<summary>ğŸ”¬ Scientific modeling, simulation V&amp;V, and uncertainty</summary>

- `Scientific Modeling and Simulation_ A Comprehensive NASA-Grade Guide.pdf`
- `Understanding Statistics & Experimental Design.pdf`
- `think-bayes-bayesian-statistics-in-python.pdf`

</details>

<details>
<summary>ğŸ“Š Statistics, regression, Bayesian, and analysis discipline</summary>

- `regression-analysis-with-python.pdf`
- `Regression analysis using Python - slides-linear-regression.pdf`
- `graphical-data-analysis-with-r.pdf`

</details>

<details>
<summary>ğŸ—ºï¸ Geospatial, remote sensing, cartography, and mapping UX</summary>

- `python-geospatial-analysis-cookbook.pdf`
- `Cloud-Based Remote Sensing with Google Earth Engine-Fundamentals and Applications.pdf`
- `making-maps-a-visual-guide-to-map-design-for-gis.pdf`
- `Mobile Mapping_ Space, Cartography and the Digital - 9789048535217.pdf`
- `compressed-image-file-formats-jpeg-png-gif-xbm-bmp.pdf`

</details>

<details>
<summary>ğŸ—„ï¸ Data management, interoperability, and scale</summary>

- `PostgreSQL Notes for Professionals - PostgreSQLNotesForProfessionals.pdf`
- `Scalable Data Management for Future Hardware.pdf`
- `Data Spaces.pdf`
- `Spectral Geometry of Graphs.pdf`

</details>

<details>
<summary>ğŸŒ Web UI, WebGL visualization, and responsive design</summary>

- `responsive-web-design-with-html5-and-css3.pdf`
- `webgl-programming-guide-interactive-3d-graphics-programming-with-webgl.pdf`

</details>

<details>
<summary>ğŸ›¡ï¸ Security, concurrency, law, humanism, and governance</summary>

- `ethical-hacking-and-countermeasures-secure-network-infrastructures.pdf` *(defensive posture only)*
- `Gray Hat Python - Python Programming for Hackers and Reverse Engineers (2009).pdf` *(defensive awareness only)*
- `concurrent-real-time-and-distributed-programming-in-java-threads-rtsj-and-rmi.pdf`
- `On the path to AI Lawâ€™s prophecies and the conceptual foundations of the machine learning age.pdf`
- `Introduction to Digital Humanism.pdf`
- `Principles of Biological Autonomy - book_9780262381833.pdf`

</details>

<details>
<summary>ğŸ“š Language &amp; tooling reference shelves (programming books index)</summary>

- `A programming Books.pdf`
- `B-C programming Books.pdf`
- `D-E programming Books.pdf`
- `F-H programming Books.pdf`
- `I-L programming Books.pdf`
- `M-N programming Books.pdf`
- `O-R programming Books.pdf`
- `S-T programming Books.pdf`
- `U-X programming Books.pdf`

</details>

---

## ğŸ•°ï¸ Version history

| Version | Date | Summary | Author |
|---:|---|---|---|
| v1.2.0 | 2026-01-11 | Aligned tests with Master Guide v13: contract-first + catalog-first gates, docs/story-node validation lane, governance trigger guidance, and tool/CLI contract testing. Removed internal placeholder evidence anchors. | KFM Engineering |
| v1.1.0 | 2026-01-09 | Tightened â€œcatalog-firstâ€ & data QA gates; added receipts/artifacts section; clarified defensive security stance; aligned CI gates with KFM engineering/testing guidance. | KFM Engineering |
| v1.0.0 | 2026-01-07 | Initial repo-wide testing README: pyramid, markers, subsystem matrix, validation + governance posture. | KFM Engineering |
