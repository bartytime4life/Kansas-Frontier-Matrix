# ğŸ§ª Tests (KFM / Kansas Matrix System)

[![CI](https://github.com/<ORG>/<REPO>/actions/workflows/ci.yml/badge.svg)](https://github.com/<ORG>/<REPO>/actions/workflows/ci.yml)
![Coverage Target](https://img.shields.io/badge/coverage-target%3A%2080%25-informational)
![Policy](https://img.shields.io/badge/policy-fail--closed-critical)
![Contracts](https://img.shields.io/badge/contracts-contract--first-blueviolet)
![Evidence](https://img.shields.io/badge/evidence-evidence--first-ff69b4)
![Stack](https://img.shields.io/badge/stack-FastAPI%20%7C%20PostGIS%20%7C%20Neo4j%20%7C%20React%20%7C%20MapLibre-blue)
![CI Gates](https://img.shields.io/badge/CI%20gates-lint%20%7C%20test%20%7C%20policy%20%7C%20schema-success)

> âœ… This folder is the **guardrail suite** for KFMâ€™s **canonical pipeline** â€” from ETL outputs, to catalog boundary artifacts (STAC/DCAT/PROV), to graph/API/UI contracts, to Story Node + Focus Mode governance.  
> ğŸ”’ If tests or policies fail, we **fail closed** (PR should not merge).  
> ğŸ§­ Tests mirror reality: every stage must produce what the next stage expects.

---

## ğŸ“Œ Contents

- [âš¡ Quickstart](#-quickstart)
- [ğŸ§­ Non-negotiable invariants](#-non-negotiable-invariants)
- [ğŸ§© Repo alignment (v13)](#-repo-alignment-v13)
- [ğŸ—‚ï¸ What belongs in `tests/`](#ï¸-what-belongs-in-tests)
- [ğŸ§ª Test suites](#-test-suites)
- [ğŸ” CI gates (what should run on PRs)](#-ci-gates-what-should-run-on-prs)
- [ğŸ§± Writing great tests (KFM style)](#-writing-great-tests-kfm-style)
- [ğŸ§¯ Troubleshooting](#-troubleshooting)
- [ğŸ“š References (repo-friendly)](#-references-repo-friendly)

---

## âš¡ Quickstart

### 0) Prereqs âœ…
- Docker + Compose (recommended for consistent PostGIS + Neo4j + API + UI)
- Python toolchain (backend + pipelines tests)
- Node toolchain (frontend tests)
- Conftest (OPA) for policy validation ğŸ”’

> ğŸ§¯ Heads-up: the most common port collisions are **5432** (Postgres/PostGIS), **7474** (Neo4j), plus typical app ports like **8000/3000**.

---

### 1) Start the dev stack ğŸ§±
```bash
# either form is fine (depending on your Docker version)
docker compose up -d
# or
docker-compose up -d
```

---

### 2) Run backend tests (Python) ğŸ
```bash
docker compose exec api pytest
# or
docker-compose exec api pytest
```

âœ… Tip: if youâ€™re iterating, run the smallest slice:
```bash
pytest tests/unit -q
```

---

### 3) Run frontend tests (React / TypeScript) âš›ï¸
```bash
docker compose exec web npm test
# or
docker-compose exec web npm test
```

---

### 4) Run policy checks (fail-closed governance) ğŸ”’
```bash
conftest test .
```

> ğŸ” Policy failures should block merges. Treat them like failing unit tests.

---

## ğŸ§­ Non-negotiable invariants

These are â€œsystem truths.â€ If a PR violates one, **tests must catch it**. âœ…

1) **Pipeline ordering is absolute**  
   `ETL â†’ Catalogs (STAC/DCAT/PROV) â†’ Graph â†’ API â†’ UI â†’ Story Nodes â†’ Focus Mode`  
   No leapfrogging. No bypassing boundary artifacts.

2) **API boundary rule**  
   The **UI must never query Neo4j (or databases) directly**. Everything goes through the governed API.

3) **Provenance-first publishing**  
   Nothing is â€œpublishedâ€ without boundary artifacts: STAC/DCAT metadata + PROV lineage.

4) **Deterministic, idempotent ETL**  
   Same inputs â‡’ stable outputs. Pipelines should be safely re-runnable.

5) **Evidence-first narrative**  
   No unsourced narrative in Story Nodes or Focus Mode. AI output must be labeled + provenance-linked.

6) **Sovereignty & classification propagation**  
   Outputs canâ€™t be â€œless restrictedâ€ than inputs. Sensitive locations must not leak through the UI.

7) **Governance fails closed** ğŸ”’  
   Missing license, missing provenance, schema violations, policy denials â‡’ pipeline/CI stops.

---

## ğŸ§© Repo alignment (v13)

This `tests/` folder is **repo-wide** and maps to the v13 subsystem homes:

```text
ğŸ“¦ repo-root/
â”œâ”€ ğŸ—‚ï¸ data/                    # raw/work/processed + catalog outputs
â”œâ”€ ğŸ§¾ schemas/                 # JSON Schemas (STAC/DCAT/PROV/story/UI/etc.)
â”œâ”€ ğŸ§± src/
â”‚  â”œâ”€ ğŸ› ï¸ pipelines/            # ETL jobs + normalization
â”‚  â”œâ”€ ğŸ•¸ï¸ graph/                # Neo4j build + constraints + migrations
â”‚  â””â”€ ğŸ”Œ server/               # API boundary + contracts + enforcement
â”œâ”€ ğŸŒ web/                     # React + MapLibre UI
â”œâ”€ ğŸ”’ policy/                  # OPA policies (tested via Conftest)
â”œâ”€ ğŸ§­ docs/                    # governed docs + story nodes
â””â”€ ğŸ§ª tests/                   # automated tests for all modules âœ…
```

### ğŸ—ºï¸ Canonical pipeline (for mental model)

```mermaid
flowchart LR
  A["Raw Sources"] --> B["ETL + Normalization"]
  B --> C["STAC Items + Collections"]
  C --> D["DCAT Dataset Views"]
  C --> E["PROV Lineage Bundles"]
  C --> G["Neo4j Graph (references catalogs)"]
  G --> H["API Layer (contracts + redaction)"]
  H --> I["Map UI â€” React Â· MapLibre Â· (optional) Cesium"]
  I --> J["Story Nodes (governed narratives)"]
  J --> K["Focus Mode (provenance-linked context bundle)"]
```

---

## ğŸ—‚ï¸ What belongs in `tests/`

**Yes** âœ…
- ğŸ”¬ Unit tests (pure logic, no network, no DB)
- ğŸ”Œ Integration tests (API â†” PostGIS/Neo4j â†” services)
- ğŸ§¾ Boundary artifact tests (STAC/DCAT/PROV schema + linkage)
- ğŸ•¸ï¸ Graph constraints + ontology/label integrity tests
- ğŸ—ºï¸ Geo correctness tests (CRS, geometry validity, GeoJSON output)
- ğŸ”’ Policy tests (Conftest/OPA)
- ğŸ§­ Story Node + Focus Mode governance checks
- ğŸ¤– AI gateway contract tests (stub by default; opt-in live runs)

**No** âŒ
- Large raw datasets (use tiny fixtures)
- Flaky tests that depend on real external services by default
- UI tests that bypass the API boundary (thatâ€™s a failure condition)

---

## ğŸ§ª Test suites

> ğŸ¯ Rule of thumb: **each pipeline boundary gets a test suite**.  
> If a stage consumes outputs from the previous stage, that is a **contract** â†’ contracts get tests.

---

### 1) ğŸ”¬ Unit tests (fast, deterministic)
**Goal:** prove core logic works without containers.

Examples:
- schema validators
- provenance builders
- parsers / normalizers
- CRS conversion utilities
- helper functions used by UI/Story Nodes

âœ… Best practice: unit tests should exercise **domain/service** logic and mock adapters.

---

### 2) ğŸ› ï¸ Pipeline/ETL tests (raw â†’ work â†’ processed)
**Goal:** ensure ETL is deterministic, idempotent, and emits stable outputs.

Recommended checks:
- output file naming + stable IDs
- hashing/signatures on inputs/outputs
- â€œre-run produces identical artifactsâ€ (or logged deltas)
- pipeline logs exist and are parseable

ğŸ“ Data staging convention:
- `data/raw/<domain>/`
- `data/work/<domain>/`
- `data/processed/<domain>/`

---

### 3) ğŸ§¾ Boundary artifact tests (STAC/DCAT/PROV)
**Goal:** enforce â€œpublishable = validated metadata + lineage.â€

Checks:
- STAC records exist (collections + items)
- DCAT dataset entry exists (title/description/license/keywords/distributions)
- PROV lineage bundle exists and links raw â†’ work â†’ processed
- cross-links are consistent (STAC â†” DCAT â†” PROV â†” graph IDs)

ğŸ“¦ Canonical catalog locations:
- `data/stac/collections/`
- `data/stac/items/`
- `data/catalog/dcat/`
- `data/prov/`

âœ… â€œBoundary artifactsâ€ are the interface to downstream stages (graph/API/UI). Treat them like APIs.

---

### 4) ğŸ•¸ï¸ Graph tests (Neo4j integrity + migrations)
**Goal:** keep graph structure stable unless migrations are deliberate.

Recommended checks:
- no orphan labels/types
- required indexes/constraints exist
- ontology changes require migration scripts
- graph nodes reference catalogs (STAC IDs / DCAT IDs) instead of duplicating payloads

---

### 5) ğŸ”Œ API integration + contract tests (FastAPI â†” PostGIS/Neo4j)
**Goal:** validate backend behavior, stability, and enforcement.

Checks:
- endpoint status codes + error shapes
- filtering/sorting/pagination
- OpenAPI contract stability (breaking change â‡’ version bump)
- policy enforcement: API calls must respect allow/deny decisions
- redaction/classification behavior (no sensitive leaks)
- provenance links returned alongside data products

âœ… Test the API as your â€œsecurity boundaryâ€ â€” because it is.

---

### 6) ğŸŒ UI tests (React + MapLibre)
**Goal:** ensure UI behavior matches contracts and never bypasses governance.

Checks:
- UI only talks to API (no direct graph/db calls)
- layer registry configs load expected layers
- accessibility sanity checks (basic a11y)
- map interactions do not leak sensitive location detail when restricted

> ğŸ§± If a UI test would be easier by calling the DB directly: **donâ€™t** â€” write an API fixture instead.

---

### 7) ğŸ§­ Story Node + Focus Mode tests
**Goal:** keep narrative machine-ingestible, provenance-linked, and safe.

Checks:
- Story Node uses the governed template
- every factual claim has citations/links to cataloged evidence
- referenced entities (people/places/events/docs) use stable graph identifiers
- fact vs interpretation is clearly separated
- Focus Mode rules enforced:
  - only provenance-linked content renders
  - AI contributions are opt-in + labeled + confidence surfaced
  - sensitive locations are generalized/blurred (no side-channel leakage)

âœ… These tests protect the â€œno hallucinationsâ€ rule.

---

### 8) ğŸ”’ Policy tests (Conftest / OPA)
**Goal:** encode governance so itâ€™s enforceable.

Run:
```bash
conftest test .
```

Patterns:
- `policy/` contains OPA rules
- `tests/policy/fixtures/` contains minimal examples
- policy tests should prove it blocks:
  - missing license
  - missing provenance
  - missing required metadata
  - schema/profile violations
  - restricted datasets exposed without authorization

---

### 9) ğŸ—ºï¸ Geospatial correctness tests (CRS, geometry, GeoJSON)
**Goal:** prevent â€œmaps that look right but are wrong.â€

Suggested checks:
- CRS sanity (expected EPSG / axis order / coordinate bounds)
- geometry validity (no self-intersections, empties, invalid rings)
- bbox correctness
- distance/area sanity (use projected CRS when required)
- GeoJSON output is complete (FeatureCollection/Feature, not geometry-only blobs)

ğŸ§  Metadata matters (test it): include map projection / coordinate system / spatial extent / scale, plus copyright & usage permissions where applicable.

---

### 10) ğŸ§­ Navigation & grid overlay tests (UTM / MGRS / lat-long)
If you ship grid overlays or coordinate readouts, test them with known fixtures:

- known lat/long â†” UTM â†” MGRS conversions
- formatting and precision rules
- â€œread right then upâ€ helpers (eastings then northings)

âœ… These are perfect for deterministic unit tests.

---

### 11) ğŸ¤– AI / local inference tests (optional, but powerful)
Treat LLMs like any external dependency: **stub by default**.

#### ğŸ§° Local-first with Ollama
Recommended test layers:
- **unit:** prompt templates + â€œno unsourced claimsâ€ guards
- **integration:** API contract to AI gateway (mock server)
- **live/manual:** opt-in tests talking to a real local Ollama instance

Example local setup:
```bash
ollama serve
ollama run <model>
```

Optional: if you route AI via env config, document test defaults (e.g., stubbed AI, local Ollama only for manual runs).

---

## ğŸ” CI gates (what should run on PRs)

> âœ… The goal is a permanently **CI-clean** main branch.

Minimum gates:
- ğŸ§¹ linting (Python + JS/TS)
- ğŸ§ª unit + integration tests (backend + pipelines)
- âš›ï¸ frontend tests
- ğŸ”’ policy checks (Conftest/OPA)
- ğŸ§¾ schema/profile validation (STAC/DCAT/PROV/story)
- ğŸ” security scans (secrets/sensitive info leaks)

### âœ… PR checklist (copy/paste)
- [ ] I added/updated tests for the behavior I changed
- [ ] Any new dataset includes required STAC/DCAT/PROV artifacts
- [ ] Any Story Node change follows the template and includes citations
- [ ] Policy checks pass (`conftest test .`)
- [ ] No sensitive data leaks (UI + API + catalogs reflect classification)

---

## ğŸ§± Writing great tests (KFM style)

### âœ… Principles (practical, not preachy)
- **Deterministic** ğŸ¯ (fixed seeds, stable fixtures)
- **Idempotent** ğŸ” (ETL can re-run safely)
- **Fast by default** âš¡ (heavy tests are opt-in / nightly)
- **Small fixtures** ğŸ§° (tiny, legible datasets > giant dumps)
- **Evidence-first failures** ğŸ§¾ (tests should explain *why* they failed)
- **Contract-first mindset** ğŸ“œ (schemas + OpenAPI + profiles are first-class)

### ğŸ·ï¸ Naming conventions
- `test_<component>__<behavior>__<expected>()`
- Prefer â€œbehaviorâ€ tests over implementation tests.

### ğŸ§ª Suggested markers (recommended)
- `@pytest.mark.unit`
- `@pytest.mark.integration`
- `@pytest.mark.e2e`
- `@pytest.mark.geo`
- `@pytest.mark.policy`

---

## ğŸ§¯ Troubleshooting

### ğŸ§¨ â€œPort already in useâ€
- Stop the conflicting local service, or change ports in `docker-compose.yml`
- Common collisions: `5432`, `7474`, `8000`, `3000`

### ğŸ§± DB container not ready / migrations not applied
```bash
docker compose down
docker compose up -d
```
Then rerun tests after DB is healthy.

### ğŸ§© Flaky e2e tests
- stabilize fixtures
- avoid time-based assumptions
- poll readiness endpoints
- stub non-critical external services

---

## ğŸ“š References (repo-friendly)

> ğŸ§­ These should live in-repo under `docs/` (and optionally `docs/library/` for PDFs).  
> Keep links stable â€” tests will rely on governed contracts and templates.

- `docs/MASTER_GUIDE_v13.md` (canonical pipeline + invariants + directory layout)
- `docs/standards/` (STAC/DCAT/PROV profiles, schema rules)
- `docs/templates/` (universal docs, Story Node template, API contract template)
- `docs/reports/story_nodes/` (governed narratives: draft â†’ published)
- `docs/governance/` (ethics, sovereignty, FAIR/CARE)
- `policy/` (OPA policies; validated via Conftest)
- `schemas/` (machine-validated contracts)

---

<details>
<summary>ğŸ§ª â€œWhere do I add my test?â€ (quick map)</summary>

| You changedâ€¦ | Add tests inâ€¦ | And runâ€¦ |
|---|---|---|
| ETL pipeline code | `tests/pipelines/` | `pytest tests/pipelines` |
| Catalog schemas/metadata | `tests/catalog/` | `pytest tests/catalog` |
| Graph ingest/ontology | `tests/graph/` | `pytest tests/graph` |
| API endpoints/contracts | `tests/api/` | `pytest tests/api` |
| UI behavior | `web/` tests | `npm test` |
| Policies | `tests/policy/` | `conftest test .` |
| Story Nodes / Focus Mode | `tests/story/` | `pytest tests/story` |

</details>