---
title: "ğŸ§ª Tests â€” Kansas Frontier Matrix (KFM) / Kansasâ€‘Matrixâ€‘System"
path: "tests/README.md"
version: "v1.3.0"
last_updated: "2026-01-13"
review_cycle: "90 days"
status: "active"
doc_kind: "Directory README"
license: "CC-BY-4.0"
owners:
  - "KFM Engineering"
tags:
  - tests
  - ci
  - determinism
  - contracts
  - catalogs
  - stac
  - dcat
  - prov
  - geo
  - eo
  - graph
  - api
  - ui
  - performance
  - governance
  - security
---

<!--
ğŸ“Œ This README defines the repo-wide testing & verification surface for KFM / Kansasâ€‘Matrixâ€‘System.
ğŸ—“ï¸ Last updated: 2026-01-13
ğŸ” Review cycle: 90 days (or anytime pipeline order / catalogs / policy pack / CI lanes change)
-->

<div align="center">

# ğŸ§ª Tests â€” Kansas Frontier Matrix (KFM) / Kansasâ€‘Matrixâ€‘System

**Trust-first testing for a contractâ€‘first, catalogâ€‘first geospatial + knowledge + modeling stack** ğŸ§¾ğŸ—ºï¸ğŸ§¬  
Determinism â€¢ Contracts â€¢ Governance â€¢ Evidence receipts â€¢ â€œFail closedâ€ gates âœ…ğŸ”’

![CI](https://img.shields.io/badge/CI-GitHub%20Actions-2ea44f?logo=githubactions&logoColor=white)
![CodeQL](https://img.shields.io/badge/Security-CodeQL-0b7285?logo=github&logoColor=white)
![Pytest](https://img.shields.io/badge/Python-pytest-blue?logo=python&logoColor=white)
![Node](https://img.shields.io/badge/Node.js-tests-brightgreen?logo=node.js&logoColor=white)
![Playwright](https://img.shields.io/badge/E2E-Playwright-0b7285?logo=playwright&logoColor=white)
![Docker](https://img.shields.io/badge/Integration-Docker%20Compose-2496ed?logo=docker&logoColor=white)
![Contracts](https://img.shields.io/badge/Contracts-OpenAPI%20%7C%20GraphQL-ff6b6b)
![Catalogs](https://img.shields.io/badge/Catalogs-STAC%20%7C%20DCAT%20%7C%20PROV-6f42c1)
![Governance](https://img.shields.io/badge/Governance-FAIR%20%2B%20CARE%20%2B%20Sovereignty-2f9e44)
![Fail%20Closed](https://img.shields.io/badge/Quality%20Gates-Fail%20Closed-red)

</div>

> KFM tests donâ€™t just check â€œit runs.â€  
> They prove that our **pipelines**, **catalogs**, **graph**, **APIs**, **docs/story nodes**, and **UI behaviors** are:
>
> âœ… **Correct** â€¢ âœ… **Reproducible** â€¢ âœ… **Governanceâ€‘compliant** â€¢ âœ… **Honest about uncertainty**  
>
> We test the **seams (boundaries + contracts)** and treat metadata/provenance as **firstâ€‘class artifacts** ğŸ—‚ï¸ğŸ§¬

> [!IMPORTANT]
> **tests/** is part of KFMâ€™s *governed surface*.  
> If a change can affect what users see or what the system asserts as â€œtruthâ€, it must be **testable**, **traceable**, and **failâ€‘closed** when requirements arenâ€™t met.

---

<details>
<summary><b>ğŸ§­ Table of contents</b></summary>

- [ğŸ”— Quick links](#-quick-links)
- [ğŸš¦ Nonâ€‘negotiables](#-non-negotiables)
- [ğŸš€ Quickstart](#-quickstart)
- [ğŸ§© KFM test matrix](#-kfm-test-matrix-subsystems--what-to-assert)
- [ğŸ§  Core invariant: governed ordering](#-core-invariant-governed-ordering)
- [ğŸ§± Architecture boundary tests](#-architecture-boundary-tests-clean-architecture)
- [ğŸ”º Test pyramid](#-test-pyramid-how-we-keep-velocity--confidence)
- [ğŸ·ï¸ Test categories & markers](#ï¸-test-categories--markers-suggested)
- [ğŸ§° Tool & CLI contract tests](#-tool--cli-contract-tests)
- [ğŸ“„ Docs, Story Nodes, & Focus Mode validation](#-docs-story-nodes--focus-mode-validation)
- [ğŸ§¾ Contract & metadata tests](#-contract--metadata-tests)
- [ğŸ§· Stable IDs & versioning tests](#-stable-ids--versioning-tests-dont-break-links)
- [ğŸ“œ License, citation, & redistribution tests](#-license-citation--redistribution-tests)
- [âœ… Data validation gates](#-data-validation-gates-fail-fast)
- [ğŸ—ºï¸ Geospatial tests](#ï¸-geospatial-tests-gis-correctness)
- [ğŸ›°ï¸ Remote sensing tests](#ï¸-remote-sensing-tests-earth-engine--imagery)
- [ğŸ§Š 3D / WebGL / 3D GIS tests](#-3d--webgl--3d-gis-tests)
- [ğŸ§  Scientific & simulation validation](#-scientific--simulation-validation)
- [ğŸ“Š ML / stats tests](#-ml--stats-tests-dont-fool-yourself)
- [ğŸ•¸ï¸ Graph tests](#ï¸-graph-tests-neo4j--algorithms)
- [ğŸ›¡ï¸ API tests](#ï¸-api-tests-fastapi--graphql)
- [ğŸŒ Web / frontend tests](#-web--frontend-test-guidance)
- [ğŸ“ˆ Performance & capacity tests](#-performance--capacity-tests-latency-throughput-cost)
- [ğŸ” Security, governance, & ethics tests](#-security-governance--ethics-tests-defensive)
- [ğŸ§¾ Test artifacts & receipts](#-test-artifacts--receipts)
- [ğŸ—‚ï¸ Suggested folder layout](#ï¸-suggested-folder-layout)
- [âœ… CI gates](#-ci-gates-non-negotiable)
- [âœ… PR checklist](#-pr-checklist-copypaste)
- [ğŸ§¯ Troubleshooting](#-troubleshooting)
- [ğŸ“š Reference pointers](#-reference-pointers-library-index)
- [ğŸ•°ï¸ Version history](#ï¸-version-history)

</details>

---

## ğŸ”— Quick links

> Paths are relative to `tests/`. If your repo differs, treat these as the **target map** and document any deltas.

- ğŸ§­ Repo overview: `../README.md`
- ğŸ“š Docs boundary (canonical): `../docs/README.md`
- ğŸ§± Master Guide (v13): `../docs/MASTER_GUIDE_v13.md` *(if present)*
- ğŸ“¦ Data lifecycle + governance: `../data/README.md`
- ğŸ§¬ Schemas registry: `../schemas/` *(STAC/DCAT/PROV/Story/UI contracts)*
- ğŸ““ MCP (methods + receipts + model cards): `../mcp/`
- ğŸ§° Tools/validators (governed command surface): `../tools/README.md` *(if present)*
- ğŸ›¡ï¸ API boundary:
  - `../api/` *(if present â€” many KFM layouts put FastAPI here)*
  - `../src/server/` *(if present â€” some layouts put API here)*
- ğŸ›¡ï¸ API contracts (OpenAPI/GraphQL): `../src/server/contracts/` *(if present)*
- ğŸ“š Story Nodes (draft/published): `../docs/reports/story_nodes/`
- ğŸŒ Web UI boundary: `../web/` *(if present)*
- ğŸ¤ CI/CD + policy pack: `../.github/` *(workflows, templates, CodeQL, SECURITY.md)*

---

## ğŸš¦ Nonâ€‘negotiables

These are KFMâ€™s â€œmust not regressâ€ invariants. If any of these become false, **CI must block merge** ğŸš«âœ…

1) **Contractâ€‘first:** schemas + API contracts are firstâ€‘class repo artifacts ğŸ§¾  
   - Breaking changes must be explicit + versioned + tested.

2) **Catalogâ€‘first:** nothing is â€œrealâ€ unless itâ€™s cataloged (STAC/DCAT) and traceable (PROV) ğŸ—‚ï¸ğŸ§¬  
   - Catalogs are **boundary artifacts** that downstream stages consume.

3) **Governed ordering is enforced in tests** ğŸ§±  
   **ETL â†’ STAC/DCAT/PROV â†’ Graph â†’ API â†’ UI â†’ Story Nodes â†’ Focus Mode**

4) **API boundary rule:** UI must never query Neo4j/DB directly ğŸ”  
   - Everything user-facing must flow through the API boundary for redaction + policy enforcement.

5) **Determinism by default:** reruns should match unless inputs/configs change ğŸ”  
   - Stochastic code must be seeded and tested by **properties** (not exact values).

6) **Sovereignty + classification propagation:** outputs canâ€™t be *less restricted* than inputs ğŸ·ï¸ğŸ›¡ï¸  
   - â€œNo downgradeâ€ is a gate, not a guideline.

7) **No network in unit tests** ğŸš«ğŸŒ  
   - Record/replay, mock adapters, or cached fixtures only.

8) **Evidence over vibes:** failures must produce inspectable artifacts (logs, diffs, screenshots) ğŸ“

9) **Metadata must compile:** unsourced/adâ€‘hoc â€œmystery layersâ€ are not allowed ğŸ§¬ğŸš«  
   - If a dataset has no license/provenance/extent, itâ€™s not publishable.

10) **Privacy by test:** location-aware UX must be optâ€‘in and minimizationâ€‘first ğŸ“ğŸ«¥  
   - If the UI can expose a sensitive location (even indirectly), tests must flag it.

> [!TIP]
> If your PR changes **spatial truth** or **what users can infer**, it must be **traceable + testable** ğŸ§¾âœ…

---

## ğŸš€ Quickstart

### 0) Preconditions (oneâ€‘time)
- ğŸ Python env ready (`venv`, `uv`, `conda`, etc.)
- ğŸŒ Node env ready (`npm`, `pnpm`, or `yarn`)
- ğŸ³ Docker installed *(recommended for integration parity)*

### 1) Fast checks (developer loop âš¡)
```bash
# Python (fast)
pytest -q -m "not integration and not e2e and not slow"

# Web (fast â€” adapt to your repo)
npm test
```

### 2) Full suite (recommended on feature branches âœ…)
```bash
pytest -q
pytest -q -m integration
npm run test:e2e
```

### 3) Integration tests with containers (preferred ğŸ³)
```bash
docker compose up -d --build
pytest -q -m integration
docker compose down -v
```

<details>
<summary>ğŸ§¾ Command cheat sheet (copy/paste)</summary>

```bash
# Contracts only (API + schemas + catalogs)
pytest -q -m contracts

# Docs/story lint + story-node schema checks
pytest -q -m docs

# Geo sanity
pytest -q -m geo

# Earth-observation sanity
pytest -q -m eo

# Scientific V&V
pytest -q -m validation

# Graph slice
pytest -q -m graph

# Performance/capacity (usually scheduled)
pytest -q -m perf

# Defensive security checks
pytest -q -m security

# Governance/policy pack
pytest -q -m governance
```
</details>

---

## ğŸ§© KFM test matrix (subsystems + what to assert)

KFM is layered (clean boundaries). Tests should **pin the seams** ğŸ”©:

| ğŸ§± Subsystem | ğŸ¯ What must never break | ğŸ§ª Best test types | ğŸ§° Typical tools |
|---|---|---|---|
| ğŸ§° Tools/CLIs | governed command surface: `--help`, safe defaults, stable exit codes, structured logs | unit âœ… + smoke âœ… | pytest, subprocess, snapshot tests |
| ğŸ§ª ETL / pipelines | deterministic outputs, idempotent reruns, schema+CRS correctness | unit âœ… + integration ğŸ”Œ + data QA gates âœ… | pytest, GDAL, GeoPandas, validators |
| ğŸ—‚ï¸ Catalogs (STAC/DCAT/PROV) | boundary artifacts exist *before* graph/UI uses data; links resolve; provenance complete | contracts ğŸ§¾ + integration ğŸ”Œ | JSON Schema, jq, custom validators |
| ğŸ•¸ï¸ Graph (Neo4j) | graph ingests **from catalogs**, not adâ€‘hoc; constraints hold | integration ğŸ”Œ + property tests ğŸ§ª | Neo4j container, Cypher assertions |
| ğŸ›¡ï¸ API (REST/GraphQL) | contract stability, authz, deterministic pagination | contracts ğŸ§¾ + integration ğŸ”Œ | OpenAPI/GraphQL validation, TestClient |
| ğŸŒ UI (SPA) | responsive + accessible, stable map behaviors, timeline correctness | unit ğŸ§© + component ğŸ§± + e2e ğŸ§­ | Jest/Vitest, Playwright/Cypress |
| ğŸ—ºï¸ Maps / 3D | symbology & overlays donâ€™t silently shift; perf budgets | visual ğŸ–¼ï¸ + e2e ğŸ§­ | screenshot diffs, WebGL harness |
| ğŸ“š Story Nodes | citations resolve; narrative ordering consistent; no unsourced claims | docs âœ… + contracts ğŸ§¾ | markdown/link validators, schema checks |
| ğŸ¯ Focus Mode (AI) | provenance-linked outputs; safe refusals; uncertainty honesty; no sensitive leakage | eval âœ… + contract-like ğŸ§¾ | golden prompts, retrieval tests |
| ğŸ” Governance | licenses, access constraints, â€œno downgradeâ€ classification | gates âœ… + integration ğŸ”Œ | OPA/Conftest policies, CI checks |
| ğŸ“ˆ Performance | latency/throughput/cost regressions are visible & explainable | perf â±ï¸ + scheduled âœ… | pytest-benchmark, k6, Locust, DB EXPLAIN |

---

## ğŸ§  Core invariant: governed ordering

> [!IMPORTANT]
> KFM enforces a **nonâ€‘negotiable** pipeline order:
>
> **ETL â†’ STAC/DCAT/PROV catalogs â†’ Graph â†’ APIs â†’ UI â†’ Story Nodes â†’ Focus Mode**

```mermaid
flowchart LR
  A[ğŸ§ª ETL] --> B[ğŸ—‚ï¸ STAC/DCAT/PROV]
  B --> C[ğŸ•¸ï¸ Graph]
  C --> D[ğŸ›¡ï¸ API]
  D --> E[ğŸŒ UI]
  E --> F[ğŸ“š Story Nodes]
  F --> G[ğŸ¯ Focus Mode]
```

### âœ… What tests should enforce (practically)
- ğŸ§ª ETL determinism (stable IDs/hashes; idempotent reruns; explicit versions)
- ğŸ—‚ï¸ Catalog records exist **before** graph/UI uses them
- ğŸ•¸ï¸ Graph loads only from catalogs (no adâ€‘hoc inserts in prod paths)
- ğŸ›¡ï¸ API is the only client boundary (UI never queries graph/DB directly)
- ğŸ” Classification/sensitivity never downgrades silently (requires audited redaction)
- ğŸ§¾ Provenance is complete (inputs â†’ activities â†’ outputs with run IDs/configs)
- ğŸ·ï¸ License is explicit before publish (block publish if missing/ambiguous)

---

## ğŸ§± Architecture boundary tests (clean architecture)

KFMâ€™s modular structure only stays maintainable if boundaries are enforced ğŸ§±âœ¨

### âœ… What to test
- **Dependency direction rules** (e.g., domain â†’ service â†’ adapters; never reverse)
- **No cross-layer shortcuts** (UI never imports DB drivers; pipelines donâ€™t import UI)
- **API boundary is the redaction/policy chokeâ€‘point** (everything public flows through it)
- **â€œIndependently testable componentsâ€ stays true** (isolated unit tests remain possible)

### ğŸ”§ Suggested patterns
- ğŸ§­ *Import-lints* that fail if forbidden imports appear (Python)  
- ğŸ§± *Module boundary tests* (TS/JS) so UI doesnâ€™t reach server internals  
- ğŸ”Œ *Contract-only integration tests* so adapters can be swapped without rewriting logic  

> [!TIP]
> Boundary tests are cheap insurance. They prevent â€œjust this onceâ€ coupling that becomes permanent. ğŸ§¯

---

## ğŸ”º Test pyramid (how we keep velocity + confidence)

Most tests should be cheap and deterministic, then fewer (higherâ€‘value) integration + E2E:

```text
          ğŸ”º E2E (few)          â†’ critical user journeys (UI + API + DB)
        ğŸ”ºğŸ”º Integration (some)  â†’ services together (DB, API, pipelines)
      ğŸ”ºğŸ”ºğŸ”º Unit (many)          â†’ pure logic, transforms, validators
```

---

## ğŸ·ï¸ Test categories & markers (suggested)

### Python (`pytest`) markers
Standardize markers so devs can run focused slices quickly:

```ini
# pytest.ini (example)
[pytest]
markers =
  unit: fast pure logic
  integration: hits db/services/filesystem
  e2e: end-to-end journeys (rare for python)
  slow: long-running tests (non-gating)
  perf: benchmarks/capacity (usually scheduled)
  contracts: schemas + catalogs + API contract validation
  docs: markdown/front-matter/story-node validation
  validation: scientific/V&V tests (tolerance-based)
  geo: GIS correctness checks
  eo: earth-observation / remote-sensing checks
  webgl: WebGL context + render sanity checks
  graph: graph (Neo4j + algorithms) checks
  api: API behavior checks (beyond schema)
  security: defensive security checks (no offensive testing)
  governance: license/classification/redaction/policy pack checks
  focus: Focus Mode contract tests (provenance + safety + uncertainty)
```

### Web tags (examples)
- Jest/Vitest: `test`, `test:unit`, `test:component`
- Playwright/Cypress: `test:e2e`
- Visual regression: `test:visual`

---

## ğŸ§° Tool & CLI contract tests

KFMâ€™s **governed toolchain** (`tools/`) is part of the contract surface. Tools must behave predictably under automation.

### âœ… What to assert for every CLI tool
- `--help` exists + includes **â‰¥2 examples**
- `--version` returns a stable value (semver or git SHA)
- safe-by-default (no writes unless `--apply`, or `--dry-run` default)
- stable exit codes (usage vs validation failure vs runtime failure)
- structured logs available (human + JSONL mode)

### ğŸ”§ Suggested tests (patterns)
- `test_tools_help_smoke()` â€” exit 0 and non-empty help
- `test_tools_version_smoke()` â€” exit 0 and prints version
- `test_tools_dry_run_does_not_mutate()` â€” run against temp dirs; confirm no writes
- `test_tools_exit_codes_are_stable()` â€” validation failure â†’ code `3` (or your chosen standard)
- `test_tools_json_logs_valid_jsonl()` â€” parse emitted JSONL lines

> [!TIP]
> If you implement core logic inside `tools/`, thatâ€™s a smell.  
> Put logic in `src/` (or `api/src/`) and keep `tools/` as a predictable CLI + validator layer ğŸ› ï¸

---

## ğŸ“„ Docs, Story Nodes, & Focus Mode validation

KFM treats documentation + narrative as governed artifacts (not â€œfreeform notesâ€).

### âœ… Docs validation should cover
- YAML frontâ€‘matter present + valid
- internal link checks (`docs/**`, `data/**`, `schemas/**`)
- image/assets exist (no broken embeds)
- required sections exist for governed doc types *(templates)*

### âœ… Story Nodes validation should cover
- lives under `docs/reports/story_nodes/{draft|published}/...`
- uses the Story Node template (v3) fields
- citations resolve to cataloged sources (STAC/DCAT/PROV)
- narrative claims do **not** introduce uncited â€œfactsâ€
- published stories meet stricter gates than drafts

### âœ… Focus Mode contract tests should cover
- context bundles only contain provenanceâ€‘linked content
- AIâ€‘generated text is **clearly labeled** (and includes model/version where permissible)
- refusal behavior works when evidence is missing
- uncertainty is surfaced (intervals, confidence notes, or â€œunknownâ€)
- AI stays **advisory**: it cannot take autonomous actions or bypass policy gates

> [!CAUTION]
> If a Story Node (or Focus Mode) could expose sensitive locations or culturally sensitive information:  
> **CI should flag it for governance review** and block publish until review completes ğŸ”’

---

## ğŸ§¾ Contract & metadata tests

KFM is **contractâ€‘first** and **catalogâ€‘first**. Tests must protect:
- ğŸ›¡ï¸ OpenAPI / GraphQL contracts (breaking changes are explicit + versioned)
- ğŸ—‚ï¸ STAC (collections/items validity + required fields)
- ğŸ·ï¸ DCAT (distributions point to real assets/endpoints)
- ğŸ§¬ PROV (inputs â†’ activities â†’ outputs; run IDs/configs recorded)
- ğŸ§¬ Crossâ€‘layer linkage (Graph references catalogs; UI references API; Story references catalogs)
- ğŸ§¾ â€œMetadata like codeâ€: validation is a compile step, not a best-effort lint

### âœ… What to validate
- JSON parses + schema passes
- links resolve (STAC assets exist; DCAT distributions point somewhere real)
- provenance completeness (raw â†’ work â†’ processed trace exists)
- stable IDs/hashes present where required
- time metadata makes sense (windows applied; plausible ranges)
- **required governance fields** present (license, sensitivity/classification, access constraints)

### Example checks (starter)
```bash
# JSON sanity
find data/stac data/catalog/dcat data/prov -name "*.json*" -print0 | xargs -0 -n 1 jq empty

# pytest contract suite
pytest -q -m contracts
```

> [!NOTE]
> Dataset PRs should run a **Catalog QA gate** (schema + links + license) before merge.  
> If metadata is incomplete (e.g., missing license, broken href), CI must fail. ğŸš«âœ…

---

## ğŸ§· Stable IDs & versioning tests (donâ€™t break links)

Stable IDs are how KFM stays citable, reversible, and auditable ğŸ§·ğŸ§¾

### âœ… What to test
- IDs remain stable across refactors and non-semantic changes
- IDs do not depend on display names alone (renames must not create new identities)
- merges/dedup donâ€™t silently rewrite public identifiers (requires migration record)
- â€œpublishedâ€ artifacts remain fetchable by prior IDs (redirect or alias map)

### ğŸ”§ Suggested patterns
- **Golden ID fixtures**: small fixture catalogs with â€œexpected IDsâ€ that must not change
- **Migration tests**: schema bumps must include a migration + tests for backward compatibility (where required)
- **Round-trip tests**: catalog â†’ graph â†’ API â†’ UI uses the same stable identifiers everywhere

> [!TIP]
> If an ID changes, you owe the repo an ADR + migration note + rollback plan. ğŸ§¾ğŸ”

---

## ğŸ“œ License, citation, & redistribution tests

Licensing is not paperwork â€” itâ€™s a publish gate ğŸ“œâœ…

### âœ… What to test (gating)
- every dataset/distribution has a **license** field before publish
- license terms are not contradictory across catalog layers (STAC/DCAT vs local metadata)
- restricted/non-commercial datasets trigger UI warnings (and/or access controls)
- attribution/citation generation works (e.g., story export includes sources list)
- presence of `CITATION.cff` for the software release (recommended)

### ğŸ”§ Suggested patterns
- `test_license_required_before_publish()` â€” fail if missing/unknown license
- `test_noncommercial_blocks_public_download()` â€” ensure policy is enforced
- `test_story_export_includes_attributions()` â€” evidence pointers flow through

> [!IMPORTANT]
> If a license is unclear, treat it as **restricted** until governance resolves it. ğŸ§¯

---

## âœ… Data validation gates (fail fast)

These gates are your â€œnoâ€‘badâ€‘data firewallâ€ ğŸ§±ğŸ”¥ â€” especially for GeoParquet + COG pipelines and publish flows.

### Ring model (recommended)
**Ring 0: Structure**
- parses (JSON/GeoJSON/Parquet/TIFF)
- schema validation (STAC/DCAT/PROV + local schemas)
- required files exist

**Ring 1: Integrity**
- checksums/manifest inventory (if used)
- deterministic IDs present where required
- atomic publish (no half-state)

**Ring 2: Semantics**
- CRS correctness + axis order
- geometry validity (and any allowed repair policy)
- raster sanity (nodata, overviews, alignment)
- time/bounds sanity (domain-specific checks)

**Ring 3: Governance & safety**
- license required before publish
- classification propagation (no downgrade)
- sensitive fields redaction rules
- policy tests (OPA/Conftest) where used
- secrets scans + dependency hygiene checks

---

## ğŸ—ºï¸ Geospatial tests (GIS correctness)

Geospatial pipelines fail in predictable waysâ€”test them explicitly:

- ğŸŒ CRS sanity: EPSG correctness; meters vs degrees; axis order
- ğŸ§± topology: geometry validity; selfâ€‘intersections per policy
- ğŸ§© overlay correctness: clip/intersect/union behaviors
- ğŸ§­ buffer correctness: distance units + projection correctness
- ğŸ§Š raster alignment: resolution, nodata handling, resampling method
- ğŸ“¦ format IO: GeoJSON/GeoPackage/GeoParquet/COG round-trips
- ğŸ§­ coordinate range checks: latitude/longitude in valid ranges
- ğŸ§® area sanity (datasetâ€‘specific): within tolerance of mask boundary

> [!TIP]
> Put CRS + units in the **fixture metadata** and test names.  
> It prevents â€œsilent degrees vs metersâ€ disasters. ğŸ¥²

---

## ğŸ›°ï¸ Remote sensing tests (Earth Engine & imagery)

Remote sensing workflows fail quietly unless assumptions are tested:
- band availability & naming
- scale / resolution
- cloud masking logic (QA bits)
- temporal compositing rules
- index calculations (NDVI) & expected ranges
- export formats & metadata consistency

Truthiness checks that catch many bugs:
- output range sanity (e.g., NDVI âˆˆ [-1, 1])
- masked pixel counts within expected bounds
- timestamps monotonic; windows applied correctly

> [!CAUTION]
> Unit tests should not call live services.  
> Prefer recorded fixtures, mock adapters, or small cached exports. âœ…

---

## ğŸ§Š 3D / WebGL / 3D GIS tests

KFM supports meaning-making beyond flat maps: 3D scenes, meshes, archaeology-grade reconstructions ğŸ§ŠğŸ—ºï¸

### âœ… What to test
- **WebGL context sanity** (creates reliably; fails gracefully; debug mode not shipped to prod)
- **Coordinate conventions** (ECEF vs local ENU vs EPSG; axis order; units)
- **LOD/tiling rules** (no â€œinfinite detailâ€ payloads; progressive loading works)
- **Georeferenced mesh validation** (mesh â†” site CRS â†” metadata alignment)
- **Visual regressions** (symbology + overlays + 3D render snapshots)

### ğŸ”§ Suggested patterns
- screenshot diffs at canonical zoom/tilt angles
- tiny deterministic scenes in fixtures (`tests/fixtures/3d/`)
- tolerance-based image diffs (antialiasing-aware) rather than pixel-perfect

> [!TIP]
> 3D can leak sensitive locations faster than 2D. Treat 3D fixtures as high-risk and keep them coarse + safe. ğŸ«¥ğŸ”’

---

## ğŸ§  Scientific / simulation validation

Treat simulation/analysis code like a **scientific instrument** ğŸ”¬:
- **verification**: implementation matches intended math
- **validation**: model matches reality within uncertainty
- **regression baselines**: detect drift across refactors

### âœ… Recommended patterns
- analytical solution comparisons (tiny cases with known answers)
- convergence tests (refinement reduces error)
- invariant checks (symmetry, conservation, monotonicity)
- tolerance-based golden files (with metadata + tolerances)
- uncertainty reporting checks (intervals, credible bands, PPC)

> [!TIP]
> If results are stochastic, test **properties** (ranges, quantiles, calibration), not exact values. ğŸ²âœ…

---

## ğŸ“Š ML / stats tests (donâ€™t fool yourself)

Data science code needs tests beyond â€œit runsâ€:
- split is leakâ€‘free (train/val/test)
- metrics stable within tolerance
- baseline comparisons exist (simple model beats random)
- diagnostics exist (residuals, leverage/outliers, calibration)
- uncertainty reporting present when relevant
- multiple comparisons / pâ€‘hacking risks handled (where applicable)

ğŸ“ On failure, attach plots as CI artifacts:
- confusion matrix
- residual plots
- calibration curve
- drift dashboards (if applicable)

> [!NOTE]
> ML tests should also protect **meaning**: avoid reporting a metric without context, uncertainty, and known failure modes. ğŸ§ ğŸ§¾

---

## ğŸ•¸ï¸ Graph tests (Neo4j + algorithms)

KFM treats the graph as **derived truth** (built from catalogs + provenance), not a writeâ€‘anywhere scratchpad.

Test categories:
- ğŸ§¾ graph build contract: rebuild from catalogs is reproducible
- ğŸ”’ constraints: uniqueness, required properties, relationship rules
- ğŸ§­ query invariants: deterministic pagination; stable ordering; filters correct
- ğŸ§  algorithm sanity: tiny deterministic graphs for spectral/routing invariants
- ğŸ§· canonical nodes: glossary terms/entities resolve consistently (no duplicate â€œalmost-sameâ€ nodes)

Example assertions:
- â€œGraph contains only entities referenced by STAC/DCAT/PROVâ€
- â€œEvery published dataset node links to a PROV Activity with run_id + config hashâ€
- â€œNo unbounded traversals in query layer (guardrails enforced)â€
- â€œSpectral metrics (where used) stay within known bounds on toy graphsâ€ ğŸ§ âš¡

---

## ğŸ›¡ï¸ API tests (FastAPI + GraphQL)

What to test:
- ğŸ§¾ OpenAPI schema validation (breaking changes explicit)
- ğŸ§¾ GraphQL schema validation + query shapes
- ğŸ” AuthN/AuthZ: role-based access, classification enforcement
- ğŸ§­ Pagination determinism: stable ordering, cursor correctness
- ğŸŒ Geo correctness: GeoJSON validity; bbox correctness; CRS behavior
- ğŸŒ CORS headers correct (UI shouldnâ€™t need workarounds)
- ğŸ§¾ â€œFail closedâ€ for missing provenance/license: endpoints should not serve ungoverned data

---

## ğŸŒ Web / frontend test guidance

### ğŸ§± Component tests (fast)
- render correctness given props/state
- event handling correctness
- accessibility checks (labels, keyboard nav, contrast)

### ğŸ“± Responsive tests (mobileâ€‘first)
- run tests across multiple viewport sizes
- verify map controls remain usable on small screens
- check touch targets + scroll locking behaviors

### ğŸ§­ E2E tests (few but powerful)
Focus on â€œmoney pathsâ€ ğŸ’¸:
- auth/login
- load a layer **from catalog**
- timeline navigation (time slider updates map + panels)
- select feature â†’ details panel updates
- export/report flow (metadata/provenance attached)

### ğŸ–¼ï¸ Visual regression (maps + WebGL)
Maps can regress visually while â€œlogic testsâ€ pass. Use screenshot diffs for:
- symbology stability
- overlay legibility at common zooms
- dark/light contrast
- WebGL rendering regressions (tolerance-based diffs)

> [!CAUTION]
> Mobile experiences can unintentionally enable tracking.  
> If you render live location or device IDs, add tests for optâ€‘in + minimization + clear user controls. ğŸ“ğŸ«¥

---

## ğŸ“ˆ Performance & capacity tests (latency, throughput, cost)

Performance tests are how we keep KFM usable as it scales ğŸ“ˆâš™ï¸

### âœ… What to measure (min set)
- **Latency distributions** (p50/p95/p99), not just averages
- **Throughput** under realistic concurrency
- **Error rates** under load (including timeouts)
- **Resource cost** (CPU/RAM/IO) per request / per pipeline run
- **DB query stability** (query plans donâ€™t regress silently)

### ğŸ§ª What to test (examples)
- API list endpoints: stable pagination under load
- graph queries: bounded traversals with sane timeouts
- tile/layer loading: payload budgets respected (no â€œmegatileâ€ surprises)
- ETL steps: runtime bounds on representative fixtures
- DB: migration impacts + index effectiveness (EXPLAIN plan snapshots)

### ğŸ•› Where these run
- PRs: **tiny perf smoke** (fast + deterministic)
- Nightly: full benchmarks + trend checks (alert on regressions)
- Release: publish-grade load profile (documented + repeatable)

> [!TIP]
> Treat performance like correctness: if it regresses, you need a reason, a measurement, and a rollback path. ğŸ§¾ğŸ”

---

## ğŸ” Security, governance, & ethics tests (defensive)

KFMâ€™s security stance is defensive: prevent leaks, enforce policy, keep audit trails.

### âœ… What to test (defensive)
- ğŸ” classification boundaries & redaction rules enforced (endâ€‘toâ€‘end)
- ğŸ•µï¸ secrets scanning (prevent committed tokens/keys)
- ğŸ“¦ dependency scanning (vulnerable libs flagged)
- ğŸ³ container scanning (base image CVEs flagged)
- ğŸ§¾ FAIR/CARE gates (required metadata present; access constraints honored)
- ğŸ§¬ auditability (publish actions produce receipts: who/what/when)
- ğŸ–¼ï¸ media hygiene: image metadata stripping; decoder safety checks for hostile inputs

### ğŸ§‘â€âš–ï¸ Governance review triggers (manual review beyond CI)
Some changes should automatically require human review:
- adding sensitive/culturally protected datasets or layers
- new AI-driven narrative features
- new external data sources (license + provenance scrutiny)
- new public-facing endpoints or downloads that could expose sensitive info
- classification/sensitivity changes (especially â€œless restrictedâ€)

> [!IMPORTANT]
> Do **not** add offensive security instructions here.  
> Security tests are for hardening, verification, and prevention. ğŸ›¡ï¸âœ…

---

## ğŸ§¾ Test artifacts & receipts

KFM work is evidenceâ€‘driven. When tests fail, make failures inspectable:

### âœ… CI artifacts to upload on failure
- structured logs (`.jsonl` or `.txt`)
- diff outputs (schema diffs, snapshot diffs)
- screenshots (UI E2E + map diffs)
- â€œminiâ€ STAC/DCAT/PROV bundles from fixtures
- performance traces (if relevant)

### ğŸ§¾ â€œReceipt mindsetâ€
For integration tests that simulate real workflows (pipeline â†’ catalogs â†’ graph â†’ API):
- record the run id / config used
- capture hashes of produced artifacts
- include a minimal PROV-like trace for the test run (even if toy)

> [!TIP]
> If a test failure canâ€™t be diagnosed from artifacts alone, itâ€™s a documentation bug. ğŸ““ğŸ§¯

---

## ğŸ—‚ï¸ Suggested folder layout

Adapt as needed, but keep intent obvious:

```text
ğŸ“¦ repo-root/
â”œâ”€ ğŸ§ª tests/
â”‚  â”œâ”€ ğŸ“„ README.md                      # you are here ğŸ‘‹
â”‚  â”œâ”€ ğŸ§· fixtures/                      # tiny, deterministic test data only
â”‚  â”‚  â”œâ”€ ğŸ—ºï¸ geo/                        # small vectors/rasters (safe + tiny)
â”‚  â”‚  â”œâ”€ ğŸ›°ï¸ eo/                         # tiny EO chips / QA bit samples
â”‚  â”‚  â”œâ”€ ğŸ§Š 3d/                         # tiny meshes/scenes (coarse + safe)
â”‚  â”‚  â”œâ”€ ğŸ–¼ï¸ media/                      # tiny images + metadata
â”‚  â”‚  â”œâ”€ ğŸ§¬ ml/                         # toy datasets / tiny model artifacts
â”‚  â”‚  â”œâ”€ ğŸ§¾ catalogs/                   # STAC/DCAT/PROV fixtures
â”‚  â”‚  â””â”€ ğŸ“˜ FIXTURES.md                 # fixture rules + provenance notes
â”‚  â”œâ”€ ğŸ“„ docs/                          # markdown/front-matter/story-node checks
â”‚  â”œâ”€ ğŸ§± architecture/                  # boundary tests (imports/dep rules)
â”‚  â”œâ”€ ğŸ§° tools_contract/                # CLI contract tests for tools/
â”‚  â”œâ”€ ğŸ python/
â”‚  â”‚  â”œâ”€ ğŸ§© unit/
â”‚  â”‚  â”œâ”€ ğŸ”Œ integration/
â”‚  â”‚  â”œâ”€ âœ… validation/                 # scientific V&V (tolerance-based)
â”‚  â”‚  â”œâ”€ â±ï¸ perf/                       # benchmarks (scheduled)
â”‚  â”‚  â”œâ”€ ğŸ” security/                   # defensive security checks
â”‚  â”‚  â””â”€ ğŸ§± conftest.py
â”‚  â”œâ”€ ğŸŒ web/
â”‚  â”‚  â”œâ”€ ğŸ§© unit/
â”‚  â”‚  â”œâ”€ ğŸ§± component/
â”‚  â”‚  â”œâ”€ ğŸ§­ e2e/
â”‚  â”‚  â””â”€ ğŸ–¼ï¸ visual/
â”‚  â”œâ”€ ğŸ—„ï¸ db/
â”‚  â”‚  â”œâ”€ ğŸ§¬ migrations/
â”‚  â”‚  â””â”€ ğŸ”Œ integration/
â”‚  â”œâ”€ ğŸ•¸ï¸ graph/
â”‚  â”‚  â”œâ”€ ğŸ”Œ integration/
â”‚  â”‚  â””â”€ ğŸ§© unit/
â”‚  â”œâ”€ ğŸ“œ contracts/
â”‚  â”‚  â”œâ”€ ğŸ›¡ï¸ api/                        # OpenAPI/GraphQL fixtures
â”‚  â”‚  â”œâ”€ ğŸ—ºï¸ stac/
â”‚  â”‚  â”œâ”€ ğŸ·ï¸ dcat/
â”‚  â”‚  â””â”€ ğŸ§¬ prov/
â”‚  â””â”€ ğŸ“˜ TEST_POLICY.md                 # merge gates + definition of done
â””â”€ ğŸ§° scripts/                          # CI glue & wrappers (optional; prefer tools/)
```

> [!NOTE]
> If you keep tests alongside code (e.g., `src/**/tests`), thatâ€™s fineâ€”just keep naming + markers consistent.

---

## âœ… CI gates (non-negotiable)

**Policy:** the pipeline must be green before merge ğŸ¤–âœ…  
CI should mirror KFMâ€™s â€œbuild â†’ validate â†’ publishâ€ discipline and keep logs/artifacts for traceability.

### âœ… Minimum PR gates (recommended)
1) ğŸ§¹ format + lint (Python + JS/TS)
2) ğŸ§± build (frontend + backend; container build if applicable)
3) ğŸ§ª unit tests
4) ğŸ§¾ docs protocol checks (frontâ€‘matter + link validation)
5) ğŸ§¾ schema validation (STAC/DCAT/PROV + story/node schemas)
6) âœ… data validation gates (CRS + geometry + raster sanity + license required)
7) ğŸ”Œ integration tests (ephemeral DB/services via Compose)
8) ğŸ•¸ï¸ graph integrity tests (constraints + rebuild invariants)
9) ğŸ›¡ï¸ API contract tests (OpenAPI/GraphQL + deterministic pagination)
10) ğŸ” security & governance scans (secrets + PII + sensitive location + classification â€œno downgradeâ€)
11) ğŸ§‘â€âš–ï¸ CodeQL/static analysis lane (recommended)
12) ğŸ“ˆ coverage thresholds (target, not a religion)

### ğŸ•› Nightly / scheduled checks (recommended)
Keep PR CI fast; push expensive checks to nightly:
- â±ï¸ benchmarks (trend monitoring)
- ğŸ§  longer ML runs (PRs use toy models)
- ğŸ—ºï¸ large raster workloads (PRs use fixtures & sampling)
- ğŸ•¸ï¸ deeper graph consistency (full rebuild + diff)
- ğŸ” deeper security scanning (if it slows PR CI)

> [!TIP]
> If a gate is â€œimportantâ€, it must be **automatable** and **repeatable**.  
> If itâ€™s not automatable, it must be a **documented manual review step** ğŸ§¾

---

## âœ… PR checklist (copy/paste)

- [ ] Unit tests added/updated
- [ ] Integration tests added (if behavior crosses boundaries)
- [ ] Boundary tests updated (if you touched architecture seams)
- [ ] Determinism confirmed (seeds + stable outputs) if ML/sim
- [ ] Tools/CLI contract checks updated (if adding/modifying tools/)
- [ ] Docs/story checks updated (frontâ€‘matter, links, story templates) if docs changed
- [ ] Contracts updated + verified (OpenAPI/GraphQL) if API changed
- [ ] Catalog/metadata tests updated (STAC/DCAT/PROV) if outputs changed
- [ ] Stable IDs preserved (or migration + ADR added) if identifiers changed
- [ ] Data validation gates updated (schema/CRS/geometry) if ETL changed
- [ ] License + governance checks pass (block publish if missing license)
- [ ] Sensitive location / â€œno downgradeâ€ checks pass (or governance review requested)
- [ ] UI changes include component tests + (if visual) snapshot updates
- [ ] CI is green (required)

---

## ğŸ§¯ Troubleshooting

### âŒ Tests fail only in CI?
- check lockfiles & pinned versions
- confirm containers match local versions
- eliminate reliance on local paths, locale, timezone, GPU availability

### ğŸ² Flaky tests?
- remove sleeps; wait on conditions
- fix randomness (seed)
- mock/record external services

### ğŸ³ Docker stack wonâ€™t start?
```bash
docker compose logs -f
docker compose config
docker compose up -d --build
```

---

## ğŸ“š Reference pointers (library index)

These repo library files inform KFMâ€™s test posture (V&V, stats rigor, GIS correctness, visualization stability, data governance, scaling, security hardening). ğŸ§ ğŸ§¾

<details>
<summary>ğŸ›ï¸ System design, contracts, governance, and â€œhow KFM is supposed to workâ€</summary>

- `docs/README.md` *(governed documentation posture)*
- `docs/MASTER_GUIDE_v13.md` *(canonical system + contract-first map, if present)*
- `Kansas Frontier Matrix (KFM) â€“ Comprehensive Technical Documentation.pdf` *(architecture + standards + QA direction)*
- `Audit of the Kansas Frontier Matrix (KFM) Repository.pdf` *(gap lens, if present)*
- `ğŸŒŸ Kansas Frontier Matrix â€“ Latest Ideas & Future Proposals.docx` *(roadmap + lanes, if present)*

</details>

<details>
<summary>ğŸ”¬ Scientific modeling, simulation V&amp;V, and uncertainty</summary>

- `Scientific Modeling and Simulation_ A Comprehensive NASA-Grade Guide.pdf`
- `Understanding Statistics & Experimental Design.pdf`
- `think-bayes-bayesian-statistics-in-python.pdf`
- `Generalized Topology Optimization for Structural Design.pdf` *(optimization run reproducibility + sensitivity mindset)*

</details>

<details>
<summary>ğŸ“Š Statistics, regression, Bayesian, and analysis discipline</summary>

- `regression-analysis-with-python.pdf`
- `Regression analysis using Python - slides-linear-regression.pdf`
- `graphical-data-analysis-with-r.pdf`
- `Understanding Machine Learning: From Theory to Algorithms.pdf` *(generalization + sample complexity mindset, if present)*

</details>

<details>
<summary>ğŸ—ºï¸ Geospatial, remote sensing, cartography, and mapping UX</summary>

- `python-geospatial-analysis-cookbook.pdf`
- `Cloud-Based Remote Sensing with Google Earth Engine-Fundamentals and Applications.pdf`
- `making-maps-a-visual-guide-to-map-design-for-gis.pdf`
- `Mobile Mapping_ Space, Cartography and the Digital - 9789048535217.pdf`
- `compressed-image-file-formats-jpeg-png-gif-xbm-bmp.pdf`
- `Archaeological 3D GIS_26_01_12_17_53_09.pdf` *(3D acquisition + validation routines)*

</details>

<details>
<summary>ğŸ—„ï¸ Data management, interoperability, and scale</summary>

- `PostgreSQL Notes for Professionals - PostgreSQLNotesForProfessionals.pdf`
- `Database Performance at Scale.pdf`
- `Scalable Data Management for Future Hardware.pdf`
- `Data Spaces.pdf`
- `Spectral Geometry of Graphs.pdf`

</details>

<details>
<summary>ğŸŒ Web UI, WebGL visualization, and responsive design</summary>

- `responsive-web-design-with-html5-and-css3.pdf`
- `webgl-programming-guide-interactive-3d-graphics-programming-with-webgl.pdf`

</details>

<details>
<summary>ğŸ›¡ï¸ Security, concurrency, law, humanism, and governance</summary>

- `ethical-hacking-and-countermeasures-secure-network-infrastructures.pdf` *(defensive posture only)*
- `Gray Hat Python - Python Programming for Hackers and Reverse Engineers (2009).pdf` *(defensive awareness only)*
- `concurrent-real-time-and-distributed-programming-in-java-threads-rtsj-and-rmi.pdf`
- `On the path to AI Lawâ€™s prophecies and the conceptual foundations of the machine learning age.pdf`
- `Introduction to Digital Humanism.pdf`
- `Principles of Biological Autonomy - book_9780262381833.pdf`

</details>

<details>
<summary>ğŸ“š Language &amp; tooling reference shelves (programming books index)</summary>

- `A programming Books.pdf`
- `B-C programming Books.pdf`
- `D-E programming Books.pdf`
- `F-H programming Books.pdf`
- `I-L programming Books.pdf`
- `M-N programming Books.pdf`
- `O-R programming Books.pdf`
- `S-T programming Books.pdf`
- `U-X programming Books.pdf`

</details>

---

## ğŸ•°ï¸ Version history

| Version | Date | Summary | Author |
|---:|---|---|---|
| v1.3.0 | 2026-01-13 | Added architecture boundary tests, stable ID/versioning lane, explicit license/citation gates, 3D/WebGL/3Dâ€‘GIS testing guidance, and performance/capacity test lane. Updated quick links and CI gates to include CodeQL/static analysis lane. | KFM Engineering |
| v1.2.0 | 2026-01-11 | Aligned tests with Master Guide v13: contract-first + catalog-first gates, docs/story-node validation lane, governance trigger guidance, and tool/CLI contract testing. Removed internal placeholder evidence anchors. | KFM Engineering |
| v1.1.0 | 2026-01-09 | Tightened â€œcatalog-firstâ€ & data QA gates; added receipts/artifacts section; clarified defensive security stance; aligned CI gates with KFM engineering/testing guidance. | KFM Engineering |
| v1.0.0 | 2026-01-07 | Initial repo-wide testing README: pyramid, markers, subsystem matrix, validation + governance posture. | KFM Engineering |