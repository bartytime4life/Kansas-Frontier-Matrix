# ğŸ§ª Tests â€” Kansas Frontier Matrix (KFM) / Kansasâ€‘Matrixâ€‘System

![CI](https://img.shields.io/badge/CI-GitHub%20Actions-2ea44f?logo=githubactions&logoColor=white)
![Coverage](https://img.shields.io/badge/Coverage-target%20%F0%9F%9A%80-informational)
![Python](https://img.shields.io/badge/Python-pytest-blue?logo=python&logoColor=white)
![Node](https://img.shields.io/badge/Node.js-tests-brightgreen?logo=node.js&logoColor=white)
![Docker](https://img.shields.io/badge/Docker-Compose-2496ed?logo=docker&logoColor=white)
![Deterministic](https://img.shields.io/badge/Deterministic-preferred-success)

> KFM is a **trust-first** geospatial + knowledge + modeling system.  
> This folder is where we continuously prove that our **code**, **pipelines**, **contracts**, and **UI behaviors** are **correct**, **reproducible**, and **honest about uncertainty**. âœ…ğŸ§¾

---

## ğŸ§­ Quick navigation

- ğŸš€ Run tests now â†’ [Quickstart](#-quickstart)
- ğŸ”’ Merge requirements â†’ [CI gates](#-ci-gates-non-negotiable)
- ğŸ§ª What to test â†’ [Test pyramid](#-test-pyramid-how-we-keep-velocity--confidence)
- ğŸ§¾ Contracts (OpenAPI + STAC/DCAT/PROV) â†’ [Contract tests](#-contract--metadata-tests)
- ğŸ—ºï¸ GIS correctness â†’ [Geospatial tests](#ï¸-geospatial-tests-gis-correctness)
- ğŸ›°ï¸ Remote sensing â†’ [Remote sensing tests](#ï¸-remote-sensing-tests-earth-engine--imagery)
- ğŸ§  Modeling/simulation validity â†’ [Scientific validation](#-scientific--simulation-validation)
- ğŸŒ Frontend â†’ [Web / frontend guidance](#-web--frontend-test-guidance)
- ğŸ§± Suggested layout â†’ [Folder layout](#ï¸-suggested-folder-layout)

---

## ğŸš€ Quickstart

### 0) Preconditions (one-time)
- ğŸ Python env ready (`venv`, `conda`, `uv`, etc.)
- ğŸŒ Node env ready (`npm`, `pnpm`, or `yarn`)
- ğŸ³ Docker installed (recommended for integration parity)

> [!TIP]
> If your PR touches **DB/API/pipelines/contracts**: run at least one Docker-backed integration pass before requesting review.

### 1) Fast checks (preâ€‘commit vibes âš¡)
```bash
# Python (fast)
pytest -q -m "not integration and not e2e and not slow"

# Web (fast â€” adjust to your repo)
npm test
```

### 2) Full suite (recommended on feature branches âœ…)
```bash
# If you have Make targets (recommended)
make test

# Or explicit slices
pytest -q
pytest -q -m integration
npm run test:e2e
```

### 3) Integration tests with containers (preferred ğŸ³)
```bash
docker compose up -d --build
pytest -q -m integration
docker compose down -v
```

---

## ğŸ§  Core invariant tests must protect

> [!IMPORTANT]
> KFM enforces a **non-negotiable** pipeline order:
>
> **ETL â†’ Catalogs (STAC/DCAT/PROV) â†’ Graph â†’ APIs â†’ UI â†’ Story Nodes â†’ Focus Mode**

```mermaid
flowchart LR
  A[ğŸ§ª ETL] --> B[ğŸ—‚ï¸ Catalogs<br/>STAC/DCAT/PROV]
  B --> C[ğŸ•¸ï¸ Graph]
  C --> D[ğŸ›¡ï¸ API]
  D --> E[ğŸŒ UI]
  E --> F[ğŸ“š Story Nodes]
  F --> G[ğŸ¯ Focus Mode]
```

### âœ… What tests should enforce (practically)
- ğŸ§ª **ETL is deterministic** (stable IDs/hashes; idempotent re-runs)
- ğŸ—‚ï¸ **Catalog records exist** for publishable outputs (STAC/DCAT/PROV) **before** graph/UI uses them
- ğŸ•¸ï¸ **Graph loads only from catalogs** (no adâ€‘hoc inserts)
- ğŸ›¡ï¸ **API is the only client boundary** (UI never queries graph/DB directly)
- ğŸ” **Sensitivity/classification does not downgrade** without an explicit, audited redaction step

---

## ğŸ§± Test pyramid (how we keep velocity + confidence)

Most tests should be cheap and deterministic. Then we add fewer (but higher-value) integration + E2E checks.

```text
          ğŸ”º E2E (few)         â†’ critical user journeys (UI + API + DB)
        ğŸ”ºğŸ”º Integration (some) â†’ services together (DB, API, pipelines)
      ğŸ”ºğŸ”ºğŸ”º Unit (many)         â†’ pure logic, transforms, validators
```

---

## ğŸ§· Core test principles (KFM style)

### âœ… Test public behavior, not private plumbing
Prefer testing **public entry points** (functions, use-cases, endpoints, contracts).  
This reduces refactor pain and encourages clean boundaries.

### ğŸ” Determinism is a feature
For research/AI/simulation code: set seeds, pin dependencies, eliminate hidden state.

**Determinism checklist:**
- [ ] seeds set (Python, NumPy, ML frameworks)
- [ ] stable sorting (donâ€™t rely on hash iteration order)
- [ ] time mocked/frozen where needed
- [ ] no network calls in unit tests (record/replay if unavoidable)
- [ ] fixtures are tiny & versioned
- [ ] floats use tolerances (`pytest.approx`, `np.testing.assert_allclose`)

### ğŸ§¾ Trust-first means we test uncertainty too
If outputs are probabilistic / estimated:
- test **ranges**, **invariants**, or **calibration** (not single-point exact values)
- attach uncertainty artifacts on failure (plots, traces, summaries)
- document tolerances + rationale (in code comments or `TEST_POLICY.md`)

---

## ğŸ—‚ï¸ Suggested folder layout

Adapt as needed, but keep intent obvious:

```text
ğŸ“¦ repo-root/
â”œâ”€ ğŸ§ª tests/
â”‚  â”œâ”€ ğŸ“„ README.md                      # you are here ğŸ‘‹
â”‚  â”œâ”€ ğŸ§· fixtures/                      # tiny, deterministic test data only
â”‚  â”‚  â”œâ”€ ğŸ—ºï¸ geo/                        # small vectors/rasters (safe + tiny)
â”‚  â”‚  â”œâ”€ ğŸ§¬ ml/                         # toy datasets / tiny model artifacts
â”‚  â”‚  â”œâ”€ ğŸ§¾ schemas/                    # JSON/YAML schemas used in tests
â”‚  â”‚  â””â”€ ğŸ“˜ FIXTURES.md                 # fixture rules + provenance notes
â”‚  â”œâ”€ ğŸ python/
â”‚  â”‚  â”œâ”€ ğŸ§© unit/
â”‚  â”‚  â”œâ”€ ğŸ”Œ integration/
â”‚  â”‚  â”œâ”€ âœ… validation/                 # scientific V&V (tolerance-based)
â”‚  â”‚  â”œâ”€ â±ï¸ perf/                       # benchmarks (nightly / non-gating)
â”‚  â”‚  â”œâ”€ ğŸ§· helpers/
â”‚  â”‚  â””â”€ ğŸ§± conftest.py
â”‚  â”œâ”€ ğŸŒ web/
â”‚  â”‚  â”œâ”€ ğŸ§© unit/
â”‚  â”‚  â”œâ”€ ğŸ§± component/
â”‚  â”‚  â”œâ”€ ğŸ§­ e2e/
â”‚  â”‚  â”œâ”€ ğŸ–¼ï¸ visual/
â”‚  â”‚  â””â”€ ğŸ§· helpers/
â”‚  â”œâ”€ ğŸ—„ï¸ db/
â”‚  â”‚  â”œâ”€ ğŸ§¬ migrations/
â”‚  â”‚  â”œâ”€ ğŸ”Œ integration/
â”‚  â”‚  â””â”€ ğŸ§ª seed/
â”‚  â”œâ”€ ğŸ“œ contracts/
â”‚  â”‚  â”œâ”€ ğŸ›¡ï¸ api/                        # OpenAPI/GraphQL fixtures
â”‚  â”‚  â”œâ”€ ğŸ—ºï¸ stac/                       # STAC contract fixtures
â”‚  â”‚  â”œâ”€ ğŸ·ï¸ dcat/                       # DCAT contract fixtures
â”‚  â”‚  â””â”€ ğŸ§¬ prov/                       # PROV contract fixtures
â”‚  â”œâ”€ ğŸ§° tools/                         # optional helpers (run scripts)
â”‚  â””â”€ ğŸ“˜ TEST_POLICY.md                 # merge gates + definition of done
â””â”€ ğŸ§° scripts/                          # optional: CI glue & utilities
```

> [!NOTE]
> If you keep tests alongside code (e.g., `src/**/tests`), thatâ€™s fineâ€”just keep **naming + markers** consistent.

---

## ğŸ·ï¸ Test categories & markers (suggested)

### Python (`pytest`) markers
If you use `pytest`, standardize markers so developers can run focused slices:

```ini
# pytest.ini (example)
[pytest]
markers =
  unit: fast pure logic
  integration: hits db/services/filesystem
  e2e: end-to-end journeys (rare for python)
  slow: long-running tests (non-gating)
  validation: scientific/V&V tests (tolerance-based)
  perf: benchmarks (nightly)
  contracts: OpenAPI + metadata contract validation
```

### Web tags
Use your stackâ€™s convention (examples):
- Jest: `test`, `test:unit`, `test:component`
- Playwright/Cypress: `test:e2e`
- Visual regression: `test:visual`

---

## ğŸ›¡ï¸ CI gates (non-negotiable)

**Policy:** the pipeline must be green before merge. ğŸ¤–âœ…

Typical PR gates:
1) ğŸ§¹ format + lint (Python + JS/TS)
2) ğŸ§ª unit tests
3) ğŸ”Œ integration tests (ephemeral DB/services)
4) ğŸŒ web build (compile check)
5) ğŸ§¾ contract/schema validation (OpenAPI + STAC/DCAT/PROV where applicable)
6) ğŸ“ˆ coverage thresholds (target, not a religion)

### ğŸ•› Nightly / scheduled checks (recommended)
Keep PR CI fast. Push expensive checks to nightly:
- â±ï¸ benchmarks (trend monitoring)
- ğŸ§  long ML training runs (PRs use toy models)
- ğŸ—ºï¸ large raster workloads (PRs use fixtures & sampling)
- ğŸ” deeper security scans (if they slow PRs)

---

## ğŸ§¾ Contract & metadata tests

KFM is **contract-first** and **catalog-first**. Tests should protect:
- ğŸ›¡ï¸ **OpenAPI / GraphQL** contracts (breaking changes are explicit + versioned)
- ğŸ—‚ï¸ **STAC** (collections/items link validity + required fields)
- ğŸ·ï¸ **DCAT** (distributions link to STAC/asset access points)
- ğŸ§¬ **PROV** (inputs â†’ activities â†’ outputs; run IDs/configs recorded)

### âœ… What to validate
- JSON parses + schema passes
- links resolve (STAC assets exist; DCAT distributions point somewhere real)
- provenance completeness (raw â†’ work â†’ processed trace exists)
- stable IDs/hashes present where required

### Example checks (starter)
```bash
# JSON sanity
find data/stac data/catalog/dcat data/prov -name "*.json*" -print0 | xargs -0 -n 1 jq empty

# pytest contract suite (example)
pytest -q -m contracts
```

---

## ğŸ Python test guidance

### ğŸ§© Unit tests
Best for:
- parsers/validators
- coordinate transforms & unit conversions
- domain rules & invariants
- pure math transforms

âœ… Tips:
- prefer tolerance-based asserts for floats
- encode invariants (monotonicity, conservation) instead of brittle constants
- include â€œsad pathsâ€ for invalid inputs

### ğŸ”Œ Integration tests
Best for:
- PostGIS interactions
- API routes (FastAPI/Flask) against a test DB
- filesystem/object store adapters
- queue/worker boundaries (smoke-level)

âœ… Tips:
- use Compose to create repeatable dependencies
- isolate state via transactions or per-test schemas
- avoid public internet in tests (mock or record/replay)

---

## ğŸ§  Scientific / simulation validation

Treat simulation/analysis code like **scientific instruments** ğŸ”¬:
- **verification**: implementation matches intended math
- **validation**: model matches reality within uncertainty
- **regression baselines**: detect drift across refactors

### âœ… Recommended patterns
- analytical solution comparisons (tiny cases with known answers)
- convergence tests (timestep/resolution refinement reduces error)
- invariant checks (symmetry, conservation, monotonicity)
- tolerance-based golden files (with metadata + tolerances)
- uncertainty reporting checks (intervals, credible bands, PPC)

> [!TIP]
> If results are stochastic, test **properties** (ranges, quantiles, calibration) rather than exact values.

---

## ğŸŒ Web / frontend test guidance

### ğŸ§± Component tests (fast)
- render correctness given props/state
- event handling correctness
- accessibility checks (labels, keyboard nav, contrast)

### ğŸ§­ E2E tests (few but powerful)
Focus on â€œmoney pathsâ€ ğŸ’¸:
- auth/login
- load a layer
- timeline navigation
- select a feature â†’ details panel updates
- export/report flow

### ğŸ–¼ï¸ Visual regression (maps + WebGL)
Maps can regress visually while â€œstill passing logic tests.â€ Use screenshot diffs where it matters:
- symbology doesnâ€™t silently change
- overlays remain legible at common zoom levels
- dark/light modes keep contrast
- WebGL rendering regressions are caught (tolerance-based diffs)

---

## ğŸ—ºï¸ Geospatial tests (GIS correctness)

Geospatial pipelines are fragile in predictable waysâ€”test these explicitly:

- ğŸŒ **CRS sanity**: EPSG correctness; meters vs degrees issues
- ğŸ§± **Topology**: geometry validity, no self-intersections when required
- ğŸ§© **Overlay correctness**: clip/intersect/union behaviors
- ğŸ§Š **Raster alignment**: resolution, nodata handling, resampling method
- ğŸ“¦ **Format IO**: GeoJSON/GeoPackage/COG round-trips

> [!NOTE]
> Always include CRS + units in test names or fixture metadata. It prevents â€œsilent degrees vs metersâ€ disasters. ğŸ¥²

---

## ğŸ›°ï¸ Remote sensing tests (Earth Engine & imagery)

Remote sensing workflows fail quietly unless you test assumptions:
- band availability & naming
- scale / resolution
- cloud masking logic (QA bits)
- temporal compositing rules
- index calculations (e.g., NDVI) & expected ranges
- export formats & metadata consistency

â€œTruthiness checksâ€ that catch many bugs:
- output range sanity (e.g., NDVI âˆˆ [-1, 1])
- masked pixel counts within expected bounds
- timestamps monotonic; windows applied correctly

---

## ğŸ“Š ML / stats tests (donâ€™t fool yourself)

Data science code needs tests beyond â€œit runsâ€:
- split is leak-free (train/val/test)
- metrics stable within tolerance
- baseline comparisons exist (simple model beats random)
- uncertainty reporting present where relevant
- multiple comparisons / p-hacking risks handled (where applicable)

> [!TIP]
> Attach sanity plots as CI artifacts on failure (confusion matrix, residuals, calibration curve). ğŸ“

---

## ğŸ§© Graphs, agents, optimization, and â€œhard mathâ€ modules

If you include:
- graph algorithms (spectral, routing, clustering)
- autonomous agents / planners
- optimization loops

Add tests that check:
- invariants (symmetry, conservation, monotonicity)
- convergence behavior (within iteration limits)
- gradient checks (finite-difference sanity, if applicable)
- known micro-benchmarks (tiny, deterministic)

---

## ğŸ—„ï¸ Database tests (Postgres/PostGIS, MySQL if applicable)

Principles:
- use ephemeral DBs (containers)
- run migrations in CI
- isolate with transactions
- seed minimal fixtures (never production dumps)

Recommended:
- migration tests: upgrade/downgrade + schema assertions
- query tests: correctness + (where critical) index usage/explain plans
- contract tests: API responses match schema

---

## ğŸ³ Docker + Compose: the integration backbone

**Pattern:**
- Compose defines `db`, `api`, maybe `worker`
- tests bring stack up, run, tear down
- CI runs the same Compose profile (parity wins ğŸ†)

Compose tips that make tests reliable:
- add `healthcheck:` and wait for readiness
- keep PR stack minimal (only required services)
- wipe state between runs (`docker compose down -v`)

---

## âœ… PR checklist (copy/paste)

- [ ] Unit tests added/updated
- [ ] Integration tests added (if behavior crosses boundaries)
- [ ] Determinism confirmed (seeds + stable outputs) if ML/sim
- [ ] Contracts updated + verified (OpenAPI/GraphQL) if API changed
- [ ] Catalog/metadata tests updated (STAC/DCAT/PROV) if data outputs changed
- [ ] UI changes include component tests + (if visual) snapshot updates
- [ ] CI is green (required)

---

## ğŸ§¯ Troubleshooting

### âŒ Tests fail only in CI?
- check lockfiles & pinned versions
- confirm containers match local versions
- eliminate reliance on local paths, locale, timezone, GPU availability

### ğŸ² Flaky tests?
- remove sleeps; wait on conditions
- fix randomness (seed)
- mock/record external services

### ğŸ³ Docker stack wonâ€™t start?
```bash
docker compose logs -f
docker compose config
docker compose up -d --build
```

---

## ğŸ“š Reference pointers (why our test strategy looks like this)

> Keep these in `docs/library/` (or your chosen path) and link them in ADRs/TEST_POLICY when needed.

- ğŸ§± Architecture & boundaries â†’ `docs/library/clean-architectures-in-python.pdf`
- ğŸ³ CI parity & Compose patterns â†’ `docs/library/Introduction-to-Docker.pdf`
- ğŸ§  Modeling V&V & uncertainty posture â†’ `docs/library/Scientific Modeling and Simulation_ A Comprehensive NASA-Grade Guide.pdf`
- âš ï¸ Statistical foot-guns â†’ `docs/library/Statistics Done Wrong - Alex_Reinhart-Statistics_Done_Wrong-EN.pdf`

---

## âœ¨ Small â€œnext improvementsâ€ (optional, high ROI)

- Add `make test`, `make test-unit`, `make test-integration`, `make test-e2e`
- Upload CI artifacts on failure (coverage HTML, E2E screenshots, diff images)
- Add metadata validation gates for outputs (STAC/DCAT/PROV) if not already present
- Add nightly benchmarks for geospatial ops + API latencies (separate from PR gates)
