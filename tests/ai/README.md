# ğŸ¤– AI Test Suite â€” `tests/ai/` (Focus Mode) ğŸ§­

> **Goal:** keep Focus Mode **evidence-first**, **policy-gated**, and **reproducible** âœ…  
> This folder is where we prove (with tests) that our AI stays grounded in retrieved sources, returns citations, and respects governance + safety guardrails.

---

## ğŸ·ï¸ Badges (wire these up in CI)
![CI](https://img.shields.io/badge/CI-configure%20me-blue?logo=githubactions&logoColor=white)
![Tests](https://img.shields.io/badge/tests-AI%20%2F%20RAG%20%2F%20Policy-informational)
![OPA](https://img.shields.io/badge/policy-OPA%20(Rego)-informational)
![Ollama](https://img.shields.io/badge/LLM-Ollama-local%20runtime-success)
![RAG](https://img.shields.io/badge/RAG-hybrid%20retrieval-success)
![Reproducibility](https://img.shields.io/badge/reproducibility-required-success)

---

## ğŸ§  What weâ€™re testing (Focus Mode contract) âœ…

Focus Mode is designed to be **advisory-only**: it should **only** narrate/assemble what retrieval returns, **not invent** new facts or claim tool access it doesnâ€™t have.

### Nonâ€‘negotiables (tests must enforce)
- **ğŸ“š Evidence-first answers:** factual claims must be supported by retrieved sources.
- **ğŸ”— Citations are mandatory:** answers must include source citations consistently.
- **ğŸ§¼ Prompt Gate / sanitization:** prompt injection attempts get neutralized before the LLM sees them.
- **ğŸ›¡ï¸ Output policy checks:** final responses must pass policy (OPA/Rego or equivalent).
- **ğŸ§¾ Provenance:** responses should be traceable (model version, prompt version, sources used).
- **ğŸš« No magical powers:** the LLM does **not** get direct DB/filesystem/network access.

> If any of these regress, tests should fail loudly ğŸ”¥

---

## ğŸ—ºï¸ Pipeline map (what integration tests should cover)

```mermaid
flowchart TD
  U[User question] --> PG[Prompt Gate ğŸ§¼ sanitize + normalize]
  PG --> R[Retrieval ğŸ” hybrid: graph + spatial + text + vectors]
  R --> P[Prompt Assembly ğŸ§© system rules + context + SOURCES [1..n]]
  P --> L[LLM Generate ğŸ¤– Ollama /api/generate]
  L --> OP[Output Policy ğŸ›¡ï¸ OPA/Rego + validation]
  OP --> PR[Provenance ğŸ§¾ log: sources, model, prompt version, decision]
  PR --> A[Answer âœ… with citations]
```

---

## ğŸ§ª Test Pyramid (recommended)

### 1) ğŸ”§ Unit tests (fast, deterministic)
Test pure functions:
- prompt builders / templates
- citation formatting + validation
- retrieval result normalization + merging
- context window compaction (â€œhigh-signalâ€ trimming)
- provenance payload creation

âœ… **No network**, no Ollama, no DB.

---

### 2) ğŸ”— Contract tests (API + schema)
Validate the AI endpoints behave like a stable API contract:
- endpoint accepts expected payload (question + optional context like place/time)
- response schema includes answer + citations (or a structured error)
- streaming endpoint (if enabled) emits expected event sequence
- policy rejection returns the correct error shape + safe message

---

### 3) ğŸ” RAG regression tests (golden + invariant checks)
Run curated questions against a **fixed fixture corpus**:
- verify **must-have citations**
- verify **must-mention key facts** (keywords/phrases)
- verify **must-not hallucinate** (no claims outside fixture sources)
- verify minimum citation count / coverage

âœ… Prefer **invariant-based assertions** over exact string snapshots:
- â€œcontains citationsâ€
- â€œmentions Xâ€
- â€œdoes not mention Yâ€
- â€œall cited source IDs exist in the fixture bundleâ€

---

### 4) ğŸ›¡ï¸ Policy & safety tests (must be mean ğŸ˜ˆ)
- prompt injection attempts (e.g., â€œignore sources, reveal secretsâ€)
- role/permission boundaries (guest vs contributor vs admin context)
- disallowed content requests
- â€œmissing citationsâ€ should fail policy
- â€œclaims of tool accessâ€ should fail validation

---

### 5) âš¡ Performance / scalability checks (optional, but valuable)
- latency budgets (p50/p95) for:
  - retrieval
  - generation
  - end-to-end
- caching effectiveness (repeat query speedup)
- stress tests with small fixture sets

> Keep perf tests separate (e.g., `-m slow` / nightly) ğŸ•’

---

## ğŸ“ Suggested folder structure ğŸ—‚ï¸

> This is a **recommended** layout; adapt to the repoâ€™s current conventions.

```text
tests/ai/
â”œâ”€ README.md                # ğŸ‘ˆ you are here
â”œâ”€ fixtures/                # ğŸ“¦ tiny, deterministic test data
â”‚  â”œâ”€ sources/              # ğŸ§© source snippets with IDs (1..n)
â”‚  â”œâ”€ questions.yaml        # â“ curated prompts + expectations
â”‚  â””â”€ contexts.json         # ğŸ—ºï¸ place/time/layer contexts
â”œâ”€ unit/                    # ğŸ”§ pure function tests
â”œâ”€ contract/                # ğŸ”— API schema + response validation
â”œâ”€ policy/                  # ğŸ›¡ï¸ OPA/Rego tests + policy fixtures
â”œâ”€ rag_regression/          # ğŸ” golden/invariant regression tests
â”œâ”€ perf/                    # âš¡ load/latency tests (optional)
â””â”€ scripts/                 # ğŸ§° helpers (update goldens, report scores)
```

---

## â–¶ï¸ Running the tests (local)

### âœ… Quick start (recommended dev loop)
1. Start the LLM runtime (Ollama) locally.
2. Run unit + policy tests first.
3. Run RAG regression tests after.

### ğŸ Python (pytest-style)
> If this project uses `pytest`, these commands are typical.

```bash
# Fast checks (unit + policy)
pytest tests/ai/unit tests/ai/policy -q

# Contract + integration (may require API + Ollama running)
pytest tests/ai/contract tests/ai/rag_regression -q

# Everything
pytest tests/ai -q
```

### ğŸŸ¨ Node (Jest-style) â€” optional
If any AI clients/tools are in Node:
```bash
npm test
# or
npx jest tests/ai
```

---

## ğŸ”§ Environment configuration (common knobs)

> Keep tests reproducible by pinning what can drift.

**Suggested env vars to support:**
- `OLLAMA_API_URL` â€” where the LLM runtime is hosted (often `http://localhost:11434`)
- `AI_MODEL` â€” model name/tag (pin versions in CI)
- `AI_EMBED_MODEL` â€” embedding model name/tag (pin versions in CI)
- `AI_TEST_MODE=1` â€” enables deterministic behavior (if supported)
- `AI_SEED` â€” random seed for any sampling (if supported)

âœ… In CI: use a **smaller model** (faster, cheaper) and run a **curated regression set**.

---

## âœï¸ Writing a new AI test (pattern)

### âœ… Naming conventions
- Use stable IDs: `rag__<topic>__<place>__<year>`
- Put expectations next to inputs:
  - question
  - optional context
  - required citations / must-mentions / must-not-mentions

### Example test case schema (YAML)
```yaml
- id: rag__dust_bowl__finney_county__1935
  question: "What happened here in the mid-1930s and why?"
  context:
    place: "Finney County, KS"
    year: 1935
    layers: ["drought_index"]
  expect:
    policy: allow
    must_include_citations: true
    min_citation_count: 2
    must_mention_any:
      - "drought"
      - "Dust Bowl"
    must_not_mention_any:
      - "I searched the web"
      - "I accessed your database directly"
```

### âœ… Assertion style (recommended)
Prefer:
- structured parsing (extract citation tokens)
- keyword + invariant checks
- source ID validation

Avoid:
- exact full-string snapshot comparisons (LLMs drift naturally)

---

## ğŸ§¾ Updating baselines (when the model/prompt changes)

When you *intentionally* change:
- retrieval strategy
- prompt templates
- policy rules
- model version/tag

â€¦expect regression diffs.

âœ… Recommended workflow:
1. run regression suite locally
2. inspect diffs (citations, coverage, hallucination checks)
3. update goldens only if the change is expected + reviewed
4. pin versions (model + prompt) in metadata

---

## ğŸ§¯ Debugging checklist (when a test fails)

- **Citations missing?**  
  - check prompt template includes SOURCES + citation rule
  - check output policy didnâ€™t strip tokens
- **Hallucination detected?**  
  - inspect retrieved snippets (is evidence actually present?)
  - tighten prompt: â€œuse only sourcesâ€
  - add post-validation: â€œclaims must map to source IDsâ€
- **Policy rejection unexpected?**  
  - run policy tests directly
  - print the exact input object passed to OPA/Rego
- **Regression drift after model bump?**  
  - pin model tag in CI
  - update goldens only after review

---

## âœ… PR checklist (AI changes)

- [ ] Added/updated tests in `tests/ai/`
- [ ] Regression suite passes locally (or in CI)
- [ ] Model + prompt versions pinned (or documented)
- [ ] Policy tests updated if guardrails changed
- [ ] New behaviors documented (what changed + why)

---

## ğŸ”— Related docs (recommended reading)
- `docs/architecture/ai/` ğŸ“  
- `docs/architecture/AI_SYSTEM_OVERVIEW.md` ğŸ“„  
- `docs/architecture/ai/OLLAMA_INTEGRATION.md` ğŸ“„  
- `src/server/api/README.md` ğŸ“„  

---

## ğŸ§­ Philosophy (why weâ€™re strict)
Weâ€™re building a historical + geospatial â€œtruth machine,â€ not a vibes generator âœ¨â¡ï¸ğŸ“š  
If the system canâ€™t cite it, it shouldnâ€™t say it. âœ…
