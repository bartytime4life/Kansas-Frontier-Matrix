# ğŸ§ª Unit Tests â€” `tests/unit/`

![Scope](https://img.shields.io/badge/scope-unit%20tests-blue?style=for-the-badge)
![Runner](https://img.shields.io/badge/runner-pytest%20%2F%20jest-purple?style=for-the-badge)
![Philosophy](https://img.shields.io/badge/philosophy-fail%20closed%20%26%20evidence--first-orange?style=for-the-badge)

Welcome to the **fastest** test layer in the Kansas Frontier Matrix / Kansas-Matrix-System. âœ…  
This folder is for **pure, deterministic, low-latency tests** that validate *core logic* without depending on live infrastructure.

> [!IMPORTANT]
> **Unit tests protect our non-negotiables**:
> - ğŸ§± **Fail-closed governance** (missing metadata / policy violations must block by default)
> - ğŸ” **Evidence-first answers** (citations are required, not optional)
> - ğŸ›¡ï¸ **Policy enforcement stays wired-in** (security & content rules canâ€™t be â€œaccidentallyâ€ bypassed)

---

## ğŸ“Œ Quick links
- [ğŸš€ Quickstart](#-quickstart)
- [âœ… What belongs in unit tests](#-what-belongs-in-unit-tests)
- [ğŸš« What does NOT belong here](#-what-does-not-belong-here)
- [ğŸ“‚ Suggested layout](#-suggested-layout)
- [ğŸ§  KFM-specific invariants we unit test](#-kfm-specific-invariants-we-unit-test)
- [ğŸ§° Patterns & helpers](#-patterns--helpers)
- [ğŸ§¯ Troubleshooting](#-troubleshooting)
- [âœ… PR checklist](#-pr-checklist)

---

## ğŸš€ Quickstart

### ğŸ Backend (Python / API)
Run unit tests (preferred):
```bash
# from repo root (recommended if running services via docker-compose)
docker-compose exec api pytest tests/unit -q
```

If you run Python tests locally (outside Docker), use your normal workflow (e.g., `pytest tests/unit`).

### ğŸŒ Frontend (React / Web)
If the web app has unit tests enabled:
```bash
npm test
```

### ğŸ§· Policy gates (OPA â€œconftestâ€, NOT pytestâ€™s `conftest.py`)
To replicate policy validation locally:
```bash
conftest test .
```

> [!TIP]
> The word â€œconftestâ€ is overloaded:
> - âœ… `conftest` (CLI) = **OPA/rego policy testing**  
> - âœ… `conftest.py` (file) = **pytest fixtures**  
> They are unrelated. Donâ€™t confuse them. ğŸ™‚

---

## âœ… What belongs in unit tests

Unit tests should hit **logic** (not infrastructure). Great targets:

- ğŸ§© **Pure functions** (transformations, parsers, formatters)
- ğŸ§¾ **Metadata validation** (required fields present, schema-ish checks)
- ğŸ” **Policy decision wiring** (inputs â†’ allow/deny/sanitize decisions)
- â›“ï¸ **Provenance builders** (PROV objects / ledger entry construction)
- ğŸ§  **Focus Mode pipeline logic** *(mock dependencies)*:
  - prompt building
  - citation marker formatting
  - post-processing & guardrails
  - â€œrefuse/sanitizeâ€ behavior on restricted content
- ğŸ§° **Utilities** (time normalization, ID formatting, slugging, etc.)

---

## ğŸš« What does NOT belong here

Keep unit tests **small + local**. These belong elsewhere (integration/e2e):

- ğŸ—„ï¸ Real database calls (PostGIS / Neo4j / any live DB)
- ğŸŒ Network calls (HTTP to external services)
- ğŸ§  Real LLM inference (Ollama/OpenAI) **in unit tests**
- ğŸ³ Docker orchestration checks
- ğŸ§± Full-stack flows (UI â†” API â†” DB)

> [!NOTE]
> If a test requires spinning up containers or depends on â€œit works on my machineâ€ state, itâ€™s not a unit test.

---

## ğŸ“‚ Suggested layout

Use whatever structure matches the repo, but aim for **discoverable + stable** organization:

```text
ğŸ“¦ tests/
 â””â”€â”€ ğŸ§ª unit/
     â”œâ”€â”€ ğŸ api/              # request parsing, response shaping, helpers
     â”œâ”€â”€ ğŸ§  ai/               # Focus Mode pipeline logic (mock retrieval + LLM)
     â”œâ”€â”€ ğŸ›¡ï¸ policy/           # policy adapters + rego-related unit checks (no network)
     â”œâ”€â”€ â›“ï¸ provenance/        # PROV builders, audit/ledger record constructors
     â”œâ”€â”€ ğŸ§° utils/            # shared pure helpers
     â”œâ”€â”€ ğŸ§· fixtures/         # tiny JSON/text fixtures used across tests
     â””â”€â”€ ğŸ“„ README.md         # (you are here)
```

> [!TIP]
> Prefer **many small test files** over one â€œmega test fileâ€.  
> Example: `test_prompt_builder.py`, `test_citation_formatter.py`, `test_rbac_decision.py`

---

## ğŸ§  KFM-specific invariants we unit test

### 1) ğŸ§± â€œFail closedâ€ behavior
If anything is missing or invalid, the safest default is **deny/block**.

âœ… Unit tests should assert:
- missing license â†’ rejected (or marked non-publishable)
- missing sensitivity label â†’ rejected (or forced to safest classification)
- missing provenance pointer/record â†’ rejected (or flagged)

### 2) ğŸ” Evidence-first: citations are mandatory
For Focus Mode / AI responses:
- citations must exist
- citation markers must map to sources
- â€œno evidenceâ€ must produce a refusal or safe fallback

âœ… Unit tests should assert:
- answer without citations â†’ fails validation / triggers refusal
- citations are stable and correctly formatted (e.g., numeric markers)
- post-processing never drops citations accidentally

### 3) ğŸ›¡ï¸ Policy is always enforced
We do not allow â€œdirect DB bypassâ€ behaviors and we donâ€™t ship code that forgets to call policy checks.

âœ… Unit tests should assert:
- policy check is invoked for protected operations
- policy denial returns a denial/sanitized result consistently
- restricted outputs are masked/sanitized when required

### 4) â›“ï¸ Audit + provenance record construction is correct
Even if a deeper storage layer is integration-tested, unit tests should validate:
- required fields exist (timestamps, actor, activity, inputs, outputs)
- stable identifiers/hashes are produced as expected
- serialization is deterministic (ordering, schema shape)

---

## ğŸ§° Patterns & helpers

### âœ… Arrangeâ€“Actâ€“Assert (AAA)
Keep each test focused and readable:

```python
def test_citation_formatter_includes_markers():
    # Arrange
    sources = [{"id": "doc_1"}, {"id": "dataset_9"}]

    # Act
    answer = format_answer_with_citations("Hello", sources)

    # Assert
    assert "[1]" in answer and "[2]" in answer
```

### ğŸ§ª Use fakes at the boundaries
Prefer these layers (from simplest â†’ heaviest):
- âœ… Fake object
- âœ… Stubbed interface
- âœ… Monkeypatch/mock
- âŒ Real dependency

Examples of what to fake:
- LLM client
- retrieval/search client
- policy client
- clock/time provider

### â±ï¸ Time & randomness must be deterministic
- freeze time (or inject a clock)
- set random seeds (or inject RNG)

> [!TIP]
> If a test sometimes fails â€œonly on CIâ€, itâ€™s usually:
> - time
> - randomness
> - hidden network/FS dependency
> - implicit ordering

---

## ğŸ§¯ Troubleshooting

### â€œMy unit tests are slowâ€¦â€
- Check for accidental network calls
- Check for real DB initialization
- Remove sleeps/timeouts and inject clocks instead

### â€œCI failed on a governance/policy ruleâ€
- Run policy checks locally:
```bash
conftest test .
```
- Fix the file the rule complains about (often metadata fields / provenance presence).

### â€œIâ€™m not sure where my test belongsâ€
Rule of thumb:
- **Unit** = no infrastructure
- **Integration** = multiple components talking together
- **E2E** = user-level flow

---

## âœ… PR checklist

Before you open a PR:

- [ ] ğŸ§ª You added/updated unit tests for new logic
- [ ] âš¡ Tests are deterministic (no time/random/network surprises)
- [ ] ğŸ§± Fail-closed paths are explicitly tested
- [ ] ğŸ” Citation enforcement is covered for Focus Mode logic (when applicable)
- [ ] ğŸ›¡ï¸ Policy checks are not bypassed (deny/sanitize behavior tested)
- [ ] ğŸ§¹ Linters pass locally (where configured)

---

### ğŸ¯ North Star
**Fast unit tests** = confident refactors + safer governance + fewer regressions. âœ…  
If it can break production behavior in a subtle way, it deserves a unit test.