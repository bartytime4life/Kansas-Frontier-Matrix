<!--
path: tests/e2e/README.md
note: According to a document from 2026-01-29, KFMâ€™s architecture is â€œpipeline-firstâ€ + â€œpolicy-as-codeâ€.
-->

# ğŸ§ª End-to-End Tests (E2E) â€” KFM

![E2E](https://img.shields.io/badge/tests-e2e-blue)
![Docker](https://img.shields.io/badge/runtime-docker%20compose-2496ED?logo=docker&logoColor=white)
![UI](https://img.shields.io/badge/web-react%20%2B%20map-61DAFB?logo=react&logoColor=black)
![API](https://img.shields.io/badge/api-fastapi-009688?logo=fastapi&logoColor=white)
![Policy](https://img.shields.io/badge/governance-OPA%20%2F%20Rego-7B3FE4)

E2E tests validate **KFMâ€™s core promise**: users experience a trusted, governed, provenance-linked systemâ€”**from UI â†’ API â†’ policy â†’ data**â€”without bypassing the canonical pipeline. ğŸ§­

---

## ğŸ” What E2E Covers (and why)

KFM is designed around a **non-negotiable pipeline** that ensures traceability and governance. E2E tests exist to catch regressions that unit/integration tests cannotâ€”things like:

- ğŸ—ºï¸ The web UI correctly loads map + layers and renders data-driven interactions.
- ğŸ” The API enforces **policy decisions** (deny / redact / sanitize).
- ğŸ§¾ Provenance and metadata are surfaced (licenses, citations, dataset records).
- ğŸ“š Story Nodes render with the right evidence links and map/timeline coupling.
- ğŸ¤– Focus Mode AI returns answers with citations and is blocked/redacted when policy says so.

> [!IMPORTANT]
> E2E tests must **never** â€œcheatâ€ by reaching into databases directly from the UI layer, hard-coding hidden data, or bypassing policy gates. If a user canâ€™t do it legitimately through KFMâ€™s contracts, your test shouldnâ€™t either.

---

## ğŸ§± System Under Test (high-level)

```mermaid
flowchart LR
  UI["ğŸŒ web/ (React + Map UI)"] -->|HTTP| API["ğŸ§  src/server/ (API boundary)"]
  API -->|read| PG["ğŸ—„ï¸ Postgres/PostGIS"]
  API -->|read| N4J["ğŸ•¸ï¸ Neo4j Graph"]
  API -->|decision| OPA["âš–ï¸ OPA / Rego Policies"]
  API -->|LLM tools + citations| LLM["ğŸ¤– Ollama (local model)"]

  Docs["ğŸ“š docs/reports/story_nodes/"] --> UI
  Catalogs["ğŸ“¦ STAC/DCAT/PROV catalogs"] --> API
```

---

## ğŸ“ Directory Layout

Suggested (and recommended) structure:

```text
ğŸ“ tests/
â””â”€â”€ ğŸ“ e2e/
    â”œâ”€â”€ ğŸ“„ README.md                 ğŸ‘ˆ you are here
    â”œâ”€â”€ ğŸ“ specs/                    âœ… test specs (grouped by feature)
    â”‚   â”œâ”€â”€ ğŸ“„ smoke.spec.ts
    â”‚   â”œâ”€â”€ ğŸ“„ map.spec.ts
    â”‚   â”œâ”€â”€ ğŸ“„ catalog.spec.ts
    â”‚   â”œâ”€â”€ ğŸ“„ storynodes.spec.ts
    â”‚   â”œâ”€â”€ ğŸ“„ policy.spec.ts
    â”‚   â””â”€â”€ ğŸ“„ focus-mode-ai.spec.ts
    â”œâ”€â”€ ğŸ“ fixtures/                 ğŸ§° small, governed test fixtures only
    â”‚   â”œâ”€â”€ ğŸ“ users/
    â”‚   â”œâ”€â”€ ğŸ“ datasets/
    â”‚   â””â”€â”€ ğŸ“ story_nodes/
    â”œâ”€â”€ ğŸ“ helpers/                  ğŸ› ï¸ shared helpers (API clients, waits, etc.)
    â”œâ”€â”€ ğŸ“ pages/                    ğŸ§© page objects (optional, but helpful)
    â”œâ”€â”€ ğŸ“ artifacts/                ğŸ“ screenshots/videos/reports (gitignored)
    â””â”€â”€ ğŸ“„ e2e.env.example           ğŸ” env vars for local runs
```

> [!TIP]
> Keep fixtures tiny. If you need â€œrealâ€ scale, add a **separate perf load test**. E2E should stay deterministic and fast.

---

## âœ… Prerequisites

- ğŸ³ **Docker + Docker Compose v2**
- ğŸŸ© **Node.js** (LTS recommended)
- ğŸ **Python tooling** only if your workflow seeds fixtures via pipeline scripts
- ğŸ¤– **Ollama** (only if running Focus Mode AI tests locally)

---

## ğŸš€ Quick Start (Local)

### 1) Bring up the dev stack ğŸ³

From the repo root:

```bash
# Option A (modern)
docker compose up -d --build

# Option B (legacy)
docker-compose up -d --build
```

Sanity checks (expected defaults):

- API: `http://localhost:8000/docs` (Swagger UI)
- Web: `http://localhost:3000`
- Neo4j: `http://localhost:7474` (if enabled)
- Postgres: `localhost:5432` (if exposed)

### 2) Configure E2E env ğŸ”

Copy the example and tweak if needed:

```bash
cp tests/e2e/e2e.env.example tests/e2e/.env
```

Example `tests/e2e/e2e.env.example`:

```dotenv
# ğŸŒ targets
E2E_BASE_URL=http://localhost:3000
E2E_API_URL=http://localhost:8000

# ğŸ§ª toggles
E2E_HEADLESS=1
E2E_TRACE=0
E2E_VIDEO=0

# ğŸ¤– AI (optional)
E2E_AI=0
E2E_AI_MODEL=llama2
```

### 3) Install and run the E2E runner ğŸ§ª

This repo may use **Playwright** (recommended) or another runner.
Below is the Playwright flow (adjust if your repo standardizes on something else):

```bash
cd tests/e2e
npm ci

# first time only
npx playwright install --with-deps

# run everything
npx playwright test

# run smoke only
npx playwright test --grep "@smoke"

# view report
npx playwright show-report
```

> [!NOTE]
> If your monorepo keeps test deps at the root, run the equivalent root scripts (e.g. `npm run e2e` / `pnpm e2e`).

---

## ğŸ§¬ Test Data & Determinism

KFMâ€™s data philosophy is **deterministic + reproducible**: given the same inputs/config, pipelines should produce the same outputsâ€”no manual steps. âœ…

For E2E, that means:

- Use a **small fixture dataset** that already conforms to the pipeline (catalog + provenance included).
- Prefer â€œseed once then reuseâ€ patterns in local dev.
- Keep E2E fixtures:
  - âœ… safe to publish
  - âœ… properly licensed
  - âœ… metadata-complete
  - âœ… governance-compliant

### Recommended E2E fixture strategy (pick one)

**A) Pre-seeded volumes (fastest)**
- Docker volumes include a minimal â€œknown-goodâ€ dataset.

**B) Seed script inside API container (most explicit)**
- A script loads a minimal dataset + graph fixture + users.
- Example pattern:
  ```bash
  docker compose exec api python -m tools.seed_e2e
  ```

**C) Pipeline-driven seed (most faithful, slowest)**
- Run a minimal ETL pipeline to generate processed output + catalogs before boot.
- Great for nightly CI, not ideal for â€œevery PRâ€.

---

## ğŸ§­ What We Assert: KFM Invariants as E2E Checks

### ğŸ§± Pipeline invariants
- UI consumes data **only** via API contracts.
- API reads from governed stores (DB/graph/catalogs), not ad-hoc files.
- Story Nodes are rendered as governed artifacts (no â€œmystery contentâ€).
- Focus Mode uses policy + citations, not unchecked model output.

### âš– Governance invariants
- Restricted/sensitive datasets are **denied or sanitized** (not leaked).
- Classification doesnâ€™t â€œdowngradeâ€ accidentally across outputs.
- Provenance artifacts exist (and are visible where appropriate).

### ğŸ¤– AI invariants (Focus Mode)
- AI answers include citations and/or references.
- AI output is checked against policy; disallowed content is blocked/redacted.
- AI uses **approved tools/APIs** (no direct DB fetches from UI).

---

## ğŸ§ª Test Suites (Suggested)

Tagging convention (example):

- `@smoke` â€” minimal â€œis it alive?â€ suite
- `@map` â€” map rendering + layers + popups
- `@catalog` â€” dataset discovery + metadata visibility
- `@policy` â€” access control + redaction + denial behavior
- `@story` â€” Story Node rendering + deep links + map coupling
- `@ai` â€” Focus Mode AI + citation checks (optional in PRs)

Example mapping:

- âœ… **Smoke**: homepage loads, API health ok, basic dataset list returns.
- âœ… **Policy**: restricted dataset request yields 403 (or redacted response).
- âœ… **Provenance**: dataset detail page shows license/citation fields.
- âœ… **Story Nodes**: citations resolve; links arenâ€™t broken.
- âœ… **Focus Mode**: `/ai/query` returns citations and refuses disallowed prompts.

---

## ğŸ§© Authoring Guidelines (to avoid flaky tests)

### âœ… Prefer stable selectors
- Use `data-testid="..."` for UI targeting.
- Avoid brittle CSS selectors, DOM order assumptions, and pixel-perfect assertions.

### âœ… Prefer contract-level expectations
- Assert â€œwhat user seesâ€ + â€œwhat API promisesâ€, not internal implementation.

### âœ… Make waits explicit
- Wait for network idle, known UI markers, or deterministic API responses.
- Avoid arbitrary `sleep()` unless youâ€™re documenting a real async dependency.

### âœ… Keep tests readable
- Use helpers for:
  - auth/login
  - dataset creation/seed checks
  - policy expectation assertions (deny vs sanitize)
  - screenshot capture on failures

---

## ğŸ§¯ Troubleshooting

<details>
<summary><strong>ğŸšª Port conflicts (5432 / 7474 / 8000 / 3000)</strong></summary>

If you already have Postgres/Neo4j running locally, Docker may fail to bind.

- Stop local services, **or**
- Change port mappings in `docker-compose.yml`.

</details>

<details>
<summary><strong>ğŸ§  Docker memory / performance issues</strong></summary>

If containers are slow or killed, raise Docker Desktop memory limits.
This matters more when loading geospatial fixtures or running graph checks.

</details>

<details>
<summary><strong>ğŸªŸ Windows/Mac volume quirks</strong></summary>

If mounted code doesnâ€™t update or containers canâ€™t write to `data/`, verify:
- volumes are configured correctly
- paths resolve properly
- permissions allow writes where needed

</details>

---

## ğŸ§· CI Notes (Recommended defaults)

For CI stability:

- Run `@smoke` on every PR âœ…
- Run `@policy` + `@story` on PRs touching governance/docs âœ…
- Run full suite nightly ğŸŒ™
- Run `@ai` only when Ollama is available + stable ğŸ¤–

Artifacts to upload on failure:
- screenshots
- videos (optional)
- traces (optional)
- HTML report

---

## ğŸ§­ Definition of Done for a New E2E Test

A new E2E spec is considered â€œdoneâ€ when it:

- âœ… Validates a user-visible behavior tied to a real KFM invariant
- âœ… Uses stable selectors (or adds them)
- âœ… Uses governed fixtures (no untracked mystery data)
- âœ… Includes a clear reason *why this matters* (comment at top of test)
- âœ… Is deterministic locally and in CI

---

## ğŸ¤ Contributing

- Keep tests small and focused âœ‚ï¸
- Prefer adding one great E2E test that protects a core invariant over many shallow ones ğŸ¯
- If your change touches:
  - ğŸ” policy
  - ğŸ“š Story Nodes
  - ğŸ“¦ STAC/DCAT/PROV metadata
  - ğŸ§  Focus Mode AI
  - ğŸ—ºï¸ map layers
â€¦add/adjust E2E coverage accordingly.

Happy breaking regressions before they break trust ğŸ§¡

