# ğŸ§ª Test Fixtures (KFM)

> ğŸ“ **Location:** `tests/fixtures/`  
> ğŸ¯ **Purpose:** small, deterministic, provenance-aware inputs for automated tests across the Kansas Frontier Matrix (KFM) pipeline (ğŸ“¥ raw â†’ ğŸ§¹ processed â†’ ğŸ—‚ï¸ catalog+prov â†’ ğŸ—„ï¸ DB â†’ ğŸ”Œ API â†’ ğŸ—ºï¸ UI).

---

## ğŸ§­ Why fixtures exist

KFM is designed to be **evidence-backed** and **provenance-first**â€”meaning we should be able to trace *any* derived output (including test outputs) back to clear inputs and metadata. Fixtures are the â€œknown-goodâ€ mini-worlds we use to verify:

- âœ… data contracts (schemas, validations)
- âœ… deterministic pipeline behavior
- âœ… API responses (FastAPI tests)
- âœ… governance checks (license + provenance requirements)
- âœ… UI rendering assumptions (GeoJSON shape, IDs, etc.)
- âœ… â€œFocus Modeâ€ / AI safety + citation formatting (when applicable)

If a test canâ€™t be reproduced from fixtures, itâ€™s not trustworthy. ğŸ§¾

---

## ğŸ†š Fixtures vs â€œsample dataâ€

Use the right home so the repo stays clean and scalable:

- **`tests/fixtures/`** â†’ *tiny* datasets and mock artifacts **only for tests**  
  - Think: â€œenough to test the logic, not enough to be a real datasetâ€.
- **`data/raw/sample/`** (or your projectâ€™s equivalent) â†’ dev/demo seed datasets used to boot the system locally (may be bigger than fixtures).  
- **Never** use fixtures as a backdoor to bypass canonical pipeline flow. ğŸš«

---

## ğŸ—‚ï¸ Recommended fixture layout

> This is a suggested structure that aligns fixtures to KFMâ€™s pipeline + subsystems.  
> Add folders only when a real test needs them.

```text
ğŸ“ tests/
â””â”€ ğŸ“ fixtures/                                   ğŸ§° governed, tiny test fixtures (goldens + seeds)
   â”œâ”€ ğŸ“„ README.md                                  ğŸ‘ˆ you are here âœ¨
   â”œâ”€ ğŸ“ manifest/                                  ğŸ§¾ fixture inventory + integrity
   â”‚  â”œâ”€ ğŸ§¾ fixtures.yaml                            âœ… fixture registry + checksums + owners
   â”‚  â””â”€ ğŸ“„ fixtures.schema.json                     â—»ï¸ optional: schema for validating fixtures.yaml
   â”œâ”€ ğŸ“ data/                                      ğŸ“¦ data fixtures (tiny + representative)
   â”‚  â”œâ”€ ğŸ“ raw/                                     ğŸ“¥ raw snapshots (tiny!)
   â”‚  â”œâ”€ ğŸ“ processed/                               ğŸ§¹ expected processed outputs (goldens)
   â”‚  â”œâ”€ ğŸ“ catalog/                                 ğŸ—‚ï¸ STAC/DCAT-style test metadata
   â”‚  â”‚  â”œâ”€ ğŸ“ stac/                                 ğŸ›°ï¸ STAC fixture objects (collections/items)
   â”‚  â”‚  â””â”€ ğŸ“ dcat/                                 ğŸ§¾ DCAT fixture objects (JSON-LD)
   â”‚  â””â”€ ğŸ“ provenance/                              ğŸ§¬ PROV-like lineage artifacts (receipts)
   â”œâ”€ ğŸ“ db/                                        ğŸ—„ï¸ database seeds + snapshots
   â”‚  â”œâ”€ ğŸ“ postgis/                                 ğŸ˜ SQL seed scripts + expected tables/views
   â”‚  â””â”€ ğŸ“ neo4j/                                   ğŸ•¸ï¸ Cypher seeds / JSON graph snapshots
   â”œâ”€ ğŸ“ api/                                       ğŸŒ API I/O fixtures (contract goldens)
   â”‚  â”œâ”€ ğŸ“ requests/                                ğŸ“¤ request payloads (JSON)
   â”‚  â”œâ”€ ğŸ“ responses/                               âœ… golden responses (JSON)
   â”‚  â””â”€ ğŸ“ errors/                                  ğŸ§¯ expected error bodies (JSON)
   â”œâ”€ ğŸ“ policy/                                    ğŸ›¡ï¸ policy-as-code fixtures (OPA-style)
   â”‚  â”œâ”€ ğŸ“ inputs/                                  ğŸ§¾ policy input JSON
   â”‚  â””â”€ ğŸ“ expected/                                âœ… expected allow/deny decisions
   â”œâ”€ ğŸ“ ai/                                        ğŸ¤– Focus Mode fixtures (prompts + expected citations)
   â”‚  â”œâ”€ ğŸ“ prompts/                                 ğŸ’¬ prompts / tool traces / contexts
   â”‚  â””â”€ ğŸ“ expected/                                âœ… expected responses + citations
   â””â”€ ğŸ“ web/                                       ğŸ–¥ï¸ UI fixtures (Story Nodes + map snippets)
      â”œâ”€ ğŸ“ story_nodes/                             ğŸ“š minimal Story Node markdown fixtures
      â””â”€ ğŸ“ map/                                     ğŸ—ºï¸ style snippets / layer configs for UI tests
```

---

## ğŸ” Fixture â€œpipeline alignmentâ€ (the golden rule)

Fixtures should be organized so tests can validate the canonical flow:

```mermaid
flowchart LR
  A[ğŸ“¥ Raw] --> B[ğŸ§¹ Processed]
  B --> C[ğŸ—‚ï¸ Catalog + ğŸ§¾ Provenance]
  C --> D[ğŸ—„ï¸ Databases]
  D --> E[ğŸ”Œ API]
  E --> F[ğŸ—ºï¸ UI]
```

This makes it easy to write tests like:

- â€œgiven `raw/`, pipeline produces **exactly** `processed/`â€
- â€œgiven `processed/`, catalog/prov generator produces **exactly** `catalog/` + `provenance/`â€
- â€œgiven `catalog/`, DB seed inserts expected rows/nodesâ€
- â€œgiven seeded DB, API returns **golden** JSON responseâ€

---

## ğŸ“œ Fixture contract

Every fixture added here should be:

- **Small** ğŸ§Š (prefer KBs, not MBs)
- **Deterministic** ğŸ¯ (stable ordering, stable IDs, no randomness)
- **Explained** ğŸ§  (a human can understand why it exists)
- **Governable** ğŸ›¡ï¸ (license + provenance expectations satisfied, even in miniature)

### âœ… Required metadata (via manifest)

Add each fixture to `tests/fixtures/manifest/fixtures.yaml`.

Suggested fields:

| Field | Why it matters |
|------|-----------------|
| `id` | stable reference across tests |
| `path` | where the file lives |
| `stage` | raw / processed / catalog / provenance / db / api / ui |
| `format` | csv / geojson / json / md / sql / cypher |
| `description` | what behavior this fixture validates |
| `license` | required for governance-style checks |
| `source` | where it came from (or â€œsyntheticâ€) |
| `sha256` | prevents silent changes / golden drift |

Example:

```yaml
# tests/fixtures/manifest/fixtures.yaml
fixtures:
  - id: geo__tiny_parcels__v1
    path: tests/fixtures/data/raw/geo/tiny_parcels_v1.geojson
    stage: raw
    format: geojson
    description: Minimal parcel polygons for bbox, CRS, and ID-stability tests
    source: synthetic
    license: CC0-1.0
    sha256: "REPLACE_WITH_REAL_HASH"
```

> Tip ğŸ§©: if your project already standardizes metadata (STAC/DCAT/PROV), keep this manifest **thin** and point to the canonical metadata fixture.

---

## ğŸ§± Naming conventions

Keep filenames predictable so tests are readable:

- Use **snake_case** for filenames  
- Prefer:  
  - `domain__topic__v1.ext` (simple)  
  - or `domain__topic__v1__expected.ext` (golden output)  

Examples:

- `trails__tiny_network__v1.geojson`
- `stac__tiny_item__v1.json`
- `prov__tiny_ingest__v1.json`
- `api__datasets_list__v1__response.json`

---

## ğŸ§ª How tests should use fixtures

### ğŸ Python / `pytest` (typical)

Use `pathlib` so paths are OS-safe:

```python
from pathlib import Path
import json

FIXTURES_DIR = Path(__file__).resolve().parent / "fixtures"

def load_json(rel_path: str) -> dict:
    p = FIXTURES_DIR / rel_path
    return json.loads(p.read_text(encoding="utf-8"))
```

### ğŸ”Œ API tests (FastAPI style)

Golden response testing pattern:

```python
from fastapi.testclient import TestClient

# example import path â€” adjust to your app structure
from api.main import app

client = TestClient(app)

def test_list_datasets_matches_golden():
    expected = load_json("api/responses/datasets__list__v1.json")
    resp = client.get("/datasets")
    assert resp.status_code == 200
    assert resp.json() == expected
```

### ğŸ§º Fixture-driven pipeline tests

Test the transformation chain:

```python
def test_pipeline_raw_to_processed(tmp_path):
    raw = FIXTURES_DIR / "data/raw/geo/tiny_parcels_v1.geojson"
    expected = FIXTURES_DIR / "data/processed/tiny_parcels_v1__expected.geojson"

    out = tmp_path / "out.geojson"
    run_pipeline(input_path=raw, output_path=out)  # your pipeline entrypoint

    assert out.read_text(encoding="utf-8") == expected.read_text(encoding="utf-8")
```

> ğŸ” **Rule of thumb:** if you canâ€™t compare it deterministically, you probably need to normalize the output (sorted keys, stable rounding, stable ID assignment).

### ğŸ§  AI / Focus Mode fixtures (when testing)

AI-related tests should **not** require a live model. Prefer:

- prompt fixtures (`ai/prompts/`)
- expected response fixtures (`ai/expected/`)
- tool-trace fixtures (if your system records them)

Use these to verify:
- citation formatting
- policy enforcement behavior (allow/deny)
- deterministic summarization behavior (if mocked)

---

## ğŸ›¡ï¸ Guardrails (non-negotiable)

### ğŸš« Never store
- secrets, API keys, tokens
- personal or sensitive info
- copyrighted datasets without permission
- â€œrealâ€ production dumps

### âœ… Prefer
- **synthetic** mini datasets
- or **downsampled** open data with clear licensing

### ğŸ“¦ Keep it tiny
If a fixture becomes â€œbig dataâ€, it belongs elsewhere (or behind Git LFS / external pointers). Fixtures should stay fast so CI stays fast. ğŸï¸

---

## â• Adding a new fixture (checklist)

1. ğŸ§© Pick the right stage folder (`data/raw`, `data/processed`, `api/responses`, etc.)
2. ğŸ“ Add/verify minimal **license + source** info
3. ğŸ§¾ Add provenance-style metadata if the test touches governance
4. ğŸ§· Update `manifest/fixtures.yaml` (include checksum)
5. âœ… Add at least **one** test that uses the fixture
6. ğŸ” Run tests locally (and ensure they pass in a clean environment)

---

## ğŸ” Troubleshooting

<details>
  <summary><strong>ğŸ§­ CRS / coordinate confusion in GeoJSON</strong></summary>

- Ensure your GeoJSON coordinates match expected CRS conventions.
- If your pipeline normalizes to a standard CRS, fixtures should reflect that.
- If output diffs are just float noise, consider rounding rules + stable serialization.

</details>

<details>
  <summary><strong>ğŸ§¨ â€œGoldenâ€ snapshots keep changing</strong></summary>

Common causes:
- nondeterministic ordering (dict ordering in serialization, DB query ordering)
- timestamps embedded in outputs
- random IDs / UUIDs

Fix:
- normalize (sort keys, sort features, freeze time, seed RNG)
- strip volatile fields before comparison

</details>

---

## ğŸ”— Related project docs (start here)

- ğŸ“š `docs/` (governed documentation) â†’ `../../docs/`
- ğŸ§± Architecture / structure â†’ `../../docs/architecture/`
- ğŸ—ƒï¸ Data + metadata layout â†’ `../../data/`
- ğŸ—ï¸ Pipelines â†’ `../../pipelines/`
- ğŸ”Œ API service â†’ `../../api/`
- ğŸ—ºï¸ Web UI â†’ `../../web/`

---

## ğŸ§· TL;DR

Fixtures are **tiny, deterministic, provenance-aware** artifacts that make tests reliable and governance verifiable.  
When in doubt: keep it small, document it, checksum it, and test it. âœ…
