# ğŸ§ª Test Fixtures (KFM)

> ğŸ“ **Location:** `tests/fixtures/`  
> ğŸ¯ **Purpose:** small, deterministic, provenance-aware inputs for automated tests across the Kansas Frontier Matrix (KFM) pipeline (ğŸ“¥ raw â†’ ğŸ§¹ processed â†’ ğŸ—‚ï¸ catalog+prov â†’ ğŸ—„ï¸ DB â†’ ğŸ”Œ API â†’ ğŸ—ºï¸ UI).

---

## ğŸ§­ Why fixtures exist

KFM is designed to be **evidence-backed** and **provenance-first**â€”meaning we should be able to trace *any* derived output (including test outputs) back to clear inputs and metadata. Fixtures are the â€œknown-goodâ€ mini-worlds we use to verify:

- âœ… data contracts (schemas, validations)
- âœ… deterministic pipeline behavior
- âœ… API responses (FastAPI tests)
- âœ… governance checks (license + provenance requirements)
- âœ… UI rendering assumptions (GeoJSON shape, IDs, etc.)
- âœ… â€œFocus Modeâ€ / AI safety + citation formatting (when applicable)

If a test canâ€™t be reproduced from fixtures, itâ€™s not trustworthy. ğŸ§¾

---

## ğŸ†š Fixtures vs â€œsample dataâ€

Use the right home so the repo stays clean and scalable:

- **`tests/fixtures/`** â†’ *tiny* datasets and mock artifacts **only for tests**  
  - Think: â€œenough to test the logic, not enough to be a real datasetâ€.
- **`data/raw/sample/`** (or your projectâ€™s equivalent) â†’ dev/demo seed datasets used to boot the system locally (may be bigger than fixtures).  
- **Never** use fixtures as a backdoor to bypass canonical pipeline flow. ğŸš«

---

## ğŸ—‚ï¸ Recommended fixture layout

> This is a suggested structure that aligns fixtures to KFMâ€™s pipeline + subsystems.  
> Add folders only when a real test needs them.

```text
tests/fixtures/
â”œâ”€â”€ README.md                         # you are here âœ¨
â”œâ”€â”€ manifest/
â”‚   â”œâ”€â”€ fixtures.yaml                 # fixture inventory + checksums
â”‚   â””â”€â”€ fixtures.schema.json          # optional: schema for the manifest
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                          # ğŸ“¥ raw snapshots (tiny!)
â”‚   â”œâ”€â”€ processed/                    # ğŸ§¹ expected processed outputs
â”‚   â”œâ”€â”€ catalog/                      # ğŸ—‚ï¸ STAC/DCAT-style test metadata
â”‚   â”‚   â”œâ”€â”€ stac/
â”‚   â”‚   â””â”€â”€ dcat/
â”‚   â””â”€â”€ provenance/                   # ğŸ§¾ PROV-like lineage artifacts
â”œâ”€â”€ db/
â”‚   â”œâ”€â”€ postgis/                      # SQL seed scripts, expected tables
â”‚   â””â”€â”€ neo4j/                        # Cypher seeds / JSON graph snapshots
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ requests/                     # request payloads (JSON)
â”‚   â”œâ”€â”€ responses/                    # golden responses (JSON)
â”‚   â””â”€â”€ errors/                       # expected error bodies
â”œâ”€â”€ policy/
â”‚   â”œâ”€â”€ inputs/                       # policy input JSON (OPA-style)
â”‚   â””â”€â”€ expected/                     # expected allow/deny decisions
â”œâ”€â”€ ai/
â”‚   â”œâ”€â”€ prompts/                      # Focus Mode prompts / tool traces
â”‚   â””â”€â”€ expected/                     # expected responses + citations
â””â”€â”€ web/
    â”œâ”€â”€ story_nodes/                  # minimal Story Node markdown fixtures
    â””â”€â”€ map/                          # style snippets / layer configs for UI tests
```

---

## ğŸ” Fixture â€œpipeline alignmentâ€ (the golden rule)

Fixtures should be organized so tests can validate the canonical flow:

```mermaid
flowchart LR
  A[ğŸ“¥ Raw] --> B[ğŸ§¹ Processed]
  B --> C[ğŸ—‚ï¸ Catalog + ğŸ§¾ Provenance]
  C --> D[ğŸ—„ï¸ Databases]
  D --> E[ğŸ”Œ API]
  E --> F[ğŸ—ºï¸ UI]
```

This makes it easy to write tests like:

- â€œgiven `raw/`, pipeline produces **exactly** `processed/`â€
- â€œgiven `processed/`, catalog/prov generator produces **exactly** `catalog/` + `provenance/`â€
- â€œgiven `catalog/`, DB seed inserts expected rows/nodesâ€
- â€œgiven seeded DB, API returns **golden** JSON responseâ€

---

## ğŸ“œ Fixture contract

Every fixture added here should be:

- **Small** ğŸ§Š (prefer KBs, not MBs)
- **Deterministic** ğŸ¯ (stable ordering, stable IDs, no randomness)
- **Explained** ğŸ§  (a human can understand why it exists)
- **Governable** ğŸ›¡ï¸ (license + provenance expectations satisfied, even in miniature)

### âœ… Required metadata (via manifest)

Add each fixture to `tests/fixtures/manifest/fixtures.yaml`.

Suggested fields:

| Field | Why it matters |
|------|-----------------|
| `id` | stable reference across tests |
| `path` | where the file lives |
| `stage` | raw / processed / catalog / provenance / db / api / ui |
| `format` | csv / geojson / json / md / sql / cypher |
| `description` | what behavior this fixture validates |
| `license` | required for governance-style checks |
| `source` | where it came from (or â€œsyntheticâ€) |
| `sha256` | prevents silent changes / golden drift |

Example:

```yaml
# tests/fixtures/manifest/fixtures.yaml
fixtures:
  - id: geo__tiny_parcels__v1
    path: tests/fixtures/data/raw/geo/tiny_parcels_v1.geojson
    stage: raw
    format: geojson
    description: Minimal parcel polygons for bbox, CRS, and ID-stability tests
    source: synthetic
    license: CC0-1.0
    sha256: "REPLACE_WITH_REAL_HASH"
```

> Tip ğŸ§©: if your project already standardizes metadata (STAC/DCAT/PROV), keep this manifest **thin** and point to the canonical metadata fixture.

---

## ğŸ§± Naming conventions

Keep filenames predictable so tests are readable:

- Use **snake_case** for filenames  
- Prefer:  
  - `domain__topic__v1.ext` (simple)  
  - or `domain__topic__v1__expected.ext` (golden output)  

Examples:

- `trails__tiny_network__v1.geojson`
- `stac__tiny_item__v1.json`
- `prov__tiny_ingest__v1.json`
- `api__datasets_list__v1__response.json`

---

## ğŸ§ª How tests should use fixtures

### ğŸ Python / `pytest` (typical)

Use `pathlib` so paths are OS-safe:

```python
from pathlib import Path
import json

FIXTURES_DIR = Path(__file__).resolve().parent / "fixtures"

def load_json(rel_path: str) -> dict:
    p = FIXTURES_DIR / rel_path
    return json.loads(p.read_text(encoding="utf-8"))
```

### ğŸ”Œ API tests (FastAPI style)

Golden response testing pattern:

```python
from fastapi.testclient import TestClient

# example import path â€” adjust to your app structure
from api.main import app

client = TestClient(app)

def test_list_datasets_matches_golden():
    expected = load_json("api/responses/datasets__list__v1.json")
    resp = client.get("/datasets")
    assert resp.status_code == 200
    assert resp.json() == expected
```

### ğŸ§º Fixture-driven pipeline tests

Test the transformation chain:

```python
def test_pipeline_raw_to_processed(tmp_path):
    raw = FIXTURES_DIR / "data/raw/geo/tiny_parcels_v1.geojson"
    expected = FIXTURES_DIR / "data/processed/tiny_parcels_v1__expected.geojson"

    out = tmp_path / "out.geojson"
    run_pipeline(input_path=raw, output_path=out)  # your pipeline entrypoint

    assert out.read_text(encoding="utf-8") == expected.read_text(encoding="utf-8")
```

> ğŸ” **Rule of thumb:** if you canâ€™t compare it deterministically, you probably need to normalize the output (sorted keys, stable rounding, stable ID assignment).

### ğŸ§  AI / Focus Mode fixtures (when testing)

AI-related tests should **not** require a live model. Prefer:

- prompt fixtures (`ai/prompts/`)
- expected response fixtures (`ai/expected/`)
- tool-trace fixtures (if your system records them)

Use these to verify:
- citation formatting
- policy enforcement behavior (allow/deny)
- deterministic summarization behavior (if mocked)

---

## ğŸ›¡ï¸ Guardrails (non-negotiable)

### ğŸš« Never store
- secrets, API keys, tokens
- personal or sensitive info
- copyrighted datasets without permission
- â€œrealâ€ production dumps

### âœ… Prefer
- **synthetic** mini datasets
- or **downsampled** open data with clear licensing

### ğŸ“¦ Keep it tiny
If a fixture becomes â€œbig dataâ€, it belongs elsewhere (or behind Git LFS / external pointers). Fixtures should stay fast so CI stays fast. ğŸï¸

---

## â• Adding a new fixture (checklist)

1. ğŸ§© Pick the right stage folder (`data/raw`, `data/processed`, `api/responses`, etc.)
2. ğŸ“ Add/verify minimal **license + source** info
3. ğŸ§¾ Add provenance-style metadata if the test touches governance
4. ğŸ§· Update `manifest/fixtures.yaml` (include checksum)
5. âœ… Add at least **one** test that uses the fixture
6. ğŸ” Run tests locally (and ensure they pass in a clean environment)

---

## ğŸ” Troubleshooting

<details>
  <summary><strong>ğŸ§­ CRS / coordinate confusion in GeoJSON</strong></summary>

- Ensure your GeoJSON coordinates match expected CRS conventions.
- If your pipeline normalizes to a standard CRS, fixtures should reflect that.
- If output diffs are just float noise, consider rounding rules + stable serialization.

</details>

<details>
  <summary><strong>ğŸ§¨ â€œGoldenâ€ snapshots keep changing</strong></summary>

Common causes:
- nondeterministic ordering (dict ordering in serialization, DB query ordering)
- timestamps embedded in outputs
- random IDs / UUIDs

Fix:
- normalize (sort keys, sort features, freeze time, seed RNG)
- strip volatile fields before comparison

</details>

---

## ğŸ”— Related project docs (start here)

- ğŸ“š `docs/` (governed documentation) â†’ `../../docs/`
- ğŸ§± Architecture / structure â†’ `../../docs/architecture/`
- ğŸ—ƒï¸ Data + metadata layout â†’ `../../data/`
- ğŸ—ï¸ Pipelines â†’ `../../pipelines/`
- ğŸ”Œ API service â†’ `../../api/`
- ğŸ—ºï¸ Web UI â†’ `../../web/`

---

## ğŸ§· TL;DR

Fixtures are **tiny, deterministic, provenance-aware** artifacts that make tests reliable and governance verifiable.  
When in doubt: keep it small, document it, checksum it, and test it. âœ…
