<!--
ğŸ“„ File: tests/fixtures/README.md
ğŸ¯ Purpose: Canonical documentation for deterministic test fixtures used across the KFM stack.
-->

# ğŸ§ª Test Fixtures

![KFM](https://img.shields.io/badge/KFM-tests%2Ffixtures-0b7285)
![Deterministic](https://img.shields.io/badge/fixtures-deterministic-1f6feb)
![Metadata](https://img.shields.io/badge/metadata-STAC%20%7C%20DCAT%20%7C%20PROV-2ea44f)
![Governance](https://img.shields.io/badge/governance-fail--closed-critical)

> [!NOTE]
> Fixtures are **tiny, deterministic, and committed** on purpose â€” they let us test the full KFM â€œraw â†’ processed â†’ catalog/provenance â†’ APIâ€ chain locally and in CI.

---

## ğŸ¯ What this folder is for

- âœ… Reproducible inputs for unit + integration tests (API, pipelines, policy).
- âœ… Minimal â€œgoldenâ€ datasets + their metadata sidecars.
- âœ… Negative fixtures (intentionally invalid) to prove we **fail closed**.
- âŒ Not production data.
- âŒ Not a dumping ground for large rasters or real-world exports.

---

## ğŸ§  Fixture philosophy (KFM-style)

KFM is **provenance-first**: even in tests, we treat data like it must pass the same gates as â€œrealâ€ layers.

- **Pipeline order matters**: raw â†’ processed â†’ catalog/prov â†’ DB â†’ API â†’ UI
- **Metadata required**: every dataset fixture should have catalog + provenance docs
- **Small & diffable**: prefer compact JSON/GeoJSON/CSV (or tiny rasters)
- **Governance baked in**: licenses + sensitivity flags are part of the â€œcontractâ€

```mermaid
flowchart LR
  A[ğŸ§± raw] --> B[ğŸ§¼ processed]
  B --> C[ğŸ—ºï¸ catalog]
  B --> D[ğŸ§¾ provenance]
  C --> E[(ğŸ—„ï¸ DB)]
  D --> E
  E --> F[ğŸ”Œ API]
  F --> G[ğŸ–¥ï¸ UI]
```

---

## ğŸ—‚ï¸ Recommended layout

> [!TIP]
> If your tests expect a different layout, update this README (or add per-pack READMEs). The goal is clarity + consistency.

```text
tests/fixtures/
â”œâ”€â”€ ğŸ“¦ datasets/
â”‚   â”œâ”€â”€ ğŸ§© <dataset_slug>/
â”‚   â”‚   â”œâ”€â”€ ğŸ§± raw/
â”‚   â”‚   â”œâ”€â”€ ğŸ§¼ processed/
â”‚   â”‚   â”œâ”€â”€ ğŸ—ºï¸ catalog/
â”‚   â”‚   â”œâ”€â”€ ğŸ§¾ provenance/
â”‚   â”‚   â””â”€â”€ ğŸ“„ README.md
â”œâ”€â”€ ğŸ”Œ api/
â”‚   â”œâ”€â”€ ğŸ§ª requests/
â”‚   â””â”€â”€ âœ… responses/
â”œâ”€â”€ ğŸ›¡ï¸ policy/
â”‚   â”œâ”€â”€ âœ… allow/
â”‚   â””â”€â”€ â›” deny/
â””â”€â”€ ğŸ“„ README.md  ğŸ‘ˆ you are here
```

---

## ğŸ§© Dataset fixture pack spec

Each dataset fixture lives in `datasets/<dataset_slug>/` and should contain:

### âœ… Required
- **Raw inputs** (`raw/`)
  - Minimal, representative sample (think: 10â€“200 rows or a handful of geometries).
- **Processed outputs** (`processed/`)
  - The â€œpost-pipelineâ€ artifact used by the API/DB layer in tests.
  - Examples: `*.geojson`, `*.jsonl`, `*.csv`, `*.parquet`, tiny `*.tif`.
- **Catalog metadata** (`catalog/`)
  - STAC Item / Collection JSON (and/or a DCAT record if your pipeline expects it).
- **Provenance log** (`provenance/`)
  - W3C PROV (or KFM-style) JSON describing lineage.

### â­ Nice-to-have
- `README.md` inside the pack explaining:
  - What the dataset represents
  - Spatial/temporal extent
  - How it is used by tests
  - Any known edge cases / â€œgotchasâ€

### ğŸ§· Naming conventions
- Use **snake_case** for slugs: `census_1900`, `railroads_1870s`, `drought_1934`
- Prefer deterministic filenames:
  - `processed/<slug>.geojson`
  - `catalog/<slug>.stac-item.json`
  - `provenance/<slug>.prov.json`

---

## â›” Negative fixtures (expected failures)

Some tests should verify that we fail safely (â€œfail closedâ€).

Common negative fixture ideas:
- Missing `provenance/*.prov.json`
- Missing / empty license in metadata
- Invalid GeoJSON geometry (self-intersections, bad coordinate order, etc.)
- Catalog JSON that violates schema (missing required fields)
- â€œRestrictedâ€ dataset requested by an unauthorized role (policy tests)

Place these either:
- inside `datasets/<slug>/` with a loud `README.md`, or
- in `datasets/_invalid/<slug>/` (if you prefer strict separation)

---

## ğŸ§ª Using fixtures in tests

### ğŸ Python (API / pipelines)

Most API tests load fixture files directly from disk and (depending on test type):
- seed a temporary DB, **or**
- use in-memory repositories/mocks, **or**
- validate outputs of pipeline steps.

```python
from pathlib import Path
import json

FIXTURES = Path(__file__).resolve().parents[2] / "fixtures"

def load_json(rel_path: str) -> dict:
    return json.loads((FIXTURES / rel_path).read_text(encoding="utf-8"))
```

Example patterns:
- Load a processed GeoJSON fixture â†’ insert into PostGIS (or use a stub repo)
- Call an endpoint via FastAPI TestClient â†’ compare response to `api/responses/*.json`

### ğŸ³ Running tests in the Docker dev stack

If youâ€™re using the compose-based dev environment, itâ€™s common to run:

```bash
docker-compose exec api pytest
```

> [!TIP]
> CI-style policy checks may also be runnable locally (e.g., via Conftest) â€” use them to catch missing metadata early.

---

## ğŸ›¡ï¸ Policy fixtures

If the repo includes OPA/Rego policies (recommended for KFM governance), keep tiny JSON inputs + expected decisions here.

Suggested approach:
- `policy/allow/*.json` â†’ should evaluate to **allow**
- `policy/deny/*.json` â†’ should evaluate to **deny**

Scenario ideas to cover:
- `accessLevel: "Restricted"` + mismatched `ownerGroup`
- dataset marked `status: "withdrawn"`
- â€œsanitized responseâ€ decisions (masking coordinates instead of returning 403)

---

## ğŸ§¼ Fixture hygiene rules

> [!IMPORTANT]
> Keep fixtures **small**, **boring**, and **stable** â€” the point is deterministic tests, not realism at scale.

- âœ… Commit only tiny artifacts
- âœ… Prefer open formats (GeoJSON/CSV/JSON)
- âœ… Avoid anything that could be PII or sensitive
- âœ… If you must reference â€œlarge dataâ€, store a **pointer + hash** (donâ€™t commit the blob)
  - (Optional) Use DVC or a remote artifact store if the project supports it

---

## ğŸ”— Handy links (repo-relative)

- `../../docs/` â€” architecture + narrative docs
- `../../pipelines/` â€” ETL scripts that generate `processed/` + metadata
- `../../policy/` â€” governance rules (OPA/Rego, AI guardrails, etc.)
- `../../api/` â€” FastAPI backend + tests
- `../../web/` â€” React + TypeScript frontend