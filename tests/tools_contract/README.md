# ğŸ§ª Tools Contract Tests â€” KFM (Kansas Frontier Matrix)

![Contract-First](https://img.shields.io/badge/Contract--First-%E2%9C%85-brightgreen)
![Provenance-First](https://img.shields.io/badge/Provenance--First-STAC%20%2B%20DCAT%20%2B%20PROV-blue)
![Policy-Pack](https://img.shields.io/badge/Policy%20Pack-OPA%20%2B%20Conftest-purple)
![Fail-Closed](https://img.shields.io/badge/Fail--Closed-%F0%9F%9B%A1%EF%B8%8F-critical)

> **Goal:** ensure every â€œtoolâ€ in KFM (API, pipeline CLI, agent action, Story Node renderer, artifact fetcher) stays **schema-stable**, **policy-compliant**, **traceable**, and **safe** â€” before it reaches the UI or Focus Mode.

---

## ğŸ§­ Why this folder exists

KFM is built around **contract-first + provenance-first** delivery:  
anything that appears in the **UI** (Map/Timeline/Story Nodes/AR) or **Focus Mode** must be **traceable back to cataloged evidence** and compliant with the **Policy Pack**.

This directory (`tests/tools_contract/`) provides **contract tests** that fail fast when a tool:

- breaks input/output schemas ğŸ”§
- bypasses the evidence triplet (**STAC + DCAT + PROV**) ğŸ§¾
- leaks sensitive locations or violates role-based access ğŸ”
- returns â€œpretty textâ€ without citations/evidence ğŸ§ 
- changes behavior without versioning (breaking changes) ğŸš¨
- drifts from the API boundary rule (UI must call APIs, not raw data) ğŸ§±

---

## ğŸ§© What counts as a â€œtoolâ€ in KFM?

A **tool** is any callable surface that produces or transforms KFM knowledge/data:

### ğŸŒ Runtime tools
- **REST endpoints** (FastAPI) â†’ `GET /datasets/{id}`, `GET /api/v1/query?table=...`, etc.
- **GraphQL resolvers** â†’ graph traversals over Neo4j (People/Places/Events/Datasets)
- **Map services** â†’ GeoJSON, vector tiles, 3D Tiles (MapLibre + Cesium client expectations)
- **Focus Mode retrieval actions** â†’ hybrid RAG + graph queries + spatial queries

### ğŸ—ï¸ Build / pipeline tools
- ingestion pipelines â†’ produce **STAC/DCAT/PROV** + load into PostGIS/Neo4j
- simulation/model runs â†’ treated as evidence with uncertainty (still cataloged + provable)
- â€œwatchersâ€ (real-time/ETL) â†’ GTFS-RT / sensor feeds with idempotency & caching headers
- artifact packaging â†’ OCI artifacts (tiles/models/packs), signed + addressable by digest

### ğŸ§¾ Narrative tools
- Story Node authoring/rendering â†’ `markdown.json` templates + evidence manifests
- evidence pack builders â†’ compile datasets/docs into reproducible narrative bundles

---

## ğŸ§± Contract layers (what we test)

```mermaid
flowchart TB
  subgraph Clients
    UI[ğŸ–¥ï¸ UI: Map / Timeline / Story Nodes / AR]
    FM[ğŸ§  Focus Mode]
  end

  subgraph Tools
    API[ğŸŒ FastAPI REST + GraphQL]
    CLI[ğŸ› ï¸ Pipelines / CLIs / Watchers]
    AG[ğŸ¤– Agents: WPE actions]
  end

  subgraph Evidence
    STAC[ğŸ—ºï¸ STAC<br/>spatiotemporal assets]
    DCAT[ğŸ“¦ DCAT<br/>dataset catalog]
    PROV[ğŸ§¬ PROV-O<br/>lineage & activities]
  end

  subgraph Guardrails
    SCHEMA[ğŸ“ JSON Schema / Pydantic / OpenAPI / GraphQL snapshots]
    POLICY[ğŸ›¡ï¸ Policy Pack<br/>OPA/Rego + Conftest]
  end

  UI --> API
  FM --> API
  CLI --> Evidence
  API --> Evidence

  Evidence --> SCHEMA
  Evidence --> POLICY
  API --> POLICY
  CLI --> POLICY
  AG --> POLICY
```

âœ… **Contract tests** enforce each layer stays aligned.

---

## ğŸ“ Expected layout

> Some of these folders may be â€œscaffolding targetsâ€ if weâ€™re still building the suite. The README defines the *intended contract testing architecture*.

```text
ğŸ§ª tests/
  ğŸ§° tools_contract/
    ğŸ“„ README.md

    ğŸ“¦ contracts/                  # versioned tool contracts (schema + invariants)
      ğŸ§¾ tools/
        focus_mode.answer.v1.json
        api.datasets.get.v1.json
        map.viewport.features.v1.json
        ingest.run_manifest.v1.json

    ğŸ§ª cases/                      # concrete â€œknown-goodâ€ and â€œknown-badâ€ fixtures
      focus_mode/
        happy_path.yaml
        missing_citations.yaml
      stac/
        collection_valid.json
        item_missing_bbox.json

    ğŸ§° harness/                    # shared test harness + validators
      validators/
        geojson.py
        stac.py
        dcat.py
        prov.py
        oci.py
        policy_pack.py
      runners/
        http_runner.py
        cli_runner.py

    ğŸ“¸ snapshots/                  # golden snapshots (break-glass updates only)
      openapi.json
      graphql_schema.graphql
      tool_registry.json
```

---

## âš¡ Quickstart

### âœ… Mode A â€” â€œOfflineâ€ contracts (no services required)
Runs schema + fixture validations (fast, deterministic):

```bash
pytest -q tests/tools_contract -m contract
```

### ğŸŒ Mode B â€” â€œIntegrationâ€ contracts (calls a running KFM)
Set a base URL and optional auth token:

```bash
export KFM_API_BASE_URL="http://localhost:8000"
export KFM_AUTH_TOKEN="..."
pytest -q tests/tools_contract -m contract_integration
```

### ğŸ›¡ï¸ Policy Pack (OPA/Rego + Conftest)
If the repo includes a policy bundle, run policy gates explicitly:

```bash
conftest test <policy_bundle_dir> -d <data_dir_or_fixture_dir>
```

> ğŸ’¡ Tip: treat policy tests as **non-negotiable gates**. If a tool output violates policy, we fail **closed**.

---

## ğŸ§· Core invariants (non-negotiables)

These are the â€œalways trueâ€ rules across KFM tools.

### 1) ğŸ“ Schema-valid I/O
- Requests must validate (types, ranges, required fields)
- Responses must validate (shape + required metadata blocks)
- Errors must be structured, not â€œmystery stringsâ€

### 2) ğŸ§¾ Evidence triplet required for anything user-facing
- **STAC** for spatial/temporal indexing + assets
- **DCAT** for dataset discovery, licensing, distributions
- **PROV** for lineage (â€œhow was this produced?â€)

### 3) ğŸ§¬ Provenance-first (traceability)
- Every response that returns data must include:
  - dataset identifiers and versions
  - provenance references (links/IDs to PROV activities/entities)
  - timestamps where relevant (especially for real-time queries)

### 4) ğŸ” Sensitivity & access control
- Sensitive layers must be:
  - hidden unless authorized, **or**
  - spatially generalized/redacted, **or**
  - returned only as aggregated summaries
- Role-based access must be enforceable at tool boundaries (API/pipeline outputs)

### 5) ğŸ§  Focus Mode must cite evidence
- Answers must include citations pointing back to cataloged sources  
- If evidence is insufficient â†’ response must say so and avoid fabrication

### 6) ğŸ§± API boundary rule (UI must not bypass governance)
- UI consumes **tools/APIs**, not raw files
- Anything shown on the map/timeline/story should be produced via governed interfaces

### 7) ğŸ§° Versioning & compatibility
- Contracts are versioned (`v1`, `v2`, â€¦)
- Breaking changes require new version + migration path
- Old versions stay supported until formally deprecated

---

## ğŸ§° Tool-type playbooks (what we test per category)

| Tool category | Examples | Contract checks âœ… |
|---|---|---|
| ğŸ—ºï¸ Map / Spatial query tools | â€œfeatures in bboxâ€, â€œlayer tilesâ€, â€œplace geometryâ€ | GeoJSON validity, CRS conventions, bbox filters, pagination, performance caps |
| ğŸ§¬ Catalog tools | STAC Collections/Items, DCAT dataset records | required fields, links across STACâ†”DCATâ†”PROV, license + distributions |
| ğŸ•¸ Graph tools | Neo4j traversals (â€œdatasets related to event Xâ€) | stable entity IDs, pagination, depth limits, ontology alignment fields |
| ğŸ§  Focus Mode tools | hybrid retrieval + summarization | evidence required, citations present, provenance references, safety redaction |
| ğŸ—ï¸ Pipeline tools | ingest, transform, tile build | Run Manifest schema, idempotency keys, output catalogs updated, policy pack passes |
| ğŸ“¦ Artifact tools | OCI packs (tiles/models/evidence packs) | digest addressability, signature presence, manifest fields, reproducibility metadata |
| ğŸ§¾ Story Node tools | `markdown.json` story nodes | schema + required narrative fields, evidence manifest presence, linkable entity IDs |

---

## ğŸ§ª What we assert (examples)

### âœ… Example: API tool returning geometry
- returns **FeatureCollection**
- each feature has `id` and `properties.dataset_id`
- geometry is valid & non-empty
- provenance block includes references to DCAT/PROV
- respects sensitivity flags

### âœ… Example: Focus Mode answer tool
We do **not** snapshot the exact prose. We snapshot the **structure**:

- `answer_md` exists
- `citations[]` exists and is non-empty
- citations point to catalog IDs (dataset/doc IDs)
- if the user lacks access â†’ tool returns redacted/denied with clear reason

### âœ… Example: pipeline run manifest tool
- produces a run manifest with:
  - `run_id`
  - `run_time`
  - `idempotency_key`
  - `canonical_digest`
  - `source_urls[]`
  - `tool_versions[]`
  - summary counts (records in/out/errors)
- digest is stable under canonicalization (RFC 8785-style normalization)

---

## ğŸ§  Handling LLM / non-determinism (Focus Mode)

To keep tests stable:

âœ… assert **invariants**, not exact tokens  
âœ… assert **citations & provenance**  
âœ… assert **redaction & safety behavior**  
âœ… assert **tool call trace** (when available)

Avoid:
- golden snapshots of the full natural-language answer text âŒ
- tests that depend on external network calls (unless explicitly flagged) âŒ

---

## ğŸ›¡ï¸ Security & governance contracts (recommended â€œred teamâ€ set)

These contract tests catch high-risk failures:

- ğŸ” **Injection-safe query surfaces** (search inputs sanitized / validated)
- ğŸ§­ **GraphQL depth & pagination limits** (no runaway recursion)
- ğŸ—ºï¸ **Sensitive location leakage** (exact coordinates never returned when restricted)
- ğŸ” **Role enforcement** (unauthorized calls fail closed with consistent errors)
- ğŸ§¾ **License compliance** (no dataset without license/distribution metadata is published)
- ğŸ§¬ **Audit trail completeness** (PROV always links â€œwhat came from whatâ€)

---

## ğŸ”„ Updating contracts (breaking changes)

1. Create `v2` contract file (donâ€™t silently mutate `v1`)
2. Add migration notes + fixtures demonstrating both versions
3. Keep `v1` tests passing until deprecation window ends
4. Update snapshots only via deliberate â€œbreak-glassâ€ PR (review required)

---

## âœ… PR checklist (for anything touching tools)

- [ ] Contract schema updated (or new version added) ğŸ§¾
- [ ] Cases added: happy + failure + edge cases ğŸ§ª
- [ ] Policy Pack still passes ğŸ›¡ï¸
- [ ] Evidence triplet preserved (STAC/DCAT/PROV) ğŸ“¦
- [ ] No sensitive data leakage ğŸ”
- [ ] OpenAPI / GraphQL / registry snapshots updated only when intended ğŸ“¸
- [ ] CI green âœ…

---

## ğŸ“š Reference docs (project design ground truth)

These docs define the expectations that contract tests enforce:

- ğŸ§­ **Architecture & features**: KFM comprehensive architecture + redesign blueprint
- ğŸ§  **AI system**: Focus Mode, hybrid retrieval, provenance + citations
- ğŸ—ºï¸ **UI system**: Map/Timeline/Story Nodes/AR, API-boundary rule
- ğŸ§¾ **Data intake**: evidence triplet (STAC/DCAT/PROV), policy pack, publishing gates
- ğŸŒŸ **Future proposals & QA**: testing strategy, security, watchers, governance
- ğŸ§ª **Markdown & Story Nodes**: `markdown.json` templates + narrative invariants
- ğŸ“¦ **Innovations**: OCI evidence packs, run manifests, pulse threads, attention nodes

> ğŸ“Œ If youâ€™re unsure what a tool *should* do, the answer is almost always: **find the contract**, then align implementation to it.

---

## ğŸ“¦ Appendix: PDF Portfolio â€œlibrariesâ€ (deep references)

Some project knowledge lives in **PDF Portfolios** (they contain multiple embedded books/docs). If you want to explore or extract them:

```bash
# List embedded docs
pdfdetach -list "docs/library/AI Concepts & more.pdf"
pdfdetach -list "docs/library/Maps-GoogleMaps-VirtualWorlds-Archaeological-Computer Graphics-Geospatial-webgl.pdf"
pdfdetach -list "docs/library/Data Managment-Theories-Architures-Data Science-Baysian Methods-Some Programming Ideas.pdf"
pdfdetach -list "docs/library/Various programming langurages & resources 1.pdf"

# Extract a specific embedded PDF (example: file #27)
pdfdetach -save 27 -o /tmp/clean_architecture_python.pdf "docs/library/Data Managment-Theories-Architures-Data Science-Baysian Methods-Some Programming Ideas.pdf"
```

These libraries inform:
- âœ… testing philosophy (CI/CD, clean architecture, contract boundaries)
- âœ… geospatial correctness (CRS/projections, map math, rendering constraints)
- âœ… AI/agent grounding (agent architecture, retrieval + evaluation patterns)
- âœ… polyglot implementation (TypeScript/Python/Node tooling and testing)

---
