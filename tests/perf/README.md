# ğŸš€ Performance & Load Testing (`tests/perf/`)

![Status](https://img.shields.io/badge/status-active%20development-orange)
![Type](https://img.shields.io/badge/tests-performance%20%26%20load-blue)
![Principle](https://img.shields.io/badge/principle-provenance--first-6f42c1)
![Target](https://img.shields.io/badge/targets-API%20%7C%20DB%20%7C%20ETL%20%7C%20AI-success)

This folder is the **repeatable performance test harness** for **Kansas-Matrix-System** (aligned to the KFM architecture): a **pipeline â†’ catalog â†’ databases â†’ API â†’ UI** platform where *all user access goes through the backend API + governance policies*. âœ…

> [!IMPORTANT]
> These tests are meant to be **reproducible** and **comparable over time** (same scenario, same dataset size, same config â†’ comparable outputs).  
> Think â€œCI-friendly perf guardrailsâ€ + â€œdeep-dive profiling runbookâ€.

---

## ğŸ§­ Table of Contents

- [ğŸ¯ Goals](#-goals)
- [ğŸ“ What We Measure](#-what-we-measure)
- [ğŸ§ª Test Types](#-test-types)
- [ğŸ—‚ï¸ Folder Layout](#ï¸-folder-layout)
- [âš¡ Quickstart](#-quickstart)
- [ğŸƒ Running Scenarios](#-running-scenarios)
- [ğŸ“ˆ Results, Baselines, Regression Rules](#-results-baselines-regression-rules)
- [ğŸ§° Profiling & Debugging Runbook](#-profiling--debugging-runbook)
- [ğŸ§© Adding a New Perf Test](#-adding-a-new-perf-test)
- [ğŸ¤– CI / Automation](#-ci--automation)
- [ğŸ§± Safety & Guardrails](#-safety--guardrails)

---

## ğŸ¯ Goals

### âœ… What â€œgoodâ€ looks like
- **Interactive** UX: map + search + story navigation feel fast.
- **Scalable** backend: query latency stays stable under concurrency.
- **Governed** access: policy enforcement doesnâ€™t silently become the bottleneck.
- **Provenance-aligned** runs: every perf run records *what* was tested, *against which dataset*, *with what settings*, and *from which git commit*.

### ğŸ§¨ What we want to catch early
- Latency regressions (p95/p99 spikes)
- Throughput collapses under load
- Memory leaks / container OOMs
- Slow database queries introduced by schema/index changes
- External adapter calls that block the system (timeouts / slow retries)
- AI â€œFocus Modeâ€ latency explosions (retrieval + generation)

---

## ğŸ“ What We Measure

> [!NOTE]
> We aim to measure **end-to-end** (client â†’ API â†’ DB/policy/AI â†’ response) *and* **component-level** (PostGIS vs Neo4j vs Search vs OPA vs Ollama) so regressions are diagnosable.

| Layer ğŸ”© | What we measure ğŸ“ | Examples |
|---|---|---|
| ğŸŒ API (FastAPI) | p50/p95/p99 latency, RPS, error rate | `/datasets`, `/search?q=...`, `/features/{id}`, `/graphql` queries |
| ğŸ§­ Policy (OPA) | decision latency, cache hit ratio | auth checks, content rules, â€œfail closedâ€ behavior |
| ğŸ—ºï¸ PostGIS | query latency, planning time, rows scanned | point-in-polygon, distance, clustering, bounding box queries |
| ğŸ§  Neo4j | traversal latency, query plan regressions | story graph navigation, entity linking queries |
| ğŸ” Search index | query latency, recall/latency tradeoffs | keyword search, embeddings similarity search |
| ğŸ§± Pipelines/ETL | ingest time, CPU/mem, IO throughput | raw â†’ processed, metadata/provenance generation |
| ğŸ¤– AI (Ollama) | time-to-first-token, tokens/sec, end-to-end answer time | retrieval + prompt build + generate |

---

## ğŸ§ª Test Types

### 1) ğŸŸ¢ Smoke (CI-friendly)
Fast checks that run on PRs:
- small dataset
- low concurrency
- short duration (30â€“90s)
- detects *obvious* regressions

### 2) ğŸŸ¡ Local â€œQuickâ€
Developer loop:
- run before/after a change
- compare to a local baseline

### 3) ğŸ”µ Nightly / Full Suite
Bigger dataset + more scenarios:
- heavier concurrency
- captures â€œreal-ishâ€ performance curves

### 4) ğŸŸ£ Soak
Long-running stability:
- leaks, GC pressure, connection pool issues
- cache drift or unbounded growth

### 5) ğŸª¶ Edge Profile (low-resource)
Targets offline/community deployments:
- constrained CPU/RAM
- validates â€œstill usableâ€ budgets

---

## ğŸ—‚ï¸ Folder Layout

> [!TIP]
> Keep perf assets **scenario-driven** and **data-versioned**. Results are artifacts: store locally or in CI artifacts, not in git.

```text
tests/perf/
â”œâ”€â”€ README.md                # ğŸ‘ˆ you are here
â”œâ”€â”€ scenarios/               # ğŸ§ª scenario definitions (yaml/json)
â”‚   â”œâ”€â”€ smoke.yaml
â”‚   â”œâ”€â”€ api_read_heavy.yaml
â”‚   â”œâ”€â”€ spatial_queries.yaml
â”‚   â”œâ”€â”€ graph_traversal.yaml
â”‚   â””â”€â”€ ai_focus_mode.yaml
â”œâ”€â”€ workloads/               # ğŸ‹ï¸ load generators (choose one â€œdefaultâ€)
â”‚   â”œâ”€â”€ locust/              # âœ… recommended (Python ecosystem)
â”‚   â”‚   â”œâ”€â”€ locustfile.py
â”‚   â”‚   â””â”€â”€ user_flows.py
â”‚   â””â”€â”€ k6/                  # optional (JS load testing)
â”‚       â””â”€â”€ api.js
â”œâ”€â”€ datasets/                # ğŸ“¦ dataset manifests + seeds (NO giant blobs)
â”‚   â”œâ”€â”€ manifests/
â”‚   â”‚   â”œâ”€â”€ small.json
â”‚   â”‚   â”œâ”€â”€ medium.json
â”‚   â”‚   â””â”€â”€ large.json
â”‚   â””â”€â”€ seeds/
â”‚       â”œâ”€â”€ postgis.sql
â”‚       â”œâ”€â”€ neo4j.cypher
â”‚       â””â”€â”€ search_index.jsonl
â”œâ”€â”€ scripts/                 # ğŸ› ï¸ orchestrators + helpers
â”‚   â”œâ”€â”€ up.sh
â”‚   â”œâ”€â”€ seed.sh
â”‚   â”œâ”€â”€ run.sh
â”‚   â”œâ”€â”€ report.py
â”‚   â””â”€â”€ diff.py
â”œâ”€â”€ docker/                  # ğŸ³ perf overlay compose (optional)
â”‚   â””â”€â”€ docker-compose.perf.yml
â””â”€â”€ results/                 # ğŸ“ˆ output artifacts (gitignored)
    â””â”€â”€ .gitkeep
```

---

## âš¡ Quickstart

### âœ… Prereqs
- Docker + Docker Compose
- The project dev stack (API + DBs) runs via compose
- (Optional for AI scenarios) **Ollama** installed and running locally, or containerized in the perf stack

> [!WARNING]
> Performance numbers are only meaningful when the machine is not overloaded. Close â€œheavy stuffâ€ (VMs, builds, video calls), and avoid running other benchmarks concurrently.

---

### 1) ğŸ³ Start the stack

**Option A â€” Use the main dev compose**
```bash
docker-compose up --build
```

**Option B â€” Use a perf overlay compose**
```bash
docker-compose -f docker-compose.yml -f tests/perf/docker/docker-compose.perf.yml up --build
```

> [!NOTE]
> Common local ports (defaults) you may need free:
> - Postgres/PostGIS: `5432`
> - Neo4j HTTP: `7474`  | Bolt: `7687`
> - FastAPI: `8000`
> - React dev server: `3000`

---

### 2) ğŸŒ± Seed the perf dataset

```bash
bash tests/perf/scripts/seed.sh --dataset small
```

Expected behavior:
- Initializes PostGIS + Neo4j (and search index if enabled)
- Loads a **known dataset manifest**
- Leaves the system in a â€œready to benchmarkâ€ state

---

### 3) ğŸŸ¢ Run smoke

```bash
bash tests/perf/scripts/run.sh smoke
```

---

## ğŸƒ Running Scenarios

### Scenario philosophy
A **scenario** is a named workload with:
- a dataset manifest (small/medium/large)
- request mix (read-heavy vs write-heavy vs mixed)
- concurrency + duration
- pass/fail budgets (optional but recommended)

Example command patterns:

```bash
# Run a named scenario
bash tests/perf/scripts/run.sh api_read_heavy --dataset small --duration 60s --users 20

# Run the full suite locally (takes longer)
bash tests/perf/scripts/run.sh suite --dataset medium
```

### Minimal environment variables (suggested)
```bash
export PERF_BASE_URL="http://localhost:8000"
export PERF_RESULTS_DIR="tests/perf/results"
export PERF_DATASET="small"
export PERF_DURATION="60s"
export PERF_USERS="20"
```

---

## ğŸ“ˆ Results, Baselines, Regression Rules

### Where results go
Each run should create a unique run folder:
```text
tests/perf/results/
â””â”€â”€ 2026-01-30T203500Z__api_read_heavy__small__git-abc123/
    â”œâ”€â”€ run.json              # ğŸ§¾ provenance: commit, host, dataset, config
    â”œâ”€â”€ summary.json          # ğŸ“Œ p50/p95/p99, rps, errors
    â”œâ”€â”€ timeseries.csv        # ğŸ“‰ optional timeline (latency over time)
    â”œâ”€â”€ locust_stats.csv      # ğŸ‹ï¸ raw generator output
    â””â”€â”€ report.html           # ğŸ§  human-friendly output
```

### Baselines (recommended)
Store baselines outside git, or in CI artifacts:
- `baseline/main` (nightly)
- `baseline/release/*` (tagged releases)

### Regression rules (starter template)
> [!TIP]
> Pick budgets that match your reality. Start loose, then tighten.

- âŒ **Fail** if error rate > `0.5%`
- âŒ **Fail** if p95 latency worsens by > `25%` vs baseline
- âŒ **Fail** if p99 latency worsens by > `35%` vs baseline
- âš ï¸ **Warn** if RPS drops by > `15%` vs baseline

---

## ğŸ§° Profiling & Debugging Runbook

### Step 0: Verify youâ€™re not benchmarking your laptopâ€™s chaos ğŸ˜…
- Docker has enough memory allocated
- No port conflicts
- No container restarts (OOM / crash loops)
- Your dataset seed finished successfully

### Step 1: Identify *which* layer regressed
Use the scenario mix to isolate:
- API only (no AI)
- DB-heavy (spatial + graph)
- Policy-heavy (OPA checks)
- AI-heavy (Focus Mode)

### Step 2: Collect the â€œstandard bundleâ€
**Always attach these when reporting perf regressions:**
- `run.json`, `summary.json`
- docker compose logs
- DB query plans for the top slow queries
- container stats (CPU/mem)

Example:
```bash
docker-compose logs --no-color > tests/perf/results/latest/docker-logs.txt
docker stats --no-stream > tests/perf/results/latest/docker-stats.txt
```

### Step 3: Database triage
- PostGIS: use `EXPLAIN (ANALYZE, BUFFERS)` on slow queries
- Neo4j: inspect query plan + indexes
- Search: check slow query logs and shard/refresh settings

### Step 4: Policy triage (OPA)
- Measure decision time (cold vs warm)
- Ensure policy evaluations are cached where safe
- Confirm â€œfail closedâ€ behavior didnâ€™t introduce retry storms

### Step 5: AI triage (Ollama)
- Confirm model choice is intentional (`OLLAMA_MODEL`)
- Record:
  - time-to-first-token
  - tokens/sec
  - context size (prompt length)
- Beware: larger models can require **significantly more RAM/VRAM**.

---

## ğŸ§© Adding a New Perf Test

1) Add a scenario file:
```text
tests/perf/scenarios/my_new_scenario.yaml
```

2) Implement the workload flow:
- add a Locust task set under `tests/perf/workloads/locust/`

3) Add/extend dataset seed artifacts if needed:
- `tests/perf/datasets/seeds/*`

4) Make sure the run produces:
- `run.json` (provenance metadata)
- `summary.json` (KPIs)

5) Add it to:
- smoke suite (if fast)
- nightly suite (if heavier)

---

## ğŸ¤– CI / Automation

> [!NOTE]
> Typical setup:
> - PRs: run `smoke`
> - Nightly: run `full suite` + upload artifacts
> - Release: run `large dataset` + publish baseline

Suggested workflow files (not in this folder):
```text
.github/workflows/perf-smoke.yml
.github/workflows/perf-nightly.yml
```

---

## ğŸ§± Safety & Guardrails

- âœ… Prefer running against **local compose** or a dedicated staging environment
- âŒ Donâ€™t point load tests at production without explicit approval
- âœ… External adapters should default to **mocks** in perf runs (avoid quotas/ToS issues)
- âœ… Keep results **out of git** (store as CI artifacts)

---

### ğŸ Done
If you can run `smoke` locally and it produces `run.json` + `summary.json`, youâ€™ve got a working perf harness foundation âœ…