# ğŸ§ª `tests/python/` â€” Python Test Suite (Kansas Frontier Matrix)

![Python](https://img.shields.io/badge/Python-3.10%2B-3776AB?logo=python&logoColor=white)
![pytest](https://img.shields.io/badge/pytest-ready-0A9EDC?logo=pytest&logoColor=white)
![Geo](https://img.shields.io/badge/geospatial-STAC%2FDCAT%2FPROV-2E7D32)
![Quality](https://img.shields.io/badge/quality-provenance--first-6A1B9A)

> [!NOTE]
> This README lives at **`tests/python/README.md`** and documents **how we test the Python parts** of the Kansas Frontier Matrix (KFM): API services, ETL/pipelines, metadata catalogs, and graph integrations.

---

## ğŸ”— Quick links

- [ğŸš€ Quickstart](#-quickstart)
- [ğŸ—ï¸ What we test](#ï¸-what-we-test)
- [ğŸ—‚ï¸ Folder layout](#ï¸-folder-layout)
- [ğŸ§ª Running tests](#-running-tests)
- [ğŸ“¦ Test data rules](#-test-data-rules)
- [ğŸ—ºï¸ Geospatial assertions](#ï¸-geospatial-assertions)
- [ğŸ§¾ Metadata & provenance tests](#-metadata--provenance-tests)
- [ğŸ•¸ï¸ Graph tests](#ï¸-graph-tests)
- [âš¡ Performance & scale checks](#-performance--scale-checks)
- [ğŸ” Security checks](#-security-checks)
- [âœ… PR â€œDefinition of Doneâ€](#-pr-definition-of-done)
- [ğŸ“š Project reference shelf](#-project-reference-shelf)

---

## ğŸš€ Quickstart

### 1) Install deps + run unit tests

From **repo root**:

```bash
python -m venv .venv
source .venv/bin/activate   # Windows: .venv\Scripts\activate
python -m pip install -U pip

# Choose ONE depending on project packaging:
pip install -r requirements.txt
# or:
pip install -e ".[dev]"

pytest -q tests/python
```

> [!TIP]
> If youâ€™re not sure which install path is correct, search for **`pyproject.toml`** vs **`requirements.txt`** at repo root and follow that.

---

### 2) (Optional) Bring up the integration stack (PostGIS / Neo4j / etc.)

If the repo includes Docker Compose:

```bash
docker compose up -d
pytest -q -m "integration or contract" tests/python
```

> [!IMPORTANT]
> Integration tests should **never** require production credentials. They must run against **ephemeral** local containers or safe mocks.

---

## ğŸ—ï¸ What we test

KFM is a â€œdata + graph + geospatial + webâ€ system. Python tests cover these categories:

### âœ… Unit tests (fast, deterministic)
- Pure functions, parsing, transforms, math, geometry utilities
- â€œDomain logicâ€ correctness (no DB, no network)

### ğŸ”„ Integration tests (real services, still deterministic)
- API routes (FastAPI) + DB adapters (PostGIS)
- Knowledge graph adapters (Neo4j)
- Pipelines that touch the filesystem and validated small fixtures

### ğŸ“¦ Contract + schema tests
- OpenAPI schemas (API contract snapshots)
- STAC/DCAT/PROV conformance & cross-link integrity
- â€œData is codeâ€ checks: schema, invariants, logical consistency

### ğŸ§± Pipeline regression tests
- Golden outputs on tiny sample inputs
- â€œNo regression in known metricsâ€ checks for key aggregates & counts

### ğŸ§¨ Negative / adversarial tests
- Bad inputs, malformed GeoJSON, invalid CRS, missing metadata
- Security-related cases (injection attempts, path traversal, unsafe filenames)

---

## ğŸ§  Test philosophy

KFMâ€™s credibility depends on **traceability** and **reproducibility**:

- **Provenance-first** ğŸ”  
  Every dataset/evidence artifact should be traceable end-to-end: raw inputs â†’ pipeline run â†’ processed outputs â†’ catalog + graph references.

- **Scientific rigor** ğŸ§ª  
  For modeling/simulation/analytics code, tests must document:
  - assumptions
  - validation approach
  - expected operating range
  - uncertainty/tolerance

- **Deterministic by default** ğŸ¯  
  If a test is flaky, we treat it as a bug.

- **â€œData is a first-class citizenâ€** ğŸ—ƒï¸  
  We test data schemas and metadata just like we test functions.

---

## ğŸ§± Architecture â†” test placement

```mermaid
flowchart TB
  subgraph Domain["ğŸ§  Domain Layer"]
    D[Entities â€¢ Value Objects â€¢ Pure logic]
  end
  subgraph Service["ğŸ”§ Service Layer"]
    S[Use-cases â€¢ Orchestration â€¢ Policies]
  end
  subgraph Integration["ğŸ”Œ Integration Layer"]
    I[Adapters: PostGIS â€¢ Neo4j â€¢ APIs â€¢ Files]
  end

  Domain --> Service --> Integration

  U["âœ… Unit tests"] --> Domain
  IT["ğŸ”„ Integration tests"] --> Service
  CT["ğŸ“¦ Contract + data tests"] --> Integration
```

---

## ğŸ—‚ï¸ Folder layout

> [!NOTE]
> Your repo may already have a structure. This shows the **intended** convention inside `tests/python/`.

```text
tests/
â””â”€â”€ python/
    â”œâ”€â”€ README.md              ğŸ‘ˆ you are here
    â”œâ”€â”€ conftest.py            ğŸ§© shared fixtures (db, tmp dirs, seeds)
    â”œâ”€â”€ unit/                  âœ… pure logic tests
    â”œâ”€â”€ integration/           ğŸ”„ DB/graph/API integration
    â”œâ”€â”€ contract/              ğŸ“¦ OpenAPI + schema checks
    â”œâ”€â”€ data/                  ğŸ—ƒï¸ tiny fixtures + golden outputs
    â”‚   â”œâ”€â”€ fixtures/
    â”‚   â”œâ”€â”€ golden/
    â”‚   â””â”€â”€ schemas/
    â”œâ”€â”€ property/              ğŸ² Hypothesis / property-based tests (optional)
    â”œâ”€â”€ perf/                  âš¡ benchmarks (usually off by default)
    â””â”€â”€ security/              ğŸ” security-focused tests (optional)
```

---

## ğŸ§ª Running tests

### Run everything (Python)
```bash
pytest tests/python
```

### Run fast unit tests only
```bash
pytest -q -m "unit" tests/python
```

### Run integration tests only
```bash
pytest -q -m "integration" tests/python
```

### Exclude slow tests
```bash
pytest -q -m "not slow" tests/python
```

### Coverage
```bash
pytest --cov --cov-report=term-missing tests/python
```

---

## ğŸ·ï¸ Markers & naming conventions

**Naming**
- Files: `test_*.py`
- Tests: `test_<behavior>_<expected_outcome>()`
- Prefer â€œbehavioralâ€ names over â€œimplementationâ€ names.

**Recommended markers**

| Marker | Purpose | Runs in CI? |
|---|---|---|
| `unit` | No DB, no network | âœ… yes |
| `integration` | Real containers / local services | âœ… yes (often) |
| `contract` | OpenAPI + schema validation | âœ… yes |
| `data` | Data invariants, golden outputs | âœ… yes |
| `slow` | Long-running / heavy | â›” usually nightly |
| `perf` | Benchmarks | â›” usually nightly |

> [!TIP]
> If you introduce markers, register them in `pytest.ini` or `pyproject.toml` so CI doesnâ€™t warn.

---

## ğŸ“¦ Test data rules

### âœ… Do
- Keep committed fixtures **tiny** (think KBâ€“a few MB).
- Prefer **synthetic** or **public-domain/clearly licensed** samples.
- Use deterministic generators for random data (fixed seeds).
- Store expected results as:
  - golden files (small)
  - snapshot JSON
  - summary stats (counts, hashes, bounds)

### âŒ Donâ€™t
- Donâ€™t commit large rasters/shapefiles unless explicitly allowed.
- Donâ€™t include sensitive locations, private data, credentials, or tokens.
- Donâ€™t make tests depend on a network call to a third-party API.

### ğŸ“Œ Large assets
If you truly need larger assets:
- Use **DVC** or **Git LFS** (and make CI fetch them explicitly).
- Gate them behind `-m slow` or `-m perf`.

---

## ğŸ—ºï¸ Geospatial assertions

Geospatial tests tend to fail for *subtle* reasons. Always assert the basics:

### Geometry validity
- GeoJSON parses âœ…
- `is_valid` âœ… (Shapely)
- No self-intersections for polygons (unless intentionally allowed)

### CRS + axis order
- Confirm CRS explicitly (avoid â€œassumed WGS84â€)
- Ensure transforms preserve bounds & orientation

### Bounds and extents
- Bounding box exists and is sane
- Lat/Lon ranges are valid (lat âˆˆ [-90, 90], lon âˆˆ [-180, 180]) when using EPSG:4326

### Raster sanity checks
- dtype + nodata
- expected value range
- pixel dimensions and geotransform consistency

> [!IMPORTANT]
> For numeric/geospatial code, prefer **tolerance-based** assertions (e.g., `abs(a-b) < eps`) to avoid brittle tests.

---

## ğŸ§¾ Metadata & provenance tests

KFM treats metadata as *required infrastructure*, not â€œnice-to-haveâ€.

### What we validate
- **STAC Collection + Item(s)** exist for assets
- **DCAT entry** exists for discovery
- **PROV bundle** exists for lineage
- Cross-links:
  - STAC â†’ actual data assets
  - DCAT â†’ STAC/distributions
  - PROV â†’ raw inputs â†’ intermediate â†’ output
  - Graph â†’ references catalog IDs (not bulky payloads)

### Typical checks
- JSON schema validation against project profiles
- Required fields present: title, description, license, spatial/temporal extent, provenance pointers
- â€œMetafile verificationâ€: if a data file is added/changed, the metadata must be updated too

---

## ğŸ•¸ï¸ Graph tests

If the repo uses Neo4j:

- Graph nodes should store **references** (STAC IDs, DOIs, stable catalog keys)
- Enforce constraints:
  - no orphan nodes without a catalog reference
  - no broken references (graph â†’ catalog must resolve)
  - relationship semantics are consistent (edge labels, direction, multiplicity)

> [!TIP]
> Keep graph tests fast by loading a tiny fixture graph or using an ephemeral container dataset.

---

## ğŸŒ API tests (FastAPI)

Common patterns:
- Use FastAPI test clients for request/response checks
- Assert:
  - status codes
  - JSON shape
  - error envelope (consistency matters)
- Add OpenAPI contract tests:
  - schema exists
  - key endpoints documented
  - snapshot diffs are reviewed intentionally

---

## âš¡ Performance & scale checks

Performance is a feature (especially for geospatial + graph + DB queries).

Recommended approach:
- Use benchmark tests (gated behind `-m perf`)
- Track:
  - query runtime budgets
  - memory spikes
  - â€œN growsâ€ behavior (small scaling checks)

> [!NOTE]
> Treat performance checks like scientific experiments: document hardware assumptions and keep comparisons fair.

---

## ğŸ” Security checks

Security lives in tests + CI gates:

- Dependency checks (known CVEs)
- Static analysis / linting
- Secrets scanning
- Negative tests for:
  - SQL injection vectors
  - path traversal in file handling
  - unsafe deserialization

> [!IMPORTANT]
> If you add a new external integration (API, scraper, connector), add a **security regression test** for its input validation.

---

## âœ… PR â€œDefinition of Doneâ€

Before merging a Python-facing PR:

- [ ] âœ… `pytest tests/python` passes locally
- [ ] âœ… Markers used appropriately (`unit` vs `integration`)
- [ ] âœ… New/changed data artifacts have updated STAC/DCAT/PROV
- [ ] âœ… Golden outputs updated intentionally (not accidentally)
- [ ] âœ… No flaky tests introduced
- [ ] âœ… No secrets in fixtures, logs, or snapshots
- [ ] âœ… CI passes (tests + linters + security scans)

---

## ğŸ§° Troubleshooting

**GDAL/PROJ install pain** ğŸ˜µâ€ğŸ’«  
- Prefer Docker/devcontainer workflows if available.
- Keep geospatial libs isolated in reproducible environments.

**Integration tests failing locally**  
- Verify containers are running (`docker compose ps`)
- Confirm env vars / connection strings point to **test** services

**Flaky numeric tests**  
- Fix seeds
- Relax tolerances where appropriate
- Assert invariants (bounds, monotonicity, conservation) instead of exact floats

---

## ğŸ“š Project reference shelf

> [!NOTE]
> This project includes a deep â€œreference libraryâ€ ğŸ“–. These documents inform our testing standards (scientific rigor, stats, geospatial correctness, scale, security, ethics).

<details>
<summary><strong>ğŸ“š Expand the full library list</strong></summary>

### ğŸ§ª Scientific rigor, modeling, statistics
- ğŸ“˜ **Scientific Modeling and Simulation_ A Comprehensive NASA-Grade Guide.pdf** â€” verification/validation mindset, reproducibility, uncertainty.
- ğŸ“— **Understanding Statistics & Experimental Design.pdf** â€” test design, sampling, power, avoiding misleading metrics.
- ğŸ“™ **think-bayes-bayesian-statistics-in-python.pdf** â€” probabilistic validation, posterior sanity checks.
- ğŸ“• **regression-analysis-with-python.pdf** â€” model evaluation, regression metrics, generalization checks.
- ğŸ“• **Regression analysis using Python - slides-linear-regression.pdf** â€” quick reference for regression workflows.

### ğŸ—ºï¸ Geospatial, mapping, remote sensing, 3D
- ğŸ—ºï¸ **python-geospatial-analysis-cookbook.pdf** â€” PostGIS/Geo workflows; good integration test inspiration.
- ğŸ§­ **making-maps-a-visual-guide-to-map-design-for-gis.pdf** â€” cartographic correctness; map output validation ideas.
- ğŸ“ **Mobile Mapping_ Space, Cartography and the Digital - 9789048535217.pdf** â€” mobile context; UX implications for map-derived outputs.
- ğŸ›°ï¸ **Cloud-Based Remote Sensing with Google Earth Engine-Fundamentals and Applications.pdf** â€” remote sensing pipelines; reproducible processing.
- ğŸ›ï¸ **Archaeological 3D GIS_26_01_12_17_53_09.pdf** â€” 3D GIS/heritage use cases; constraints + validation concerns.
- ğŸ® **webgl-programming-guide-interactive-3d-graphics-programming-with-webgl.pdf** â€” rendering pipelines; useful for future visual regression tests.
- ğŸ–¼ï¸ **compressed-image-file-formats-jpeg-png-gif-xbm-bmp.pdf** â€” raster formats & pitfalls (metadata, compression artifacts).

### ğŸ§± Data engineering, DB, scale
- ğŸ˜ **PostgreSQL Notes for Professionals - PostgreSQLNotesForProfessionals.pdf** â€” SQL/Postgres reference; query correctness tests.
- ğŸ“ˆ **Database Performance at Scale.pdf** â€” performance testing discipline; bottleneck hunting.
- ğŸ§¬ **Scalable Data Management for Future Hardware.pdf** â€” systems/performance thinking; concurrency and hardware implications.
- ğŸ§© **Data Spaces.pdf** â€” governance, interoperability, trust & security as cross-cutting â€œtestableâ€ concerns.

### ğŸ” Security & adversarial thinking
- ğŸ›¡ï¸ **ethical-hacking-and-countermeasures-secure-network-infrastructures.pdf** â€” threat modeling; defensive testing patterns.
- ğŸ **Gray Hat Python - Python Programming for Hackers and Reverse Engineers (2009).pdf** â€” adversarial mindset; input hardening tests.

### ğŸ§  Systems, autonomy, ethics, governance
- ğŸ¤ **Introduction to Digital Humanism.pdf** â€” transparency, accountability, algorithmic harms.
- âš–ï¸ **On the path to AI Lawâ€™s prophecies and the conceptual foundations of the machine learning age.pdf** â€” governance & compliance framing.
- ğŸ§« **Principles of Biological Autonomy - book_9780262381833.pdf** â€” systems thinking; â€œclosureâ€ and robustness metaphors.

### ğŸ§° Full-stack + general engineering references (compendiums)
- ğŸ“š **A programming Books.pdf**
- ğŸ“š **B-C programming Books.pdf**
- ğŸ“š **D-E programming Books.pdf**
- ğŸ“š **F-H programming Books.pdf**
- ğŸ“š **I-L programming Books.pdf**
- ğŸ“š **M-N programming Books.pdf**
- ğŸ“š **O-R programming Books.pdf**
- ğŸ“š **S-T programming Books.pdf**
- ğŸ“š **U-X programming Books.pdf**
- ğŸŒ **responsive-web-design-with-html5-and-css3.pdf** â€” front-end correctness (useful when Python outputs feed UI).

### ğŸ§­ Core KFM docs (project-specific)
- ğŸ§¾ **Kansas Frontier Matrix (KFM) â€“ Comprehensive Technical Documentation.pdf** â€” system architecture, CI expectations, testing scope.
- ğŸ§· **MARKDOWN_GUIDE_v13.md.gdoc** â€” markdown + metadata standards, STAC/DCAT/PROV rules.
- ğŸ§¬ **Scientific Method _ Research _ Master Coder Protocol Documentation.pdf** â€” documentation + reproducibility protocol.
- ğŸ—ºï¸ **Kansas-Frontier-Matrix_ Open-Source Geospatial Historical Mapping Hub Design.pdf** â€” pipeline + catalog + CI design patterns.
- ğŸ““ **Comprehensive Markdown Guide_ Syntax, Extensions, and Best Practices.docx** â€” doc/testing hygiene for markdown-heavy repos.
- ğŸ§  **Deep Learning for Coders with fastai and PyTorch - Deep.Learning.for.Coders.with.fastai.and.PyTorchpdf** â€” ML evaluation & reproducibility (if/when used).

</details>

---

ğŸ§­ **Goal:** Every PR leaves KFM more reliable, more reproducible, and more explainable. âœ…
