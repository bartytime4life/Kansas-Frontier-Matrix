# ğŸŒ Geo Test Suite (`tests/geo/`)

![tests](https://img.shields.io/badge/tests-geo%20suite-brightgreen)
![pytest](https://img.shields.io/badge/runner-pytest-blue)
![spatial](https://img.shields.io/badge/focus-CRS%20%7C%20topology%20%7C%20PostGIS-orange)

> ğŸ§­ **Intent:** Keep Kansas Frontier Matrix geospatial pipelines *correct, consistent, and explainable* â€” from CRS transforms to topology rules to metadata/provenance.

---

## ğŸ“Œ What lives here

This folder is dedicated to **geospatial correctness tests** that protect:

- ğŸ—ºï¸ **CRS & projection invariants** (EPSG rules, axis order, bounding sanity)
- ğŸ§© **Geometry validity + topology rules** (no self-intersections, no overlaps where forbidden)
- ğŸ§ª **Spatial predicate correctness** (contains/within/intersects/distance semantics)
- ğŸ›°ï¸ **Raster sanity checks** (CRS, nodata, alignment, resolution assumptions)
- ğŸ§¾ **Metadata + provenance lints** (STAC-ish catalog completeness and consistency)

---

## ğŸš€ Quickstart

### Run all geo tests (local)
```bash
pytest -q tests/geo
```

### Run a focused subset
```bash
pytest -q tests/geo -k crs
pytest -q tests/geo -k topology
pytest -q tests/geo -k postgis
```

### Run inside containers (if your stack uses Compose)
```bash
docker compose exec api pytest -q tests/geo
```

> âœ… Tip: Keep `tests/geo` fast. Put heavy workloads behind explicit markers like `@pytest.mark.slow`.

---

## ğŸ§± Test layout

Suggested structure (adapt to the repo as it evolves):

```text
tests/geo/
â”œâ”€ README.md                 ğŸ‘ˆ you are here
â”œâ”€ conftest.py               ğŸ§° geo fixtures (CRS helpers, sample AOIs)
â”œâ”€ unit/                     ğŸ§ª pure Python tests (no DB)
â”‚  â”œâ”€ test_crs_rules.py
â”‚  â”œâ”€ test_geojson_bounds.py
â”‚  â”œâ”€ test_topology_rules.py
â”‚  â””â”€ test_raster_sanity.py
â”œâ”€ integration/              ğŸ”Œ needs services (PostGIS, tile server, etc.)
â”‚  â”œâ”€ test_postgis_predicates.py
â”‚  â”œâ”€ test_ingest_pipeline_spatial.py
â”‚  â””â”€ test_catalog_metadata_geo.py
â”œâ”€ fixtures/                 ğŸ“¦ tiny inputs (GeoJSON / GeoTIFF / WKT)
â”‚  â”œâ”€ ks_aoi.geojson
â”‚  â”œâ”€ counties_min.geojson
â”‚  â””â”€ dem_tiny.tif
â””â”€ golden/                   ğŸ† expected outputs (stable, reviewed)
   â”œâ”€ catalog_item_expected.json
   â””â”€ provenance_expected.json
```

---

## âœ… Geo invariants we enforce

### 1) ğŸ§­ CRS & projection rules

**Core idea:** spatial data must declare (or reliably imply) its CRS, and tests should catch â€œlooks right but is wrong.â€

Checklist:
- âœ… Default lon/lat datasets behave like **WGS84 / EPSG:4326**
- âœ… Lon/lat coordinates remain within valid world bounds
- âœ… Reprojection steps are explicit and repeatable (no silent CRS guessing)
- âœ… If using web-map tiles, data aligns with **EPSG:3857** expectations
- âœ… If using local/state projections (SPCS / Kansas-specific), distance/area calculations must be performed in an appropriate projected CRS

Recommended assertions:
- `feature_collection_has_crs_or_assumed_default()`
- `all_lonlat_within_bounds()`
- `reproject_roundtrip_is_stable()` (within tolerance)
- `axis_order_is_correct()` (especially when swapping between libraries)

---

### 2) ğŸ§© Geometry validity & topology

**Core idea:** invalid geometry poisons everything downstream (joins, overlays, stats, rendering).

We typically enforce:
- âœ… geometries are valid (or a consistent â€œmake validâ€ strategy is applied)
- âœ… polygons donâ€™t self-intersect
- âœ… polygon overlap rules (dataset-dependent):
  - administrative boundaries: overlaps usually **not allowed**
  - time-sliced layers: overlaps might be allowed **within a time window**, but must be explicit
- âœ… networks (roads/rails/rivers): node/edge connectivity assumptions remain true after transforms

Suggested test names:
- `test_geom_is_valid()`
- `test_no_polygon_overlaps()` (or `test_overlaps_only_when_allowed()`)
- `test_network_snapping_tolerance()`

---

### 3) ğŸ“ Spatial predicates behave as expected

When the system relies on spatial predicates (Python or PostGIS), we lock in semantics:

Common expectations:
- `within(A, B)` implies `intersects(A, B)`
- `distance(A, B) == 0` for overlapping/intersecting geometries (depending on geometry types)
- `dwithin(A, B, r)` matches the chosen distance units + CRS

Suggested tests:
- `test_within_intersects_consistency()`
- `test_dwithin_matches_expected_units()`
- `test_intersects_is_symmetric()`

---

### 4) ğŸ›°ï¸ Raster sanity checks

For small, test-friendly rasters:
- âœ… CRS is present and matches expectations
- âœ… transform + resolution are stable
- âœ… nodata is defined (or an explicit default is used)
- âœ… (optional) alignment checks when clipping/masking (pixel edges vs. vector boundaries)

Suggested tests:
- `test_raster_has_crs_and_transform()`
- `test_raster_clip_is_deterministic()`

---

### 5) ğŸ§¾ Metadata & provenance lints

Geospatial outputs are only useful if we can **find, interpret, and trust** them later.

We lint for:
- âœ… catalog entry exists for each processed dataset
- âœ… metadata includes spatial/temporal extent
- âœ… projection/CRS is captured
- âœ… provenance log exists and references inputs + transforms
- âœ… license/source attribution isnâ€™t missing for new data

Suggested tests:
- `test_catalog_item_exists_for_processed_dataset()`
- `test_catalog_extent_matches_data_bounds()`
- `test_provenance_references_inputs_and_steps()`

---

## ğŸ§ª Writing a new geo test

### ğŸ§° Choose the right level

| Level | Folder | Fast? | Needs DB? | Use whenâ€¦ |
|---|---|---:|---:|---|
| Unit | `unit/` | âœ… | âŒ | CRS rules, geometry validity, file parsing |
| Integration | `integration/` | âš ï¸ | âœ…/âŒ | PostGIS predicates, ingest pipelines, catalog/provenance |
| Slow / E2E | `integration/` + marker | âŒ | âœ… | tile rendering, big joins, performance guardrails |

### ğŸ§· Keep fixtures tiny

Rules of thumb:
- Prefer 5â€“200 features (not 50k)
- Prefer rasters under ~1â€“5 MB
- Use â€œKansas AOIâ€ cutouts for anything spatially heavy
- Golden outputs must be stable and reviewed in PR

---

## ğŸ§  Debug toolbox

Helpful one-liners:

```bash
# show full assertion diffs
pytest -q tests/geo -vv

# stop on first failure
pytest -q tests/geo -x

# run only marked tests
pytest -q tests/geo -m postgis
pytest -q tests/geo -m slow
```

Common failure causes:
- CRS mismatch between fixtures and code defaults
- silent reprojection (or missing reprojection)
- floating precision + topology edge cases
- using geographic CRS for distance/area math

---

## ğŸ”’ CI expectations

- âœ… Geo tests should be deterministic
- âœ… Failures should be explainable (clear assertion messages)
- âœ… Missing metadata/provenance should fail fast
- âœ… If a PR adds/changes spatial outputs, it should also update relevant golden files

---

## ğŸ“š Project reading list (handy references)

- ğŸ§± *Kansas Frontier Matrix (KFM) â€“ Comprehensive Technical Blueprint*
- ğŸ—ºï¸ *Making Maps: A Visual Guide to Map Design for GIS*
- ğŸ§­ *Map Reading & Land Navigation*
- ğŸ§© *Archaeological 3D GIS* (topology + spatial relationships)
- ğŸ›°ï¸ *Cloud-Based Remote Sensing with Google Earth Engine* (raster workflows + scale pitfalls)
- ğŸ§° *Python Geospatial Analysis Cookbook* (EPSG + PostGIS examples)

---

