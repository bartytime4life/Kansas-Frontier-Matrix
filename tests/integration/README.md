# ğŸ§ª Integration Tests (KFM)

![scope](https://img.shields.io/badge/scope-integration-blue)
![stack](https://img.shields.io/badge/stack-docker%20compose%20%2B%20pytest-0aa)
![mindset](https://img.shields.io/badge/mindset-provenance--first%20%26%20fail--closed-6a5acd)

Welcome to `tests/integration/` âœ…  
These tests validate **KFMâ€™s real service boundaries** (API â†” databases â†” pipelines â†” policy gates) using a **containerized dev stack** â€” not mocks.

---

## ğŸ§­ What counts as â€œintegrationâ€ in KFM?

KFM is designed as a **pipeline â†’ catalog/provenance â†’ database â†’ API â†’ UI** system. Integration tests should **prove the seams hold**:

- âœ… API talks to **real** PostGIS + graph DB containers
- âœ… Data follows the canonical flow (no â€œUI talks to DBâ€ shortcuts)
- âœ… Governance checks (policy-as-code) are enforced in the same repo reality as CI
- âœ… â€œSmoke pathsâ€ remain stable: `/docs`, GraphQL (if enabled), DB connectivity, seed data pipelines, etc.

> [!IMPORTANT]
> **Integration tests should only touch databases through the API layer** (by design).  
> If a test directly queries DB tables to â€œverify UI behavior,â€ youâ€™re probably writing the wrong test.

---

## ğŸ“¦ Recommended folder layout

```text
tests/
â””â”€â”€ integration/
    â”œâ”€â”€ README.md                ğŸ‘ˆ you are here
    â”œâ”€â”€ conftest.py              ğŸ§° pytest fixtures (base_url, clients, retries, cleanup)
    â”œâ”€â”€ test_health.py           â¤ï¸ /health or minimal â€œAPI upâ€ checks
    â”œâ”€â”€ test_datasets_api.py     ğŸ—‚ï¸ list/read datasets endpoints
    â”œâ”€â”€ test_graphql_smoke.py    ğŸ§¬ optional GraphQL smoke tests
    â”œâ”€â”€ test_provenance.py       ğŸ§¾ provenance artifacts exist for seeded data
    â”œâ”€â”€ test_policy_gates.py     ğŸ›¡ï¸ policy outcomes observable via API
    â””â”€â”€ test_ai_focus_mode.py    ğŸ¤– optional (only if AI backend configured)
```

---

## âœ… Prerequisites

### Required
- ğŸ³ **Docker + Docker Compose** (Compose V2 recommended)
- ğŸ Python environment available **inside the API container** (tests generally run there)

### Optional (but recommended)
- ğŸ›¡ï¸ **Conftest** for running policy gates locally (mirrors CIâ€™s policy checks)
- ğŸ¤– AI backend (e.g., Ollama or hosted provider) if you want to run AI integration tests

---

## ğŸš€ Quickstart: run integration tests locally

> [!TIP]
> Run integration tests from the **repo root** so Compose paths & env files resolve cleanly.

### 1) Start the dev stack

```bash
# Option A (classic)
docker-compose up -d

# Option B (Compose V2)
docker compose up -d
```

### 2) Run tests inside the API container

```bash
# Run all integration tests (recommended marker approach if configured)
docker-compose exec api pytest -m integration

# Or run just this folder
docker-compose exec api pytest tests/integration -q
```

### 3) Tear down when done

```bash
docker-compose down -v
```

---

## ğŸŒ± Seeding sample data (so tests have something real to hit)

Many integration tests are more meaningful with a tiny â€œknown-goodâ€ dataset seeded into:

- `data/raw/`
- `data/processed/`
- `data/catalog/`
- `data/provenance/`
- plus DB inserts done via pipelines

Suggested pattern:

```bash
# Example: run a one-off pipeline inside the API container
docker-compose exec api python pipelines/import_rainfall.py
```

> [!NOTE]
> If your repo doesnâ€™t ship `import_rainfall.py`, keep the pattern and swap the script name for whatever sample pipeline exists.

---

## ğŸ” Manual smoke checks (fast sanity before you debug tests)

These are â€œhuman integration testsâ€ that quickly confirm your stack is wired correctly.

### âœ… API docs
- Open: `http://localhost:8000/docs`

### âœ… GraphQL (if enabled)
- Open: `http://localhost:8000/graphql`
- Example query:
```graphql
query {
  storyNodes {
    id
    title
    yearRange
  }
}
```

### âœ… Web UI (if running)
- Open: `http://localhost:3000`

### âœ… Databases (for debugging only)
- PostGIS: `localhost:5432`
- Graph DB UI (e.g., Neo4j): `http://localhost:7474`

> [!WARNING]
> Use DB UIs for debugging, not as your â€œtest harness.â€
> Integration truth should be asserted through the API responses and system outputs.

---

## ğŸ§ª Test conventions (please follow)

### âœ… Naming & structure
- Files: `test_*.py`
- Tests: `test_<behavior>__<expected_outcome>()`
- Pattern: **Arrange â†’ Act â†’ Assert**
- Prefer **black-box** assertions:
  - â€œcalling endpoint returns expected schemaâ€
  - â€œseeded dataset appears in `/datasets`â€
  - â€œpolicy prevents restricted access (403/deny response)â€

### âœ… Keep tests stable
- Avoid time-based flake: add small retry helpers for service readiness
- Prefer idempotent setup:
  - write tests that can run twice without requiring manual cleanup
- Keep payload sizes tiny; integration tests should be fast enough for CI

---

## ğŸ›¡ï¸ Policy checks (OPA/Rego) in the integration workflow

KFM uses â€œpolicy-as-codeâ€ to prevent non-compliant changes (e.g., missing license metadata, missing provenance artifacts, etc.).  
You should run these checks locally when your change touches:

- `data/processed/`
- `data/catalog/`
- `data/provenance/`
- policy files / AI prompt configs

### Run policy checks locally (if Conftest is installed)

```bash
conftest test .

# Targeted check example:
conftest test data/processed/mydata.csv
```

> [!TIP]
> If CI fails on a policy gate, treat it like a **test failure**: fix the inputs until the rule passes.

---

## ğŸ§¯ Troubleshooting

<details>
<summary><strong>âš ï¸ Port conflicts</strong> (Postgres 5432, Graph 7474, API 8000, Web 3000)</summary>

If you already run Postgres locally (or another service binds these ports), you may see failures.

**Fix options:**
- Stop the conflicting service
- Change host port mappings in `docker-compose.yml`
- Restart the stack after updating `.env` or compose config
</details>

<details>
<summary><strong>ğŸ¢ Containers start but API canâ€™t reach DB</strong></summary>

Common causes:
- DB container not ready when API starts
- Missing `depends_on` or health checks
- First boot takes longer on low-resource machines

**Try:**
```bash
docker-compose logs api
docker-compose logs db
docker-compose up -d
```
</details>

<details>
<summary><strong>ğŸ—‚ï¸ Volume / permissions issues</strong> (especially macOS/Windows)</summary>

If the API writes into `data/` and you see permission errors:
- Ensure repo folders are writable by Docker
- Check volume mounts are correct
- Rebuild with `--build` if dependencies changed
</details>

---

## â• Adding a new integration test

1. Pick the seam youâ€™re validating:
   - API â†” PostGIS
   - API â†” Graph DB
   - API â†” pipeline outputs
   - API â†” policy decision surface
2. Add/extend fixtures in `tests/integration/conftest.py`
3. Keep assertions **API-facing**
4. Ensure:
   - test passes on a clean `docker-compose up -d`
   - test does not depend on your local machine state

> [!TIP]
> If you need â€œknown data,â€ prefer adding a tiny seed pipeline step (or a sample dataset) rather than hardcoding DB inserts in tests.

---

## ğŸ¯ Definition of Done (DoD) for integration test PRs

- [ ] `docker-compose up -d` works on a fresh checkout
- [ ] Integration tests pass: `pytest tests/integration`
- [ ] Policy checks pass (when relevant): `conftest test .`
- [ ] No direct DB coupling for UI behavior
- [ ] Test failures are actionable (clear asserts + helpful messages)

---

### ğŸ§¾ Related docs (recommended)
- `docs/architecture/` ğŸ“
- Root `README.md` ğŸ—ºï¸
- `policy/` ğŸ›¡ï¸
- `pipelines/` ğŸ”