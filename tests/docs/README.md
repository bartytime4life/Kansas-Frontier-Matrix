<!--
ğŸ“„ File: tests/docs/README.md
ğŸ§­ Purpose: Single entry point for *how we test* the Kansas Frontier Matrix (KFM) / Kansas-Matrix-System.
-->

# ğŸ§ª Tests Docs Hub â€” Kansas Frontier Matrix (KFM)

![Project](https://img.shields.io/badge/KFM-Kansas%20Frontier%20Matrix-blue?style=flat-square)
![Testing](https://img.shields.io/badge/testing-provenance--first-success?style=flat-square)
![CI](https://img.shields.io/badge/CI-TODO-lightgrey?style=flat-square)
![Coverage](https://img.shields.io/badge/coverage-TODO-lightgrey?style=flat-square)
![Security](https://img.shields.io/badge/security-TODO-lightgrey?style=flat-square)

> [!IMPORTANT]
> **KFM is provenance-first.** Tests are not only â€œdoes it work?â€ checks â€” they are **auditable evidence** that:
> - the data pipelines are reproducible ğŸ§¬  
> - the geospatial layers are consistent ğŸ—ºï¸  
> - the models are validated (within declared limits) ğŸ§®  
> - the UI is trustworthy + accessible ğŸ§‘â€ğŸ¤â€ğŸ§‘  
> - the system remains maintainable as requirements evolve ğŸ”

---

## ğŸ§­ What lives in `tests/docs/`

This folder is the **documentation spine** for testing in this repo.

- âœ… **Test philosophy** (what we optimize for)
- âœ… **Test pyramid** (what we run most often vs. least often)
- âœ… **Fixtures & provenance** (how we make data testable + auditable)
- âœ… **Geospatial QA** (CRS/topology/raster/vector sanity)
- âœ… **Model & simulation validation** (numerics, regression, uncertainty)
- âœ… **Performance & scale** (DB + pipeline budgets)
- âœ… **Security & resilience** (defensive posture + guardrails)
- âœ… **Reference library index** (project PDFs that shape our approach) ğŸ“š

---

## âš¡ Quick start (developer ergonomics)

> [!NOTE]
> Commands below are **intentionally tool-agnostic** (Python/Node/R/Java may coexist here).
> Replace `â€¦` with your repoâ€™s real scripts once they exist.

### 1) Smoke tests (fastest signal)
```bash
# recommended: <60s, runs on every PR
make test-smoke
```

### 2) Full local validation (slower, thorough)
```bash
make test
```

### 3) Narrow runs (when youâ€™re iterating)
```bash
make test-unit
make test-integration
make test-e2e
make test-perf
make test-security
```

### 4) Test data / fixtures (rebuild deterministically)
```bash
make fixtures
make fixtures-verify
```

---

## ğŸ—ï¸ Testing model (pyramid + provenance)

```mermaid
flowchart TD
  A[ğŸ§© Unit Tests\nPure logic, domain rules\nFast + deterministic] --> B[ğŸ”Œ Integration Tests\nDB, storage, GIS libs, adapters]
  B --> C[ğŸŒ E2E Tests\nAPI + UI + DB\nUser flows + accessibility]
  C --> D[âš¡ Performance + Scale\nbudgets + baselines]
  C --> E[ğŸ” Security + Resilience\ndefensive checks]
  A --> F[ğŸ§ª Data QA + Provenance\nfixtures, checksums, metadata]
```

**Rule of thumb ğŸ§ **
- Run **Unit + Data QA** constantly.
- Run **Integration** often.
- Run **E2E** when flows change.
- Run **Perf/Security** at least nightly + before releases.

---

## ğŸ—‚ï¸ Suggested test directory layout

> [!TIP]
> Mirror **clean architecture boundaries**: Domain â†’ Services â†’ Adapters â†’ UI.

```text
ğŸ“ tests/
â”œâ”€ğŸ“ docs/                         ğŸ“„ this file + testing playbooks
â”‚  â””â”€ğŸ“„ README.md
â”œâ”€ğŸ“ unit/                         ğŸ§© pure logic + deterministic math
â”œâ”€ğŸ“ integration/                  ğŸ”Œ db, storage, external interfaces
â”œâ”€ğŸ“ e2e/                          ğŸŒ full-stack flows (API + UI)
â”œâ”€ğŸ“ performance/                  âš¡ benchmarks, budgets, regressions
â”œâ”€ğŸ“ security/                     ğŸ” static checks, threat-driven tests
â”œâ”€ğŸ“ data/                         ğŸ§ª fixtures, manifests, goldens
â”‚  â”œâ”€ğŸ“ fixtures/
â”‚  â”œâ”€ğŸ“ goldens/
â”‚  â””â”€ğŸ“ schemas/
â””â”€ğŸ“ tools/                        ğŸ§° helper scripts (deterministic!)
```

---

## âœ… Test categories (what we verify)

### ğŸ§© Unit tests
Focus: correctness of **domain entities + business rules**.
- deterministic, no network, no DB
- property-based testing encouraged (where it increases confidence)
- numerical tests must declare tolerances explicitly

### ğŸ”Œ Integration tests
Focus: correctness of **interfaces & adapters**.
Examples:
- Postgres queries / migrations / indexing assumptions ğŸ˜
- geospatial libraries producing expected output (GDAL/GeoPandas equivalents)
- storage adapters (local FS / object store)
- remote sensing fetch logic (mocked, cached, or replayable)

### ğŸŒ End-to-end tests
Focus: **user workflows** and trust signals.
Examples:
- â€œsearch â†’ map â†’ inspect provenance â†’ exportâ€
- accessibility checks (keyboard nav, ARIA, contrast)
- visual regressions (maps + WebGL layers) with tolerance thresholds

### ğŸ—ºï¸ Geospatial QA tests (vector + raster)
Focus: integrity of spatial truth.
- CRS correctness & explicit transforms (no silent CRS mixing)
- topology checks (self-intersections, invalid polygons, winding rules)
- bounding-box / tile sanity
- deterministic reprojection + rounding strategy

### ğŸ›°ï¸ Remote sensing & Earth observation tests
Focus: pipeline reproducibility and metadata integrity.
- stable dataset IDs and timestamps
- repeatable preprocessing (masking, clipping, compositing)
- compare **summary statistics** (not only pixel-perfect) when appropriate
- enforce metadata completeness (sensor, band, resolution, projection, QA flags)

### ğŸ¤– ML/AI evaluation tests
Focus: *measurable* model quality + safety guardrails.
- fixed dataset snapshots + split manifests
- baseline comparisons (donâ€™t regress silently)
- calibration & uncertainty checks (where relevant)
- â€œevidence-backed outputsâ€ requirements (citations + traceability)

### ğŸ§® Modeling & simulation validation (V&V mindset)
Focus: numerical stability + real-world plausibility.
- sanity tests: units, bounds, conservation rules (if applicable)
- convergence tests: grid refinement / timestep checks
- regression tests: known inputs â†’ expected outputs (within tolerance)
- uncertainty: document what the test *doesnâ€™t* prove

### âš¡ Performance & scale tests
Focus: budgets + regression detection.
- query latency budgets and throughput targets
- ETL pipeline runtime budgets
- memory ceilings for large rasters/tiles
- avoid flaky perf tests: prefer â€œrelative to baselineâ€ + robust statistics

### ğŸ” Security & resilience tests (defensive)
Focus: hardening, not exploitation.
- dependency / supply-chain scanning
- secrets detection
- config linting (CORS, CSP, headers)
- authz tests (deny-by-default)
- abuse cases: rate limits, payload size, invalid geometry bombs, etc.

---

## ğŸ§¬ Fixtures, provenance, and â€œtest data that can testifyâ€

> [!IMPORTANT]
> A fixture without provenance is a liability.  
> Every fixture must be attributable, reproducible, and minimally scoped.

### âœ… Fixture rules
- **Small**: minimal size that still catches the bug.
- **Stable**: checksum + versioned schema.
- **Explainable**: include `manifest.yml` (below).
- **Rebuildable**: include `build.py` / `build.sh` that deterministically recreates it.

### ğŸ“„ Fixture manifest template
```yaml
# tests/data/fixtures/<fixture_name>/manifest.yml
id: kfm.fixture.<name>.<version>
version: 1
description: "Short explanation of what this fixture is meant to test."
domain: [geospatial, remote_sensing, database, ui, ml, simulation]

sources:
  - name: "Source name (dataset, paper, archival map, etc.)"
    uri: "https://â€¦ OR internal path"
    license: "SPDX or human-readable"
    retrieved_at: "YYYY-MM-DD"
    checksum: "sha256:â€¦"

processing:
  steps:
    - name: "clip"
      tool: "gdalwarp|geopandas|python|r|sql|â€¦"
      params:
        bbox: [-101.2, 37.0, -101.0, 37.2]
    - name: "normalize"
      tool: "python"
      params:
        rounding: 6

expectations:
  - type: "golden"
    path: "expected.json"
    comparison: "exact|tolerant"
    tolerance:
      abs: 1e-6
      rel: 1e-4

provenance:
  owner: "@team-or-handle"
  reviewed_by: "@reviewer"
  notes: "Any caveats / known limitations."
```

### ğŸ” Golden files & snapshot policy
Use goldens when:
- output is structured + stable (GeoJSON after canonicalization, JSON schemas, SQL plans)
- UI output can be snapshotted with controlled fonts/sizes
- model outputs have a clear baseline

Avoid goldens when:
- output is inherently noisy (perf metrics, nondeterministic GPU shading)
- output is huge (prefer summary metrics or hashing strategies)

---

## ğŸ§· Statistical tests (avoid â€œp-value cosplayâ€)

When tests use statistics (e.g., regression baselines, model metrics, performance):
- **pre-register** the metric + threshold in the test (donâ€™t â€œeyeball passâ€)
- track **effect size** and **variance**, not just pass/fail
- treat multiple comparisons carefully (donâ€™t create false failures)
- prefer robust summaries (median, percentiles) for noisy signals

---

## ğŸ›ï¸ Determinism & randomness policy

- Seed all RNGs (Python/JS/R) âœ…  
- Log seeds on failure âœ…  
- Keep tests hermetic (no live network) âœ…  
- If a test *must* use nondeterminism (rare), isolate it under:
  - `performance/` or `chaos/` style suites
  - explicit rerun logic + statistical pass criteria

---

## ğŸ§° CI gates (recommended default)

> [!TIP]
> Optimize for fast feedback + high confidence.

**PR gates (fast):**
- format + lint
- unit tests
- fixture verification (checksums + schemas)
- a small integration slice (DB migrations + 1 representative query)
- a small e2e slice (one critical flow)

**Nightly gates (deep):**
- full integration suite
- e2e across key browsers/devices (if applicable)
- perf regression suite
- security scan suite

---

## ğŸ§‘â€ğŸ”§ Contributing tests (the â€œDefinition of Doneâ€ checklist)

A PR is â€œdoneâ€ when:
- [ ] new behavior includes tests
- [ ] fixtures include manifests + checksums
- [ ] tests explain **why** the check exists (not just how)
- [ ] tolerances are justified (numerics/geo)
- [ ] performance claims include budgets + measurement method
- [ ] security-sensitive changes include threat-driven tests
- [ ] docs updated (this folder) when behavior changes

---

## ğŸ“š Project reference library index (all project files)

> [!NOTE]
> These PDFs are the **shared research backbone** used to shape testing strategy across:
> scientific computing, geospatial integrity, statistics, ML, performance, security, and UI.

<details>
<summary>ğŸ“¦ Click to expand the full library list (37 files)</summary>

### ğŸ§  Science, statistics, ML, and modeling
- **`Scientific Modeling and Simulation_ A Comprehensive NASA-Grade Guide.pdf`** â€” verification/validation mindset, numerical rigor ğŸ§®  
- **`Understanding Statistics & Experimental Design.pdf`** â€” experiment design, error control, interpretation âœ…  
- **`regression-analysis-with-python.pdf`** â€” regression workflows + diagnostics ğŸ“ˆ  
- **`Regression analysis using Python - slides-linear-regression.pdf`** â€” quick linear-regression reference ğŸ“‰  
- **`think-bayes-bayesian-statistics-in-python.pdf`** â€” Bayesian thinking for uncertainty + decision tests ğŸ²  
- **`Basics of Linear Algebra for Machine Learning.pdf`** â€” linear algebra foundations for numerical tests â—  
- **`Understanding Machine Learning: From Theory to Algorithms.pdf`** â€” learning theory + evaluation principles ğŸ¤–  
- **`Deep Learning for Coders with fastai and PyTorch - Deep.Learning.for.Coders.with.fastai.and.PyTorchpdf`** â€” pragmatic ML training/eval patterns ğŸ§   
- **`Principles of Biological Autonomy - book_9780262381833.pdf`** â€” complex systems framing (useful for agent/eco models) ğŸŒ¿  
- **`Spectral Geometry of Graphs.pdf`** â€” graph algorithms + spectral methods (test invariants + bounds) ğŸ•¸ï¸  
- **`Generalized Topology Optimization for Structural Design.pdf`** â€” optimization + simulation validation patterns ğŸ§±  

### ğŸ—ºï¸ GIS, cartography, remote sensing, 3D
- **`Kansas Frontier Matrix (KFM) â€“ Comprehensive Technical Documentation.pdf`** â€” architecture + provenance-first principles ğŸ§¾  
- **`python-geospatial-analysis-cookbook.pdf`** â€” geospatial processing recipes (good fixture patterns) ğŸ—ºï¸  
- **`making-maps-a-visual-guide-to-map-design-for-gis.pdf`** â€” cartographic QA + visual validation ğŸ¨  
- **`Mobile Mapping_ Space, Cartography and the Digital - 9789048535217.pdf`** â€” field/mobile mapping considerations ğŸ“  
- **`Cloud-Based Remote Sensing with Google Earth Engine-Fundamentals and Applications.pdf`** â€” remote sensing pipelines + reproducibility ğŸ›°ï¸  
- **`Archaeological 3D GIS_26_01_12_17_53_09.pdf`** â€” 3D GIS concepts + volumetric thinking ğŸ§Š  
- **`compressed-image-file-formats-jpeg-png-gif-xbm-bmp.pdf`** â€” image formats & constraints (raster IO tests) ğŸ–¼ï¸  
- **`webgl-programming-guide-interactive-3d-graphics-programming-with-webgl.pdf`** â€” WebGL rendering/precision considerations ğŸ®  

### ğŸ—„ï¸ Data management, performance, and architecture
- **`Database Performance at Scale.pdf`** â€” performance budgeting, workload thinking âš¡  
- **`PostgreSQL Notes for Professionals - PostgreSQLNotesForProfessionals.pdf`** â€” Postgres fundamentals for integration tests ğŸ˜  
- **`Scalable Data Management for Future Hardware.pdf`** â€” scaling mindset, hardware-aware constraints ğŸ§°  
- **`Data Spaces.pdf`** â€” data interoperability + architecture framing ğŸ§©  
- **`Flexible Software Design.pdf`** â€” designing for change (tests as change insurance) ğŸ”  

### ğŸ” Security, concurrency, and â€œsharp toolsâ€ (defensive use)
- **`ethical-hacking-and-countermeasures-secure-network-infrastructures.pdf`** â€” defensive security testing themes ğŸ›¡ï¸  
- **`Gray Hat Python - Python Programming for Hackers and Reverse Engineers (2009).pdf`** â€” security awareness (use responsibly) ğŸ§¯  
- **`concurrent-real-time-and-distributed-programming-in-java-threads-rtsj-and-rmi.pdf`** â€” concurrency hazards + test strategies ğŸ§µ  

### ğŸŒ UI, web, and language reference collections
- **`responsive-web-design-with-html5-and-css3.pdf`** â€” responsive UI test coverage ğŸ“±  
- **`graphical-data-analysis-with-r.pdf`** â€” EDA patterns for QA + anomaly detection ğŸ”  
- **`A programming Books.pdf`** â€” language reference compendium (Aâ€“*) ğŸ§°  
- **`B-C programming Books.pdf`** â€” language reference compendium (Bâ€“C) ğŸ§°  
- **`D-E programming Books.pdf`** â€” language reference compendium (Dâ€“E) ğŸ§°  
- **`F-H programming Books.pdf`** â€” language reference compendium (Fâ€“H) ğŸ§°  
- **`I-L programming Books.pdf`** â€” language reference compendium (Iâ€“L) ğŸ§°  
- **`M-N programming Books.pdf`** â€” language reference compendium (Mâ€“N) ğŸ§°  
- **`O-R programming Books.pdf`** â€” language reference compendium (Oâ€“R) ğŸ§°  
- **`S-T programming Books.pdf`** â€” language reference compendium (Sâ€“T) ğŸ§°  
- **`U-X programming Books.pdf`** â€” language reference compendium (Uâ€“X) ğŸ§°  
- **`On the path to AI Lawâ€™s prophecies and the conceptual foundations of the machine learning age.pdf`** â€” governance + accountability framing âš–ï¸  
- **`Introduction to Digital Humanism.pdf`** â€” human-centered safeguards + transparency ğŸ§‘â€ğŸ¤â€ğŸ§‘  

</details>

---

## ğŸ§¾ Glossary (shared vocabulary)

- **Fixture**: a minimal dataset used to make a test meaningful and reproducible.
- **Golden**: an â€œexpected outputâ€ artifact used for snapshot/regression comparisons.
- **Provenance**: the traceable chain from source â†’ processing â†’ output (including metadata).
- **V&V**: verification (correctly built) + validation (correctly represents reality within limits).
- **Budget**: explicit performance constraint (latency, memory, runtime) that tests enforce.

---

## ğŸ—ºï¸ Next docs to add (recommended)
- ğŸ“„ `tests/docs/fixtures.md` â€” fixture standards + manifest schema details
- ğŸ“„ `tests/docs/geospatial.md` â€” CRS/topology/raster/vector checklists
- ğŸ“„ `tests/docs/ml-eval.md` â€” dataset snapshotting, metrics, calibration, drift
- ğŸ“„ `tests/docs/performance.md` â€” budgets, baselines, how to avoid flaky perf tests
- ğŸ“„ `tests/docs/security.md` â€” defensive test suite + threat model mapping
