---
id: kfm-web-tokens-experiments-readme
title: Token Experiments (Web Chart Assets)
status: active
owner: web-ui
last_updated: 2026-01-16
---

# ğŸ§ªğŸ“Š Token Experiments â€” Web Chart Assets

![Provenance](https://img.shields.io/badge/provenance-first-2ea44f)
![Contracts](https://img.shields.io/badge/data%20contracts-required-blue)
![No%20Mystery%20Layers](https://img.shields.io/badge/no%20mystery%20layers-ever-critical)
![Telemetry](https://img.shields.io/badge/telemetry-governed-orange)

> âœ… **What â€œtokensâ€ means here:** **LLM usage tokens** (prompt/input + completion/output) for KFMâ€™s AI-enabled features (ex: Focus Mode), **not** auth/security tokens.  
> ğŸ§¾ This directory is part of KFMâ€™s â€œ**no data without provenance**â€ philosophy: every chart should be traceable back to the run + config that produced it.

---

## ğŸ¯ Purpose

This folder contains **render-ready** (or near render-ready) **chart assets** used by the web UI to visualize **token usage experiments**:

- ğŸ’¸ *Cost + efficiency* tracking (tokens â†’ estimated cost)
- ğŸ§  Prompt + workflow comparisons (baseline vs variant)
- â±ï¸ Latency vs tokens tradeoffs
- ğŸ§° Regression watch (token spikes after changes)
- ğŸ” Governance visibility (telemetry + review gates)

KFM emphasizes provenance-first and contract-first designâ€”**chart artifacts are treated like governed outputs**, not ad-hoc screenshots.  [oai_citation:0â€¡Kansas Frontier Matrix (KFM) â€“ Comprehensive Technical Documentation.pdf](file-service://file-AkqwUuYPp5zePf7pv5SMxi)

---

## ğŸ—ºï¸ Where This Fits in the KFM Pipeline

KFMâ€™s pipeline is ordered and traceable (raw â†’ processed â†’ catalog/graph â†’ API â†’ UI â†’ narratives). Token experiments are **telemetry-derived** and should follow the same discipline: **measure â†’ validate â†’ publish â†’ render**.  [oai_citation:1â€¡MARKDOWN_GUIDE_v13.md.gdoc](file-service://file-UYVruFXfueR8veHMUKeugU)

```mermaid
flowchart LR
  A["ğŸ§¾ Telemetry / Run Logs<br/>Focus Mode, Story Nodes, API calls"] --> B["ğŸ§¹ Aggregation / Cleaning<br/>Redaction + normalization"]
  B --> C["ğŸ“¦ Experiment Bundle<br/>manifest + data + prov"]
  C --> D["ğŸ“Š Chart Assets (this folder)<br/>spec + thumbnails + summaries"]
  D --> E["ğŸ–¥ï¸ Web UI renders<br/>comparisons + dashboards"]
```

> ğŸ”¥ **Rule:** `web/assets/...` is **not** the source of truth.  
> It should store *publishable, UI-friendly* assets derived from governed logs.

---

## ğŸ“ Directory Layout

Recommended layout (you can evolve this, but keep it consistent):

```text
web/assets/charts/tokens/experiments/
â”œâ”€â”€ README.md                      # â† you are here
â”œâ”€â”€ index.json                     # experiment registry (optional but recommended)
â”œâ”€â”€ schemas/                       # JSON Schemas (optional but highly recommended)
â”‚   â”œâ”€â”€ token-experiment.schema.json
â”‚   â””â”€â”€ token-run.schema.json
â””â”€â”€ 2026-01-16__EXP-0001__focusmode-citations/
    â”œâ”€â”€ manifest.json              # required: experiment contract
    â”œâ”€â”€ prov.jsonld                # required: provenance lineage (lightweight is fine)
    â”œâ”€â”€ data.summary.json          # required: aggregated metrics for UI
    â”œâ”€â”€ data.runs.jsonl            # optional: row-per-run (keep small)
    â”œâ”€â”€ chart.vl.json              # optional: Vega-Lite spec (or your rendererâ€™s spec)
    â”œâ”€â”€ chart.png                  # optional: thumbnail
    â””â”€â”€ notes.md                   # optional: narrative + interpretation
```

### ğŸ§· Naming Convention

Use stable, sortable names:

- Folder: `YYYY-MM-DD__EXP-####__short-slug/`
- IDs: `EXP-0001`, `EXP-0002`, â€¦ (aligns with standardized protocol numbering)  [oai_citation:2â€¡Scientific Method _ Research _ Master Coder Protocol Documentation.pdf](file-service://file-HTpax4QbDgguDwxwwyiS32)

---

## âœ… Minimum Required Files Per Experiment

Every experiment folder **must** include:

### 1) `manifest.json` (ğŸ“œ contract)
Defines the experimentâ€™s identity, purpose, and parameters (so results are comparable).

**Minimum fields (recommended):**
- `id`, `slug`, `date_utc`, `owner`
- `question` (what are we trying to learn?)
- `hypothesis` (testable expectation)
- `baseline` and `variant` definitions (what changed?)
- `metrics` (what we measure + units)
- `inputs` (task set / scenarios)
- `environment` (commit, runtime, model version)
- `privacy` (redaction status, sensitivity flags)

### 2) `data.summary.json` (ğŸ“Š UI-friendly metrics)
Small file the UI can load quickly (totals, means, percentiles, deltas).

### 3) `prov.jsonld` (ğŸ§¬ provenance)
Keep it lightweight but real:
- where run logs came from
- what script/version aggregated them
- what redactions were applied
- the commit hash / build id

> KFMâ€™s guiding idea: **no â€œmystery layersâ€**â€”if itâ€™s rendered, it must be traceable.  [oai_citation:3â€¡MARKDOWN_GUIDE_v13.md.gdoc](file-service://file-UYVruFXfueR8veHMUKeugU)

---

## ğŸ“ Suggested Data Contract (Example)

> You can adapt this to your renderer + telemetry format. The key is: **stable fields + versioning**.

<details>
<summary><strong>ğŸ“„ Example: manifest.json</strong></summary>

```json
{
  "schema_version": "1.0.0",
  "id": "EXP-0001",
  "slug": "focusmode-citations",
  "date_utc": "2026-01-16",
  "owner": "web-ui",

  "question": "Do inline citations reduce total tokens without degrading answer usefulness?",
  "hypothesis": "Adding structured citation prompts will reduce output tokens by >= 10% while preserving relevance.",

  "baseline": {
    "name": "focusmode-default",
    "prompt_ref": "prompts/focusmode-default@v3",
    "model": "gpt-4.1",
    "params": { "temperature": 0.2 }
  },
  "variant": {
    "name": "focusmode-citations-structured",
    "prompt_ref": "prompts/focusmode-citations@v1",
    "model": "gpt-4.1",
    "params": { "temperature": 0.2 }
  },

  "metrics": {
    "unit": "tokens",
    "fields": [
      "input_tokens",
      "output_tokens",
      "total_tokens",
      "latency_ms",
      "estimated_cost_usd"
    ],
    "primary": "total_tokens"
  },

  "inputs": {
    "taskset_id": "kfm-focusmode-qna-20",
    "n_runs": 40,
    "sampling": "paired",
    "notes": "Same questions; randomized order; cached context disabled."
  },

  "environment": {
    "repo_commit": "GIT_SHA_HERE",
    "runner": "node@20",
    "os": "linux",
    "timestamp_utc": "2026-01-16T00:00:00Z"
  },

  "privacy": {
    "contains_user_text": false,
    "contains_sensitive_locations": false,
    "redactions_applied": ["prompt_text_removed", "user_ids_hashed"]
  }
}
```
</details>

<details>
<summary><strong>ğŸ“Š Example: data.summary.json</strong></summary>

```json
{
  "schema_version": "1.0.0",
  "experiment_id": "EXP-0001",
  "baseline": {
    "mean_total_tokens": 1820,
    "p50_total_tokens": 1790,
    "p95_total_tokens": 2310,
    "mean_latency_ms": 2400
  },
  "variant": {
    "mean_total_tokens": 1610,
    "p50_total_tokens": 1585,
    "p95_total_tokens": 2105,
    "mean_latency_ms": 2480
  },
  "delta": {
    "mean_total_tokens_pct": -11.5,
    "mean_latency_ms_pct": 3.3
  },
  "notes": [
    "Variant reduces output tokens more than input tokens.",
    "Latency increased slightly; within tolerance."
  ]
}
```
</details>

---

## ğŸ§  Experiment Design: â€œNASA-gradeâ€ Discipline (Applied to Tokens)

Even though token experiments arenâ€™t â€œphysics simulations,â€ they benefit from the same rigor:
- ğŸ“Œ **Version everything** (code + prompts + tasksets)
- ğŸ§ª **Pre-write the protocol** (what youâ€™ll change, what youâ€™ll measure)
- ğŸ§¾ **Log everything needed to reproduce**
- âœ… **Validate + sanity check** before publishing charts
- ğŸ“‰ **Report uncertainty** (variance / percentiles, not just single numbers)

NASA-style modeling & simulation guidance stresses reproducibility, version traceability, and uncertainty reporting as part of credibility.  [oai_citation:4â€¡Scientific Modeling and Simulation_ A Comprehensive NASA-Grade Guide.pdf](file-service://file-LuWF23hffNAZJaZm2Gzvcd)

---

## ğŸ§¾ Protocol Template (Copy/Paste)

Use this to keep experiments comparable (and reviewable).  [oai_citation:5â€¡Scientific Method _ Research _ Master Coder Protocol Documentation.pdf](file-service://file-HTpax4QbDgguDwxwwyiS32)

```markdown
# EXP-#### â€” <short title>

## Objective ğŸ¯
- What decision will this experiment inform?

## Background ğŸ“š
- What do we already know? Link prior experiments.

## Hypothesis ğŸ”®
- Testable statement (include expected direction + threshold if possible).

## Method ğŸ§ª
- Baseline:
- Variant:
- Controls (what is held constant):
- Taskset / dataset:
- Run count:
- Randomization / pairing:
- Stop criteria:

## Metrics ğŸ“
- Primary:
- Secondary:
- Reporting: mean + p50 + p95 + max + deltas

## Data Handling ğŸ”’
- Sensitive content rules:
- Redaction applied:
- Storage location:

## Results ğŸ“Š
- Summary:
- Charts:
- Notes:

## Conclusion âœ…
- Support/refute hypothesis?
- Risks / tradeoffs?

## Next Steps ğŸ”
- Follow-up experiments:
```

---

## ğŸ§¯ Common Pitfalls (Token Experiments)

A few ways token experiments can accidentally lie:

- ğŸ¯ **Cherry-picked prompts/tasks** â†’ results donâ€™t generalize
- ğŸ§ª **Changing multiple variables at once** â†’ unclear causality
- â¹ï¸ **Optional stopping** (â€œran it until it looked goodâ€) â†’ biased outcomes
- ğŸ“° **Publication bias** (only saving wins) â†’ misleading dashboard history

These are standard experimental design hazardsâ€”treat token experiments like real studies, not vibes.  [oai_citation:6â€¡Understanding Statistics & Experimental Design.pdf](file-service://file-SdX6LMgi1uDRk5kd4H4Bg3)

---

## ğŸ” Privacy, Governance, and â€œNo Side-Channelsâ€

Token telemetry can indirectly leak sensitive info (e.g., prompt text, user fragments, location strings). Keep this folder safe:

- âŒ Donâ€™t commit raw prompts containing private or sensitive data
- âœ… Prefer hashes / stable IDs for tasks and users
- âœ… Store only *aggregated* metrics in `web/assets/` when possible
- âœ… Ensure classification is not â€œdowngradedâ€ by publishing

KFMâ€™s governance model expects CI gates for secrets, PII, and classification consistency.  [oai_citation:7â€¡MARKDOWN_GUIDE_v13.md.gdoc](file-service://file-UYVruFXfueR8veHMUKeugU)

---

## âœ… Validation Gates (What Should Fail CI)

If you wire CI checks for this folder, these are the â€œdefinition of doneâ€ checks:

- ğŸ“„ JSON schema validation for `manifest.json` / `data.summary.json`
- ğŸ”— Link/reference validation (no broken internal refs)
- ğŸ”’ Secret scanning (never commit keys/tokens)
- ğŸ§¬ Provenance completeness (prov file exists + non-empty)
- ğŸ§¼ Size budget (no giant logs in `web/assets/`)
- ğŸ§ª Smoke render (chart specs parse + render at least once)

KFMâ€™s v13 guidance explicitly recommends validation gates and schema checks for telemetry/UI config.  [oai_citation:8â€¡MARKDOWN_GUIDE_v13.md.gdoc](file-service://file-UYVruFXfueR8veHMUKeugU)

---

## ğŸ§­ FAQ

### â€œWhere do raw logs live?â€
Prefer outside `web/assets/`. This folder should contain **publishable** artifacts only. If you must keep run-level data here, keep it tiny and sanitized.

### â€œWhat chart spec format should we use?â€
Pick one renderer and standardize it. **Vega-Lite JSON** is a strong default because itâ€™s portable and data-driven, but any stable spec is fine if you keep it versioned.

### â€œHow do we compare experiments fairly?â€
Use **paired runs** (same taskset, randomized order) and report distributions (p50/p95), not just means.

---

## ğŸ“š Project References (Grounding)

- Kansas Frontier Matrix (KFM) â€” Comprehensive Technical Documentation  [oai_citation:9â€¡Kansas Frontier Matrix (KFM) â€“ Comprehensive Technical Documentation.pdf](file-service://file-AkqwUuYPp5zePf7pv5SMxi)  
- MARKDOWN_GUIDE_v13 (repo invariants, validation gates, pipeline ordering)  [oai_citation:10â€¡MARKDOWN_GUIDE_v13.md.gdoc](file-service://file-UYVruFXfueR8veHMUKeugU)  
- Scientific Method / Research â€” Master Coder Protocol Documentation (experiment protocol discipline)  [oai_citation:11â€¡Scientific Method _ Research _ Master Coder Protocol Documentation.pdf](file-service://file-HTpax4QbDgguDwxwwyiS32)  
- Understanding Statistics & Experimental Design (biases + rigorous comparisons)  [oai_citation:12â€¡Understanding Statistics & Experimental Design.pdf](file-service://file-SdX6LMgi1uDRk5kd4H4Bg3)  
- Scientific Modeling and Simulation â€” A Comprehensive NASA-Grade Guide (reproducibility + uncertainty discipline)  [oai_citation:13â€¡Scientific Modeling and Simulation_ A Comprehensive NASA-Grade Guide.pdf](file-service://file-LuWF23hffNAZJaZm2Gzvcd)  

---

## ğŸ§© TODOs (Future Nice-to-Haves)

- ğŸ§± Add `schemas/` and validate in CI
- ğŸ—‚ï¸ Add a top-level `index.json` so the UI can auto-list experiments
- ğŸ§® Add a tiny `aggregate.ts` / `aggregate.py` to build `data.summary.json` deterministically
- ğŸ§¾ Add â€œexperiment statusâ€ (`draft`, `review`, `published`) to manifest + UI filters
