---
title: "ğŸ§ª Modeling Mappers â€” Evidence Artifacts â†” Domain (Uncertainty â€¢ Reproducibility â€¢ Provenance)"
path: "api/src/adapters/mappers/modeling/README.md"
version: "v0.1.0"
last_updated: "2026-01-11"
status: "draft"
doc_kind: "Module README"
license: "CC-BY-4.0"

# KFM governance header
fair_category: "FAIR+CARE"
care_label: "Public"
sensitivity: "public"
classification: "open"
pipeline_ordering: "ETL â†’ Catalogs â†’ Graph â†’ API â†’ UI â†’ Story Nodes â†’ Focus Mode"
---

![Layer](https://img.shields.io/badge/layer-adapters-informational)
![Module](https://img.shields.io/badge/module-mappers%2Fmodeling-7b2cbf)
![Discipline](https://img.shields.io/badge/discipline-modeling%20%26%20simulation-0ea5e9)
![Evidence](https://img.shields.io/badge/rule-evidence--first-f97316)
![Repro](https://img.shields.io/badge/rule-reproducible%20runs-22c55e)
![Uncertainty](https://img.shields.io/badge/outputs-uncertainty%20included-111827)
![Safety](https://img.shields.io/badge/safety-no%20I%2FO%20%7C%20no%20secrets-ef4444)

# ğŸ§ª Modeling Mappers (`api/src/adapters/mappers/modeling/`)

This folder contains **pure, deterministic** mapping utilities for *modeling & simulation outputs* (â€œevidence artifactsâ€) at the API boundary.

Modeling mappers translate between:
- ğŸ§  **Domain results** (runs, experiments, analyses)  
  â†’ ğŸ“¦ **Stable, client-safe DTOs** (REST/GraphQL/event outputs)
- ğŸ“¦ **Inbound modeling requests** (params, ROI, time windows)  
  â†’ ğŸ§  **Domain commands/queries**
- ğŸ§¾ **Provenance + catalog references** (STAC/DCAT/PROV IDs)  
  â†’ embedded metadata that keeps results auditable
- ğŸ“ˆ **Uncertainty & diagnostics** (CI/credible intervals, residual summaries, confusion matrices, etc.)  
  â†’ consistent summary shapes (no hand-wavy â€œconfidenceâ€)

> [!IMPORTANT]
> Modeling mappers are an **anti-corruption layer** for science-grade outputs:
> - âœ… map + validate + normalize + annotate
> - âœ… enforce reproducibility metadata (run IDs, hashes, versions)
> - âœ… enforce uncertainty semantics (what kind, what units, what meaning)
> - âŒ no DB/Neo4j/PostGIS access
> - âŒ no filesystem/network calls
> - âŒ no â€œcurrent timeâ€ generation (inject timestamps from services)
> - âŒ no business decisions (â€œwhat should we run?â€) â€” services decide

---

## ğŸ”— Neighbor links

- ğŸ“¦ Parent: `ğŸ“ api/src/adapters/mappers/README.md`
- ğŸ§° Shared primitives: `ğŸ“ api/src/adapters/mappers/common/README.md`
- ğŸ—ºï¸ Geo helpers (ROI, bbox, CRS): `ğŸ“ api/src/adapters/mappers/geo/README.md`
- ğŸ“š Catalog mappers (STAC/DCAT/PROV): `ğŸ“ api/src/adapters/mappers/catalog/README.md`
- ğŸ›¬ Inbound adapters: `ğŸ“ api/src/adapters/inbound/README.md`
- ğŸ›« Outbound adapters: `ğŸ“ api/src/adapters/outbound/README.md`
- ğŸ§¯ Adapter errors: `ğŸ“„ api/src/adapters/errors.py`

---

## ğŸ§­ Table of contents

- [ğŸ“ Folder map](#-folder-map)
- [ğŸ¯ What belongs here](#-what-belongs-here)
- [ğŸ§  Modeling artifacts KFM should treat as â€œevidenceâ€](#-modeling-artifacts-kfm-should-treat-as-evidence)
- [ğŸ“¦ Canonical evidence DTO shape](#-canonical-evidence-dto-shape)
- [ğŸ›ï¸ Parameters, hashes, and reproducibility](#ï¸-parameters-hashes-and-reproducibility)
- [ğŸ“ˆ Uncertainty & diagnostics mapping](#-uncertainty--diagnostics-mapping)
- [ğŸ—ºï¸ Spatial & temporal semantics](#ï¸-spatial--temporal-semantics)
- [ğŸ“¤ Outputs, assets, and payload size](#-outputs-assets-and-payload-size)
- [ğŸ” Classification, redaction, and safety](#-classification-redaction-and-safety)
- [ğŸ§¯ Error codes](#-error-codes)
- [ğŸ§ª Testing strategy](#-testing-strategy)
- [ğŸ§‘â€ğŸ’» Templates](#-templates)
- [âœ… Definition of done](#-definition-of-done)
- [ğŸ“š Project bookshelf](#-project-bookshelf)

---

## ğŸ“ Folder map

```text
ğŸ“ api/
  ğŸ“ src/
    ğŸ“ adapters/
      ğŸ“ mappers/
        ğŸ“ modeling/                     ğŸ§ª modeling & simulation mapping (pure)
          ğŸ“„ README.md                   ğŸ‘ˆ you are here
          ğŸ“„ __init__.py                 ğŸ§¬ package init (optional)

          ğŸ“„ requests.py                 ğŸ§¾ inbound DTO â†’ domain commands/queries
          ğŸ“„ results.py                  ğŸ“¤ domain results â†’ response DTOs
          ğŸ“„ metrics.py                  ğŸ“ˆ metrics + diagnostics shaping
          ğŸ“„ uncertainty.py              ğŸ² CI/credible intervals + error bounds
          ğŸ“„ artifacts.py                ğŸ“¦ asset refs (plots, COGs, CSV, parquet) + safe href rules
          ğŸ“„ manifests.py                ğŸ—ƒï¸ run manifests (params_hash, data_hash, versions)
          ğŸ“„ normalize.py                ğŸ§¼ stable normalization (floats, enums, time)
          ğŸ“„ validate.py                 âœ… pure validation (ranges, required fields)
          ğŸ“„ errors.py                   ğŸ§¯ modeling-mapper error codes (optional)
```

> [!TIP]
> If your repo uses different filenames, keep the *separation by concern*:
> `requests / results / uncertainty / manifests / artifacts`.

---

## ğŸ¯ What belongs here

### âœ… In scope
- â€œEvidence artifactâ€ DTOs (summary + detail variants)
- Stable `run_id`, `model_id`, `params_hash`, `data_hash` shaping
- Strict mapping for modeling inputs:
  - ROI (bbox/polygon), CRS, time range
  - parameter sets + units
  - seeds and reproducibility flags
- Standardized metrics and diagnostics:
  - regression metrics (RMSE, MAE, RÂ², residual summaries)
  - classification metrics (precision/recall/F1, confusion matrix summary)
  - Bayesian summaries (posterior mean/median, HDI/credible interval)
  - simulation diagnostics (convergence flags, step size, stability notes)
- Uncertainty objects (explicit semantics + units)
- Asset reference shaping (plots and large arrays are links, not inline blobs)
- Provenance/citations fields (STAC/DCAT/PROV refs, method disclosure)

### âŒ Out of scope
- Running simulations
- Training models
- Querying PostGIS/Neo4j
- Exporting COGs/tiles
- Writing STAC/DCAT/PROV JSON to storage (catalog mappers + outbound handle that)
- Any â€œhelpfulâ€ guessing that changes meaning (units/CRS/timezone)

---

## ğŸ§  Modeling artifacts KFM should treat as â€œevidenceâ€

Modeling results should be considered **publishable artifacts** (like datasets), not â€œtemporary responsesâ€:

- ğŸ§ª **Scientific simulations** (forward models, scenario runs, numerical solvers)
- ğŸ“ˆ **Regression & statistical analyses** (linear/logistic, diagnostics, residual analysis)
- ğŸ² **Bayesian inference outputs** (posterior summaries, credible intervals, priors)
- ğŸ¤– **Machine learning training runs** (model versioning, dataset splits, metrics)
- ğŸ›°ï¸ **Remote sensing classifications** (accuracy assessment + uncertainty)
- ğŸ§  **Graph analytics results** (centrality/community outputs as derived evidence)
- ğŸ§± **Optimization results** (topology optimization outputs, objective/constraints, convergence)
- ğŸ—ºï¸ **Geospatial transformations** that imply claims (change detection, suitability scoring)

> [!NOTE]
> If a result changes an interpretation, it should carry the metadata of a dataset:
> *inputs, method, parameters, uncertainty, and provenance refs* ğŸ§¾

---

## ğŸ“¦ Canonical evidence DTO shape

KFM-friendly evidence outputs should be predictable, and â€œrefs-firstâ€.

### âœ… Recommended top-level fields

```json
{
  "kind": "model_run",
  "run_id": "run_...",
  "model": {
    "model_id": "landcover_rf_v3",
    "model_version": "3.2.1",
    "method": "random_forest",
    "software_versions": {
      "pipeline": "kfm-pipeline@abc123",
      "python": "3.11.x"
    }
  },
  "inputs": {
    "data_refs": [
      { "stac_item_id": "stac_item_...", "role": "training" }
    ],
    "roi": { "bbox": [-99, 37, -94, 40], "crs": "EPSG:4326" },
    "time_range": { "start": "1870-01-01T00:00:00Z", "end": "1870-12-31T23:59:59Z" }
  },
  "parameters": {
    "params_hash": "sha256:...",
    "summary": { "n_trees": 500, "max_depth": 12 }
  },
  "results": {
    "metrics": { "accuracy": 0.91, "f1_macro": 0.88 },
    "uncertainty": [
      { "kind": "confidence_interval", "level": 0.95, "value": { "low": 0.89, "high": 0.93 }, "units": "probability" }
    ]
  },
  "artifacts": {
    "asset_links": [
      { "rel": "preview", "href": "s3://.../preview.png", "type": "image/png" },
      { "rel": "data", "href": "s3://.../output.tif", "type": "image/tiff; application=geotiff; profile=cloud-optimized" }
    ]
  },
  "provenance": {
    "prov_activity_id": "prov_act_...",
    "prov_bundle_id": "prov_bundle_...",
    "dcat_dataset_id": "dcat_...",
    "stac_item_id": "stac_item_output_..."
  },
  "classification": "public",
  "redaction_notes": []
}
```

> [!TIP]
> Return **summaries** inline. Put heavy arrays/rasters/plots in storage and reference them via STAC assets and safe hrefs ğŸ“¦ğŸ”—

---

## ğŸ›ï¸ Parameters, hashes, and reproducibility

### Why this matters ğŸ§¬
Modeling results are only trustworthy if they can be reproduced or at least audited.

**Modeling mappers should shape and preserve:**
- `params_hash` â€” stable fingerprint of normalized parameters
- `data_hash` / `input_refs` â€” stable identifiers to inputs
- `run_id` â€” stable ID (not random unless supplied by domain)
- `seed` â€” when applicable
- `environment` / `software_versions` â€” pipeline version, dependencies (when available)
- `runtime_profile` â€” optional summary (elapsed time, hardware hints), but never secrets

### Normalization rules (recommended)
- sort keys deterministically
- normalize floats (avoid `repr()` drift)
- forbid NaN/inf
- strip/normalize strings
- allowlist enums (method names, metrics names)

> [!CAUTION]
> If a mapper changes floats differently on two machines, hashes will differ and provenance breaks. Keep float formatting stable.

---

## ğŸ“ˆ Uncertainty & diagnostics mapping

Modeling outputs without uncertainty are â€œpretty picturesâ€ â€” not evidence.

### Uncertainty kinds to support ğŸ²
- ğŸ“Š `confidence_interval` (frequentist)
- ğŸ² `credible_interval` (Bayesian)
- ğŸ§® `standard_error` / `std_dev`
- ğŸ§Š `error_bound` (numerical)
- âœ… `confusion_matrix_summary` (classification; include labels + counts, not huge matrices by default)

### Rules (must be explicit)
- include `kind`, `level` (where relevant), and `units`
- never label something â€œconfidenceâ€ without specifying *what* it means
- if a metric is computed on a subset/split, state the split (`train/val/test`)
- include diagnostic flags (e.g., `converged`, `diverged`, `ill_conditioned`, `non_identifiable`)

> [!TIP]
> For Bayesian outputs, clearly separate:
> - prior description (or `prior_ref`)
> - posterior summary
> - sampling diagnostics (effective sample size, R-hat) if available

---

## ğŸ—ºï¸ Spatial & temporal semantics

Modeling often depends on where and when:

### ROI (region of interest) ğŸ—ºï¸
- support bbox + polygon ROI (if the project supports it)
- carry CRS explicitly
- if ROI is generalized/redacted, mark it (and never â€œincrease precisionâ€ later)

### Time semantics â³
- ISO-8601 on the wire
- normalize to UTC internally
- support â€œfuzzyâ€ time when historically uncertain:
  - `start`, `end`, `certainty`, `source_ref`
- avoid inventing precision (no fake timestamps)

---

## ğŸ“¤ Outputs, assets, and payload size

### The cardinal rule ğŸ§±
**Do not inline huge outputs in API responses.**

Instead:
- emit a small summary DTO
- store heavy artifacts in storage
- reference them:
  - as `asset_links[]`
  - and via STAC assets/catalog refs

Common artifact types:
- ğŸ§Š COG rasters (GeoTIFF)
- ğŸ§± MVT tileset references
- ğŸ“„ CSV/parquet tables (metrics, time series)
- ğŸ–¼ï¸ PNG/JPEG previews
- ğŸ“„ PDF reports
- ğŸ§ª JSON manifests (run config + hashes)
- ğŸ“ˆ plot images (residual plots, calibration curves, ROC/PR)

> [!TIP]
> Include a **thumbnail/preview** asset whenever possible. Humans trust what they can see ğŸ‘€

---

## ğŸ” Classification, redaction, and safety

Modeling outputs can leak sensitive info by inference:
- precise locations
- rare event patterns
- individual-level measurements (even if anonymized poorly)

Mapper-level safety rules:
- never downgrade classification
- propagate redaction notes
- enforce precision policies on geo fields
- do not log raw payloads (use sanitized views)
- do not embed signed URLs/tokens in hrefs (emit references instead)

---

## ğŸ§¯ Error codes

Keep mapper errors stable and boring:

- `INVALID_MODEL_ID`
- `INVALID_RUN_ID`
- `INVALID_PARAMS`
- `INVALID_UNITS`
- `INVALID_TIME_RANGE`
- `INVALID_ROI`
- `INVALID_METRIC`
- `INVALID_UNCERTAINTY`
- `PAYLOAD_TOO_LARGE`
- `UNSAFE_HREF`
- `CLASSIFICATION_DOWNGRADE_ATTEMPT`

> [!IMPORTANT]
> Treat error codes like API surface. Changing them is a breaking change ğŸ“œğŸ’¥

---

## ğŸ§ª Testing strategy

### âœ… Unit tests
- deterministic hashing (same params â†’ same hash)
- uncertainty mapping correctness (kind/level/units)
- metric naming allowlists
- ROI/time validation edge cases
- â€œclassification never downgradesâ€

### âœ… Golden fixtures
```text
ğŸ§ª tests/
  ğŸ“ fixtures/
    ğŸ“ modeling/
      ğŸ“„ model_run_summary_regression_v1.json
      ğŸ“„ model_run_summary_bayes_v1.json
      ğŸ“„ model_run_summary_classification_v1.json
      ğŸ“„ simulation_run_summary_v1.json
      ğŸ“„ optimization_run_summary_v1.json
      ğŸ“„ problem_invalid_params.json
      ğŸ“„ problem_invalid_uncertainty.json
```

### âœ… Property tests (optional, high value)
- parameter normalization is stable across ordering changes
- float normalization is stable across platforms
- â€œrefs-firstâ€: asset links never include secret-like query params

---

## ğŸ§‘â€ğŸ’» Templates

### 1) Evidence summary DTO (illustrative) ğŸ§¾
```python
# ğŸ“„ api/src/adapters/mappers/modeling/results.py

from dataclasses import dataclass
from typing import Any, Optional

@dataclass(frozen=True)
class EvidenceSummaryDTO:
    kind: str
    run_id: str
    model_id: str
    model_version: Optional[str]
    params_hash: str
    data_refs: list[dict[str, Any]]
    metrics: dict[str, Any]
    uncertainty: list[dict[str, Any]]
    asset_links: list[dict[str, str]]
    provenance: dict[str, str]
    classification: Optional[str] = None
    redaction_notes: list[str] = None
```

### 2) Mapping a domain â€œrunâ€ to an evidence summary ğŸ“¤
```python
# ğŸ“„ api/src/adapters/mappers/modeling/results.py

def to_evidence_summary(domain_run) -> EvidenceSummaryDTO:
    # domain_run is produced by services/use-cases (already validated at business level)
    return EvidenceSummaryDTO(
        kind=domain_run.kind,
        run_id=domain_run.run_id,
        model_id=domain_run.model.model_id,
        model_version=getattr(domain_run.model, "model_version", None),
        params_hash=domain_run.params_hash,
        data_refs=[r.to_ref_dict() for r in domain_run.input_refs],
        metrics=dict(domain_run.metrics),
        uncertainty=[u.to_dict() for u in getattr(domain_run, "uncertainty", [])],
        asset_links=[a.to_link_dict() for a in getattr(domain_run, "asset_links", [])],
        provenance={
            "stac_item_id": getattr(domain_run.provenance, "stac_item_id", ""),
            "dcat_dataset_id": getattr(domain_run.provenance, "dcat_dataset_id", ""),
            "prov_activity_id": getattr(domain_run.provenance, "prov_activity_id", ""),
            "prov_bundle_id": getattr(domain_run.provenance, "prov_bundle_id", ""),
        },
        classification=getattr(domain_run, "classification", None),
        redaction_notes=list(getattr(domain_run, "redaction_notes", [])),
    )
```

### 3) Stable parameter hashing helper (delegates to common) #ï¸âƒ£
```python
# ğŸ“„ api/src/adapters/mappers/modeling/manifests.py

from typing import Any, Dict
# from api.src.adapters.mappers.common.hashing import stable_hash_dict  # recommended

def normalize_params(params: Dict[str, Any]) -> Dict[str, Any]:
    # Minimal normalization: sort keys; coerce simple scalar types.
    # Real implementation should:
    # - normalize floats deterministically
    # - enforce allowlists for enums/units
    # - reject NaN/inf
    return dict(sorted(params.items(), key=lambda kv: kv[0]))

def params_fingerprint(params: Dict[str, Any]) -> str:
    normalized = normalize_params(params)
    # return stable_hash_dict(normalized)
    # placeholder:
    import hashlib, json
    payload = json.dumps(normalized, sort_keys=True, separators=(",", ":"), ensure_ascii=False)
    return "sha256:" + hashlib.sha256(payload.encode("utf-8")).hexdigest()
```

> [!NOTE]
> Keep hashing deterministic and consistent with `mappers/common/` so all mappers agree on fingerprints.

---

## âœ… Definition of done

For any new/changed modeling mapper:

- [ ] Pure mapping (no I/O, no driver objects)
- [ ] Repro metadata included (run_id, model_id/version, params_hash, input refs)
- [ ] Uncertainty semantics explicit (kind, level, units)
- [ ] Diagnostics included when available (convergence, split info)
- [ ] Outputs are refs-first (large artifacts are links, not inline blobs)
- [ ] Catalog/provenance refs included (STAC/DCAT/PROV)
- [ ] Classification/redaction propagation enforced (no downgrade)
- [ ] Stable error codes and safe messages
- [ ] Unit tests + golden fixtures added
- [ ] Docs/examples updated with contract changes

---

## ğŸ“š Project bookshelf

<details>
<summary>ğŸ“š Click to expand â€” how the full project library informs modeling mapper rules</summary>

### ğŸ§ª Modeling, simulation, statistics, and uncertainty (core)
- ğŸ“„ **Scientific Modeling and Simulation_ A Comprehensive NASA-Grade Guide.pdf** â†’ reproducibility norms, verification/validation mindset, run documentation
- ğŸ“„ **Understanding Statistics & Experimental Design.pdf** â†’ experimental discipline, assumptions, reporting standards
- ğŸ“„ **regression-analysis-with-python.pdf** â†’ regression outputs, diagnostics shaping, metrics vocabulary
- ğŸ“„ **Regression analysis using Python - slides-linear-regression.pdf** â†’ compact summaries and reporting conventions
- ğŸ“„ **think-bayes-bayesian-statistics-in-python.pdf** â†’ credible intervals, priors/posteriors, uncertainty semantics
- ğŸ“„ **graphical-data-analysis-with-r.pdf** â†’ exploratory outputs as evidence artifacts (summaries + plots as assets)
- ğŸ“„ **Deep Learning for Coders with fastai and PyTorch - Deep.Learning.for.Coders.with.fastai.and.PyTorchpdf** â†’ ML run/version metadata, metrics and evaluation artifacts *(library item)*

### ğŸ›°ï¸ Remote sensing (model outputs must carry accuracy/uncertainty)
- ğŸ“„ **Cloud-Based Remote Sensing with Google Earth Engine-Fundamentals and Applications.pdf** â†’ classification products, accuracy assessment expectations, long-running workflows

### ğŸ•¸ï¸ Graph & optimization (derived results are evidence too)
- ğŸ“„ **Spectral Geometry of Graphs.pdf** â†’ graph-derived measures; encourages careful definitions and reproducible summaries
- ğŸ“„ **Generalized Topology Optimization for Structural Design.pdf** â†’ optimization runs: objectives/constraints/convergence + parameter tracking

### ğŸ—ºï¸ Geospatial representation (ROIs, precision, map-facing outputs)
- ğŸ“„ **python-geospatial-analysis-cookbook.pdf** â†’ CRS hygiene, geometry conventions, practical GIS output shaping
- ğŸ“„ **making-maps-a-visual-guide-to-map-design-for-gis.pdf** â†’ representation matters; avoid misleading precision; include previews/thumbnails
- ğŸ“„ **Mobile Mapping_ Space, Cartography and the Digital - 9789048535217.pdf** â†’ scale/context and privacy implications; mobile delivery constraints
- ğŸ“„ **compressed-image-file-formats-jpeg-png-gif-xbm-bmp.pdf** â†’ correct preview asset typing and compression tradeoffs
- ğŸ“„ **webgl-programming-guide-interactive-3d-graphics-programming-with-webgl.pdf** â†’ interactive client needs; avoid bloated payloads
- ğŸ“„ **responsive-web-design-with-html5-and-css3.pdf** â†’ predictable web contracts; stable DTO shapes

### ğŸ—„ï¸ Data systems & scaling (why refs-first + deterministic outputs)
- ğŸ“„ **PostgreSQL Notes for Professionals - PostgreSQLNotesForProfessionals.pdf** â†’ typing discipline; careful numeric/time handling
- ğŸ“„ **Scalable Data Management for Future Hardware.pdf** â†’ performance constraints; stable caching keys; avoid repeated serialization
- ğŸ“„ **Data Spaces.pdf** â†’ interoperability/federation framing; metadata as glue across systems

### ğŸ§  Humanism, accountability, and governance (why provenance is required)
- ğŸ“„ **Introduction to Digital Humanism.pdf** â†’ transparency and human-facing accountability
- ğŸ“„ **On the path to AI Lawâ€™s prophecies and the conceptual foundations of the machine learning age.pdf** â†’ accountability expectations for ML-derived claims
- ğŸ“„ **Principles of Biological Autonomy - book_9780262381833.pdf** â†’ systems thinking; track â€œwhy/howâ€ metadata for adaptive behavior

### ğŸ›¡ï¸ Security mindset (why safe hrefs and sanitized logs exist)
- ğŸ“„ **ethical-hacking-and-countermeasures-secure-network-infrastructures.pdf** â†’ threat modeling for input validation, exfiltration risks
- ğŸ“„ **Gray Hat Python - Python Programming for Hackers and Reverse Engineers (2009).pdf** â†’ adversarial thinking; donâ€™t trust inputs; donâ€™t leak secrets

### ğŸ§µ Concurrency/distributed runs (why explicit time/IDs matter)
- ğŸ“„ **concurrent-real-time-and-distributed-programming-in-java-threads-rtsj-and-rmi.pdf** â†’ distributed execution: retries, idempotency, determinism

### ğŸ§© Project direction docs (how modeling fits KFM)
- ğŸ“„ **Kansas Frontier Matrix (KFM) â€“ Comprehensive Technical Documentation.docx** â†’ system boundaries, API pipeline placement
- ğŸ“„ **ğŸŒŸ Kansas Frontier Matrix â€“ Latest Ideas & Future Proposals.docx** â†’ future-facing modeling/analysis integration direction

### ğŸ§° Programming compendium shelf (implementation reference)
- ğŸ“„ **A programming Books.pdf**
- ğŸ“„ **B-C programming Books.pdf**
- ğŸ“„ **D-E programming Books.pdf**
- ğŸ“„ **F-H programming Books.pdf**
- ğŸ“„ **I-L programming Books.pdf**
- ğŸ“„ **M-N programming Books.pdf**
- ğŸ“„ **O-R programming Books.pdf**
- ğŸ“„ **S-T programming Books.pdf**
- ğŸ“„ **U-X programming Books.pdf**

</details>

