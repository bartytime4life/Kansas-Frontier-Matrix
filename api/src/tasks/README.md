# ğŸ§© Background Tasks (api/src/tasks)

![scope](https://img.shields.io/badge/scope-api%2Fsrc%2Ftasks-blue)
![pattern](https://img.shields.io/badge/pattern-async%20jobs-orange)
![ops](https://img.shields.io/badge/ops-retries%20%7C%20idempotency%20%7C%20observability-success)

This folder defines **background jobs** (a.k.a. tasks) that run **outside** the request/response path.

They exist so the API can **authenticate + validate**, then **enqueue work**, return a **jobId**, and let the client **poll for status/results** without blocking the web server. âœ…

---

## ğŸ¯ What belongs here

- âœ… Long-running compute (simulation, ML retrain, geoprocessing)
- âœ… IO-heavy work (batch imports, external API pulls, large file transforms)
- âœ… Event-driven handlers (react to new data arrivals / internal events)
- âœ… Fan-out/fan-in orchestration (split work, aggregate outputs)
- âœ… Task status + progress reporting hooks

## ğŸš« What should NOT belong here

- âŒ HTTP request routing / controllers (API layer concern)
- âŒ UI formatting (frontend concern)
- âŒ One-off scripts with no operational contract (put in `/scripts` or `/tools`)
- âŒ Tiny synchronous work that should finish inside the request (<~100ms)

---

## ğŸ” Task lifecycle

> Mermaid note: this diagram avoids emojis + parentheses in labels to reduce GitHub/Mermaid parsing issues.

```mermaid
graph LR
  UI[UI Client] -->|POST action| API[API Endpoint]
  API -->|auth validate| AUTH[AuthN AuthZ]
  API -->|enqueue return jobId| Q[Queue Broker]
  Q -->|pull| W[Worker Pool]
  W -->|execute| T[Task Handler]
  T -->|store results| STORE[Storage]
  UI -->|GET status jobId| API
  API -->|read status results| STORE
```

### ğŸ§¯ Plain-text fallback (always works)

```text
UI/Client -> API -> auth/validate -> queue -> worker -> task -> storage
UI/Client -> API -> status/results -> storage
```

---

## ğŸ“¦ Task contract

Every task should read like a product: **name, version, inputs, outputs, and behavior**.

### Required metadata

- **name**: stable identifier (example: `simulation.run`)
- **version**: bump when *meaning* or *output schema* changes
- **input**: JSON-serializable payload (validated before enqueue)
- **output**: stored result payload OR a pointer (object key / URL)
- **status**: `PENDING | RUNNING | SUCCEEDED | FAILED | CANCELLED`
- **progress** (optional): `0..100`
- **timestamps**: created/started/finished (recommended)

### Operational guarantees âœ…

- **Idempotent where possible**
- **Retry-safe** (no double-charging, no duplicate writes)
- **Deterministic** given the same inputs + same version
- **Observable**: logs include `jobId`, `taskName`, `taskVersion`

---

## ğŸ—‚ï¸ Suggested layout

> Adjust to match the real codebase â€” the goal is consistent boundaries and discoverability.

ğŸ“ `api/src/tasks/`  
- ğŸ“„ `README.md` (this file)  
- ğŸ§° `registry.py` (task registration + routing)  
- ğŸ§¾ `schemas/` (input/output validation models)  
- ğŸ§  `handlers/` (task implementations)  
- ğŸ§ª `tests/` (unit + integration tests)  
- ğŸ§± `adapters/` (queue/db/object-store wrappers, if needed)

---

## ğŸ“¨ Enqueue pattern (pseudo-code)

### API layer

```python
# API controller/service layer
payload = validate_request(body)          # schema + domain validation
job_id = tasks.enqueue(
    name="simulation.run",
    payload=payload,
    requested_by=user.id,
)
return {"jobId": job_id}
```

### Worker side

```python
def simulation_run(job_id: str, payload: dict) -> None:
    tasks.mark_running(job_id)

    result = run_simulation(payload)

    tasks.store_result(job_id, result)
    tasks.mark_succeeded(job_id)
```

### Status polling

```python
# API layer
status = tasks.get_status(job_id)
# optionally: include result pointer if SUCCEEDED
return status
```

---

## ğŸ•’ Scheduling vs queuing

Use **queuing** when work is triggered by:
- a user action (button click / API call),
- an internal event (new data arrived),
- a webhook or external integration.

Use **scheduling** when work runs on a cadence (hourly/daily/weekly):
- ingestion refresh,
- backfills,
- maintenance tasks,
- periodic evaluation / retraining checks.

**Rule of thumb**
- ğŸ•°ï¸ Simple cadence: cron
- ğŸ›« DAGs with dependencies + monitoring: Airflow (or equivalent)

---

## ğŸ§· Reliability patterns (do these by default)

### ğŸ” Retries + backoff
- Prefer bounded retries with **exponential backoff**
- Classify failures:
  - transient (retry): timeouts, 5xx, network flake
  - permanent (fail fast): schema mismatch, forbidden, invalid inputs

### ğŸ§¬ Idempotency + dedup
- Introduce a **dedup key** when tasks can be triggered multiple times
- Prefer **upsert** semantics for data writes where appropriate
- Make re-runs safe (no duplicated outputs)

### ğŸ§¾ Lineage (traceability)
- Persist enough metadata so you can answer:
  - â€œWhat input produced this output?â€
  - â€œWhich job run generated this dataset/layer?â€

---

## ğŸ“¡ Observability checklist

- [ ] Every log line includes: `jobId`, `taskName`, `taskVersion`
- [ ] Progress updates are throttled (donâ€™t spam DB/queue)
- [ ] Errors store both:
  - human-readable message
  - machine-readable code/category
- [ ] Timeouts are explicit per task
- [ ] â€œPoison pillâ€ inputs are quarantined (donâ€™t retry forever)

---

## ğŸ§  Common task types in KFM-style systems

### ğŸŒ¾ Simulation runs
- long-running, heavy compute
- output: summary metrics + artifacts

### ğŸ›°ï¸ Geospatial processing
- raster/vector transforms
- tiling, aggregation, indexing
- output: layers + metadata

### ğŸ¤– ML retraining / batch inference
- scheduled or manual triggers
- output: model registry entries + evaluation artifacts

---

## âœ… Adding a new task (quick recipe)

1. ğŸ§¾ Define input/output schema (`schemas/`)
2. ğŸ§  Implement handler (`handlers/`)
3. ğŸ§° Register it (`registry.py`)
4. ğŸ§ª Add tests (`tests/`)
5. ğŸ“ Document it here:
   - name + version
   - payload example
   - expected outputs
   - retry + idempotency notes

---

## ğŸ§­ Next docs to add (recommended)

- `api/src/tasks/CONTRIBUTING.md` (local runner, test strategy, conventions)
- `api/src/tasks/ERRORS.md` (canonical error codes + retry categories)
- `api/src/tasks/STATUS.md` (status machine + transitions)
