# ğŸ§ª API Test Suite (`api/tests`)

![Tests](https://img.shields.io/badge/tests-automated-blue)
![Style](https://img.shields.io/badge/style-clean%20architecture-3c4)
![API](https://img.shields.io/badge/API-REST%20%2B%20GeoJSON-7aa)

Welcome to the **API boundary test suite** for the Kansas Frontier Matrix (KFM) backend.  
This folder is for testing the **web/API layer**: routing, request/response validation, auth, serialization, and orchestration behavior.

---

## ğŸ¯ What lives here?

This directory focuses on **API-facing behavior**:

- âœ… Request parsing + validation (query params, request bodies, headers)
- âœ… Authentication & authorization behavior (401/403, role checks, token expiry)
- âœ… Response shapes (JSON / GeoJSON / CSV / tiles) and status codes
- âœ… Error semantics (400 vs 404 vs 422 vs 500) + safe messages
- âœ… â€œThin controllerâ€ rules: API layer calls **use-cases/services** and formats output

> ğŸ’¡ The API should stay **thin**: it validates inputs, delegates to application logic, and formats outputs.

---

## ğŸ§­ Test Philosophy (KFM-friendly)

### ğŸ§¼ Clean Architecture first
- **Core logic should be testable without the framework.**
- API tests are **not** where business rules live â€” they verify the boundary glue.

### ğŸ§Š Deterministic & reproducible
- No live calls to remote sensing services / Earth Engine / external APIs.
- Prefer fakes/stubs/recorded responses for external dependencies.
- Seed randomness when ML/simulation endpoints are involved.

### ğŸ§¯ Safe by default
- Never point tests at production infrastructure.
- Integration tests must use **test databases** and disposable containers.

---

## âš¡ Quickstart

From repo root:

```bash
python -m pytest api/tests -q
```

Run a single test file:

```bash
python -m pytest api/tests/test_timeseries.py -q
```

Run tests matching a keyword:

```bash
python -m pytest api/tests -k timeseries -q
```

Stop on first failure + show locals:

```bash
python -m pytest api/tests -x --showlocals
```

> ğŸ§© If your repo provides a `Makefile`, itâ€™s recommended to add:
> - `make test`
> - `make test-api`
> - `make test-integration`

---

## ğŸ—‚ï¸ Suggested folder layout

> If your test suite grows, use a structure like this ğŸ‘‡

```text
ğŸ“¦ api/
â””â”€â”€ ğŸ§ª tests/
    â”œâ”€â”€ ğŸ“„ README.md
    â”œâ”€â”€ ğŸ§° conftest.py              # shared fixtures (client, tokens, db, etc.)
    â”œâ”€â”€ ğŸ§© unit/                    # pure unit tests for API helpers (no DB)
    â”‚   â”œâ”€â”€ test_validation.py
    â”‚   â””â”€â”€ test_serialization.py
    â”œâ”€â”€ ğŸŒ routes/                  # endpoint tests (FastAPI/Flask routes)
    â”‚   â”œâ”€â”€ test_health.py
    â”‚   â”œâ”€â”€ test_fields_timeseries.py
    â”‚   â”œâ”€â”€ test_simulation_run.py
    â”‚   â””â”€â”€ test_data_upload.py
    â”œâ”€â”€ ğŸ” auth/                    # authn/authz tests
    â”‚   â”œâ”€â”€ test_jwt.py
    â”‚   â””â”€â”€ test_rbac.py
    â”œâ”€â”€ ğŸ”Œ integration/             # DB + repository + API boundary
    â”‚   â”œâ”€â”€ test_postgis_queries.py
    â”‚   â””â”€â”€ test_task_queue.py
    â”œâ”€â”€ ğŸ¤ contract/                # OpenAPI/schema expectations
    â”‚   â””â”€â”€ test_openapi_contract.py
    â””â”€â”€ ğŸ§ª fixtures/                # small, synthetic JSON/GeoJSON/CSV inputs
        â”œâ”€â”€ field_minimal.json
        â”œâ”€â”€ timeseries_ndvi.json
        â””â”€â”€ geometry_county.geojson
```

---

## ğŸ§± Test Types we expect in KFM

### ğŸ§© Unit tests (fast)
**Goal:** test small pieces of API logic (pure functions / helpers)

Examples:
- query param parsing
- response serialization (entity â†’ JSON)
- validation rules for enums/ranges
- error mapping (domain error â†’ HTTP error)

âœ… Should run in < 1s locally.

---

### ğŸ”Œ Integration tests (real DB / real adapters)
**Goal:** verify behavior across layers when a real dependency matters:
- Postgres/PostGIS queries
- repository adapters (SQLAlchemy/raw SQL)
- migrations + schema expectations
- file/object storage stubs (local temp dirs)

**Recommended approach:**
- run via `docker compose` using a disposable test database container
- isolate using unique schemas or re-create DB per run

---

### ğŸ¤ Contract tests (API shape guarantees)
**Goal:** keep the API predictable for front-end and external integrators.

Examples:
- OpenAPI is generated and includes expected endpoints
- response schema for `timeseries` is stable
- error responses always include `error_code` + `message` (if thatâ€™s the project rule)

---

### ğŸ›°ï¸ Geospatial / Remote Sensing endpoint tests
KFM is geospatial-heavy, so we treat these as first-class:

- GeoJSON validity (geometry type present, coordinates parse)
- correct content-type headers
- bbox sanity checks (west < east, south < north)
- coordinate reference expectations (documented + consistent)
- tile endpoints return expected binary content + caching headers (if used)

---

### ğŸ§µ Long-running jobs (simulations, ML, heavy processing)
For â€œstart jobâ€ endpoints:
- âœ… returns a `job_id`
- âœ… returns quickly (typically `202 Accepted` or similar)
- âœ… status polling endpoint transitions correctly
- âœ… unauthorized users cannot query othersâ€™ jobs

---

### ğŸ“¡ Real-time updates (WebSockets/SSE)
If enabled:
- handshake tests
- subscription authorization tests
- basic message schema tests

---

## ğŸ§ª Writing a new API test

### âœ… Minimum checklist per new endpoint
For each new route, include:

- âœ… **happy path** (200/201/202)
- âœ… **invalid input** (400/422)
- âœ… **not found** (404) when applicable
- âœ… **unauthorized** (401)
- âœ… **forbidden** (403) for role-gated endpoints
- âœ… **serialization** shape check (keys + types that matter)

### ğŸ§  Keep tests readable
Prefer **Arrange â†’ Act â†’ Assert** and short fixtures.

```python
# Arrange
# Act
# Assert
```

---

## ğŸ§° Fixtures & Test Data Rules

### ğŸ“¦ Fixtures
- Put shared fixtures in `conftest.py`
- Prefer small, explicit fixtures over massive â€œkitchen sinkâ€ objects
- Keep fixture names domain-specific (e.g., `field_id`, `ndvi_timeseries`)

### ğŸ§¬ Synthetic test data
- Donâ€™t copy production datasets into tests
- Use *small*, *representative* samples
- Store reusable payloads under `fixtures/` as JSON/GeoJSON/CSV

---

## ğŸ” Auth testing tips

If using JWT-like tokens:
- include fixtures for:
  - valid user token
  - expired token
  - admin token
  - token with missing role claims

Also verify:
- endpoints enforce ownership rules (e.g., field belongs to user/org)

---

## ğŸ§¯ Common pitfalls

- ğŸ•’ **Timezones & date parsing** (always specify ISO-8601 in tests)
- ğŸ² **Randomness** (seed any non-deterministic code paths)
- ğŸ—ƒï¸ **DB state leakage** (ensure cleanup / transactions / recreate DB)
- ğŸŒ **External API calls** (block network in CI unless explicitly allowed)

---

## âœ… Definition of Done (DoD) for API work

A change is â€œdoneâ€ when:

- [ ] tests added/updated for new or changed behavior
- [ ] tests pass locally (`pytest api/tests`)
- [ ] CI is green (no merges on red)
- [ ] contract/shape remains stable (or is versioned intentionally)
- [ ] any new fixtures are minimal + documented

---

## ğŸ§¾ KFM-flavored endpoint examples to cover

These are typical patterns we want strong coverage for:

- `GET /api/field/{field_id}/timeseries?var=ndvi`
- `POST /api/simulation/run`
- `POST /api/data/upload` (admin / privileged)
- any GeoJSON or tile-serving endpoints (if present)

---

## ğŸ“š Handy pytest patterns

- Filter by keyword: `-k timeseries`
- Re-run last failures: `--lf`
- Show slow tests: `--durations=20`
- Add markers for test classes (recommended):
  - `@pytest.mark.unit`
  - `@pytest.mark.integration`
  - `@pytest.mark.contract`
  - `@pytest.mark.slow`

---

## ğŸ”„ Next improvements (nice-to-have)

- ğŸ§± Add `docker-compose.test.yml` for integration tests
- ğŸ“ˆ Add coverage reporting in CI (`pytest --cov ...`)
- ğŸš« Block outbound network in CI to enforce deterministic tests
- ğŸ§¾ Add OpenAPI contract test that fails on accidental breaking changes