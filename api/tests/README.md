<!-- According to a document from 2026-02-03 -->

<div align="center">

# ğŸ§ª API Tests (KFM) â€” `api/tests/`

**Evidence-first. Policy-gated. Reproducible.**  
ğŸ§· *â€œNo Source, No Answerâ€* is a **feature**, not a suggestion.

<br/>

![Python](https://img.shields.io/badge/Python-3.x-blue)
![pytest](https://img.shields.io/badge/pytest-test%20runner-brightgreen)
![FastAPI](https://img.shields.io/badge/FastAPI-API%20server-teal)
![Docker](https://img.shields.io/badge/Docker-Compose-informational)
![PostGIS](https://img.shields.io/badge/PostGIS-spatial%20db-blueviolet)
![Neo4j](https://img.shields.io/badge/Neo4j-knowledge%20graph-important)
![OpenAPI](https://img.shields.io/badge/OpenAPI-contracts-orange)
![GraphQL](https://img.shields.io/badge/GraphQL-optional-ff69b4)
![OPA](https://img.shields.io/badge/OPA-Policy%20as%20Code-purple)
![Conftest](https://img.shields.io/badge/Conftest-OPA%20CLI-6f42c1)
![Ollama](https://img.shields.io/badge/Ollama-local%20LLM-black)

</div>

---

## ğŸ§­ Why this test suite exists

KFM is **not** â€œjust a CRUD API.â€ Itâ€™s a **governed geospatial knowledge platform** with:

- ğŸ—ºï¸ Spatial + temporal datasets (PostGIS)  
- ğŸ§  Knowledge graph relationships (Neo4j)  
- ğŸ” Search + retrieval for evidence-backed answers  
- ğŸ¤– Focus Mode (RAG) that must produce **cited** outputs  
- ğŸ›¡ï¸ Runtime + CI policy gates (OPA/Rego + Conftest)  
- ğŸ§¾ Provenance / audit hooks (log what sources + policies produced what output)

This folder is where we encode those guarantees into executable proof.

> [!IMPORTANT]
> If the API returns a â€œnice answerâ€ but itâ€™s **uncited**, **policy-bypassing**, or **un-auditable**â€¦ thatâ€™s a **bug**.

---

## ğŸ—ºï¸ Table of contents

- [ğŸ“¦ What lives here](#-what-lives-here)
- [ğŸ›ï¸ Architectural contracts we enforce](#ï¸-architectural-contracts-we-enforce)
- [âš¡ Quickstart](#-quickstart)
- [ğŸ§ª How to run tests](#-how-to-run-tests)
- [ğŸ§· pytest markers](#-pytest-markers)
- [ğŸ—‚ï¸ Suggested folder layout](#ï¸-suggested-folder-layout)
- [ğŸ¤– Focus Mode test playbook](#-focus-mode-test-playbook)
- [ğŸ›¡ï¸ Policy, security, and governance tests](#ï¸-policy-security-and-governance-tests)
- [ğŸ“œ Contract tests](#-contract-tests)
- [ğŸ§° Troubleshooting](#-troubleshooting)
- [âœ… Contribution checklist](#-contribution-checklist)
- [ğŸ”— Useful cross-links](#-useful-cross-links)

---

## ğŸ“¦ What lives here?

This directory contains the **backend API test suite** for KFM:

- ğŸŒ REST endpoints (OpenAPI / Swagger)
- ğŸ§¬ GraphQL endpoint (if enabled)
- ğŸ¤– Focus Mode AI endpoint (RAG + citations + policy checks)
- ğŸ›¡ï¸ Security & governance behaviors (AuthN/AuthZ + RBAC + sensitivity rules)
- ğŸ”Œ Integration with core services (PostGIS, Neo4j, search, vector store, object storage, etc.)
- ğŸ§¾ Provenance / audit hooks (answers and data releases must be traceable)

---

## ğŸ›ï¸ Architectural contracts we enforce

KFMâ€™s â€œtrust storyâ€ is built on a few non-negotiable contracts. Tests exist to prevent drift.

### 1) âœ… The Truth Path must be respected
**Raw âœ Processed âœ Catalog âœ Databases âœ API âœ UI/AI**

No â€œback doors,â€ no direct DB calls from UI, no unpublished data leaking via internal endpoints.

### 2) âœ… Fail closed by default
If metadata is missing, policy is uncertain, or evidence is insufficient â†’ the system must **block**.

### 3) âœ… Focus Mode is advisory-only + explainable
The model is treated like an **untrusted narrator**:
- It can generate text
- It must cite
- It cannot bypass governance
- Its output must be policy-checked and auditable

---

## âš¡ Quickstart

> [!TIP]
> Keep the Compose stack running while you develop so integration tests can hit real services.

### 1) Bring up the dev stack ğŸ³
```bash
docker compose up -d
# legacy:
docker-compose up -d
```

### 2) Sanity-check the API is reachable ğŸ”
```bash
# Swagger UI (typical default)
open http://localhost:8000/docs

# Optional: readiness endpoint (if implemented)
curl -sS http://localhost:8000/readyz || true
```

### 3) Run the backend tests âœ…
```bash
# common service name:
docker compose exec api pytest

# some stacks name the service api-server:
docker compose exec api-server pytest
```

### 4) Run policy checks (Conftest) ğŸ›¡ï¸
```bash
# run all policies (repo-wide)
conftest test .

# or target the policy directory
conftest test policy/
```

---

## ğŸ§ª How to run tests

### Run fast-by-default_patch-first ğŸ§©
```bash
pytest -m "not integration and not slow"
```

### Run a single suite ğŸ“Œ
```bash
pytest api/tests/unit
pytest api/tests/contract
pytest api/tests/integration
pytest api/tests/ai
pytest api/tests/security
```

### Run with higher signal logs ğŸ§¯
```bash
pytest -q --disable-warnings -rA
```

### Parallelize locally (optional) âš¡
```bash
pytest -n auto
```

---

## ğŸ§· pytest markers

Markers keep CI fast and local runs intentional:

- `unit` â€” pure logic, no IO  
- `contract` â€” schema & endpoint shape  
- `integration` â€” requires Docker services running  
- `ai` â€” Focus Mode (mocked by default)  
- `security` â€” authn/authz + negative access tests  
- `policy` â€” policy pack behavior (OPA/Rego)  
- `slow` â€” big queries / heavy seeds / expensive paths  
- `external` â€” hits a remote API (off by default)

Example:
```bash
pytest -m unit
pytest -m "contract and not slow"
pytest -m "integration and not slow"
pytest -m "ai and not slow"
pytest -m "security or policy"
```

> [!NOTE]
> Add marker definitions to `pytest.ini` (or `pyproject.toml`) so unknown-markers donâ€™t silently rot.

---

## ğŸ—‚ï¸ Suggested folder layout

> Your exact structure may vary â€” but keep *intent* obvious.

```text
api/tests/
  README.md                ğŸ‘ˆ you are here ğŸ“
  conftest.py              ğŸ§© shared pytest fixtures
  unit/                    ğŸ§ª fast tests (no network / no DB)
  contract/                ğŸ“œ OpenAPI + schema + response-shape tests
  integration/             ğŸ§± requires Docker services (PostGIS/Neo4j/etc.)
  ai/                      ğŸ¤– Focus Mode + citation + policy gating tests
  security/                ğŸ›¡ï¸ authn/authz + negative access tests
  policy/                  ğŸ§· targeted Rego/OPA behavior tests (if needed)
  performance/             â±ï¸ smoke tests + regression guardrails (optional)
  fixtures/                ğŸ§° tiny deterministic datasets & payloads
  golden/                  ğŸ† â€œgoldenâ€ expected outputs (optional; keep small)
```

---

## ğŸ§© Fixtures & patterns

### âœ… FastAPI test client
Prefer in-process clients (`TestClient` / `httpx`) for unit/contract tests:
- Avoid â€œreal networkâ€ for unit tests  
- Use dependency overrides:
  - DB session / repository layer
  - policy engine client
  - model client (Ollama/OpenAI/etc.)
  - clock/time provider (for reproducible timestamps)

### ğŸ§ª Deterministic test data
- Keep fixtures tiny and readable (`api/tests/fixtures/`)
- Prefer explicit IDs (example style: `ks_hydrology_1880`)
- Use migrations/seeding **only** in integration tests
- Treat fixture files like public API: stable, small, and versioned

### ğŸ§¾ Audit/provenance assertions
Where applicable, validate:
- an audit event was emitted
- the event includes source IDs, policy decision outcome, and request metadata
- the logging path is append-only (implementation-dependent)

---

## ğŸ¤– Focus Mode test playbook

Focus Mode is a trust-critical surface. A â€œcorrect but uncitedâ€ answer is a failing behavior.

### ğŸ” Minimum trust guarantees to test

#### âœ… Citations required
- Answer contains bracket citations: `[1]`, `[2]`, â€¦
- Citations map to actual source metadata in the response payload (if your API returns it)

#### âœ… Prompt injection is neutralized
- Injection attempts do not alter system policy or reveal restricted data
- Prompt Gate behavior is observable (sanitize/strip/escape)

#### âœ… Policy enforces the output contract (OPA/Rego)
- **Missing citations â‡’ denied**
- **Disallowed content â‡’ denied**
- **Role violation â‡’ denied**
- Safe, non-leaky error messaging when denied

#### âœ… Provenance is recorded
- Question, selected sources, model ID/prompt version, and policy decision are logged

---

### ğŸ§ª â€œNo Source, No Answerâ€ regression suite (required)

Keep a dedicated suite that protects the core UX + trust mechanic:

- ğŸš« missing citations â‡’ denied  
- ğŸš« hallucinated citation IDs â‡’ rejected (if you validate citation IDs)  
- ğŸ§¾ citation mapping returns metadata (title/license/source id/etc.)  
- ğŸ” citation order deterministic when inputs deterministic (avoid flaky tests)

> [!WARNING]
> These tests prevent the worst regression class: â€œit still answersâ€¦ but without sources.â€

---

### ğŸ§  Mocked vs real LLM runs

**Default**: run Focus Mode tests with a mocked model client that returns deterministic outputs.

**Optional**: allow real-model runs behind a flag/marker (developer machine or specialized CI job).

Suggested pattern:
- `pytest -m ai` runs mocked model tests  
- `pytest -m ai --run-real-ollama` runs end-to-end (if Ollama service is available)

Example behaviors to assert with a real model:
- citation formatting survives generation
- policy check still denies unsafe outputs
- response latency doesnâ€™t exceed a basic smoke threshold (non-flaky)

> [!TIP]
> If you ever run AI regression tests in CI, prefer a smaller model + CPU mode to keep it stable.

---

## ğŸ›¡ï¸ Policy, security, and governance tests

KFM governance is **runtime-enforced** and **CI-enforced**.

### ğŸ§· Policy checks (OPA/Rego + Conftest)
Validate that:
- policy bundles load successfully
- key rules deny on missing metadata (license, sensitivity labels, provenance pointers)
- policy decisions produce expected HTTP codes (`401/403`) and safe error messages
- policy changes donâ€™t silently weaken gates (tests should fail loudly)

### ğŸ” RBAC & sensitivity
Test matrix should cover:
- Public viewer vs contributor vs maintainer vs admin roles (whatever your system defines)
- restricted dataset access blocked by default
- sensitive fields masked/redacted where policy demands it
- GraphQL introspection/admin fields locked down (if applicable)

---

## ğŸ“œ Contract tests

Contract tests prevent â€œworks on my machineâ€ API drift.

### Health & readiness ğŸ©º
If implemented, assert:
- `GET /healthz` returns a stable shape
- `GET /readyz` indicates dependency readiness
- `GET /version` includes semantic version/build metadata

### OpenAPI shape ğŸ”§
- `GET /openapi.json` returns valid JSON
- OpenAPI includes core endpoints (datasets, catalog/search, AI if enabled)
- response schemas match what UI expects (especially citations payload)

### Data catalog & datasets ğŸ—ƒï¸
Contract + integration tests for:
- metadata includes `license`, `title`, `description` at minimum
- catalog search supports filters (keyword, bbox, time range) where implemented
- dataset data endpoint supports format selection (e.g., GeoJSON) + bbox filtering where implemented

### GraphQL (if enabled) ğŸ§¬
- `/graphql` is reachable
- baseline introspection works (or is intentionally restricted)
- core query paths are stable and versioned

---

## ğŸ§° Troubleshooting

### ğŸ³ Compose flakiness / startup timing
- Re-run Compose if a dependency wasnâ€™t ready:
  ```bash
  docker compose up -d
  ```
- Check service logs:
  ```bash
  docker compose logs api
  docker compose logs db
  docker compose logs neo4j
  ```

### ğŸ”Œ Port conflicts
Common defaults:
- PostGIS: `5432`
- Neo4j: `7474`
- API: `8000`
- Web: `3000`
- Ollama: `11434`

If something is already using a port, stop local services or remap ports in `docker-compose.yml`.

### ğŸ§± Permissions on mounted volumes
If containers canâ€™t write to mounted paths:
- ensure `data/` (or the configured mount) is writable
- verify UID/GID mapping if on Linux
- on macOS/Windows, double-check Docker file sharing settings

### ğŸŒ± Missing sample data
If the API expects seeded data for integration tests:
- run whatever â€œseed/initâ€ script your repo provides (if present)
- or run a pipeline import inside the API container

Example pattern (only if your repo provides these scripts):
```bash
docker compose exec api python scripts/init_sample_data.py
# or
docker compose exec api python pipelines/import_some_dataset.py
```

---

## âœ… Contribution checklist

Before opening a PR:

- [ ] New endpoint? Add **contract + behavior** tests  
- [ ] Bug fix? Add a **regression test** (prove it never comes back)  
- [ ] Touch Focus Mode? Add **citation + policy gating** coverage  
- [ ] Touch metadata/pipelines? Ensure **policy checks** still pass  
- [ ] CI green locally: `pytest` + policy checks  
- [ ] Tests explain the **trust guarantee** they protect

---

## ğŸ”— Useful cross-links

From this directory (`api/tests/`), these are usually relevant:

- `../../src/server/api/README.md` â€” API surface area & examples  
- `../../docs/architecture/ai/OLLAMA_INTEGRATION.md` â€” Focus Mode / RAG pipeline behavior  
- `../../docs/architecture/AI_SYSTEM_OVERVIEW.md` â€” AI system architecture & guardrails  
- `../../docs/architecture/system_overview.md` â€” system layering & â€œtruth pathâ€  
- `../../policy/` â€” OPA/Rego policies (governance gates)  
- `../../pipelines/README.md` â€” ingestion/build steps that affect integration tests  

---

## ğŸ§  Guiding principle

> If a test canâ€™t explain **what trust guarantee it protects**, it probably belongs in `unit/`â€¦ or doesnâ€™t belong at all. ğŸ˜‰