# ğŸ›°ï¸ STAC Schemas (KFM Contract Artifacts)

![Contract-first](https://img.shields.io/badge/contract--first-âœ…-blue)
![Evidence-first](https://img.shields.io/badge/evidence--first-ğŸ“-brightgreen)
![Metadata](https://img.shields.io/badge/metadata-STAC%20%7C%20DCAT%20%7C%20PROV-6f42c1)
![Governance](https://img.shields.io/badge/FAIR%2BCARE-âš–ï¸-orange)

> [!IMPORTANT]
> **This folder is part of KFMâ€™s â€œcontract-firstâ€ boundary.**  
> If you change a schema here, you are changing what the API, pipelines, and UI are allowed to produce/consume. Treat this like changing code interfaces. ğŸ§©

**Quick links:**  
[Why STAC in KFM](#why-stac-in-kfm) â€¢ [What lives here](#what-lives-here) â€¢ [Folder layout](#folder-layout) â€¢ [Schema strategy](#schema-strategy) â€¢ [Validation](#validation--ci-gates) â€¢ [KFM STAC profile](#kfm-stac-profile) â€¢ [Governance](#governance--safety) â€¢ [Contributing](#contributing--change-workflow)

---

## Why STAC in KFM

KFM is built as a **Kansas â€œliving atlasâ€** where datasets, maps, and model outputs are:

- discoverable ğŸ§­  
- mappable ğŸ—ºï¸  
- auditable ğŸ”  
- reproducible â™»ï¸  

To make that real (and interoperable with the broader geospatial ecosystem), **each published dataset is described by a STAC JSON record** and paired with provenance metadata and discovery metadata (PROV + DCAT). This is an explicit architectural requirement in the KFM documentation and repo audit (STAC for spatial/temporal + assets, PROV for lineage, DCAT for discovery).  

> [!NOTE]
> In KFMâ€™s canonical pipeline ordering, **STAC/DCAT/PROV are â€œboundary artifactsâ€ required before data is considered published** (before graph ingestion, API exposure, UI layers, or Story Nodes). âœ…

---

## What lives here

This directory defines **JSON Schema contracts** for STAC objects **as KFM uses and serves them**.

Typical responsibilities:

- âœ… Validate **STAC Items / Collections / Catalogs** produced by pipelines
- âœ… Validate STAC objects **served by the API** (REST and/or GraphQL wrappers)
- âœ… Enforce **KFM-specific profile rules** (namespaced fields, governance tags, provenance hooks)
- âœ… Provide **examples** used in CI to prevent regressions

Non-goals:

- âŒ This is **not** the runtime metadata store (that lives under `data/stac/**`)
- âŒ This is **not** where real datasets or assets belong
- âŒ This is **not** a place to â€œjust add a fieldâ€ without updating the profile + validators

---

## Folder layout

> [!TIP]
> Keep schemas **boring and predictable**: stable names, stable `$id`s, and versioned changes only.

Recommended structure (add files as needed):

```text
ğŸ“ api/contracts/schemas/stac/
â”œâ”€ ğŸ“„ README.md                         ğŸ‘ˆ you are here
â”œâ”€ ğŸ“ upstream/                         ğŸ”’ vendored STAC core schemas (pinned snapshot)
â”‚  â”œâ”€ ğŸ“„ catalog.schema.json
â”‚  â”œâ”€ ğŸ“„ collection.schema.json
â”‚  â””â”€ ğŸ“„ item.schema.json
â”œâ”€ ğŸ“ kfm/                              ğŸŒ¾ KFM overlays (KFM profile â€œmust-havesâ€)
â”‚  â”œâ”€ ğŸ“„ kfm.stac.catalog.schema.json
â”‚  â”œâ”€ ğŸ“„ kfm.stac.collection.schema.json
â”‚  â”œâ”€ ğŸ“„ kfm.stac.item.schema.json
â”‚  â””â”€ ğŸ“„ kfm.shared.schema.json         (shared defs: classification, provenance refs, etc.)
â”œâ”€ ğŸ“ extensions/                       ğŸ§© optional: vendored/pinned extensions used by KFM
â”‚  â”œâ”€ ğŸ“„ projection.schema.json
â”‚  â”œâ”€ ğŸ“„ eo.schema.json
â”‚  â””â”€ ğŸ“„ raster.schema.json
â””â”€ ğŸ“ examples/                         âœ… â€œgoldenâ€ STAC objects used by CI + docs
   â”œâ”€ ğŸ“„ example.item.public.json
   â”œâ”€ ğŸ“„ example.item.restricted.redacted.json
   â””â”€ ğŸ“„ example.collection.json
```

---

## Schema strategy

KFMâ€™s schema approach follows **open-standards-first + project overlays**:

### 1) Vendor upstream STAC schemas (donâ€™t fork)

- Keep upstream STAC schemas **verbatim** under `upstream/`
- Pin them to a specific version/commit in your tooling (so CI is deterministic)
- Never â€œpatchâ€ upstream files in-place â€” layer KFM constraints in `kfm/`

### 2) Apply KFM rules via overlays (`allOf` + `$ref`)

Your KFM schema should generally look like:

- `allOf: [ { $ref: "../upstream/item.schema.json" }, { ...kfm constraints... } ]`

This preserves interoperability while enforcing KFM invariants.

### 3) Namespaced custom fields only

To avoid collisions with STAC core and community extensions:

- âœ… Use `kfm:*` for project-specific fields  
- âŒ Do not introduce bare top-level properties like `"classification"` or `"run_id"` without namespacing

---

## Validation + CI gates

> [!IMPORTANT]
> KFMâ€™s docs require that **no dataset is accepted without valid metadata** and that CI rejects missing/invalid catalog artifacts.

### Local validation (examples)

Use whatever validator is standard in the repo tooling; typical options:

#### âœ… Python (jsonschema)
```bash
python -m jsonschema \
  -i data/stac/items/<domain>/<item>.json \
  api/contracts/schemas/stac/kfm/kfm.stac.item.schema.json
```

#### âœ… Node (ajv)
```bash
npx ajv-cli validate \
  -s api/contracts/schemas/stac/kfm/kfm.stac.item.schema.json \
  -d data/stac/items/<domain>/<item>.json \
  --strict=false
```

### CI expectations (recommended)

A PR that adds or updates a dataset should:

- validate all new/changed `data/stac/**` records against these schemas
- fail if:
  - required fields are missing (license, provenance pointers, classification, etc.)
  - links are broken / assets are missing
  - â€œrestrictedâ€ inputs appear as â€œpublicâ€ outputs (classification propagation violation)

---

## KFM STAC profile

KFM treats STAC as the **asset-level truth** for geospatial (and even â€œmostly non-spatialâ€) datasets.

### Required alignment across catalogs (KFM policy)

Every new dataset/evidence artifact must have:

- **STAC Collection + Item(s)** (asset indexing; spatial/temporal extent; license)  
- **DCAT Dataset entry** (high-level discovery + distributions that point to STAC or downloads)  
- **PROV activity bundle** (raw â†’ work â†’ processed lineage; run/config identifiers)

And the cross-links must remain consistent:

- STAC Items â†’ link to actual assets in `data/processed/**` or stable API endpoints  
- DCAT â†’ distributions should reference STAC and/or the underlying resources  
- PROV â†’ must link the full chain and identify pipeline run/config (run id / commit hash)  
- Graph â†’ should store **references** to catalog IDs (not bulky payloads)

### KFM-specific fields (typical)

KFM extends STAC with project metadata that supports governance, provenance, and uncertainty.

> [!NOTE]
> The authoritative definition belongs in:  
> `docs/standards/KFM_STAC_PROFILE.md` (and sibling DCAT/PROV profiles).

Commonly expected KFM fields live in `properties` and are namespaced:

- `kfm:dataset_id` â€” stable dataset identifier (human-readable slug)
- `kfm:domain` â€” domain module (e.g., `air-quality`, `soils`, `historical`)
- `kfm:topic` â€” short topic tag (optional but useful for search)
- `kfm:classification` â€” sensitivity tag (see [Governance](#governance--safety))
- `kfm:redaction` â€” if coordinates/attributes were generalized, how + why
- `kfm:provenance` â€” pointers to PROV bundles + run identifiers
- `kfm:uncertainty` â€” uncertainty indicators (if applicable for modeled/estimated layers)

### Minimal example (STAC Item + KFM additions)

```json
{
  "type": "Feature",
  "stac_version": "1.0.0",
  "stac_extensions": [],
  "id": "kfm.ks.soils.sda.2024.v1__tile_001",
  "collection": "kfm.ks.soils.sda.2024.v1",
  "geometry": { "type": "Polygon", "coordinates": [[[0,0],[1,0],[1,1],[0,1],[0,0]]] },
  "bbox": [0, 0, 1, 1],
  "properties": {
    "datetime": "2024-01-15T00:00:00Z",

    "kfm:dataset_id": "kfm.ks.soils.sda.2024.v1",
    "kfm:domain": "soils",
    "kfm:classification": "public",

    "kfm:provenance": {
      "prov_bundle": "data/prov/soils/sda/kfm.ks.soils.sda.2024.v1.prov.jsonld",
      "run_id": "pipeline_run_2024-01-15T00:00:00Z"
    }
  },
  "assets": {
    "cog": {
      "href": "data/processed/soils/sda/kfm.ks.soils.sda.2024.v1.tile_001.cog.tif",
      "type": "image/tiff; application=geotiff; profile=cloud-optimized",
      "roles": ["data"]
    }
  },
  "links": [
    { "rel": "describedby", "href": "data/catalog/dcat/kfm.ks.soils.sda.2024.v1.jsonld", "type": "application/ld+json" },
    { "rel": "provenance", "href": "data/prov/soils/sda/kfm.ks.soils.sda.2024.v1.prov.jsonld", "type": "application/ld+json" }
  ]
}
```

---

## Versioning rules

> [!IMPORTANT]
> KFM is versioned at **dataset level** and **system level**, and breaking contract changes require coordination.

### Dataset versioning

When a dataset is updated or reprocessed:

- publish a new dataset version
- link revisions in DCAT and PROV (e.g., `prov:wasRevisionOf`)
- keep prior versions discoverable (or explicitly sunset with notice)
- prefer persistent identifiers when available (e.g., DOI/ARK) for citation stability

### Contract versioning (schemas in this folder)

When changing schema behavior:

- âœ… add/adjust `examples/` to lock behavior in CI  
- âœ… bump contract version (per repo conventions)  
- âœ… ensure compatibility (or introduce a versioned parallel schema)  

---

## Governance + safety

KFM governance is not a side policy â€” it is encoded into the pipeline and contracts.

### Classification propagation (non-negotiable)

> [!WARNING]
> **No output artifact can be less restricted than its inputs.**  
> If raw data is confidential, the STAC record must reflect that classification, and downstream UI/API must not â€œleakâ€ it.

### Redaction is multi-layered

When sensitive data requires redaction/generalization:

- âœ… apply redaction in `data/processed/**`
- âœ… flag it in STAC/DCAT metadata (so itâ€™s transparent what changed)
- âœ… enforce access controls/redaction in the API
- âœ… maintain UI safeguards (no client-side bypass)

### Recommended schema-level enforcement

In your `kfm.*.schema.json` overlays, enforce:

- `kfm:classification` enum (e.g., `public | restricted | confidential | sacred | ...` per governance)
- required presence of `kfm:redaction` object when classification implies generalization
- provenance pointers required (`kfm:provenance.prov_bundle`, run id / commit hash)

---

## How this fits the larger KFM pipeline

```mermaid
flowchart LR
  RAW["data/raw/**"] --> ETL["ETL + Normalization"]
  ETL --> STAC["STAC Items + Collections"]
  STAC --> DCAT["DCAT Dataset Views"]
  STAC --> PROV["PROV Lineage Bundles"]
  STAC --> GRAPH["Neo4j Graph (references to catalogs)"]
  GRAPH --> API["API Layer (contracts + redaction)"]
  API --> UI["Map UI + Story Nodes + Focus Mode"]
```

---

## Contributing + change workflow

### If youâ€™re adding a dataset âœ…

1. Put raw inputs under `data/raw/<domain>/`
2. Produce processed outputs under `data/processed/<domain>/`
3. Generate boundary artifacts:
   - `data/stac/collections/**`
   - `data/stac/items/**`
   - `data/catalog/dcat/**`
   - `data/prov/**`
4. Validate STAC against these schemas (local + CI)
5. Ensure Story Nodes (if any) only cite cataloged assets (no uncited narrative)

### If you need a new metadata field ğŸ§©

1. Update the profile doc first: `docs/standards/KFM_STAC_PROFILE.md`
2. Update `api/contracts/schemas/stac/kfm/**` to match
3. Add/modify `examples/**` so CI locks the behavior
4. If breaking, follow versioning rules (parallel schema or bump)

---

## Related docs (repo internal)

- ğŸ“˜ `docs/MASTER_GUIDE_v13.md` â€” canonical pipeline ordering + invariants  
- ğŸ“ `docs/standards/KFM_STAC_PROFILE.md` â€” KFM STAC profile definition  
- ğŸ§¾ `docs/standards/KFM_DCAT_PROFILE.md` / `docs/standards/KFM_PROV_PROFILE.md` â€” sibling metadata profiles  
- ğŸ§­ `docs/templates/TEMPLATE__API_CONTRACT_EXTENSION.md` â€” how to change contracts safely  
- ğŸ—‚ï¸ Domain examples:
  - `docs/data/historical/land-treaties/README.md`
  - `docs/data/air-quality/README.md`
  - `docs/data/soils/sda/README.md`

---

## FAQ

### â€œWhere do the real STAC JSON files live?â€
In the data catalogs, typically:

- `data/stac/collections/**`
- `data/stac/items/**`

This folder is **only the schema contract** the rest of the system depends on.

### â€œDo we implement STAC API endpoints?â€
Thatâ€™s an API-layer decision. If/when we do, **those endpoints should reference these schema contracts**, and the OpenAPI spec should remain the source of truth for request/response shapes.

### â€œCan non-spatial datasets use STAC?â€
Yes. KFMâ€™s policy is that even â€œnon-spatialâ€ datasets often carry a STAC Collection for consistency, and many have some spatial/temporal footprint (even if coarse). If not, record the rationale in metadata.

ğŸ§©ğŸŒ¾
