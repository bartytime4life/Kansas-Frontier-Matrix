# ğŸ—„ï¸ Database Scripts â€” `api/scripts/db`

![PostgreSQL](https://img.shields.io/badge/PostgreSQL-Database-336791?logo=postgresql&logoColor=white)
![PostGIS](https://img.shields.io/badge/PostGIS-Spatial%20SQL-2E7D32)
![Neo4j](https://img.shields.io/badge/Neo4j-Knowledge%20Graph-008CC1?logo=neo4j&logoColor=white)
![Docker](https://img.shields.io/badge/Docker-Compose-2496ED?logo=docker&logoColor=white)
![STAC/DCAT/PROV](https://img.shields.io/badge/Metadata-STAC%20%7C%20DCAT%20%7C%20PROV-7B1FA2)
![Idempotent](https://img.shields.io/badge/Scripts-Idempotent%20by%20default-success)

> According to a document from **January 12, 2026**, KFMâ€™s DB layer centers on **PostGIS** (fast bounding-box queries & spatial indexing) plus **tile generation/caching** for web mapping. :contentReference[oaicite:0]{index=0}

---

## ğŸ¯ What this folder is

This directory is the **DB runbook + automation toolbox** for Kansas Frontier Matrix (KFM). Itâ€™s where we keep scripts that:

- ğŸ§± **Provision** local/dev/test databases (and optionally staging)
- ğŸ§¬ **Enable extensions** (PostGIS, etc.) and create schemas
- ğŸ§° **Apply migrations** and keep schema evolution repeatable
- ğŸŒ± **Seed** baseline reference tables (domains, vocabularies, system defaults)
- ğŸ—ºï¸ **Load spatial layers** into PostGIS (vector/raster metadata where relevant)
- ğŸ§ª **Validate** data quality (geometry validity, foreign keys, provenance links)
- ğŸ•¸ï¸ **Export / refresh graph references** used by Neo4j (graph = relationships, not payload) :contentReference[oaicite:1]{index=1}
- ğŸ’¾ **Backup/restore** (pg_dump, snapshots, and smoke tests)

> KFM also uses **Neo4j** for semantic/relationship queries aligned with standards like CIDOC-CRM, GeoSPARQL, and OWL-Time. :contentReference[oaicite:2]{index=2}

---

## ğŸ§­ Table of contents

- [ğŸš¦ Safety first](#-safety-first)
- [ğŸ“¦ Folder layout](#-folder-layout)
- [âš™ï¸ Prerequisites](#ï¸-prerequisites)
- [ğŸ” Configuration](#-configuration)
- [ğŸš€ Quickstart workflows](#-quickstart-workflows)
- [ğŸ—ºï¸ PostGIS patterns we rely on](#ï¸-postgis-patterns-we-rely-on)
- [ğŸ§¾ Metadata + provenance contract](#-metadata--provenance-contract)
- [ğŸ•¸ï¸ Neo4j graph refresh](#ï¸-neo4j-graph-refresh)
- [ğŸ§ª Testing + QA](#-testing--qa)
- [ğŸ›¡ï¸ Security notes](#ï¸-security-notes)
- [âš¡ Performance notes](#-performance-notes)
- [ğŸ“š Project library](#-project-library)

---

## ğŸš¦ Safety first

> [!WARNING]
> Many DB scripts are **destructive** (drop/reset). Never point them at production unless you *explicitly* mean to.

Recommended guardrails:
- Require `--yes-really` for destructive actions
- Require `ENV=local|test|staging|prod` and refuse `prod` unless `ALLOW_PROD=1`
- Print the resolved connection string **host + dbname** before doing anything
- Default to **idempotent** operations (e.g., `CREATE EXTENSION IF NOT EXISTS`, `CREATE TABLE IF NOT EXISTS`)

---

## ğŸ“¦ Folder layout

> This is the **recommended** structure for this directory (adjust to match what exists in the repo).

```text
ğŸ“‚ api/scripts/db/
â”œâ”€â”€ ğŸ“„ README.md                         # ğŸ‘ˆ you are here
â”œâ”€â”€ ğŸ“‚ docker/                           # ğŸ³ local containers (optional)
â”‚   â””â”€â”€ ğŸ“„ compose.db.yml
â”œâ”€â”€ ğŸ“‚ postgres/                         # ğŸ˜ Postgres/PostGIS SQL
â”‚   â”œâ”€â”€ ğŸ“„ 00_extensions.sql             # ğŸ§© postgis, pgcrypto, pg_stat_statements...
â”‚   â”œâ”€â”€ ğŸ“„ 01_roles.sql                  # ğŸ‘¥ least-privilege roles
â”‚   â”œâ”€â”€ ğŸ“„ 02_schemas.sql                # ğŸ—ï¸ catalog / geodata / audit / app
â”‚   â”œâ”€â”€ ğŸ“„ 03_tables.sql                 # ğŸ§± core tables
â”‚   â”œâ”€â”€ ğŸ“„ 04_indexes.sql                # âš¡ spatial + btree + gin
â”‚   â”œâ”€â”€ ğŸ“„ 05_views.sql                  # ğŸ‘“ read models
â”‚   â”œâ”€â”€ ğŸ“„ 06_functions.sql              # ğŸ§  helpers, triggers, utilities
â”‚   â””â”€â”€ ğŸ“‚ seeds/                        # ğŸŒ± deterministic seeds
â”‚       â”œâ”€â”€ ğŸ“„ 00_reference.sql
â”‚       â””â”€â”€ ğŸ“„ 10_demo_data.sql
â”œâ”€â”€ ğŸ“‚ migrations/                       # ğŸ” alembic/flyway/sql-based migrations
â”‚   â””â”€â”€ ğŸ“„ ...                           # (tool-specific)
â”œâ”€â”€ ğŸ“‚ neo4j/                            # ğŸ•¸ï¸ graph constraints/imports (optional)
â”‚   â”œâ”€â”€ ğŸ“„ 00_constraints.cypher         # ğŸ”’ uniqueness constraints
â”‚   â”œâ”€â”€ ğŸ“„ 10_import_nodes.cypher
â”‚   â”œâ”€â”€ ğŸ“„ 20_import_edges.cypher
â”‚   â””â”€â”€ ğŸ“„ 99_post_import.cypher
â”œâ”€â”€ ğŸ“‚ qa/                               # ğŸ§ª validation + smoke checks
â”‚   â”œâ”€â”€ ğŸ“„ 00_smoke.sql                  # âœ… quick health checks
â”‚   â”œâ”€â”€ ğŸ“„ 10_geometry_validity.sql      # ğŸ§­ ST_IsValid checks
â”‚   â”œâ”€â”€ ğŸ“„ 20_catalog_links.sql          # ğŸ”— STAC/DCAT/PROV referential checks
â”‚   â””â”€â”€ ğŸ“„ validate_catalogs.py          # ğŸ§¾ metadata sanity checks (no secrets)
â””â”€â”€ ğŸ“‚ tools/                            # ğŸ› ï¸ CLI wrappers / runbooks
    â”œâ”€â”€ ğŸ“„ db_up.sh                      # â–¶ï¸ start local DBs
    â”œâ”€â”€ ğŸ“„ db_down.sh                    # â¹ï¸ stop local DBs
    â”œâ”€â”€ ğŸ“„ db_reset.sh                   # ğŸ’¥ drop + recreate (guarded)
    â”œâ”€â”€ ğŸ“„ db_migrate.sh                 # ğŸ” apply migrations
    â”œâ”€â”€ ğŸ“„ db_seed.sh                    # ğŸŒ± seed reference data
    â”œâ”€â”€ ğŸ“„ db_load_geodata.sh            # ğŸ—ºï¸ import processed layers
    â”œâ”€â”€ ğŸ“„ db_backup.sh                  # ğŸ’¾ pg_dump
    â””â”€â”€ ğŸ“„ db_restore.sh                 # â™»ï¸ restore
```

---

## âš™ï¸ Prerequisites

- ğŸ³ Docker + Docker Compose (recommended for local/dev)
- ğŸ˜ `psql` + `pg_dump` (PostgreSQL client tooling)
- ğŸ§­ PostGIS-enabled PostgreSQL (local container is easiest)
- ğŸ•¸ï¸ Neo4j (optional, only if youâ€™re running the graph layer locally)

---

## ğŸ” Configuration

Typical env vars (adapt to your `.env` conventions):

| Variable | Example | Notes |
|---|---:|---|
| `DATABASE_URL` | `postgresql://kfm_app:pass@localhost:5432/kfm` | Prefer a single URL |
| `POSTGRES_HOST` | `localhost` | If not using `DATABASE_URL` |
| `POSTGRES_DB` | `kfm` |  |
| `POSTGRES_USER` | `kfm_app` | App role |
| `POSTGRES_PASSWORD` | `...` | Donâ€™t commit |
| `NEO4J_URI` | `bolt://localhost:7687` | Optional |
| `NEO4J_USER` | `neo4j` | Optional |
| `NEO4J_PASSWORD` | `...` | Optional |
| `KFM_DATA_DIR` | `./data` | Used for loading assets |

> [!NOTE]
> If your repo follows KFMâ€™s metadata contract, your DB loads should treat **`data/processed/**`** as the stable output location, with catalogs written to `data/stac/`, `data/catalog/dcat/`, and `data/prov/`. :contentReference[oaicite:3]{index=3}

---

## ğŸš€ Quickstart workflows

### 1) Bring up local DBs ğŸ³
```bash
# recommended
./tools/db_up.sh
```

### 2) Initialize core schema ğŸ§±
```bash
# enable extensions + schemas + base tables
psql "$DATABASE_URL" -f postgres/00_extensions.sql
psql "$DATABASE_URL" -f postgres/02_schemas.sql
psql "$DATABASE_URL" -f postgres/03_tables.sql
psql "$DATABASE_URL" -f postgres/04_indexes.sql
```

> If you prefer a dedicated schema (instead of dumping everything into `public`), thatâ€™s a common PostGIS workflow: `CREATE EXTENSION postgis; CREATE SCHEMA ...;` :contentReference[oaicite:4]{index=4}

### 3) Apply migrations ğŸ”
```bash
./tools/db_migrate.sh
```

### 4) Seed reference data ğŸŒ±
```bash
./tools/db_seed.sh
```

### 5) Load processed geodata ğŸ—ºï¸
```bash
# loads from data/processed/** and registers metadata links
./tools/db_load_geodata.sh
```

### 6) Validate everything âœ…
```bash
psql "$DATABASE_URL" -f qa/00_smoke.sql
psql "$DATABASE_URL" -f qa/10_geometry_validity.sql
python qa/validate_catalogs.py
```

---

## ğŸ—ºï¸ PostGIS patterns we rely on

### Spatial indexing + bbox queries
KFMâ€™s API expects **fast â€œwhatâ€™s in this map view?â€** queries (bbox / viewport). PostGIS provides spatial indexing and SQL patterns for this. :contentReference[oaicite:5]{index=5}

**Example (viewport bbox):**
```sql
-- bbox in WGS84 (EPSG:4326) for example purposes
SELECT id, name, geom
FROM geodata.features
WHERE geom && ST_MakeEnvelope(:min_lon, :min_lat, :max_lon, :max_lat, 4326)
  AND ST_Intersects(geom, ST_MakeEnvelope(:min_lon, :min_lat, :max_lon, :max_lat, 4326));
```

**Index to support it:**
```sql
CREATE INDEX IF NOT EXISTS idx_features_geom_gist
ON geodata.features
USING GIST (geom);
```

### Tiles (vector tiles / caching)
For web mapping, KFM mentions generating vector tiles and caching for performance. :contentReference[oaicite:6]{index=6}

Where DB scripts fit:
- precompute / refresh tile sources (views/materialized views)
- ensure geometry is valid + simplified appropriately per zoom level
- keep â€œtile-readyâ€ queries stable and versioned

---

## ğŸ§¾ Metadata + provenance contract

KFM treats metadata artifacts as **boundary contracts** between pipelines â†’ graph â†’ API â†’ UI. The required trio:

- **STAC** (Collections + Items) for geospatial assets
- **DCAT** dataset entry for discovery
- **PROV** for lineage (inputs â†’ steps â†’ outputs) :contentReference[oaicite:7]{index=7}

Cross-linking expectations include:
- STAC Items link to actual assets in `data/processed/**` (or stable storage) :contentReference[oaicite:8]{index=8}
- DCAT distributions point to STAC or data resources :contentReference[oaicite:9]{index=9}
- PROV records connect raw â†’ work â†’ processed and capture run/config identifiers :contentReference[oaicite:10]{index=10}

### DB implication ğŸ§©
Your DB scripts should **not** â€œsilently ingestâ€ data:
- every load/import should record:
  - dataset IDs (STAC collection/item IDs)
  - provenance IDs (PROV bundle identifiers)
  - license + source attribution pointers (DCAT fields)

> If your KFM profiles include project-specific fields (e.g., provenance references & uncertainty indicators), DB scripts should validate those exist before â€œpublishingâ€ an asset. :contentReference[oaicite:11]{index=11}

---

## ğŸ•¸ï¸ Neo4j graph refresh

**Graph rule of thumb:** the graph stores **relationships + references** (IDs/links), not bulky payloads. :contentReference[oaicite:12]{index=12}

### Typical workflow
1. DB exports a â€œgraph-readyâ€ dataset:
   - nodes (entities)
   - edges (relationships)
   - references back to STAC/DCAT/PROV IDs
2. Neo4j scripts ingest those datasets
3. Post-import scripts build derived relationships / constraints

> If youâ€™re using Cypher imports or py2neo merges, make your scripts idempotent and constrain uniqueness so reruns donâ€™t duplicate nodes. (See examples of `MERGE`-like patterns.) :contentReference[oaicite:13]{index=13}

---

## ğŸ§ª Testing + QA

KFM highlights integration-style tests that simulate: **API request â†’ DB query â†’ response** and verify correctness & performance. :contentReference[oaicite:14]{index=14}

Recommended QA checks in `qa/`:
- âœ… DB can connect + required extensions exist
- âœ… required schemas/tables/views exist
- âœ… `ST_IsValid(geom)` for all spatial tables
- âœ… â€œcatalog linkâ€ checks:
  - every loaded layer has STAC Item ID
  - every dataset has DCAT + PROV pointers
- âœ… sample bbox query returns within a threshold

---

## ğŸ›¡ï¸ Security notes

- Always use parameterized queries from the API layer.
- Sanitize + validate any inputs that land in SQL (including admin scripts).
- SQL injection is real even in â€œinternalâ€ toolsâ€”donâ€™t concatenate strings into statements. :contentReference[oaicite:15]{index=15}

---

## âš¡ Performance notes

### Caching & reuse
For expensive queries, **reuse** can matter (materialized views, cached intermediate outputs, etc.). Caching intermediate results can improve repeated-query workloads. :contentReference[oaicite:16]{index=16}

Where to reflect this in scripts:
- `postgres/05_views.sql` â†’ build stable read models
- optional `postgres/05_materialized_views.sql` â†’ refresh policies
- `tools/db_refresh_views.sh` â†’ on-demand refresh

### Keep structure vs process distinct ğŸ§ 
Treat the DB schema as the **structure model** and ETL loads as **process**. This separation helps keep responsibilities clean and scripts composable. :contentReference[oaicite:17]{index=17}

---

## ğŸ§© Scaling + future-proofing (roadmap)

The projectâ€™s â€œfuture proposalsâ€ emphasize scaling, federation/sharding, caching, and containerized deployment. :contentReference[oaicite:18]{index=18}

DB script implications:
- migration strategy that works across multiple environments
- reproducible seeds and fixtures for CI
- export/import tooling that can target partitions/shards later
- â€œdata productâ€ boundaries (datasets with IDs + metadata) rather than monolithic loads

---

## ğŸ“š Project library

### Core KFM docs (high signal)
- ğŸ“˜ KFM Comprehensive Technical Documentation :contentReference[oaicite:19]{index=19}
- ğŸ’¡ KFM Latest Ideas & Future Proposals :contentReference[oaicite:20]{index=20}
- ğŸ§¾ Metadata + repo standards (STAC/DCAT/PROV) :contentReference[oaicite:21]{index=21}

### System design + reproducibility
- ğŸ—ï¸ Open-Source Geospatial Historical Mapping Hub Design (includes DVC plan + repo structure) :contentReference[oaicite:22]{index=22}

### Engineering & implementation references
- ğŸ˜ PostgreSQL notes (practical SQL/ops) :contentReference[oaicite:23]{index=23}
- ğŸ§­ Geospatial analysis cookbook (PostGIS workflows) :contentReference[oaicite:24]{index=24}
- ğŸ•¸ï¸ Data Spaces (polyglot storage + graph DB context) :contentReference[oaicite:25]{index=25}

### â€œProject bundlesâ€ (programming & applied math) â€” used as shared reference shelf
> These are here so contributors can align patterns, style, and tooling across the repo.

- ğŸ§  Implementing Programming Languages :contentReference[oaicite:26]{index=26}  
- ğŸ“ MATLAB Notes for Professionals :contentReference[oaicite:27]{index=27}  
- ğŸš Bash Notes for Professionals :contentReference[oaicite:28]{index=28}  
- ğŸ¤– Understanding Machine Learning (theory â†” practice) :contentReference[oaicite:29]{index=29}  
- â— Linear Algebra / ML math reference :contentReference[oaicite:30]{index=30}  

---

## âœ… Contribution checklist for DB scripts

> [!TIP]
> If you add or modify scripts here, aim for â€œclone â†’ run â†’ reproduceâ€.

- [ ] Script is idempotent (or clearly marked destructive)
- [ ] Script prints the target DB host/dbname before acting
- [ ] Script supports `--help` and reads config from env
- [ ] If it loads data, it also records/validates STAC/DCAT/PROV links :contentReference[oaicite:31]{index=31}
- [ ] QA checks added/updated in `qa/`
- [ ] CI-friendly (non-interactive mode available)

---

ğŸ’¬ **If youâ€™re wiring new DB scripts:** keep them small, composable, and â€œpipeline-safeâ€ so they can be called from Make/CI without surprises. ğŸ§©

