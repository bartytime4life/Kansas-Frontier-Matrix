# ğŸ•¸ï¸ `api/scripts/graph` â€” Graph Build, Load & QA (KFM)

![KFM](https://img.shields.io/badge/KFM-Kansas%20Frontier%20Matrix-1f6feb)
![Graph](https://img.shields.io/badge/Graph-Knowledge%20Graph%20%2F%20Neo4j-00a3e0)
![Geospatial](https://img.shields.io/badge/Geo-PostGIS%20%2F%20STAC%20%2F%20DCAT-2ea44f)
![Ops](https://img.shields.io/badge/Ops-Idempotent%20Scripts%20%26%20Snapshots-orange)
![Status](https://img.shields.io/badge/Status-Contract%20%2B%20Runbook-purple)

> **Goal:** turn KFM catalogs + curated story evidence into a **queryable knowledge graph** (property graph) that powers search, linking, provenance, and UI explorationâ€”**without duplicating** catalog metadata.

---

## ğŸ§­ Quick Links

- [ğŸ¯ What this folder is for](#-what-this-folder-is-for)
- [ğŸ§© Pipeline position](#-pipeline-position)
- [ğŸ“¦ Inputs & outputs](#-inputs--outputs)
- [ğŸš€ Quickstart](#-quickstart)
- [ğŸ§° Script catalog](#-script-catalog)
- [ğŸ§¬ Graph model](#-graph-model)
- [âœ… Quality gates](#-quality-gates)
- [âš¡ Performance notes](#-performance-notes)
- [ğŸ” Security & governance](#-security--governance)
- [ğŸ—ºï¸ UI & visualization hooks](#-ui--visualization-hooks)
- [ğŸ§¯ Troubleshooting](#-troubleshooting)
- [ğŸ“š Project library used](#-project-library-used)

---

## ğŸ¯ What this folder is for

This directory contains **operational scripts** (repeatable + safe) for the **Graph subsystem**:

- ğŸ—ï¸ **Build** graph node/edge sets from STAC/DCAT/PROV catalogs + Story Nodes
- ğŸ“¤ **Export** to Neo4j-friendly formats (CSV + Cypher)
- ğŸ“¥ **Load** (bulk import or online ingestion)
- ğŸ§ª **Validate** schema, constraints, and â€œevidence-firstâ€ provenance rules
- ğŸ“¸ **Snapshot** reproducible graph states for APIs and UI

> **Philosophy:** the graph is an **index** and **linker**, not a second metadata store. Keep it lean, reference the catalogs, and attach provenance/evidence to anything that looks like a â€œclaim.â€

---

## ğŸ§© Pipeline position

The graph is intentionally **downstream** of your catalogs and upstream of API/UI layers.

```mermaid
flowchart LR
  A[ETL & Ingestion] --> B[STAC / DCAT / PROV Catalogs]
  B --> C[Neo4j Knowledge Graph]
  C --> D[API Layer]
  D --> E[UI / Map Explorer]
  E --> F[Story Nodes (curated)]
  F --> G[Focus Mode outputs (citation-first)]
```

**Why it matters:** if catalog metadata changes, the graph should remain consistent because it **references** canonical IDs rather than copying fields.

---

## ğŸ“¦ Inputs & outputs

### Inputs (typical)
- ğŸ“š **Catalogs**: STAC items/collections, DCAT dataset listings, PROV lineage
- ğŸ§¾ **Story Nodes**: curated narratives + citations (human/AI-assisted, but evidence-locked)
- ğŸ—ºï¸ **Geospatial sources**: PostGIS tables, GeoJSON, GPKG, tilesets, gazetteers
- ğŸ›°ï¸ **Remote sensing outputs** (optional): raster/vector summaries produced elsewhere

### Outputs (recommended conventions)
- ğŸ“ `data/graph/csv/` â†’ Neo4j bulk import CSVs (nodes + relationships)
- ğŸ“ `data/graph/cypher/` â†’ constraints + indexes + migrations
- ğŸ“ `data/graph/snapshots/<run_id>/` â†’ immutable build artifacts + manifests
- ğŸ“ `data/graph/reports/<run_id>/` â†’ QA metrics, coverage, and diffs

> âœ… **Tip:** treat every graph build as a â€œscientific runâ€ with a `run_id`, manifest, and QA report.

---

## ğŸš€ Quickstart

### 1) Prereqs
- âœ… Neo4j (local Docker or hosted)
- âœ… Python/Node runtime (depending on implementation)
- âœ… Access to catalogs + PostGIS (if building geo-derived edges)

### 2) Environment variables
Create a `.env` (or equivalent secrets mechanism):

| Variable | Purpose |
|---|---|
| `NEO4J_URI` | Neo4j bolt/http endpoint |
| `NEO4J_USER` | username |
| `NEO4J_PASSWORD` | password |
| `KFM_CATALOG_ROOT` | path to STAC/DCAT/PROV catalogs |
| `KFM_GRAPH_OUT` | output dir (default `data/graph/`) |
| `POSTGRES_DSN` | Postgres/PostGIS connection string (optional) |
| `RUN_ID` | reproducible run label (recommended) |

### 3) Minimal run (conceptual)
```bash
# 0) Ensure catalogs exist and are current
# 1) Build export artifacts (CSV + Cypher)
python -m api.scripts.graph.build \
  --catalog "$KFM_CATALOG_ROOT" \
  --out "$KFM_GRAPH_OUT" \
  --run-id "${RUN_ID:-local}"

# 2) Apply schema constraints / indexes
python -m api.scripts.graph.migrate \
  --cypher "$KFM_GRAPH_OUT/cypher" \
  --run-id "${RUN_ID:-local}"

# 3) Load into Neo4j (bulk or online)
python -m api.scripts.graph.load \
  --csv "$KFM_GRAPH_OUT/csv" \
  --run-id "${RUN_ID:-local}"

# 4) Run QA gates
python -m api.scripts.graph.qa \
  --run-id "${RUN_ID:-local}"
```

> ğŸ§  If your repo uses different filenames, map these steps to the closest equivalents: **build â†’ migrate â†’ load â†’ qa â†’ snapshot**.

---

## ğŸ§° Script catalog

> This README defines a **contract**. If the exact scripts differ, keep the intent, flags, and outputs consistent.

### Recommended structure

```text
ğŸ“ api/scripts/graph/
  ğŸ“„ README.md
  ğŸ“„ build.(py|ts)              # catalog â†’ nodes/edges exports
  ğŸ“„ load.(py|ts|sh)            # CSV/Cypher â†’ Neo4j
  ğŸ“„ migrate.(py|ts)            # constraints/indexes/migrations
  ğŸ“„ qa.(py|ts)                 # quality gates + reports
  ğŸ“„ snapshot.(py|ts)           # pin run_id â†’ immutable snapshot
  ğŸ“ cypher/
  â”‚   â”œâ”€ 001_constraints.cypher
  â”‚   â”œâ”€ 002_indexes.cypher
  â”‚   â””â”€ 100_migrations.cypher
  ğŸ“ schemas/
  â”‚   â”œâ”€ ontology.yml
  â”‚   â””â”€ property_contract.json
  ğŸ“ queries/
  â”‚   â”œâ”€ search.cypher
  â”‚   â”œâ”€ provenance.cypher
  â”‚   â””â”€ coverage.cypher
```

### Script interface contract âœ…
All scripts **should** support:

- `--run-id` (string, required in CI)
- `--dry-run` (no DB writes)
- `--log-json` (structured logs)
- `--fail-fast` (stop on first violation)
- `--out` (where outputs go)
- `--config` (optional config file)

---

## ğŸ§¬ Graph model

### Core idea: **IDs are canonical**
- Nodes that represent real catalog entities (datasets, maps, layers) use the **same ID** as STAC/DCAT.
- The graph stores **references**, plus:
  - derived metrics (e.g., degree, centrality)
  - thin searchable text
  - provenance pointers (what source supports this edge?)

### Baseline node labels (suggested)
| Label | Example | Notes |
|---|---|---|
| `Dataset` | STAC/DCAT dataset | **ID must match catalog** |
| `Map` | historical map sheet | points to STAC asset(s) |
| `Layer` | vector/raster layer | references tiles/COG |
| `Place` | county, town, river | ideally linked to gazetteer |
| `Person` | explorer, cartographer | should be evidence-backed |
| `Org` | agency, publisher | evidence-backed |
| `Event` | treaty, battle | time-bounded |
| `Document` | scanned doc, PDF | stored as pointer + hash |
| `StoryNode` | curated narrative | must cite sources |
| `Claim` | atomic statement | always has provenance |
| `Citation` | source pointer | ties claims to evidence |
| `Run` | build run metadata | reproducibility anchor |

### Relationship types (suggested)
| Type | From â†’ To | Meaning |
|---|---|---|
| `HAS_ASSET` | Dataset â†’ Layer/Map | catalog assets |
| `MENTIONS` | StoryNode/Document â†’ Place/Person/Org/Event | entity linking |
| `ABOUT` | Document â†’ Event/Place | thematic link |
| `DERIVED_FROM` | Layer â†’ Dataset | lineage |
| `SUPPORTED_BY` | Claim/Edge â†’ Citation | evidence requirement |
| `LOCATED_IN` | Place â†’ Place | admin containment |
| `INTERSECTS` | Layer â†’ Layer | spatial overlap edge |
| `NEAR` | Place â†’ Place | proximity relationship |
| `IN_RUN` | Node/Edge â†’ Run | build provenance |

> ğŸ§· **Rule of thumb:** if a relationship implies something a historian could dispute, it must be `SUPPORTED_BY` at the claim level.

---

## ğŸ—ºï¸ Geospatial edge generation (PostGIS-first)

Many KFM edges can be produced deterministically from geometry:

- `INTERSECTS`
- `WITHIN`
- `CONTAINS`
- `NEAR` (distance-threshold)
- `ROUTE_CONNECTS` (network topology)

**Pattern:** compute in PostGIS âœ export IDs âœ load into Neo4j as relationships.

### Example: proximity edges
- Use PostGIS to compute nearby features (`ST_DWithin`)
- Export `(src_id, dst_id, distance_m)` rows
- Load into Neo4j as `(:Place)-[:NEAR {meters:...}]->(:Place)`

### Example: routing edges (optional)
If you have a routable network, generate shortest paths or travel-time edges and store **summaries** (not full routes). Routes can be recomputed on demand.

---

## ğŸ“ˆ Analytics hooks (graph math without the drama)

This folder may include QA/analytics scripts for:

- ğŸ§© **Connected components** (detect accidental fragmentation)
- ğŸ“ **Degree distributions** (spot bad joins)
- ğŸ›ï¸ **Spectral checks** (graph Laplacian health signals)
- ğŸ§­ **Centrality & hubs** (useful for UI suggestions)
- ğŸ§  **Uncertainty tagging** (Bayesian credibility for inferred edges)

> âœ… Keep analytics *derived* and reproducibleâ€”never let analytics overwrite canonical truth.

---

## âœ… Quality gates

Every graph build should emit a QA report and fail CI if any gate fails.

### Gate A â€” Referential integrity
- Every `Dataset/Map/Layer` node must map back to a real STAC/DCAT ID
- Orphans are errors unless explicitly whitelisted

### Gate B â€” No metadata duplication
- Graph must not drift from catalogs by copying large catalog fields
- Store only **IDs**, **pointers**, and **derived** attributes

### Gate C â€” Provenance required
- Any `Claim` or â€œinterpretiveâ€ edge requires at least one citation
- Story Nodes must be citation-backed (focus-mode compatible)

### Gate D â€” Determinism
- Exports must be stable across runs given identical inputs
- Sort outputs; do not rely on hash/dict iteration order

### Gate E â€” Scientific rigor
- If you attach scores/weights, ship:
  - model version
  - training/eval summary
  - residual diagnostics / sanity plots (where applicable)

---

## âš¡ Performance notes

When graphs get big, performance is a feature:

- ğŸ§± Prefer bulk imports for initial loads
- ğŸ§° Create constraints/indexes early (especially on IDs)
- ğŸ§µ Use chunked processing for exports and QA
- ğŸ§  Cache high-cost geospatial joins (store intermediate tables/views)

**Anti-patterns**
- Loading millions of edges row-by-row online
- Recomputing spatial joins every run without caching
- Storing huge blobs of metadata inside the graph

---

## ğŸ” Security & governance

Graph pipelines touch â€œpowerfulâ€ data surfaces (catalogs, DBs, entity linking). Keep these non-negotiable:

- âœ… Use parameterized queries (Postgres/Neo4j)
- âœ… Treat Story Nodes as **evidence-based** to prevent hallucinated claims entering the graph
- âœ… Redact secrets from manifests/logs
- âœ… Enforce least privilege for DB accounts (read catalogs, write graph, nothing else)
- âœ… Keep a clear audit trail per `run_id`

---

## ğŸ–¥ï¸ UI & visualization hooks

The graph exists to serve exploration:

- ğŸ—ºï¸ **Map UI:** graph edges should support map-centric exploration (place â†’ map â†’ layer â†’ story)
- ğŸ§Š **3D/2D Graph Views:** export lightweight graph slices as JSON (nodes+edges) for WebGL renderers
- ğŸ“± **Responsive UI:** keep payloads slim and paginated

### Recommended exports for UI
- `graph_slice.json` (bounded node/edge subset)
- `entity_card.json` (summary for Place/Person/Map)
- `provenance_chain.json` (why is this edge here?)

---

## ğŸ§¯ Troubleshooting

### â€œGraph loads but search is emptyâ€
- Check constraints/indexes exist
- Confirm IDs match STAC/DCAT exactly (case, prefixes, namespaces)

### â€œQA fails: too many orphansâ€
- Catalog path mismatch
- Changed ID scheme without migrations
- Missing mapping layer between external IDs and KFM canonical IDs

### â€œBulk import is slowâ€
- Split files by label/type
- Increase heap/page cache (Neo4j)
- Pre-sort CSVs and dedupe edges before import

---

## ğŸ“š Project library used

This README is intentionally cross-disciplinary ğŸ§©  
KFMâ€™s graph work touches: **geospatial catalogs**, **graph theory**, **data engineering**, **UX**, **security**, and **ethics**.

<details>
<summary><strong>ğŸ“¦ Library touchpoints (how they inform Graph scripts)</strong></summary>

- ğŸ›°ï¸ Remote sensing â†’ feature extraction + built-environment layers feeding the graph  
- ğŸ—ºï¸ Map design â†’ graph outputs must support legible mapping & storytelling  
- ğŸ•¸ï¸ Spectral graph theory â†’ QA invariants + analytics hooks  
- ğŸ§° PostGIS/PostgreSQL â†’ deterministic spatial joins that become graph edges  
- âš¡ Scalable data mgmt â†’ chunked, parallel, cache-friendly builds  
- ğŸ§ª Modeling & statistics â†’ verification/validation, uncertainty tagging  
- ğŸ” Security & humanism â†’ privacy, integrity, safe-by-default scripts  
- ğŸŒ WebGL + responsive UI â†’ graph slices tailored for frontend exploration  

</details>

---

## ğŸ¤ Contributing

- Keep scripts **idempotent**
- Always emit a `manifest.json` per run
- Add a QA gate when you add a new edge type
- Prefer **derivation** over â€œmanual truthâ€ unless itâ€™s a curated Story Node
- Write migrations when schema changes

---

## âœ… Checklist (before you ship)

- [ ] Node IDs match catalogs (STAC/DCAT)
- [ ] Constraints/indexes applied
- [ ] QA report generated and clean
- [ ] Snapshot saved with `run_id`
- [ ] Provenance is attached for any interpretive links
- [ ] Logs are safe (no secrets, no PII unless explicitly allowed)

---

_ğŸ•¸ï¸ Build graphs like a scientist, load them like an engineer, and present them like a cartographer._

