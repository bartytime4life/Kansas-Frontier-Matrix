# âš™ï¸ Optimization Scripts (`api/scripts/optimization`) ğŸ§ ğŸ“ˆ

![Status](https://img.shields.io/badge/status-active--dev-blue)
![Provenance-first](https://img.shields.io/badge/provenance-first-brightgreen)
![Deterministic](https://img.shields.io/badge/runs-deterministic-success)
![Policy-Gated](https://img.shields.io/badge/governance-policy--gated-orange)
![Backend](https://img.shields.io/badge/backend-FastAPI%20%7C%20PostGIS%20%7C%20Neo4j-4c1)
![Metadata](https://img.shields.io/badge/metadata-STAC%20%7C%20DCAT%20%7C%20PROV--O-informational)

> **ğŸ“Œ Purpose:** This folder is the **offline + CI-friendly optimization toolbox** for Kansas Frontier Matrix (KFM).
> We use it to **tune model parameters, pipeline knobs, and performance tradeoffs** without sacrificing KFMâ€™s core principles:
> **determinism, traceability (PROV), validation gates, and human-centered governance**.

[â¬…ï¸ Back to repo README](../../../README.md) Â· [ğŸ§­ Docs](../../../docs) Â· [ğŸ§ª Validation tooling](../../../tools/validation)

---

## ğŸ§­ Contents

- [What â€œoptimizationâ€ means in KFM](#what-optimization-means-in-kfm)
- [Optimization lifecycle](#optimization-lifecycle)
- [Folder layout & conventions](#folder-layout--conventions)
- [Run configuration contract](#run-configuration-contract)
- [Outputs & artifacts](#outputs--artifacts)
- [Optimization recipes](#optimization-recipes)
- [Quality gates](#quality-gates)
- [Reproducibility & experimental design](#reproducibility--experimental-design)
- [Observability & energy](#observability--energy)
- [Security, ethics, and legal safety rails](#security-ethics-and-legal-safety-rails)
- [Adding a new optimization script](#adding-a-new-optimization-script)
- [Reference shelf](#reference-shelf)

---

## What â€œoptimizationâ€ means in KFM

In KFM, â€œoptimizationâ€ is **not** just â€œfind the best number.â€ Itâ€™s:

âœ… **Calibration** (match model outputs to observed data)  
âœ… **Sensitivity analysis** (what matters / what doesnâ€™t)  
âœ… **Search under constraints** (performance, memory, governance, fairness)  
âœ… **Multi-objective tradeoffs** (accuracy vs latency vs interpretability)  
âœ… **Offline performance tuning** (indexes, cache plans, tile strategies)  
âœ… **Artifact production** (publish *evidence*, not vibes)

> **Rule of thumb ğŸ§ :** If a script produces something that might influence a map, story, or decision, it must produce **evidence artifacts** (STAC/DCAT/PROV + metrics + config) so it can be inspected, re-run, and audited.

---

## ğŸ§¬ Optimization lifecycle

```mermaid
flowchart LR
  A[Inputs\n(datasets + bounds + constraints)] --> B[Plan\n(objective + metrics + budget)]
  B --> C[Search\n(grid / random / bayes / gradient / evo)]
  C --> D[Evaluate\n(simulation / query / pipeline / UI perf)]
  D --> E[Artifacts\nmetrics + plots + prov + stac]
  E --> F{Policy + QA Gate}
  F -- pass âœ… --> G[Publish\nPR / catalog entry / worker job]
  F -- fail âŒ --> H[Quarantine\nreport + no publish]
```

---

## ğŸ“ Folder layout & conventions

> This directory is intentionally â€œscripts-firstâ€: small, composable executables that can be run locally, in a worker, or in CI.

Recommended structure (adjust to what already exists in the repo):

```text
api/scripts/optimization/
â”œâ”€ README.md                      ğŸ‘ˆ you are here
â”œâ”€ _shared/                        â™»ï¸ common helpers
â”‚  â”œâ”€ config.py                    (load/validate config)
â”‚  â”œâ”€ provenance.py                (PROV emission helpers)
â”‚  â”œâ”€ artifacts.py                 (standard output layout)
â”‚  â”œâ”€ metrics.py                   (common metrics + schemas)
â”‚  â””â”€ io.py                        (dataset fetch, caching, hashing)
â”œâ”€ model_calibration/              ğŸ§ª tune scientific models
â”œâ”€ pipeline_tuning/                ğŸ—ï¸ optimize ETL knobs
â”œâ”€ db_query_tuning/                ğŸ—„ï¸ PostGIS/Neo4j tuning runs
â”œâ”€ map_delivery_tuning/            ğŸ—ºï¸ tiling + compression + LOD
â””â”€ reports/                        ğŸ“ˆ plots + summaries
```

### âœ… Script invariants (non-negotiable)

Every optimization script should be:

- **Deterministic**: fixed RNG seeds + explicit config + stable outputs.
- **Idempotent**: same inputs/config â†’ same run_id and same artifacts (or clearly versioned outputs).
- **Contract-first**: config schemas and outputs are versioned, validated, and reviewed.
- **Provenance-emitting**: every run writes a PROV lineage record.
- **Validation-gated**: artifacts donâ€™t get published unless QA/policy checks pass.
- **Atomic-publish**: stage â†’ validate â†’ publish (no partial output promotion).

> **Tip ğŸ§©:** Treat scripts like pipelines. The same discipline that applies to ingestion applies here too.

---

## ğŸ§¾ Run configuration contract

All scripts should support **config-driven runs** (YAML or JSON), plus a small set of CLI overrides.

### Minimal config schema (recommended)

```yaml
run:
  name: "example_optimization"
  seed: 1337
  idempotency_key: "example_2026-01-12"
  notes: "Short explanation of intent"
objective:
  direction: "min"         # min|max
  primary_metric: "rmse"   # required
  secondary_metrics:       # optional
    - "mae"
    - "latency_ms"
search:
  algorithm: "random"      # grid|random|bayes|gradient|evo
  budget: 50               # total trials / evals
  early_stop:
    enabled: true
    patience: 10
  bounds:
    param_a: [0.0, 1.0]
    param_b: [10, 200]
data:
  inputs:
    - dataset_id: "kfm.ks.example.1900_2000.v1"
      version: "2026-01-01"
  split:
    method: "time"         # time|space|kfold
    holdout: 0.2
publish:
  mode: "none"             # none|staging|pr
observability:
  otel: true
  energy_report: true
```

### CLI expectations

Each script should implement:

- `--config <path>`
- `--seed <int>` (overrides config)
- `--budget <int>` (overrides config)
- `--output-dir <path>`
- `--dry-run` (no publish / no DB writes)
- `--print-resolved-config` (debugging determinism)

Example pattern:

```bash
python -m api.scripts.optimization.model_calibration.calibrate \
  --config configs/optimization/air_quality.yaml \
  --seed 1337 \
  --budget 80 \
  --output-dir data/work/optimization_runs
```

> **Note ğŸ§¾:** If these modules donâ€™t exist yet, keep the interface anywayâ€”this README defines the **contract**, not a specific implementation.

---

## ğŸ“¦ Outputs & artifacts

Every run should create a **single, self-contained run folder** with:

- **Resolved config** (exact parameters actually used)
- **Trial table** (all attempts, not just â€œthe bestâ€)
- **Metrics** (JSON schema-stable)
- **Plots** (optional but encouraged)
- **PROV lineage**
- **STAC/DCAT records** when artifacts are publishable
- **A small human summary** (markdown)

Recommended layout:

```text
data/work/optimization_runs/<run_id>/
â”œâ”€ config.resolved.yaml
â”œâ”€ metrics.json
â”œâ”€ trials.csv
â”œâ”€ best.json
â”œâ”€ summary.md
â”œâ”€ prov.jsonld
â”œâ”€ stac_item.json                 (if producing a dataset artifact)
â”œâ”€ dcat_dataset.json              (if publishing to catalog)
â”œâ”€ plots/
â”‚  â”œâ”€ convergence.png
â”‚  â”œâ”€ pareto.png
â”‚  â””â”€ residuals.png
â””â”€ logs/
   â”œâ”€ run.log
   â””â”€ otel_trace.json             (optional export)
```

> **Golden rule ğŸ¥‡:** If someone canâ€™t reproduce your â€œbest paramsâ€ from your artifacts, it doesnâ€™t count.

---

## ğŸ§ª Optimization recipes

### 1) Scientific model calibration (simulation-first) ğŸ›°ï¸ğŸŒ¾ğŸŒŠ

Use when you have:
- A forward model (deterministic runner)
- Observations (sensors, surveys, remote sensing products)
- A calibration objective (RMSE, NSE, likelihood)

Recommended methods:
- **Grid / factorial** for low-dimensional problems
- **Random search** for broad exploration (seeded!)
- **Bayesian optimization** for expensive evaluations
- **Gradient-based** only when objective is differentiable and stable

Artifacts to include:
- Calibration curves, residual plots, uncertainty ranges
- Sensitivity report (which params matter most)
- Clear â€œdomain validityâ€ bounds (donâ€™t optimize nonsense)

---

### 2) Remote sensing pipeline tuning (cloud-to-catalog) â˜ï¸ğŸ›°ï¸ğŸ—ºï¸

Use when youâ€™re tuning:
- Cloud masks, compositing windows, classification thresholds
- Spatial/temporal resolution tradeoffs
- Post-processing filters (smoothing, gap fill)

Recommended methods:
- DOE-style exploration (factor ranges + replicates)
- Holdout by **space** (counties/tiles) or **time** (years/seasons)
- Metrics beyond accuracy: coverage %, artifacts, latency

Artifacts to include:
- Confusion matrices or agreement with reference labels (if available)
- Spatial error maps (where it fails matters)
- Runtime + cost estimates (compute-to-data philosophy)

---

### 3) Database / query optimization (offline logs â†’ online speed) ğŸ—„ï¸âš¡

Typical tasks:
- Index suggestions (PostGIS/JSONB/graph)
- Query shape constraints (pagination, expensive joins)
- Cache placement for repeated query sequences

Recommended approach:
- Collect query logs â†’ cluster sequences â†’ replay â€œprobe queriesâ€
- Compare plans with `EXPLAIN (ANALYZE, BUFFERS)` in controlled runs
- Publish only after a performance regression test passes

Artifacts to include:
- Before/after latency distributions
- Query plans (sanitized), index DDL proposals
- Impact analysis (write overhead / storage cost)

> **Tip:** Prefer improvements that are **auditable and reversible** (migration scripts, feature flags, config knobs).

---

### 4) Map delivery tuning (tiles, LOD, compression) ğŸ—ºï¸ğŸ§ŠğŸ›ï¸

Targets:
- Tile generation strategy (pre-tile vs on-demand + cache)
- Vector simplification rules per zoom level
- Raster encoding choices (PNG/JPEG/WebP), COG tiling parameters

Metrics:
- P95 tile latency
- Client FPS / GPU load proxies (when measurable)
- Bandwidth per view and cache hit rate

Artifacts:
- Performance dashboards
- Representative screenshots for â€œvisual correctnessâ€ regression
- A/B comparisons with â€œacceptance thresholdsâ€

---

## ğŸš¦ Quality gates

Before anything leaves `data/work/`:

### âœ… Required checks
- Config schema validation (and version compatibility)
- Dataset bounds checks (Kansas extent, CRS expectations, etc.)
- Provenance emission present (`prov.jsonld`)
- If producing a dataset artifact: STAC/DCAT records exist

### ğŸ§ª Strongly recommended checks
- Catalog QA gate (`tools/validation/catalog_qa`)
- Policy pack evaluation (OPA/Conftest or equivalent)
- Regression test on a small â€œgoldenâ€ dataset

> **Warning âš ï¸:** If an optimization run changes user-facing outputs (maps, stories, recommendations),
> it must go through **human review** before publish.

---

## ğŸ§  Reproducibility & experimental design

Optimization can accidentally turn into **p-hacking** if weâ€™re not disciplined. Keep it science-grade:

### Do this âœ…
- Predefine objective + metrics + stopping rules (write it in config)
- Log **all trials** (not just the best)
- Use appropriate splits (time/space leakage is real)
- Report uncertainty (confidence/credible intervals where possible)
- Prefer simpler models when performance is comparable (interpretability wins)

### Donâ€™t do this âŒ
- â€œTune until it looks goodâ€ without recording the search
- Change the metric after seeing results without documenting why
- Optimize on the same data you claim as validation

> **Best practice ğŸ§ª:** Treat each optimization run like an experiment:
> hypothesis â†’ plan â†’ run â†’ artifacts â†’ review â†’ publish.

---

## ğŸ“¡ Observability & energy

If optimization is compute-heavy, it must be observable:

- Emit OpenTelemetry spans (trial-level spans are ğŸ”¥ for debugging)
- Record runtime, memory, and IO stats
- Track energy usage when feasible (especially for large sweeps)

> **Why?** KFM explicitly values responsible compute + transparent operationsâ€”not just â€œfast.â€

---

## ğŸ”’ Security, ethics, and legal safety rails

Optimization scripts often touch:
- Sensitive datasets (restricted layers, private contributors)
- API keys (Earth Engine / cloud resources)
- Derived artifacts that could be misinterpreted

Minimum safety posture:
- Never hardcode secrets; use env + secret managers
- Redact logs by default (no tokens, no PII)
- If an artifact is AI-generated or model-derived, label it clearly and attach provenance
- Ensure outputs remain **advisory** unless explicitly governed otherwise

> **Reminder ğŸ§‘â€âš–ï¸:** Responsible, explainable, evidence-backed behavior is a design requirementâ€”not a nice-to-have.

---

## â• Adding a new optimization script

### Checklist âœ…
- [ ] Add script under the right subfolder (model_calibration / db_query_tuning / â€¦)
- [ ] Implement CLI: `--config --seed --budget --output-dir --dry-run`
- [ ] Validate config schema (versioned)
- [ ] Emit artifacts with standard layout
- [ ] Emit PROV lineage
- [ ] Add a smoke test + a tiny fixture dataset
- [ ] Update this README (or subfolder README) with usage

### Naming conventions ğŸ·ï¸
- Script names: `opt_<domain>_<target>.py` (predictable + searchable)
- Dataset IDs: `kfm.<region>.<theme>.<time_range>.v<version>` (when applicable)
- Run IDs: deterministic hash from `(idempotency_key + resolved_config + inputs)`

---

## ğŸ“š Reference shelf

<details>
<summary><strong>ğŸ§± Project â€œsource of truthâ€ docs</strong> (click to expand)</summary>

- **Kansas Frontier Matrix (KFM) â€“ Comprehensive Technical Documentation** â€” system architecture, governance, pipelines, and contracts.
- **ğŸŒŸ Kansas Frontier Matrix â€“ Latest Ideas & Future Proposals** â€” forward-looking designs (simulation runner, bias correction, policy, telemetry).
- **Kansas-Frontier-Matrix Design Audit â€“ Gaps and Enhancement Opportunities** â€” whatâ€™s missing + where optimization work creates leverage.
- **Kansas-Frontier-Matrix Open-Source Geospatial Historical Mapping Hub Design** â€” mapping hub framing & architecture.
- **MARKDOWN_GUIDE_v13 (Kansas Matrix System)** â€” contract-first, deterministic pipeline, evidence artifact requirements.

</details>

<details>
<summary><strong>ğŸ§ª Modeling, simulation, stats, and optimization</strong></summary>

- *Scientific Modeling and Simulation (NASA-grade guide)* â€” V&V, UQ, sensitivity, scientific rigor.
- *Understanding Statistics & Experimental Design* â€” DOE discipline, avoiding bias.
- *Regression Analysis with Python* + *Linear Regression slides* â€” objective functions, optimization tooling, cross-validation patterns.
- *Think Bayes* â€” Bayesian estimation ideas for calibration and uncertainty.
- *Generalized Topology Optimization for Structural Design* â€” constraint-driven optimization thinking (great analogy for governed outputs).
- *Spectral Geometry of Graphs* â€” graph analysis foundations (relevant to knowledge graph optimization).
- *Graphical Data Analysis with R* â€” diagnostic plots and exploratory validation.

</details>

<details>
<summary><strong>ğŸ—„ï¸ Data platforms & performance</strong></summary>

- *Scalable Data Management for Future Hardware* â€” offline optimization using query logs, compiled pipelines, workload-aware tuning.
- *PostgreSQL Notes for Professionals* â€” indexing and query tuning reference (esp. JSONB patterns).
- *Data Spaces* â€” governance and interoperability lenses for federated data systems.
- *Concurrent Real-Time and Distributed Programming in Java* â€” concurrency patterns (useful when designing workers and schedulers).

</details>

<details>
<summary><strong>ğŸ—ºï¸ GIS, remote sensing, cartography, and delivery</strong></summary>

- *Cloud-Based Remote Sensing with Google Earth Engine* â€” cloud processing patterns & remote sensing workflows.
- *Python Geospatial Analysis Cookbook* â€” practical PostGIS/GDAL pipeline guidance.
- *Making Maps (GIS map design)* â€” cartographic clarity (optimization must preserve meaning).
- *Mobile Mapping: Space, Cartography and the Digital* â€” UX context (performance is user experience).
- *WebGL Programming Guide* â€” GPU/render constraints (tile + vector payload tuning).
- *Responsive Web Design (HTML5/CSS3)* â€” responsive UI performance implications.
- *Compressed Image File Formats (JPEG/PNG/GIF/BMP)* â€” compression tradeoffs for tiles and thumbnails.

</details>

<details>
<summary><strong>ğŸ” Ethics, human-centered design, and security</strong></summary>

- *Introduction to Digital Humanism* â€” human-centered constraints and accountability.
- *AI Lawâ€™s prophecies / conceptual foundations of ML age* â€” governance framing for ML-derived artifacts.
- *Ethical Hacking and Countermeasures* â€” defensive mindset for handling keys, infra, and auditability.
- *Gray Hat Python* â€” security awareness reference (use responsibly; no offensive use in KFM).
- *Principles of Biological Autonomy* â€” systems thinking reference (useful for agent-like orchestration patterns).

</details>

<details>
<summary><strong>ğŸ“¦ Programming mega-compilations (language reference library)</strong></summary>

- **A programming Books.pdf**
- **B-C programming Books.pdf**
- **D-E programming Books.pdf**
- **F-H programming Books.pdf**
- **I-L programming Books.pdf**
- **M-N programming Books.pdf**
- **O-R programming Books.pdf**
- **S-T programming Books.pdf**
- **U-X programming Books.pdf**

</details>

---

### ğŸ§© If you only remember 3 thingsâ€¦

1) **Config + artifacts are the product.**  
2) **Determinism + provenance are mandatory.**  
3) **Nothing publishes without QA + policy gates.** ğŸš¦âœ…

ğŸŒ¾ğŸ—ºï¸ğŸš€

