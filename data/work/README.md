# ğŸ§° `data/work/` â€” Working Data Sandbox (Nonâ€‘Authoritative)

![purpose](https://img.shields.io/badge/purpose-working%20sandbox-blue)
![data](https://img.shields.io/badge/data-non--authoritative-orange)
![provenance](https://img.shields.io/badge/provenance-required-success)
![policy](https://img.shields.io/badge/no%20bypasses-truth%20path-critical)

> [!WARNING]
> **Nothing in `data/work/` is â€œsource of truth.â€**  
> `work/` is the **intermediate + scratch stage** of KFMâ€™s governed truth path. It is **regenerable**, and **not served to end users**.
>
> Anything destined for publication must travel the governed pipeline:  
> **Raw âœ Work âœ Processed âœ Catalog & Lineage (STAC/DCAT/PROV) âœ Runtime Stores âœ API âœ UI/AI**

---

## ğŸ¯ What this folder is for

`data/work/` is the **handsâ€‘on workshop** for the Kansas Frontier Matrix (KFM) data pipeline: quick prototypes, staging, exploratory transformations, QA checks, and intermediate artifacts created while you learn/iterate.

Typical uses:
- ğŸ§ª **Exploratory ETL**: rough transforms, schema experiments, trial joins, validation spikes
- ğŸ§Š **Caches**: downloaded source bundles, API responses (when allowed), temporary tiles
- ğŸ§¹ **Preâ€‘QA / QA**: profiling, row counts, geometry checks, bounding boxes, sampling
- ğŸ§¾ **Run manifests**: lightweight logs + provenance notes (â€œhow we got hereâ€)
- ğŸ—ºï¸ **Preview outputs**: *tiny* sample GeoJSON, plots, screenshots, debug tiles (small only)

> [!IMPORTANT]
> **Work is allowed to be messy. It is not allowed to be untraceable.**  
> If a work artifact influences a decision or promotion, it must have a manifest + provenance notes.

---

## ğŸ§­ Where `work/` fits in the KFM â€œtruth pathâ€

```mermaid
flowchart LR
  A[Raw ğŸ“¥<br/>data/raw/&lt;domain&gt;/] --> W[Work ğŸ§ª<br/>data/work/&lt;domain&gt;/]
  W --> P[Processed âœ…<br/>data/processed/&lt;domain&gt;/]
  P --> C[Catalog & Lineage ğŸ—‚ï¸<br/>data/stac Â· data/catalog/dcat Â· data/prov]
  C --> D[Runtime Stores ğŸ—ƒï¸<br/>PostGIS Â· Neo4j Â· Search/Vector]
  D --> E[API ğŸŒ<br/>FastAPI + Policy]
  E --> F[UI/AI ğŸ—ºï¸ğŸ¤–]

  W -. scratch outputs .-> P
  W -. download cache .-> A
  W -. provenance notes .-> C
```

**Rule:** `work/` can *assist* any stage, but **must not replace** any stage.  
**Rule:** Nothing moves â€œforwardâ€ without satisfying the **policy gates** for that transition.

---

## ğŸ“ Recommended structure

Keep `work/` predictable so tools and humans can find things fast.

> [!NOTE]
> The examples below assume the canonical KFM staging layout:  
> `data/raw/<domain>/` âœ `data/work/<domain>/` âœ `data/processed/<domain>/`

```text
data/
â”œâ”€ raw/ ğŸ“¥
â”‚  â””â”€ <domain>/...              # immutable source snapshots (governed)
â”œâ”€ work/ ğŸ§ª                     # (YOU ARE HERE) intermediate + scratch (NOT authoritative)
â”‚  â”œâ”€ tmp/ ğŸ§¯                   # throwaway files (safe to delete anytime)
â”‚  â”œâ”€ cache/ ğŸ§Š                 # re-download avoidance (safe to delete anytime)
â”‚  â”œâ”€ runs/ ğŸƒ                  # one folder per run/experiment (recommended)
â”‚  â”œâ”€ experiments/ ğŸ§ª           # notebooks / ad-hoc spikes (small outputs only)
â”‚  â””â”€ reports/ ğŸ“Š               # QA summaries, profiling results (small text/plots)
â”œâ”€ processed/ âœ…
â”‚  â””â”€ <domain>/...              # canonical cleaned outputs (governed)
â”œâ”€ stac/ ğŸ—ºï¸
â”‚  â”œâ”€ collections/              # STAC Collections (governed)
â”‚  â””â”€ items/                    # STAC Items (governed)
â”œâ”€ catalog/ ğŸ—‚ï¸
â”‚  â””â”€ dcat/                     # DCAT JSON-LD outputs (governed)
â”œâ”€ prov/ ğŸ§¾                     # PROV lineage bundles (governed)
â””â”€ graph/ ğŸ•¸ï¸                    # graph import/export artifacts (governed)
   â”œâ”€ csv/
   â””â”€ cypher/
```

> [!TIP]
> Treat `tmp/` and most of `cache/` as **rebuildable**. If it canâ€™t be rebuilt, it doesnâ€™t belong here.

---

## ğŸƒ Run folder contract

Each meaningful experiment should get a dedicated run folder.

```text
data/work/runs/YYYY-MM-DD__<pipeline>__<dataset_slug>/
â”œâ”€ manifest.yml                 # what you did + where inputs came from
â”œâ”€ provenance.jsonld            # optional but recommended (PROV-style record)
â”œâ”€ logs/                        # stdout/stderr, validation logs
â”œâ”€ checks/                      # schema checks, geometry checks, QA outputs
â”œâ”€ inputs/                      # small samples only (or pointers)
â”œâ”€ outputs/                     # small outputs only (or pointers)
â””â”€ notes.md                     # decisions, issues, next steps
```

### `manifest.yml` (template)

```yaml
run_id: "2026-02-03__ingest__ks_dasc_counties"
owner: "@your-handle"
created_at: "2026-02-03T20:05:00Z"
goal: "Validate geometry + normalize CRS; prep for processed promotion"
domain: "boundaries"
dataset_slug: "ks_dasc_counties"
truth_path_stage: "work"

inputs:
  - name: "DASC counties layer"
    raw_path: "data/raw/boundaries/ks_dasc_counties.zip"
    source: "https://â€¦"
    retrieved_at: "2026-02-03T20:00:00Z"
    license: "TBD"
    checksum_sha256: "TBD"
    sensitivity:
      classification: "public"   # public | internal | confidential | restricted
      notes: ""

processing:
  steps:
    - "download (raw snapshot)"
    - "inspect schema + nulls"
    - "reproject to EPSG:4326"
    - "fix invalid geometries"
    - "write small sample outputs for QA review"
  code_ref:
    git_sha: "TBD"
    entrypoint: "src/pipelines/boundaries/ks_dasc_counties.py"
  environment:
    container: "TBD"
    tool_versions:
      python: "TBD"
      gdal: "TBD"

outputs:
  - name: "counties_sample.geojson"
    path: "data/work/runs/2026-02-03__ingest__ks_dasc_counties/outputs/counties_sample.geojson"
    size_bytes: 123456
    notes: "sample only; full data promoted to processed stage"

promotion_intent:
  target_stage: "processed"
  target_paths:
    - "data/processed/boundaries/ks_counties.geojson"
  required_checks:
    - "license verified"
    - "schema validated"
    - "CRS validated"
    - "geometry validity checks passed"
    - "sensitivity classification recorded"
    - "provenance recorded (PROV bundle planned)"
  catalog_artifacts_expected:
    - "data/stac/items/<item>.json"
    - "data/catalog/dcat/<dataset>.jsonld"
    - "data/prov/<bundle>.jsonld"

notes:
  risks:
    - "TBD"
  next_steps:
    - "TBD"
```

---

## âœ… Promotion checklist

Before anything leaves `work/` and becomes â€œrealâ€:

1. ğŸ”’ **License & rights check**
   - Confirm allowed use + redistribution
   - Record license string & source link in metadata

2. ğŸ§¾ **Provenance captured (â€œmap behind the mapâ€)**
   - Source URL(s), retrieval date/time, checksums
   - Toolchain + parameters + code reference (git SHA)
   - Record what was *changed* and what was *discarded*

3. ğŸ§ª **Quality gates**
   - Schema validation (types, null rules)
   - Spatial validation (CRS, geometry validity, bbox sanity)
   - Basic profiling (row counts, uniqueness, join keys)

4. ğŸ—‚ï¸ **Catalog & lineage artifacts prepared**
   - STAC (items/collections): bbox, time range, links to assets
   - DCAT: dataset-level description, license, distributions
   - PROV: inputs â†’ transformations â†’ outputs (agents + timestamps)

5. ğŸ—ƒï¸ **Load & serve through the platform boundary (no bypasses)**
   - No UI direct-to-DB shortcuts
   - Publish via the service layer (API + policy enforcement)

> [!IMPORTANT]
> Promotion is a **one-way mindset**: once promoted, the governed copies become the referenceâ€”not the scratch files in `work/`.

---

## ğŸ›¡ï¸ Sovereignty, sensitivity, and redaction

KFM enforces **FAIR + CARE** expectations through policy gates and governance review.

**If you touch sensitive data in `work/`, you must treat it as sensitive everywhere:**
- Apply redaction/generalization at **processed outputs**
- Reflect redaction/sensitivity in **STAC/DCAT metadata**
- Ensure **API/Policy** does not serve unredacted data
- Ensure **UI** doesnâ€™t leak sensitive data (even indirectly)

> [!WARNING]
> Some additions (e.g., culturally sensitive layers, detailed locations of protected sites, Indigenous data sovereignty contexts) may require **manual governance review** before promotion.

---

## ğŸ§¼ What NOT to put in `data/work/`

**Hard â€œnoâ€ list:**
- ğŸ”‘ Secrets (API keys, tokens, `.env`, credentials)
- ğŸ§ PII / sensitive records unless explicitly approved + governed
- ğŸ‹ï¸ Huge binaries (rasters, LiDAR, full tilesets) committed to git
- ğŸ“Œ Anything â€œproduction-criticalâ€ that canâ€™t be recreated

> [!NOTE]
> Large artifacts belong in object storage + referenced via metadata (STAC items, manifests, or catalog pointers), not committed here.

---

## ğŸ§· Git hygiene (keep the repo healthy)

Recommended approach:
- âœ… Commit: `README.md`, run manifests (`manifest.yml`), tiny QA reports, tiny samples
- âŒ Do not commit: big downloads, big intermediate outputs, database dumps

If needed, keep empty dirs with a `.gitkeep`:
```text
data/work/tmp/.gitkeep
data/work/cache/.gitkeep
```

Optional `.gitignore` pattern (adjust to taste):
```gitignore
# data/work is scratch by default
data/work/tmp/**
data/work/cache/**
data/work/runs/**/outputs/**

# allow small committed breadcrumbs
!data/work/**/manifest.yml
!data/work/**/notes.md
!data/work/**/reports/**
```

---

## ğŸ”— Related docs (inside the repo)

- `../../docs/architecture/` ğŸ›ï¸ *(system overview, truth path, governance)*
- `../../src/pipelines/` ğŸ§° *(ETL entrypoints, dataset recipes, loaders)*
- `../../docs/data/` ğŸ—‚ï¸ *(metadata standards, catalog format, naming rules)*
- `../../schemas/` ğŸ§¾ *(STAC/DCAT/PROV + validation schemas)*
- `../../tools/` ğŸ”§ *(validators, helpers, dev utilities)*

> If these paths differ in your checkout, update links here to match the repo layout.

---

## ğŸ™Œ Quick start (copy/paste)

```bash
# 1) Create a new run folder
mkdir -p data/work/runs/$(date +%F)__<pipeline>__<dataset_slug>/{logs,checks,inputs,outputs}

# 2) Add a manifest + notes
touch data/work/runs/$(date +%F)__<pipeline>__<dataset_slug>/manifest.yml
touch data/work/runs/$(date +%F)__<pipeline>__<dataset_slug>/notes.md

# 3) Do your work, then promote outputs into governed stages
#    raw/ -> work/ -> processed/ -> (stac + dcat + prov) -> db -> api -> ui/ai
```

---

## ğŸ“Œ Maintainership

- Default owner: **Data / Pipeline maintainers**
- PR expectation: If you add a new workflow, include at least:
  - `manifest.yml` (or equivalent)
  - a short `notes.md`
  - a clear promotion plan (where it lands in the governed pipeline)

âœ¨ Keep it rebuildable. Keep it traceable. Keep it honest.
