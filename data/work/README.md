# ğŸ§° `data/work/` â€” Working Data Sandbox (Nonâ€‘Authoritative)

![purpose](https://img.shields.io/badge/purpose-working%20sandbox-blue)
![data](https://img.shields.io/badge/data-non--authoritative-orange)
![provenance](https://img.shields.io/badge/provenance-required-success)
![policy](https://img.shields.io/badge/no%20bypasses-truth%20path-critical)

> [!WARNING]
> **Nothing in `data/work/` is â€œsource of truth.â€**  
> This directory exists for *iteration*, *experiments*, and *scratch outputs* only. Anything destined for publication must be promoted into the governed pipeline (Raw âœ Processed âœ Catalog âœ Databases âœ API âœ UI/AI).

---

## ğŸ¯ What this folder is for

`data/work/` is the **hands-on workshop** for the Kansas Frontier Matrix (KFM) data pipeline: quick prototypes, staging, exploratory transformations, QA checks, and intermediate artifacts created while you learn/iterate.

Typical uses:
- ğŸ§ª **Exploratory ETL**: rough transforms, schema experiments, trial joins, validation spikes
- ğŸ§Š **Caches**: downloaded source bundles, API responses (when allowed), temporary tiles
- ğŸ§¹ **Pre-QA / QA**: profiling, row counts, geometry checks, bounding boxes, sampling
- ğŸ§¾ **Run manifests**: lightweight logs + provenance notes to preserve â€œhow we got hereâ€
- ğŸ—ºï¸ **Preview outputs**: small sample GeoJSON, plots, screenshots, debug tiles (small only)

---

## ğŸ§­ Where `work/` fits in the â€œtruth pathâ€

```mermaid
flowchart LR
  A[Raw ğŸ“¥] --> B[Processed ğŸ§¼] --> C[Catalog ğŸ—‚ï¸] --> D[Databases ğŸ—ƒï¸] --> E[API ğŸŒ] --> F[UI/AI ğŸ—ºï¸ğŸ¤–]
  W[(work ğŸ§°)]
  W -. scratch outputs .-> B
  W -. download cache .-> A
  W -. notes & manifests .-> C
```

**Rule:** `work/` can *assist* any stage, but **must not replace** any stage.

---

## ğŸ“ Recommended structure

Keep `work/` predictable so tools and humans can find things fast:

```text
data/
â”œâ”€ raw/ ğŸ“¥                  # immutable source snapshots (governed)
â”œâ”€ processed/ ğŸ§¼            # cleaned/standardized outputs (governed)
â”œâ”€ catalog/ ğŸ—‚ï¸             # metadata, STAC/DCAT, provenance (governed)
â””â”€ work/ ğŸ§°                 # (YOU ARE HERE) scratch + iteration (NOT governed)
   â”œâ”€ tmp/ ğŸ§¯               # throwaway files (safe to delete anytime)
   â”œâ”€ cache/ ğŸ§Š             # re-download avoidance (safe to delete anytime)
   â”œâ”€ runs/ ğŸƒ              # one folder per experiment/run (recommended)
   â”œâ”€ experiments/ ğŸ§ª       # notebooks / ad-hoc spikes (small outputs only)
   â””â”€ reports/ ğŸ“Š           # QA summaries, profiling results (small text/plots)
```

> [!TIP]
> Treat `tmp/` and most of `cache/` as **rebuildable**. If it canâ€™t be rebuilt, it doesnâ€™t belong here.

---

## ğŸƒ Run folder contract (âœ… do this)

Each meaningful experiment should get a dedicated run folder:

```text
data/work/runs/YYYY-MM-DD__<pipeline>__<dataset_slug>/
â”œâ”€ manifest.yml             # what you did + where inputs came from
â”œâ”€ provenance.jsonld        # optional but recommended: PROV-style record
â”œâ”€ logs/                    # stdout/stderr, validation logs
â”œâ”€ inputs/                  # small samples only (or pointers)
â”œâ”€ outputs/                 # small outputs only (or pointers)
â””â”€ notes.md                 # decisions, issues, next steps
```

### `manifest.yml` (template)
```yaml
run_id: "2026-02-03__ingest__ks_dasc_counties"
owner: "@your-handle"
goal: "Validate geometry + normalize CRS; prep for processed promotion"

inputs:
  - name: "DASC counties layer"
    source: "https://â€¦"
    retrieved_at: "2026-02-03T20:00:00Z"
    license: "TBD"
    checksum_sha256: "TBD"

processing:
  steps:
    - "download"
    - "inspect schema"
    - "reproject to EPSG:4326"
    - "fix invalid geometries"
  code_ref:
    git_sha: "TBD"
    entrypoint: "pipelines/â€¦"
  environment:
    container: "TBD"
    tool_versions:
      python: "TBD"
      gdal: "TBD"

outputs:
  - name: "counties_sample.geojson"
    path: "data/work/runs/.../outputs/counties_sample.geojson"
    size_bytes: 123456
    notes: "sample only; full data promoted elsewhere"

promotion_intent:
  target_stage: "processed"
  required_checks:
    - "license verified"
    - "schema validated"
    - "provenance recorded"
```

---

## âœ… Promotion checklist (work âœ governed pipeline)

Before anything leaves `work/` and becomes â€œrealâ€:

1. ğŸ”’ **License & rights check**
   - Confirm allowed use + redistribution
   - Record license string & source link in metadata

2. ğŸ§¾ **Provenance captured (â€œmap behind the mapâ€)**
   - Source URL(s), retrieval date/time, checksums
   - Toolchain + parameters + code reference (git SHA)

3. ğŸ§ª **Quality gates**
   - Schema validation (types, null rules)
   - Spatial validation (CRS, geometry validity, bbox sanity)
   - Basic profiling (row counts, uniqueness, join keys)

4. ğŸ—‚ï¸ **Catalog entry created**
   - Minimum viable metadata (title, description, extent, license, lineage)
   - Add STAC/DCAT/PROV artifacts *where your repo expects them*

5. ğŸ—ƒï¸ **Load & serve through the API (no bypasses)**
   - No UI direct-to-DB shortcuts
   - Publish via the service layer

> [!IMPORTANT]
> Promotion is a **one-way mindset**: once promoted, the governed copies become the referenceâ€”not the scratch files in `work/`.

---

## ğŸ§¼ What NOT to put in `data/work/`

**Hard â€œnoâ€ list:**
- ğŸ”‘ Secrets (API keys, tokens, `.env`, credentials)
- ğŸ§ PII / sensitive records unless explicitly approved + governed
- ğŸ‹ï¸ Huge binaries (rasters, LiDAR, full tilesets) committed to git
- ğŸ“Œ Anything â€œproduction-criticalâ€ that canâ€™t be recreated

> [!NOTE]
> Large artifacts belong in object storage + referenced via metadata (STAC items, manifests, or catalog pointers), not committed here.

---

## ğŸ§· Git hygiene (keep the repo healthy)

Recommended approach:
- âœ… Commit: `README.md`, run manifests (`manifest.yml`), small QA reports, tiny samples
- âŒ Do not commit: big downloads, big intermediate outputs, database dumps

If needed, keep empty dirs with a `.gitkeep`:
```text
data/work/tmp/.gitkeep
data/work/cache/.gitkeep
```

---

## ğŸ§  Workflow philosophy (why this exists)

This folder is intentionally designed for:
- **Iterative cycles** (try âœ observe âœ refine âœ promote)
- **Fast feedback** with minimal ceremony *until* youâ€™re ready to govern the output
- **Transparent decisions** via lightweight manifests + notes

---

## ğŸ”— Related docs (inside the repo)

- `../../docs/architecture/` ğŸ›ï¸ *(system overview, truth path, governance)*
- `../../pipelines/` ğŸ§° *(ETL entrypoints, dataset recipes, loaders)*
- `../../docs/data/` ğŸ—‚ï¸ *(metadata standards, catalog format, naming rules)*

> If these paths differ in your checkout, update links here to match the repo layout.

---

## ğŸ™Œ Quick start (copy/paste)

```bash
# 1) Create a new run folder
mkdir -p data/work/runs/$(date +%F)__<pipeline>__<dataset_slug>/{logs,inputs,outputs}

# 2) Add a manifest
touch data/work/runs/$(date +%F)__<pipeline>__<dataset_slug>/manifest.yml

# 3) Do your work, then promote outputs into governed stages
#    raw/ -> processed/ -> catalog/ -> db -> api -> ui/ai
```

---

## ğŸ“Œ Maintainership

- Default owner: **Data / Pipeline maintainers**
- PR expectation: If you add a new workflow, include at least:
  - `manifest.yml` (or equivalent)
  - a short `notes.md`
  - a clear promotion plan (where it lands in the governed pipeline)

âœ¨ Keep it rebuildable. Keep it traceable. Keep it honest.