# ğŸ§° `data/work/` â€” Pipeline Workbench (Intermediate Artifacts)

![Data Stage](https://img.shields.io/badge/data%20stage-work-yellow)
![Governance](https://img.shields.io/badge/governance-fail--closed-critical)
![Provenance](https://img.shields.io/badge/provenance-PROV-informational)
![Principles](https://img.shields.io/badge/principles-FAIR%20%2B%20CARE-blueviolet)

> âœ… **TL;DR:** `data/work/` is the **scratch + staging zone** for repeatable ETL runs.  
> Nothing here should be treated as â€œpublishedâ€ or â€œAPI-ready.â€  
> When youâ€™re confident, **promote** outputs to `data/processed/` *with* catalogs + provenance.

---

## ğŸ¯ What `data/work/` is for

Use this directory to hold **intermediate, reproducible artifacts** produced while transforming data:

- ğŸ§ª **Intermediate transforms** (normalized tables, cleaned-but-not-final layers)
- ğŸ§© **Join outputs** (e.g., â€œtreaty layer joined to census block IDsâ€)
- ğŸ§¹ **QA/QC reports** (row counts, null scans, geometry validity summaries)
- ğŸ§° **Pipeline logs** + run manifests (so runs can be re-audited)
- ğŸ—ƒï¸ **Caches** to speed up iteration (safe to delete / regenerate)

---

## ğŸš« What does **NOT** belong in `data/work/`

- âœ… **Final datasets intended for release** â†’ put them in `data/processed/`
- ğŸª„ **AI/analysis outputs that will be referenced as evidence** â†’ treat as a first-class dataset and store in `data/processed/...` with full lineage
- ğŸ”‘ **Secrets** (API keys, tokens, credentials) â€” never store here
- ğŸ§ **Sensitive or disallowed location/identity data** â€” if you must work with it locally, keep it out of Git and apply redaction rules before promotion
- âœï¸ **Manual edits to â€œmake it workâ€** â€” fix the pipeline instead (so results are reproducible)

---

## ğŸ” Canonical data flow (where `work` sits)

KFM/KMS expects a **layered pipeline** where each stage depends on the previous stage.

```mermaid
flowchart LR
  A[ğŸ§Š data/raw<br/>immutable snapshots] --> B[ğŸ§° data/work<br/>intermediate artifacts]
  B --> C[âœ… data/processed<br/>publishable outputs]

  C --> D[ğŸ“š data/catalog & data/stac<br/>discovery metadata]
  C --> E[ğŸ§¾ data/prov or data/provenance<br/>lineage bundles]

  D --> F[ğŸ—„ï¸ Database/Graph]
  F --> G[ğŸ”Œ API Layer]
  G --> H[ğŸ—ºï¸ UI / Stories]
```

**Rule of thumb:** if itâ€™s not in `data/processed/` **and** not described by catalogs + provenance, itâ€™s not â€œrealâ€ to the platform.

---

## ğŸ—‚ï¸ Recommended folder layout

Organize by **domain** â†’ **dataset** â†’ **stages**.

```text
ğŸ“ data/
â”œâ”€ ğŸ“ raw/                                   ğŸ§¾ source snapshots (immutable / read-only)
â”œâ”€ ğŸ“ work/                                  ğŸ§ª pipeline workspace (intermediate + ephemeral artifacts)
â”‚  â”œâ”€ ğŸ“„ README.md                            ğŸ‘ˆ you are here
â”‚  â””â”€ ğŸ“ <domain>/                            ğŸ§­ thematic bucket (e.g., historical/, hydrology/, soils/)
â”‚     â””â”€ ğŸ“ <dataset_slug>/                   ğŸ·ï¸ dataset workspace (one dataset at a time)
â”‚        â”œâ”€ ğŸ“ _run/                          ğŸ§· run manifests + pipeline fingerprints
â”‚        â”‚  â”œâ”€ ğŸ“„ run.json                     âœ… required: run config + tool versions + params
â”‚        â”‚  â””â”€ ğŸ“ logs/                        ğŸªµ pipeline logs (stdout/stderr, timings, warnings)
â”‚        â”œâ”€ ğŸ“ stage_01_extract/               ğŸ“¦ unpack/unzip/extract outputs
â”‚        â”œâ”€ ğŸ“ stage_02_clean/                 ğŸ§¼ cleaning (nulls, types, geometry fixes, de-dupe)
â”‚        â”œâ”€ ğŸ“ stage_03_normalize/             ğŸ§© canonicalization (schema, CRS, domains, IDs)
â”‚        â”œâ”€ ğŸ“ stage_04_join/                  ğŸ”— joins/enrichment (crosswalks, lookups, merges)
â”‚        â”œâ”€ ğŸ“ qc/                             ğŸ§ª QA/QC outputs (reports, expectations, diffs)
â”‚        â”œâ”€ ğŸ“ exports/                        ğŸ“¤ short-lived shareables (NOT final / not canonical)
â”‚        â””â”€ ğŸ“ tmp/                            ğŸ—‘ï¸ delete-anytime scratch
â”œâ”€ ğŸ“ processed/                              âœ… final outputs consumed by DB/API/UI
â”œâ”€ ğŸ“ catalog/                                ğŸ—‚ï¸ discovery catalog (DCAT)
â”œâ”€ ğŸ“ stac/                                   ğŸ›°ï¸ geospatial catalog (STAC)
â””â”€ ğŸ“ prov/                                   ğŸ§¬ provenance bundles (or ğŸ“ provenance/)
```

### âœ… Domain expansion pattern

When adding a new domain, keep it isolated and consistent:

- `data/raw/<new-domain>/` â†’ source snapshots
- `data/work/<new-domain>/` â†’ intermediate processing
- `data/processed/<new-domain>/` â†’ final outputs
- Add a domain runbook under `docs/data/<new-domain>/` (and link it from your higher-level docs)

---

## ğŸ·ï¸ Naming conventions (so artifacts stay searchable)

**Prefer â€œself-describingâ€ filenames** that wonâ€™t confuse future-you:

**Recommended pattern**
- `dataset__stage__vX__YYYY-MM-DD__<optional-short-hash>.<ext>`

**Examples**
- `census_1900__normalize__v1__2026-01-29.parquet`
- `land_treaties__join_census__v2__2026-01-29__a1b2c3.geojson`
- `landsat_2010_kansas__cloudmask__v1__2026-01-29.tif`

**Always capture somewhere (file name or manifest):**
- ğŸŒ CRS / projection
- ğŸ•’ time span represented
- ğŸ”¢ units + any transforms
- ğŸ” pipeline + config version used

---

## ğŸ§¾ Run manifests & provenance (make work auditable)

Even though `data/work/` is â€œintermediate,â€ it should still be **traceable**.  
Every pipeline run should write a manifest that links:

- inputs (raw)
- intermediate outputs (work)
- final outputs (processed, if produced)
- pipeline version + config used

<details>
<summary>ğŸ§¾ Suggested <code>_run/run.json</code> template</summary>

```json
{
  "run_id": "2026-01-29T18:42:11Z__census_1900__normalize__v1",
  "git_commit": "abc1234",
  "pipeline": "pipelines/census_1900",
  "config": {
    "profile": "default",
    "crs_out": "EPSG:4326",
    "notes": "normalize + standardize units"
  },
  "inputs": [
    {"path": "data/raw/census/1900.csv", "sha256": "â€¦"}
  ],
  "intermediate_outputs": [
    {"path": "data/work/census/census_1900/stage_03_normalize/census_1900__normalize__v1__2026-01-29.parquet", "sha256": "â€¦"}
  ],
  "qc": {
    "rows_in": 123456,
    "rows_out": 123456,
    "null_rates": {"population": 0.0},
    "geometry_valid": true
  },
  "timestamp_utc": "2026-01-29T18:42:11Z"
}
```

</details>

> ğŸ§¾ **Provenance expectation:** lineage should reference **raw inputs â†’ work intermediates â†’ processed outputs**, plus the **pipeline run/config**.

---

## ğŸ” Quality gates you should run in `work/` (before promotion)

Use `data/work/` to catch issues early. A good minimum checklist:

- [ ] **Schema**: expected columns present, types correct
- [ ] **Ranges**: numeric fields sane (no impossible negatives, etc.)
- [ ] **Nulls**: missingness understood and documented
- [ ] **Duplicates**: key uniqueness confirmed (or intentional duplicates explained)
- [ ] **CRS**: known + consistent; any reprojection documented
- [ ] **Geometry** (if spatial): valid geometries, no obviously flipped coords
- [ ] **Time** (if temporal): time range verified + encoded consistently
- [ ] **Sampling sanity**: a quick map/view confirms it â€œlooks rightâ€
- [ ] **License + rights**: known before anything is considered publishable

---

## ğŸš€ Promotion checklist: `work/` â†’ `processed/` âœ…

When an artifact is ready to become a real dataset:

1. **Export final output** â†’ `data/processed/<domain>/...`
2. **Generate boundary artifacts** (required for â€œpublishedâ€ state):
   - ğŸ—‚ï¸ STAC item/collection
   - ğŸ·ï¸ DCAT dataset entry
   - ğŸ§¾ PROV lineage bundle
3. **Validate** (schema + basic sanity)
4. **Commit + PR**
   - include processed data + metadata + provenance
   - let CI enforce requirements

> ğŸ”’ **Governance note:** the project is designed to **fail closed** â€” if licensing/metadata/provenance is missing, promotion should be blocked.

---

## ğŸ§¹ Cleaning & retention

`data/work/` is allowed to be big and messy **during iteration** â€” but it should remain safe to prune.

**Good hygiene**
- Keep throwaway output in `tmp/`
- Keep caches in `cache/` (if you add it) and document how to rebuild
- Keep anything required for auditing in `_run/`

<details>
<summary>ğŸ§½ Example cleanup patterns</summary>

```bash
# clear only scratch + tmp (keep run manifests)
rm -rf data/work/<domain>/<dataset_slug>/tmp/*
rm -rf data/work/<domain>/<dataset_slug>/exports/*

# optional: remove *all* intermediates for a dataset (rebuild from raw)
rm -rf data/work/<domain>/<dataset_slug>/*
```

</details>

---

## ğŸ”’ Safety, privacy & licensing reminders

Geospatial work frequently contains sensitive location information:

- ğŸ“ **Locational privacy**: treat any location traces as potentially sensitive until proven otherwise.
- ğŸ§¾ **Metadata + copyright**: track origin, rights, and allowed uses early â€” *before* you publish anything.
- âœ… **Interoperability**: prefer standard formats and document coordinate/projection info.

### ğŸ§¾ Minimum metadata fields to capture (even while in `work/`)
When promoting, ensure you can provide at least:

- Identification (what it is)
- Quality (limits + confidence)
- Spatial data organization (how geometry is stored)
- Spatial reference (CRS/projection)
- Entity/attribute descriptions
- Distribution + use policy
- Citation info
- Temporal info
- Contact info

---

## ğŸ”— Quick navigation

- ğŸ“ `../raw/` â€” immutable inputs
- âœ… `../processed/` â€” publishable outputs
- ğŸ“š `../catalog/` â€” catalog metadata (DCAT)
- ğŸ—‚ï¸ `../stac/` â€” STAC items/collections
- ğŸ§¾ `../prov/` *(or `../provenance/` depending on branch)* â€” lineage bundles

---

## ğŸ“š Project reference shelf (handy for doing â€œworkâ€ correctly)

<details>
<summary>ğŸ“– Internal docs used to define conventions here</summary>

- ğŸ§± Kansas Frontier Matrix (KFM) â€” Comprehensive Technical Blueprint  
- ğŸ§­ Master Guide (v13) â€” required staging layout + catalog/prov expectations  
- ğŸ—ºï¸ *Making Maps* â€” metadata, copyright, interoperability, and privacy considerations  
- ğŸ›°ï¸ Cloud-Based Remote Sensing w/ Earth Engine â€” practical QC + metadata habits  
- ğŸ§­ Map Reading & Land Navigation â€” coordinate systems + ground-truth thinking  
- ğŸ“± Mobile Mapping (Project Muse) â€” field capture patterns + GPS considerations  

</details>

---

<!--
ğŸ“Œ Source grounding (for maintainers)

Pipeline order + governance (fail closed) + PR/CI expectations:
- Fail closed + FAIR/CARE: :contentReference[oaicite:0]{index=0}
- Canonical pipeline order: :contentReference[oaicite:1]{index=1}
- Stage changes + CI checks catalog/provenance: :contentReference[oaicite:2]{index=2}

Staging layout + work folder intent:
- Raw â†’ Work â†’ Processed (required staging): :contentReference[oaicite:3]{index=3}
- Domain expansion pattern + docs guidance: :contentReference[oaicite:4]{index=4}
- Evidence artifacts belong in processed: :contentReference[oaicite:5]{index=5}

Metadata + privacy/copyright reminders:
- Metadata fields + locational privacy/copyright/interoperability: :contentReference[oaicite:6]{index=6}
-->

