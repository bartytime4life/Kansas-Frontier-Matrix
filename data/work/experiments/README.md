# ğŸ§ª Experiments Lab â€” `data/work/experiments`

![MCP](https://img.shields.io/badge/MCP-Scientific%20Method%20%2B%20Clean%20Code-2ea44f)
![Reproducible](https://img.shields.io/badge/Reproducibility-Required-blue)
![Provenance](https://img.shields.io/badge/Data%20Provenance-Tracked-purple)
![Human-Centered](https://img.shields.io/badge/Digital%20Humanism-Human%20in%20the%20Loop-orange)

> [!NOTE]
> This folder is the **electronic lab notebook + artifact vault** for *working* experiments: geospatial pipelines, ML training runs, statistical studies, simulations, and prototypes.  
> The goal is simple: **anyone can rerun (or audit) any result** with minimal guessing.

---

## ğŸ§­ What belongs here?

âœ… **YES (belongs in `data/work/experiments`)**
- ğŸ§ª Experiment folders (protocol + code + configs + outputs + report)
- ğŸ“Š EDA notebooks with clear narrative + conclusions
- ğŸ§± Pipeline trials (new ETL stages, new feature engineering, new raster/vector processing)
- ğŸ¤– Model training/evaluation runs (with parameters + metrics + seeds + environment snapshot)
- ğŸŒ GIS/remote sensing processing trials (COG/GeoJSON/tile generation, validation checks)
- ğŸ›°ï¸ Simulation runs + calibration/validation attempts + uncertainty notes

ğŸš« **NO (belongs elsewhere)**
- ğŸ”’ **Raw source downloads** that should be immutable (prefer `data/raw/` or an external store + pointer)
- ğŸ§¾ Long-term â€œfinalâ€ documentation (prefer `/docs/` or `/mcp/` if your repo uses it)
- ğŸ§¨ Secrets, API keys, private credentials (never commitâ€”use env vars/secrets managers)
- ğŸ˜ Massive artifacts committed directly to Git (use DVC / LFS / object storage)

> [!TIP]
> If a file is too big to comfortably diff, itâ€™s probably too big for Git. Track it (or its hash + location) instead.

---

## ğŸ§· Quick Start (new experiment in 5 minutes)

1) **Create a new experiment folder** using the naming convention below  
2) **Write the protocol first** (question â†’ hypothesis â†’ method)  
3) **Run it** (capture config + environment + seeds)  
4) **Write a short report** (what happened + why it matters)  
5) **Update the registry** (so we can find it later)

---

## ğŸ·ï¸ Naming & ID conventions

### Experiment IDs
Use a predictable, sortable ID:

- `EXP-YYYY-MM-DD_<short-slug>`  
  Example: `EXP-2026-01-04_drought-nowcast-baseline`

Optional domain prefixes (helpful for filtering):
- `EXP-GIS-...` ğŸŒ
- `EXP-ML-...` ğŸ¤–
- `EXP-SIM-...` ğŸ›°ï¸
- `EXP-STAT-...` ğŸ“ˆ

### Run IDs (inside an experiment)
Use timestamped run folders to avoid collisions:

- `runs/run-YYYYMMDD-HHMMSSZ`  
  Example: `runs/run-20260104-214233Z`

---

## ğŸ—‚ï¸ Recommended directory layout

```text
data/work/experiments/
â”œâ”€ ğŸ§ªğŸ“¦ experiments/                             # experiment workspace root (you are here)
â”œâ”€ ğŸ§°ğŸ§¾ templates/                               # copy/paste scaffolds for consistent runs
â”‚  â”œâ”€ ğŸ§¾ğŸ§ª experiment_report_template.md          # report.md starter: goal â†’ method â†’ results â†’ decision
â”‚  â”œâ”€ ğŸ§·ğŸ“‹ run_manifest_template.json             # run â€œflight recorderâ€ (inputs/params/env/hashes)
â”‚  â””â”€ ğŸ›ï¸ğŸ§¬ params_template.yaml                  # params/hyperparams/CLI snapshot template
â”œâ”€ ğŸ—‚ï¸ğŸ§¾ registry/                                # indexes so experiments are discoverable
â”‚  â”œâ”€ ğŸ§­ğŸ—’ï¸ experiments_index.md                  # human-friendly index (browse + links)
â”‚  â””â”€ ğŸ“ŠğŸ§¾ experiments_registry.csv              # machine-friendly registry (filters/sorts/ingest)
â””â”€ ğŸ§ªğŸ§· EXP-YYYY-MM-DD_<slug>/                    # one experiment = protocol + runs + report
   â”œâ”€ ğŸ“„âœ¨ README.md                              # tl;dr + links to protocol/report/runs/artifacts
   â”œâ”€ ğŸ§¾ğŸ§  protocol.md                            # written BEFORE running (hypothesis + metrics)
   â”œâ”€ ğŸ“ğŸ“Š report.md                              # written AFTER running (results + decision)
   â”œâ”€ ğŸ›ï¸ğŸ§¬ params/                                # configs, hyperparams, CLI args snapshots
   â”‚  â””â”€ ğŸ›ï¸ğŸ“„ params.yaml                         # pinned params for reproducibility
   â”œâ”€ ğŸ§©ğŸ§  src/                                   # scripts or small runnable modules
   â”œâ”€ ğŸ““ğŸ”¬ notebooks/                             # EDA / prototypes (narrative required)
   â”œâ”€ ğŸ—ƒï¸ğŸ§ª data/                                  # pointers, small samples, or DVC-tracked artifacts
   â”œâ”€ ğŸƒâ€â™‚ï¸ğŸ“¦ runs/                                 # execution history (immutable-ish)
   â”‚  â””â”€ ğŸ•’ğŸƒ run-YYYYMMDD-HHMMSSZ/                # a single run instance
   â”‚     â”œâ”€ ğŸ§·ğŸ§¾ manifest.json                     # provenance + env + inputs + outputs (hashes)
   â”‚     â”œâ”€ ğŸ“ŸğŸ“„ stdout.log                        # console output / progress
   â”‚     â”œâ”€ ğŸ“ˆğŸ§¾ metrics.json                      # metrics + eval summaries
   â”‚     â””â”€ ğŸ§°ğŸ–¼ï¸ artifacts/                        # plots, tiles, model weights (track smartly)
   â””â”€ ğŸ—’ï¸ğŸ§­ notes/                                 # scratch notes, review comments, decisions
```

> [!IMPORTANT]
> Every experiment must have **protocol.md** + **report.md**. If itâ€™s not written down, it didnâ€™t happen. ğŸ§¾

---

## ğŸ” Experiment lifecycle

```mermaid
flowchart LR
  A[ğŸ’¡ Question / Problem] --> B[ğŸ“š Background / Prior Art]
  B --> C[ğŸ§  Hypothesis]
  C --> D[ğŸ§ª Protocol + Variables + Metrics]
  D --> E[âš™ï¸ Run + Capture Config/Env/Seeds]
  E --> F[ğŸ“Š Analyze + Validate]
  F --> G[ğŸ§¾ Report + Decision]
  G --> H[ğŸš€ Promote / Ship / Archive]
  G --> A
```

---

## âœ… Minimum â€œExperiment Recordâ€ (Definition of Done)

Use this checklist before calling an experiment â€œdoneâ€:

- [ ] **Question** is explicit (what decision will this inform?)
- [ ] **Hypothesis** is testable (expected direction or outcome)
- [ ] **Method** is reproducible (steps, variables, controls/baselines)
- [ ] **Data provenance** is recorded (source, version/hash, transformations)
- [ ] **Code version** is pinned (git commit hash or tag)
- [ ] **Environment** is captured (requirements/conda/Docker + hardware notes)
- [ ] **Seeds** are recorded (and set for deterministic runs where possible)
- [ ] **Metrics** are defined *before* running (what counts as â€œbetterâ€?)
- [ ] **Results** include uncertainty where relevant (CI, error bars, UQ notes)
- [ ] **Validation** includes at least one sanity check or replication attempt
- [ ] **Report** states conclusion + limitations + next steps

---

## ğŸ§¾ Protocol-first rule (what â€œprotocol.mdâ€ must contain)

A protocol is a **pre-registered plan** for what you are about to do.

### Required sections
- ğŸ¯ Objective / Decision it supports
- ğŸ” Background & assumptions
- ğŸ§  Hypothesis (or expected outcome)
- ğŸ§° Materials/tools (datasets, libraries, services)
- ğŸ§ª Method (steps + variables + controls/baselines)
- ğŸ“ Metrics (primary + secondary)
- ğŸ§¯ Risks & failure modes (including data quality concerns)
- ğŸ§· Repro notes (seed plan, compute plan, environment plan)

---

## ğŸ§· Run manifest (what â€œmanifest.jsonâ€ must capture)

Every `runs/<run-id>/manifest.json` should include:

- `experiment_id`, `run_id`
- `timestamp_utc`
- `git_commit`
- `data_inputs` (dataset IDs/paths + hashes or DVC refs)
- `params` (inline or linked)
- `environment` (python version, libs, container image hash, GPU/CPU)
- `seeds`
- `outputs` (paths + hashes)
- `notes` (what was different / why this run exists)

> [!TIP]
> Treat `manifest.json` like a **flight recorder** âœˆï¸ â€” itâ€™s what youâ€™ll need when something breaks or a result gets questioned.

---

## ğŸ“Š Statistical hygiene (avoid fooling ourselves)

> [!WARNING]
> â€œToo perfectâ€ results are a smell. Optional stopping, selective reporting, and uncontrolled multiple comparisons can make noise look like signal.

Basic rules:
- âœ… Decide metrics and stopping rules **before** running
- âœ… Prefer effect sizes + confidence intervals, not just p-values
- âœ… Correct for multiple comparisons when exploring many hypotheses
- âœ… Record *all* runs (including â€œfailedâ€ / negative results)
- âœ… If power/sample-size matters, estimate it early
- âœ… If itâ€™s observational, explicitly address confounders (stratify/regress/match)

---

## ğŸŒ GIS / Remote Sensing experiment notes

For geospatial processing experiments (COGs, GeoJSON, tiles, time layers):
- Always record: CRS, resolution, bounding boxes, temporal coverage
- Store a **catalog/metadata** entry (source URL/archive ref + processing steps)
- Validate alignment (spot check control points / overlay checks)
- Keep conversions scripted (not â€œclick ops onlyâ€)

Artifacts may include:
- ğŸ—ºï¸ COGs, vector layers, generated tiles, KML/KMZ exports
- ğŸ“Œ Validation screenshots or checksums + â€œlooks-rightâ€ notes

---

## ğŸ¤– ML experiment notes

For training runs:
- Log: dataset split strategy, leakage checks, features, hyperparameters
- Keep baseline models (so we know what â€œbetterâ€ means)
- Save metrics per epoch/step if relevant, but summarize key outcomes
- Create **model cards** for models that might be promoted/deployed

Suggested run artifacts:
- `metrics.json`, `confusion_matrix.png`, `roc.png`
- `model.pkl` / `model.pt` / `onnx/` (track with DVC/LFS as needed)

---

## ğŸ›°ï¸ Simulation experiment notes

For simulation & modeling:
- Record physical assumptions, parameter ranges, and calibration steps
- Include verification/validation notes (what did we compare against?)
- Track uncertainty (inputs â†’ outputs), especially where decisions depend on risk
- When hardware/software changes: do a **back-to-back** comparison run if feasible

---

## ğŸ§‘â€ğŸ¤â€ğŸ§‘ Ethics & human-centered guardrails

Experiments should preserve:
- ğŸ§­ Human agency & oversight (humans remain decision-makers)
- ğŸ” Transparency (assumptions + uncertainty are visible)
- âš–ï¸ Fairness / non-discrimination where models touch people
- ğŸ” Privacy & data governance for sensitive layers

> [!NOTE]
> If your experiment touches sensitive or person-adjacent data, add a short â€œEthics & Riskâ€ section to the report with mitigations.

---

## ğŸ§° Handy command-line snippets (safe + useful)

```bash
# Find experiments
ls -1 data/work/experiments | grep "^EXP-"

# Search for an experiment id across reports
grep -R "EXP-" -n data/work/experiments 2>/dev/null | head

# List biggest folders (helps avoid committing huge artifacts)
du -h -d 2 data/work/experiments | sort -h | tail -n 20

# Find manifests and quickly inspect them (requires jq)
find data/work/experiments -name manifest.json -print
# jq '.experiment_id, .run_id, .git_commit' path/to/manifest.json
```

---

## ğŸ§¾ Templates

### 1) Experiment report template (`templates/experiment_report_template.md`)

Copy/paste into `report.md`:

```markdown
---
doc_kind: experiment-report
experiment_id: EXP-YYYY-MM-DD_slug
status: draft
owners: [ "TBD" ]
created: YYYY-MM-DD
updated: YYYY-MM-DD
git_commit: "TBD"
data:
  inputs:
    - dataset_id: "TBD"
      version: "TBD"
      location: "TBD"
  outputs:
    - artifact: "TBD"
      location: "TBD"
repro:
  seeds: [1337]
  env:
    method: "conda|pip|docker"
    lockfile: "TBD"
ethics:
  sensitive_data: false
  care_label: "TBD"
---

# ğŸ§ª EXP-YYYY-MM-DD_slug â€” Report

## ğŸ“˜ Overview
- **Goal:**  
- **Decision this informs:**  
- **Primary metric:**  
- **Baseline:**  

## â“ Question
## ğŸ§  Hypothesis
## ğŸ§° Method
## ğŸ“Š Results
## âœ… Validation & Sanity Checks
## ğŸ§© Interpretation (What it means)
## âš ï¸ Limitations / Risks
## ğŸ” Next Steps
## ğŸ”— Links
- Runs:
- Notebooks:
- Artifacts:
```

### 2) Run manifest template (`templates/run_manifest_template.json`)

```json
{
  "experiment_id": "EXP-YYYY-MM-DD_slug",
  "run_id": "run-YYYYMMDD-HHMMSSZ",
  "timestamp_utc": "YYYY-MM-DDTHH:MM:SSZ",
  "git_commit": "TBD",
  "data_inputs": [
    { "dataset_id": "TBD", "version": "TBD", "path": "TBD", "hash": "TBD" }
  ],
  "params_path": "params/params.yaml",
  "environment": {
    "python": "TBD",
    "os": "TBD",
    "container_image": "TBD",
    "cpu": "TBD",
    "gpu": "TBD"
  },
  "seeds": [1337],
  "outputs": [
    { "path": "runs/<run-id>/artifacts", "hash": "TBD" }
  ],
  "notes": "What changed / why this run exists"
}
```

---

## ğŸ“š Reference shelf (project docs that inspired this folder)

- ğŸ“˜ Master protocol for scientific method + documentation rigor  
- ğŸ§  KFM technical reference (architecture, ML, GIS, validation, ethics)  
- ğŸ›°ï¸ Scientific modeling & simulation rigor (validation, UQ, workflow discipline)  
- ğŸ“‰ Statistical â€œgotchasâ€ (publication bias / optional stopping / multiple comparisons)  
- ğŸ§‘â€âš–ï¸ Digital humanism & human-centered AI guardrails  

---

## ğŸ§¹ Housekeeping rules (keep the lab usable)

- Keep each experiment folder readable (a new person should understand it in <10 minutes)
- Prefer small scripts + automatable pipelines over â€œmystery notebooksâ€
- If an experiment becomes a feature, **promote it** into the main system (and leave a link)
- Periodically archive/purge stale runs (but never lose the report + manifest trail)

---

### âœ… If you only remember 3 thingsâ€¦

1) **Protocol first** ğŸ§¾  
2) **Manifests always** ğŸ§·  
3) **Repro or it didnâ€™t happen** ğŸ”

