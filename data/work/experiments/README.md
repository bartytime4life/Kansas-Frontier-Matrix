# ğŸ§ª Experiments Lab â€” `data/work/experiments/`

![Provenance First](https://img.shields.io/badge/provenance-first-2ea44f)
![Reproducible](https://img.shields.io/badge/reproducible-by%20default-blue)
![Truth Path](https://img.shields.io/badge/pipeline-Raw%E2%86%92Processed%E2%86%92Catalog%E2%86%92DB%E2%86%92API%E2%86%92UI-8a2be2)
![Scope](https://img.shields.io/badge/scope-Kansas%20Frontier%20Matrix%20(KFM)-gold)

Welcome to the KFM **sandbox** ğŸ§°â€”where we test ideas, validate hypotheses, benchmark performance, prototype GIS workflows, and trial AI models **without breaking the â€œtruth path.â€**  

> âœ… Goal: turn experiments into **traceable, shippable** datasets, layers, models, and stories.  
> âŒ Not a dumping ground: if it canâ€™t be reproduced (or cited), it doesnâ€™t belong here.

---

## ğŸ”¥ Quick Start (the 60-second ritual)

1) **Create a new experiment folder**
```bash
mkdir -p data/work/experiments/2026-02-03__kansas_river_ndvi__yourname
cd data/work/experiments/2026-02-03__kansas_river_ndvi__yourname
```

2) **Add the three required files**
- `experiment.yml` (what/why/how)
- `run.*` (how to reproduce)
- `RESULTS.md` (what happened)

3) **Run it, record it, package it**
- outputs go into `./artifacts/`
- figures go into `./figures/`
- logs go into `./logs/`

---

## ğŸ—‚ï¸ Folder Layout (recommended)

```text
data/work/experiments/
â”œâ”€â”€ README.md  ğŸ‘ˆ you are here
â”œâ”€â”€ _templates/ ğŸ§© reusable manifests, scripts, report skeletons
â””â”€â”€ 2026-02-03__kansas_river_ndvi__yourname/
    â”œâ”€â”€ experiment.yml           âœ… required
    â”œâ”€â”€ run.sh | run.py | run.R  âœ… required (one-click reproduction)
    â”œâ”€â”€ RESULTS.md               âœ… required (human summary)
    â”œâ”€â”€ notebooks/               ğŸ““ optional (exploration only)
    â”œâ”€â”€ src/                     ğŸ§  code used by run.*
    â”œâ”€â”€ configs/                 âš™ï¸ params, AOIs, style specs
    â”œâ”€â”€ inputs/                  ğŸ”— pointers only (NEVER raw dumps)
    â”œâ”€â”€ artifacts/               ğŸ“¦ generated data/products (non-final)
    â”œâ”€â”€ figures/                 ğŸ–¼ï¸ charts, maps, screenshots
    â”œâ”€â”€ logs/                    ğŸ§¾ stdout/stderr + run metadata
    â””â”€â”€ provenance/              ğŸ§¬ citations + lineage notes
```

### âœ… What goes where?
| Thing | Put it here | Notes |
|---|---|---|
| Hypothesis / design | `experiment.yml` | keep it crisp + testable |
| One-click reproduction | `run.*` | no manual steps |
| Raw external data | **not here** | store in `data/raw/` via ingestion pipeline |
| Generated interim outputs | `artifacts/` | okay to be messy, but reproducible |
| Final shippable datasets | `data/processed/` | after review + metadata |
| Metadata (STAC/DCAT) | `data/catalog/` | required for publish |
| Lineage (W3C PROV style) | `data/provenance/` | required for publish |

---

## ğŸ§­ The Lab Rules (nonâ€‘negotiables)

### 1) Donâ€™t bypass the Truth Path ğŸš¦
Experiments may explore freely, but anything that becomes **public-facing** must travel:
**Raw â†’ Processed â†’ Catalog â†’ Database â†’ API â†’ UI/AI**  
No shortcuts. No â€œtemporaryâ€ backdoors. No â€œjust this once.â€  

### 2) Inputs are **references**, not dumps ğŸ”—
Inside an experiment folder, `inputs/` should contain:
- dataset IDs
- URLs
- query parameters
- bounding boxes / AOIs
- commit hashes
- checksums

### 3) Every chart/map needs a breadcrumb trail ğŸ¥¾
If a figure lands in `figures/`, it must be traceable back to:
- source dataset(s)
- processing steps
- parameters
- code entrypoint (`run.*`)

### 4) Reproducibility beats cleverness â™»ï¸
If the result canâ€™t be recreated by someone else (or you-in-2-weeks), itâ€™s not a result yet.

---

## ğŸ§¾ Experiment Manifest (`experiment.yml`) âœ…

Use this as your baseline:

```yaml
id: 2026-02-03__kansas_river_ndvi__yourname
title: "Kansas River NDVI trend (2016â€“2025)"
owner: "yourname"
status: "draft" # draft | validated | shipped | archived
type:
  - remote_sensing
  - time_series
hypothesis: >
  NDVI along the Kansas River corridor shows distinct seasonal signatures and detectable multi-year change.
questions:
  - "What is the NDVI seasonal profile by county?"
  - "Are there statistically meaningful trends after controlling for seasonality?"

inputs:
  datasets:
    - id: "usgs_landsat_collection2"
      access: "stac"   # stac | api | file
      subset:
        aoi: "aoi/kansas_river_corridor.geojson"
        time: ["2016-01-01", "2025-12-31"]
  notes:
    - "inputs are pointers; no raw dumps in this folder"

methods:
  steps:
    - name: "fetch_scenes"
      tool: "stac-client"
    - name: "cloud_mask"
      tool: "python"
    - name: "compute_ndvi"
      tool: "python"
    - name: "aggregate"
      tool: "postgis/sql"
  parameters:
    cloud_threshold: 0.2
    ndvi_scale: 10000
    spatial_unit: "county"
    temporal_unit: "month"

outputs:
  artifacts:
    - path: "artifacts/ndvi_monthly.parquet"
    - path: "figures/ndvi_trends.png"
  candidate_publish:
    processed_dataset_id: "ks_ndvi_kansas_river_monthly_v1"

evaluation:
  checks:
    - "spot-check 10 scenes visually"
    - "validate AOI coverage"
    - "sanity bounds: NDVI âˆˆ [-1, 1]"
  metrics:
    - "coverage_pct"
    - "missingness_rate"

reproducibility:
  entrypoint: "run.sh"
  environment:
    python: "3.11"
    containers: true
  randomness:
    seed: 1337

provenance:
  citations_file: "provenance/SOURCES.md"
  lineage_file: "provenance/LINEAGE.md"

notes:
  - "If this ships: add STAC/DCAT + PROV docs and move dataset into the canonical pipeline."
```

---

## ğŸ§¬ Provenance Pack (minimum viable)

Create these two files:

### `provenance/SOURCES.md`
- Bullet list of **every** external dataset / document used
- License + attribution notes
- Access date(s)
- IDs, URLs, or catalog references

### `provenance/LINEAGE.md`
A human-readable chain like:
- **Input datasets** â†’ **transforms** â†’ **outputs**
- Include command lines + script names + key parameters

> Pro tip: treat this like â€œthe map behind the mapâ€ ğŸ—ºï¸â€”someone should be able to audit the result.

---

## ğŸ§  Common Experiment Types (pick your flavor)

### ğŸ—ºï¸ Geospatial / GIS
- topology validation
- overlays / joins
- routing & networks
- tile generation sanity checks

### ğŸ›°ï¸ Remote Sensing
- STAC scene ingest tests
- COG/PMTiles generation experiments
- change detection, classification baselines

### ğŸ¤– ML / AI
- baseline models, ablations, feature studies
- RAG retrieval quality checks
- bias/leakage audits (train/test hygiene)

### âš¡ Performance / Scalability
- API latency under load
- tile serving throughput
- DB query plans & caching behavior

### ğŸ›ï¸ UI / Map UX
- layer styling trials
- time slider usability tests
- narrative â€œstory nodeâ€ prototypes

---

## âš¡ Performance Experiments (rules of clean measurements)

If youâ€™re load-testing:
- keep load generators **separate** from the system under test
- write down the workload model (open vs closed, think time, concurrency)
- record response time + throughput + utilization, not just â€œit felt fastâ€

Suggested structure:
```text
perf/
â”œâ”€â”€ test_plan.md
â”œâ”€â”€ scenarios/
â”œâ”€â”€ scripts/
â”œâ”€â”€ raw_results/
â””â”€â”€ report.md
```

---

## ğŸ“¦ From Experiment â†’ Shippable Output (the promotion ceremony)

When an experiment proves useful:

1) **Freeze the run**
- tag inputs (dataset IDs / commit hashes)
- lock dependencies
- make `run.*` one-command reproducible

2) **Promote artifacts**
- move cleaned outputs to `data/processed/<dataset_id>/...`
- write catalog metadata in `data/catalog/<dataset_id>.{json,yaml}`
- create lineage docs in `data/provenance/<dataset_id>/...`

3) **Integrate**
- load into DB / index as appropriate
- expose through API
- verify UI renders + citations show up

---

## ğŸ§© Templates

Use these starter files:
- `data/work/experiments/_templates/experiment.yml`
- `data/work/experiments/_templates/RESULTS.md`
- `data/work/experiments/_templates/SOURCES.md`
- `data/work/experiments/_templates/LINEAGE.md`

---

## âœ… RESULTS.md (recommended format)

```markdown
# Results â€” <experiment id>

## TL;DR
- âœ… what worked
- âŒ what didnâ€™t
- ğŸ¤” what surprised us

## What changed vs baseline?
- bullet list of deltas

## Artifacts
- `artifacts/...`
- `figures/...`

## Quality checks
- what you validated + outcomes

## Decision
- [ ] Archive
- [ ] Iterate
- [ ] Promote to `data/processed/` (with metadata + provenance)
```

---

## ğŸ§  Mermaid: experiment lifecycle (birdâ€™s-eye view)

```mermaid
flowchart TD
  A[Idea / Hypothesis] --> B[Experiment Folder]
  B --> C[Run + Log + Artifacts]
  C --> D{Validated?}
  D -- No --> E[Iterate / Archive]
  D -- Yes --> F[Promote to Processed]
  F --> G[Catalog Metadata (STAC/DCAT)]
  G --> H[Provenance (W3C PROV-style)]
  H --> I[DB / Index]
  I --> J[API]
  J --> K[UI / AI Answers]
```

---

## ğŸ§¯ When to Archive an Experiment
Archive (donâ€™t delete) if:
- it answered the question
- or the hypothesis was falsified
- or a better approach replaced it

Mark `status: archived` in `experiment.yml` and add a final note in `RESULTS.md`.

---

## ğŸ™Œ Philosophy (the vibe)
Weâ€™re building an evidence-first geospatial knowledge systemâ€”so experiments must be:
**testable âœ…, reproducible â™»ï¸, and traceable ğŸ§¾**.

Happy hacking. Keep the breadcrumbs. ğŸ¥–âœ¨