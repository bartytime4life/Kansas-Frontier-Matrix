# ğŸ§° `data/work/<domain>` â€” Workbench (Intermediate Artifacts)

![stage](https://img.shields.io/badge/stage-data%2Fwork-blue)
![purpose](https://img.shields.io/badge/purpose-intermediate%20artifacts%20%26%20receipts-brightgreen)
![rule](https://img.shields.io/badge/rule-raw%20is%20immutable-critical)
![trust](https://img.shields.io/badge/trust-provenance--first%20%E2%9C%85-informational)
![policy](https://img.shields.io/badge/policy-gates%20%2B%20QA%20required-orange)

> [!TIP]
> Replace every `<domain>` placeholder in this README with your domain slug (example: `sims`, `rivers`, `landcover`, `boundaries`, `census`, `hazards`).

> [!IMPORTANT]
> `data/work/<domain>/` is **staging + scratch + receipts**.  
> âœ… Use it for intermediate transforms, caches, QA reports, and run manifests.  
> âŒ Do **not** treat it as a published source of truth.

---

## ğŸ§­ Quick Links

- ğŸ”’ Raw inputs (read-only): `../../raw/<domain>/`
- âœ… Published outputs: `../../processed/<domain>/`
- ğŸ§¾ Catalog + provenance (publish gates): `../../catalog/` + `../../provenance/`
- ğŸ§ª Pipeline code (recommended): `../../../pipelines/<domain>/`
- ğŸ“š Domain docs (recommended): `../../../docs/data/<domain>/`

---

## ğŸ§  Why this folder exists

KFM follows a **managed promotion lifecycle**:

- **`data/raw/`** â†’ immutable source bytes (evidence)
- **`data/work/`** â†’ *temporary + intermediate* transformations and experiments
- **`data/processed/`** â†’ authoritative datasets ready to be served
- **`data/catalog/` + `data/provenance/`** â†’ discovery + traceability backbone (STAC/DCAT/PROV)

This domain folder exists so we can:
- normalize inputs (schema, CRS, time indexes) ğŸ§¹
- generate intermediate artifacts (tiles, indexes, embeddings, QA results) ğŸ§ª
- keep **receipts** (checksums, manifests, logs) for reproducibility ğŸ§¾
- promote only *validated* outputs to `data/processed/<domain>/` âœ…

---

## âœ… Golden Rules (nonâ€‘negotiable)

1) **Never modify `data/raw/`**  
   If bytes change, that happens in `data/work/` (intermediate) or `data/processed/` (publishable).

2) **Everything is reproducible**
   - Prefer scripted, config-driven transforms over â€œhand editsâ€
   - Capture environment + command + inputs in a run manifest

3) **No â€œmystery layersâ€**
   - If an artifact could be served, it must be promotable into `data/processed/` + documented with metadata/provenance.

4) **Safety & sensitivity**
   - Treat location-sensitive / culturally-sensitive / private data as **restricted**
   - Obfuscate or aggregate *before* promotion (example: distribution maps vs exact points)

---

## ğŸ—‚ï¸ Directory Layout (recommended)

```text
data/work/<domain>/
â”œâ”€ ğŸ‘‹ğŸ“„ README.md                          # ğŸ‘‹ you are here ğŸ“Œ What belongs in work/, retention rules, and promotion to processed/
â”œâ”€ ğŸ§¹ _tmp/                               # ğŸ§¹ Throwaway scratch (safe to delete; never relied on by pipelines)
â”œâ”€ âš¡ cache/                              # âš¡ Deterministic caches (rebuildable; keyed by inputs+params+versions)
â”œâ”€ ğŸ§ª runs/                               # ğŸ§ª Every pipeline execution produces one run folder (append-only)
â”‚  â””â”€ ğŸ·ï¸ <run_id>/                        # e.g. 2026-01-24T031500Z_a1b2c3d (UTC timestamp + short suffix)
â”‚     â”œâ”€ ğŸ§¾ manifest/                     # ğŸ§¾ â€œWhat happenedâ€ receipts (inputs/config/tooling/policy)
â”‚     â”‚  â”œâ”€ ğŸ§¾ğŸ” run_manifest.json          # Run ledger: who/what/when + params + inputs/outputs + hashes/pointers
â”‚     â”‚  â”œâ”€ ğŸ”’ğŸ§¾ inputs.lock.json           # Frozen pointers to raw inputs (paths/URIs + checksums) for reproducibility
â”‚     â”‚  â”œâ”€ ğŸ§°ğŸ§¾ tool_versions.txt          # Toolchain versions (python/node/gdal/tippecanoe/etc.) used by the run
â”‚     â”‚  â””â”€ ğŸš¦ğŸ§¾ policy_decisions.json      # Optional: OPA/governance decisions (pass/fail, findings, waivers)
â”‚     â”œâ”€ ğŸ”§ staging/                      # ğŸ”§ Intermediate transforms (not final; may be pruned)
â”‚     â”œâ”€ âœ… qa/                           # âœ… Validation outputs (anomaly scans, link checks, geo sanity, metrics)
â”‚     â”‚  â”œâ”€ ğŸ“ğŸ“„ qa_report.md              # Human QA summary (what was checked + key findings)
â”‚     â”‚  â””â”€ ğŸ“ŠğŸ§¾ qa_metrics.json           # Machine QA metrics (counts, thresholds, anomaly rates)
â”‚     â”œâ”€ ğŸ“¦ outputs/                      # ğŸ“¦ Candidate artifacts (promote to processed/ when accepted)
â”‚     â”œâ”€ ğŸ“œ logs/                         # ğŸ“œ Run logs (structured; sanitize secrets/PII)
â”‚     â”‚  â”œâ”€ ğŸ“ˆğŸ§¾ telemetry.ndjson          # Append-only NDJSON events (timings, counters, audit-safe signals)
â”‚     â”‚  â””â”€ ğŸªµğŸ“„ run.log                   # Human-readable log (high-level; useful for quick debugging)
â”‚     â””â”€ ğŸ” receipts/                     # ğŸ” Integrity + traceability receipts (tamper evidence)
â”‚        â”œâ”€ ğŸ”ğŸ“„ checksums.sha256          # sha256 sums for key files/artifacts in this run bundle
â”‚        â””â”€ ğŸ§¾ file_inventory.json         # Inventory of produced files (paths, sizes, roles, digests, pointers)
â”œâ”€ ğŸ““ notebooks/                          # ğŸ““ Exploration notebooks (keep small; link to runs/ and manifests)
â””â”€ ğŸ—ï¸ reports/                            # ğŸ—ï¸ Human-readable summaries across runs (rollups, trends, release notes)
```

> [!NOTE]
> A good â€œrule of thumbâ€: **anything youâ€™d be sad to lose belongs in `data/processed/` + catalogs**, not here.

---

## ğŸ†” Run IDs & Naming Conventions

### Run ID format (recommended)
Use a sortable timestamp + short hash:

- `<run_id> = YYYY-MM-DDThhmmssZ_<shortsha>`
  - example: `2026-01-24T031500Z_a1b2c3d`

### Dataset/artifact naming (recommended)
When you create intermediate or candidate outputs:

- `<dataset_id>__<variant>__v<semver>.<ext>`
  - example: `landcover__county_summary__v1.2.0.parquet`
  - example: `rivers__gauges_latest__v0.1.0.geojson`

### Geospatial conventions
- Serve-friendly CRS: **WGS84 (EPSG:4326)**  
- Keep the original CRS recorded in manifests/metadata
- If you generalize/simplify geometries for performance, record *how* and *why* (zoom thresholds, tolerance)

---

## ğŸ§¾ Required Run Artifacts (minimum contract)

Every `runs/<run_id>/` should include:

- [ ] `manifest/run_manifest.json`
- [ ] `receipts/checksums.sha256` (or multihash equivalent)
- [ ] `logs/telemetry.ndjson` (or structured JSON logs)
- [ ] `qa/qa_report.md` (or an equivalent machine+human report)
- [ ] `manifest/tool_versions.txt` (or lockfile snapshot)

### Suggested `run_manifest.json` shape

```json
{
  "run_id": "2026-01-24T031500Z_a1b2c3d",
  "domain": "<domain>",
  "pipeline": "pipelines/<domain>/pipeline.py",
  "git": {
    "repo": "Kansas-Frontier-Matrix",
    "commit": "<commit_sha>",
    "dirty": false
  },
  "inputs": [
    {
      "path": "data/raw/<domain>/source_a/file.ext",
      "sha256": "<hex>",
      "source_url": "<optional>",
      "license": "<spdx-or-text>",
      "sensitivity": "public|restricted|embargoed"
    }
  ],
  "steps": [
    { "name": "extract", "ts_start": "...", "ts_end": "...", "params": {} },
    { "name": "transform", "ts_start": "...", "ts_end": "...", "params": {} },
    { "name": "validate", "ts_start": "...", "ts_end": "...", "params": {} }
  ],
  "outputs": [
    {
      "path": "data/work/<domain>/runs/â€¦/outputs/candidate.parquet",
      "sha256": "<hex>",
      "candidate_promotion_target": "data/processed/<domain>/candidate.parquet"
    }
  ]
}
```

> [!TIP]
> If you support *idempotency keys*, hash a canonicalized manifest (or config) and store it as `manifest/idempotency_key.txt` so reruns can detect duplicates.

---

## ğŸ§ª What belongs in `outputs/` vs `staging/`

### `staging/` (ğŸ”§ intermediate)
Put here:
- extracted raw tables before schema alignment
- reprojection scratch files
- partial merges / joins
- temporary tiles or intermediate rasters
- intermediate embeddings / indexes (if rebuildable)

### `outputs/` (ğŸ“¦ promotion candidates)
Put here:
- final-cleaned candidate tables
- candidate GeoParquet / Parquet / GeoJSON
- COGs / derived rasters ready for cataloging
- vector tile bundles (PMTiles / MBTiles) ready for packaging
- scenario/simulation results intended to become publishable layers
- any AI artifacts intended for product features (only if reproducible + documented)

---

## ğŸ§¼ Hygiene: what should **never** live here

- âŒ secrets (API keys, tokens, credentials)
- âŒ untracked binaries with no manifest
- âŒ â€œfinalâ€ data that bypasses `data/processed/` + metadata/provenance
- âŒ private data in plaintext without governance clearance

---

## ğŸ” Sensitivity, Ethics, and Governance

This is where **safe-by-design** happens:

- If a dataset contains sensitive coordinates (endangered species, cultural sites, private stations):
  - produce an **obfuscated/aggregated** output here
  - promote only the safe form
  - record the transformation + rationale in the manifest and QA report

> [!WARNING]
> â€œWe can hide it in the UIâ€ is **not** a safety plan. If it shouldnâ€™t be public, donâ€™t promote it.

### Policy gates (recommended)
When applicable, store decision artifacts under `manifest/`:
- OPA / Conftest decision output
- sensitivity classification checks
- license compliance checks
- schema & metadata lint results

---

## âœ… Promotion Checklist (Work âœ Processed)

Before copying/moving anything from:

`data/work/<domain>/runs/<run_id>/outputs/*`
to
`data/processed/<domain>/â€¦`

confirm:

- [ ] Output passes schema validation
- [ ] Geometry validity checked (if spatial)
- [ ] CRS recorded + standard served CRS confirmed
- [ ] License recorded + compatible
- [ ] Sensitivity classification confirmed
- [ ] Checksums recorded
- [ ] Catalog metadata prepared (STAC/DCAT as applicable)
- [ ] Provenance record prepared (PROV / JSON-LD as applicable)
- [ ] CI-friendly: can be regenerated in a clean environment

---

## ğŸ—ºï¸ Common Domain Artifact Patterns

<details>
<summary><strong>ğŸ§© Geospatial (MapLibre / tiles / PostGIS)</strong></summary>

Typical artifacts you might stage in this domain workbench:

- **Vector**: GeoJSON, GeoParquet, Parquet
- **Raster**: COG (Cloud-Optimized GeoTIFF), XYZ tile caches
- **Tiles**: MVT, MBTiles, PMTiles
- **3D**: 3D Tiles, CZML (for Cesium), KML regionated tiles (legacy)

If you generate tiles:
- keep raw geometry in authoritative storage (usually processed + DB)
- store tile generation parameters (minzoom/maxzoom, simplification tolerances, layers)
- ensure the tile outputs are traceable back to their source

</details>

<details>
<summary><strong>ğŸ§  AI / Focus Mode artifacts</strong></summary>

Allowed here *only when reproducible + documented*:

- embeddings generation scratch (with model/version noted)
- vector index snapshots (if rebuildable)
- retrieval evaluation reports (precision/recall metrics)
- prompt/test fixtures for explainable Q&A

If an AI output is user-facing, it must be:
- attributable (citations/evidence)
- reproducible (manifest)
- policy-checked (sensitivity + license)

</details>

<details>
<summary><strong>â±ï¸ Simulations & scenario outputs</strong></summary>

Keep scenario sweeps here first:

- parameter grids
- intermediate model state
- scenario outputs (per run)
- model cards + assumptions + caveats

Then promote â€œpublished scenariosâ€ to `data/processed/<domain>/` with the same rigor as any dataset.

</details>

---

## ğŸ” Pipeline Flow (mental model)

```mermaid
flowchart LR
  RAW[ğŸ“¥ data/raw domain - immutable] --> WORK[ğŸ§° data/work domain - intermediate]
  WORK --> PROC[ğŸ“¦ data/processed domain - authoritative]
  PROC --> META[ğŸ§¾ data/catalog + ğŸ§¬ data/provenance - STAC DCAT PROV]
  META --> API[ğŸ”Œ API + ğŸ§± tiles + ğŸ•¸ï¸ graph]
  API --> UI[ğŸ—ºï¸ UI - 2D 3D timeline ğŸ¤– focus mode]
```

---

## ğŸ â€œNew Domainâ€ Setup Checklist

When adding a new domain:

- [ ] Create:
  - `data/raw/<domain>/README.md`
  - `data/work/<domain>/README.md` âœ… (this file)
  - `data/processed/<domain>/README.md`
  - `docs/data/<domain>/README.md` (domain docs + data contract)
- [ ] Add pipeline scaffold: `pipelines/<domain>/`
- [ ] Add validation hooks and promotion checks (CI-friendly)
- [ ] Define data contract + sensitivity defaults

---

## ğŸ“š Project Reference Docs (what shaped this README)

<details>
<summary><strong>ğŸ“¦ Project file library (click to expand)</strong></summary>

These project documents inform the patterns used here:

- ğŸ“š **KFM Data Intake â€“ Technical & Design Guide** (pipeline, provenance-first, QA gates)
- ğŸ—ï¸ **KFM Comprehensive Architecture, Features, and Design** (STAC/DCAT/PROV backbone, managed promotion)
- ğŸ§± **KFM Comprehensive Technical Documentation** (formats, WGS84, tiles, validation + contracts)
- ğŸ§­ğŸ¤– **KFM AI System Overview** (policy gates, explainability, agent workflows)
- ğŸ–¥ï¸ **KFM Comprehensive UI System Overview** (map behind the map, offline packs, AR/3D, simulations)
- ğŸ’¡ **Innovative Concepts to Evolve KFM** (future simulation + governance ideas)
- ğŸ§  **Additional Project Ideas** (evidence-first manifests, run receipts, integrity patterns)
- ğŸŒŸ **Latest Ideas & Future Proposals** (security, roles, rollback + provenance repair)
- ğŸ“¦ **AI Concepts & more** (reference library; may be a PDF portfolio)
- ğŸŒ **Maps / Virtual Worlds / WebGL** (reference library; may be a PDF portfolio)
- ğŸ§° **Various programming languages & resources** (reference library; may be a PDF portfolio)
- ğŸ—ƒï¸ **Data management theories / Bayesian / architectures** (reference library; may be a PDF portfolio)

</details>

---

## ğŸ§¹ Cleanup

Safe cleanup targets:
- `data/work/<domain>/_tmp/`
- old `runs/<run_id>/` directories that were never promoted
- rebuildable `cache/` entries

> [!TIP]
> If you delete a run, consider keeping at least its `run_manifest.json` and QA report in `reports/` if it influenced decisions.

---

## âœ… Status

- This folder is intentionally **run-oriented** and **disposable**.
- Promote only validated artifacts into `data/processed/<domain>/`.
- Keep receipts, keep trust. âœ¨


