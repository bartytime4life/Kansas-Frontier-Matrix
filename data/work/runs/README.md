# ğŸ§ª `data/work/runs/` â€” Run Artifacts (Logs â€¢ Manifests â€¢ Validation)

![stage](https://img.shields.io/badge/stage-work-blue)
![artifacts](https://img.shields.io/badge/artifacts-runs%20%7C%20logs%20%7C%20manifests-informational)
![reproducible](https://img.shields.io/badge/pipeline-deterministic%20%26%20idempotent-success)
![governance](https://img.shields.io/badge/governance-fail--closed-critical)
![provenance](https://img.shields.io/badge/provenance-PROV%20linked-8A2BE2)

> This folder is the **execution-time â€œflight recorderâ€** for KFM pipelines.  
> It stores **run-scoped artifacts** (manifests, logs, metrics, and validation reports) that prove what happened during a pipeline run and support replay/debug â€” **before** promotion to canonical outputs and published provenance.

---

## ğŸ¯ Purpose

KFMâ€™s nonâ€‘negotiables: **determinism**, **traceability**, and **governed publication**.  
This directory exists to:

- ğŸ“Œ Capture **what ran** (pipeline + config snapshot + environment hints)
- ğŸ” Capture **what it touched** (inputs/outputs with hashes and paths)
- ğŸ§¾ Capture **what it proved** (validation reports + policy gate results)
- ğŸ§  Support **reproducibility** (re-run with the same inputs â†’ same outputs)
- ğŸ§° Keep dev/CI runs **inspectable** without polluting `data/processed/`

---

## âœ… What belongs here

- ğŸ§¾ **Run manifest(s)** (inputs, outputs, checksums, timestamps)
- ğŸ§ª **Validation reports** (schema checks, geometry checks, QA summaries, policy checks)
- ğŸ§µ **Logs** (structured logs preferred: JSONL)
- ğŸ“ˆ **Metrics** (runtime, row counts, feature counts, memory, etc.)
- ğŸ§Š **Config snapshots** (the exact config used for the run)
- ğŸ§° **Intermediate scratch** that is needed for *debugging* / *replay* (not long-term)

---

## ğŸš« What does NOT belong here

- ğŸ›ï¸ **Canonical published data** â†’ `data/processed/**`
- ğŸ—‚ï¸ **Canonical catalogs & governance artifacts** â†’ `data/stac/**`, `data/catalog/**`, `data/prov/**` (or legacy `data/provenance/**`)
- ğŸ”‘ **Secrets / tokens / credentials** (ever)
- ğŸ“¦ **Gigantic permanent artifacts** that should be treated as datasets (promote them properly)

> [!IMPORTANT]
> Treat `data/work/runs/**` as **ephemeral**.  
> If an artifact must be **audited** or **referenced by the system**, it must be promoted to:
> - `data/processed/**` (data product)  
> - `data/stac/**` + `data/catalog/**` + `data/prov/**` (published metadata + lineage)

---

## ğŸ§­ Where `runs/` sits in the canonical pipeline

```mermaid
flowchart LR
  A["data/raw/** ğŸ§± Raw inputs"] --> B["data/work/** ğŸ§ª Intermediate"]
  B --> C["data/processed/** ğŸ Final outputs"]
  C --> D["data/stac/** ğŸ—ºï¸ STAC"]
  C --> E["data/catalog/** ğŸ§¾ DCAT"]
  B --> F["data/prov/** ğŸ§¬ PROV (lineage)"]
  C --> F
  D --> G["Graph ğŸ§ "]
  E --> H["API ğŸ§©"]
  H --> I["UI ğŸ—ºï¸"]
  I --> J["Story Nodes ğŸ“š"]
  J --> K["Focus Mode ğŸ”"]
```

**`data/work/runs/`** is the *run artifact sink* that supports **B â†’ C â†’ (D/E/F)** with evidence.

---

## ğŸ—‚ï¸ Recommended layout

We keep the structure predictable so tooling can:
- discover runs
- summarize outcomes
- upload CI artifacts
- generate PROV bundles that reference run IDs

```text
data/work/runs/
â”œâ”€â”€ README.md
â”œâ”€â”€ .gitkeep                          (optional)
â”œâ”€â”€ <pipeline_name>/                  (e.g. import_census, soils_sda, ocr_newspapers)
â”‚   â””â”€â”€ <run_id>/                     (unique per execution)
â”‚       â”œâ”€â”€ run.yaml                  # primary run metadata (human + machine friendly)
â”‚       â”œâ”€â”€ status.json               # {state: running|success|failed|aborted, ...}
â”‚       â”œâ”€â”€ config/                   # snapshot of configs used for this run
â”‚       â”‚   â”œâ”€â”€ pipeline.yaml
â”‚       â”‚   â””â”€â”€ params.json
â”‚       â”œâ”€â”€ manifests/
â”‚       â”‚   â”œâ”€â”€ inputs.json           # paths + checksums + versions
â”‚       â”‚   â””â”€â”€ outputs.json          # paths + checksums + row counts
â”‚       â”œâ”€â”€ logs/
â”‚       â”‚   â”œâ”€â”€ pipeline.jsonl        # structured logs (preferred)
â”‚       â”‚   â””â”€â”€ stderr.log
â”‚       â”œâ”€â”€ validation/
â”‚       â”‚   â”œâ”€â”€ schema_report.json
â”‚       â”‚   â”œâ”€â”€ geometry_report.json
â”‚       â”‚   â””â”€â”€ policy_gate.json
â”‚       â”œâ”€â”€ metrics/
â”‚       â”‚   â”œâ”€â”€ runtime.json
â”‚       â”‚   â””â”€â”€ counters.json
â”‚       â”œâ”€â”€ scratch/                  # optional: intermediate files (ephemeral)
â”‚       â””â”€â”€ _SUCCESS                  # sentinel file (or _FAILED with reason)
```

> [!TIP]
> If your pipeline writes intermediate files under `data/work/<domain>/...`, keep them there.  
> This `data/work/runs/` folder is for **run-level evidence & reporting**, not domain staging.

---

## ğŸ§¾ Run ID conventions

Pick a run ID that is:
- unique âœ…
- sortable by time âœ…
- easy to paste into PROV âœ…

**Recommended formats:**
- **ULID**: `01J2N7FZ2B9Q7W8KZK3K8Q2Y2D`  
- **Timestamp + commit**: `2026-02-03T21-14-08Z__import_census__a1b2c3d`

> [!NOTE]
> PROV records should reference the **run ID or commit hash** so lineage can link:
> `raw â†’ work â†’ processed` with a specific, replayable activity.

---

## âœ… Minimum â€œdefinition of doneâ€ for a run

A run is â€œpublishableâ€ only when these are true:

- [ ] `status.json` ends in `success`
- [ ] `manifests/inputs.json` contains hashes for *every* input
- [ ] `manifests/outputs.json` contains hashes + counts for *every* output
- [ ] `validation/**` exists and indicates pass/fail (no silent skips)
- [ ] Outputs are **promoted** to `data/processed/**`
- [ ] Metadata is updated/created in `data/stac/**` and `data/catalog/**`
- [ ] A PROV lineage record is written to `data/prov/**` (or legacy `data/provenance/**`)
- [ ] No output is **less restricted** than its inputs (classification propagation)

---

## ğŸ“„ Suggested `run.yaml` (template)

```yaml
run_id: "2026-02-03T21-14-08Z__import_census__a1b2c3d"
pipeline:
  name: "import_census"
  entrypoint: "pipelines/import_census.py"
  version:
    git_commit: "a1b2c3d"
    repo_dirty: false
runtime:
  started_at: "2026-02-03T21:14:08Z"
  finished_at: "2026-02-03T21:18:44Z"
  host: "devbox-01"
  runner: "local"   # local|ci|prod
actor:
  invoked_by: "admin"   # user or service account
inputs_manifest: "manifests/inputs.json"
outputs_manifest: "manifests/outputs.json"
validation:
  schema_report: "validation/schema_report.json"
  geometry_report: "validation/geometry_report.json"
  policy_gate: "validation/policy_gate.json"
promotion:
  processed_paths:
    - "data/processed/census/1900_population.geojson"
prov:
  target_path: "data/prov/import_census/2026-02-03/import_census__run.jsonld"
notes: "Byte-identical rerun expected with same raw inputs."
```

---

## ğŸ§¾ Suggested `manifests/inputs.json` (template)

```json
{
  "run_id": "2026-02-03T21-14-08Z__import_census__a1b2c3d",
  "generated_at": "2026-02-03T21:14:10Z",
  "inputs": [
    {
      "path": "data/raw/census_1900/census_1900.csv",
      "sha256": "â€¦",
      "size_bytes": 12345678
    }
  ]
}
```

---

## ğŸ§¹ Retention & cleanup

Because `data/work/runs/**` is ephemeral, choose a retention policy that fits your workflow:

- ğŸ§ª Local dev: keep last **10â€“50** runs per pipeline
- ğŸ¤– CI: keep only runs attached to **PR artifacts** (upload zipped runs)
- ğŸ›ï¸ Published runs: keep only whatâ€™s necessary to regenerate PROV + audits (and ensure itâ€™s copied to `data/prov/**`)

> [!WARNING]
> Never delete a run until its **immutable provenance** has been created in `data/prov/**` (or legacy `data/provenance/**`).

---

## ğŸ” Security & governance guardrails

- ğŸ”’ **No secrets** in logs/manifests/config snapshots
- ğŸ§ª Validation should **fail closed** (missing metadata = block promotion)
- ğŸ§­ Keep **policy gate results** (OPA/rego decisions, schema compliance)
- ğŸ§¬ Ensure run artifacts are sufficient to **audit** and **replay**

---

## ğŸ§© Git hygiene (recommended)

Typically, only this README is committed. Everything else is local/CI artifact output.

```gitignore
# Keep the docs, ignore run artifacts
data/work/runs/**
!data/work/runs/README.md
```

---

## ğŸ”— Related docs (repo paths)

- ğŸ“˜ `docs/MASTER_GUIDE_v13.md`
- ğŸ§¬ `docs/standards/KFM_PROV_PROFILE.md`
- ğŸ—ºï¸ `docs/standards/KFM_STAC_PROFILE.md`
- ğŸ§¾ `docs/standards/KFM_DCAT_PROFILE.md`
- ğŸ§± `docs/standards/KFM_REPO_STRUCTURE_STANDARD.md`

---

## ğŸ†˜ Common issues

<details>
  <summary><strong>ğŸ³ Docker volume permission issues writing under <code>data/</code></strong></summary>

If a container user doesnâ€™t match host permissions, pipelines may fail to write run artifacts.

**Symptoms**
- `Permission denied` writing to `data/work/runs/...`

**Fix ideas**
- Ensure `data/` is writable on host
- Align container user/group IDs with host user
- Use a dedicated writable volume mount for `data/`

</details>

<details>
  <summary><strong>ğŸ§ª â€œRun succeeded, but results changedâ€ (non-determinism)</strong></summary>

Deterministic pipelines should produce byte-identical outputs for identical inputs.

**Check**
- Did an external dependency change?
- Did you capture config snapshots in `config/`?
- Are timestamps embedded into outputs?

**Mitigation**
- Move timestamps into manifests/logs, not into processed dataset content
- Hash inputs and compare manifests across runs

</details>