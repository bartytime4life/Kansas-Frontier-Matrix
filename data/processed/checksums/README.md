<div align="center">

# ğŸ” Kansas Frontier Matrix â€” Processed Data Checksums

`data/processed/checksums/`

**Mission:** Safeguard the integrity and traceability of all processed datasets through reproducible
**SHA-256 checksums**, ensuring **scientific verifiability**, **data provenance**, and **end-to-end reproducibility** across the Kansas Frontier Matrix (KFM) ecosystem.

[![Build & Deploy](https://github.com/bartytime4life/Kansas-Frontier-Matrix/actions/workflows/site.yml/badge.svg)](../../../.github/workflows/site.yml)
[![STAC Validate](https://github.com/bartytime4life/Kansas-Frontier-Matrix/actions/workflows/stac-validate.yml/badge.svg)](../../../.github/workflows/stac-validate.yml)
[![CodeQL](https://github.com/bartytime4life/Kansas-Frontier-Matrix/actions/workflows/codeql.yml/badge.svg)](../../../.github/workflows/codeql.yml)
[![Trivy](https://github.com/bartytime4life/Kansas-Frontier-Matrix/actions/workflows/trivy.yml/badge.svg)](../../../.github/workflows/trivy.yml)
[![Pre-Commit](https://github.com/bartytime4life/Kansas-Frontier-Matrix/actions/workflows/pre-commit.yml/badge.svg)](../../../.github/workflows/pre-commit.yml)
[![Docs Â· MCP](https://img.shields.io/badge/Docs-MCP-blue)](../../../docs/)
[![License: Data](https://img.shields.io/badge/License-CC--BY%204.0-green)](../../../LICENSE)
[![License: Code](https://img.shields.io/badge/License-MIT-yellow)](../../../LICENSE)

</div>

---

## ğŸ§© Versioning

| Field            | Value                                    |
| :--------------- | :--------------------------------------- |
| **Version**      | `v1.4.0`                                 |
| **Status**       | Stable                                   |
| **Maintainers**  | KFM Data Engineering Team                |
| **Last Updated** | 2025-10-12                               |
| **Applies To**   | All datasets in `data/processed/`        |
| **Provenance**   | MCP v1.0 â€¢ STAC 1.0.0 â€¢ SHA-256 Verified |

---

## ğŸ“š Table of Contents

* [Overview](#overview)
* [Purpose](#purpose)
* [Directory Layout](#directory-layout)
* [Checksum Generation](#checksum-generation)
* [Verification Workflow](#verification-workflow)
* [Integration with MCP & STAC](#integration-with-mcp--stac)
* [Adding or Updating Checksums](#adding-or-updating-checksums)
* [Best Practices](#best-practices)
* [Mermaid Data Flow](#mermaid-data-flow)
* [Change Log](#change-log)
* [References](#references)

---

## ğŸ§  Overview

This directory houses **cryptographic checksum manifests (`.sha256`)** validating the byte-level integrity of all
processed datasets within `data/processed/`.

Checksums provide tamper detection, reproducibility validation, and traceable data lineage across every processing stage.
Each 64-character digest uniquely represents a dataset version and integrates directly into the
**MCP Provenance** and **STAC Catalog** metadata structures.

---

## ğŸ¯ Purpose

| Objective                  | Description                                                                   |
| :------------------------- | :---------------------------------------------------------------------------- |
| ğŸ§© **Integrity Assurance** | Ensures that processed datasets remain unmodified post-validation.            |
| ğŸ” **Reproducibility**     | Enables deterministic, verifiable reconstruction of results.                  |
| âš™ï¸ **Automation**          | Supports CI/CD checksum validation workflows.                                 |
| ğŸ”— **Provenance Linkage**  | Ties `mcp_provenance` fields in metadata and STAC Items to verifiable hashes. |

---

## ğŸ§± Directory Layout

```bash
data/
â””â”€â”€ processed/
    â””â”€â”€ checksums/
        â”œâ”€â”€ terrain/
        â”‚   â”œâ”€â”€ dem_1m_ks_filled.tif.sha256
        â”‚   â”œâ”€â”€ dem_30m_ned_ks.tif.sha256
        â”œâ”€â”€ hydrology/
        â”‚   â”œâ”€â”€ flow_dir_d8_1m_ks.tif.sha256
        â”‚   â”œâ”€â”€ watermask_ks.tif.sha256
        â”œâ”€â”€ landcover/
        â”‚   â”œâ”€â”€ nlcd_2021_ks.tif.sha256
        â”‚   â”œâ”€â”€ vegetation_mask_ks.tif.sha256
        â”œâ”€â”€ climate/
        â”‚   â”œâ”€â”€ precip_total_annual_1895_2024.tif.sha256
        â”‚   â”œâ”€â”€ drought_spi12_1895_2024.tif.sha256
        â”œâ”€â”€ hazards/
        â”‚   â”œâ”€â”€ tornado_tracks_1950_2024.geojson.sha256
        â”‚   â”œâ”€â”€ fema_disasters_1953_2024.geojson.sha256
        â”œâ”€â”€ tabular/
        â”‚   â”œâ”€â”€ tornado_counts_1950_2024.csv.sha256
        â”‚   â”œâ”€â”€ county_population_1850_2020.csv.sha256
        â”œâ”€â”€ text/
        â”‚   â”œâ”€â”€ newspapers_1854_1925_cleaned.jsonl.sha256
        â”‚   â”œâ”€â”€ nlp_entities_extracted.json.sha256
        â””â”€â”€ README.md
```

Each file stores a 64-character SHA-256 digest ensuring byte-for-byte identity across all environments.

---

## âš™ï¸ Checksum Generation

Checksums are generated automatically during ETL or manually when verifying datasets.

```bash
# âœ… Generate checksum for one file
sha256sum dem_1m_ks_filled.tif > checksums/terrain/dem_1m_ks_filled.tif.sha256

# ğŸ” Generate checksums recursively
find data/processed -type f \( -name "*.tif" -o -name "*.csv" -o -name "*.jsonl" \) \
  -exec sha256sum {} \; > data/processed/checksums/_manifest_all.sha256
```

ğŸ’¡ Use `--binary` mode (`sha256sum --binary`) for cross-platform consistency.

---

## ğŸ§ª Verification Workflow

```bash
# Verify all checksums in a subdirectory
sha256sum -c checksums/terrain/*.sha256

# Verify a specific file
sha256sum -c data/processed/checksums/climate/precip_total_annual_1895_2024.tif.sha256
```

**Automated CI/CD Integration**
Validation runs in `.github/workflows/stac-validate.yml` for every push or pull request.
Any checksum mismatch halts the pipeline, ensuring no altered or corrupted data enters production.

---

## ğŸ§© Integration with MCP & STAC

| Layer              | Integration Mechanism                                                                         |
| :----------------- | :-------------------------------------------------------------------------------------------- |
| **MCP Provenance** | Metadata JSON contains `"mcp_provenance": "sha256:<digest>"`.                                 |
| **STAC Catalog**   | STAC Item property `properties.checksum:sha256` mirrors the same digest for cross-validation. |
| **Data Lineage**   | Links checksum â†’ STAC Item â†’ metadata â†’ dataset â†’ CI logs.                                    |

This creates a **verifiable, multi-layer integrity chain** across both the **scientific** and **geospatial** domains.

---

## ğŸ§® Adding or Updating Checksums

1. **Generate**

   ```bash
   sha256sum <file> > data/processed/checksums/<path>/<file>.sha256
   ```
2. **Validate**

   ```bash
   sha256sum -c data/processed/checksums/<path>/<file>.sha256
   ```
3. **Commit**
   Add both the data and checksum files to Git.
4. **Push**
   CI/CD auto-verifies checksums.
5. **Update Provenance**
   If data changes, refresh checksum and increment `version` + `mcp_provenance` in metadata.

---

## ğŸ§  Best Practices

| Category                 | Guideline                                                  |
| :----------------------- | :--------------------------------------------------------- |
| âœ… **Completeness**       | Every processed dataset must include a matching `.sha256`. |
| ğŸ”„ **Update Policy**     | Always regenerate checksums after any file change.         |
| ğŸ§¾ **Schema Compliance** | Ensure metadata JSON validates against MCP/STAC schemas.   |
| ğŸ§ª **CI Enforcement**    | Automated tests must reject mismatched hashes.             |
| ğŸ§° **Transparency**      | Maintain `_manifest_all.sha256` for full dataset audits.   |

---

## ğŸ§­ Mermaid Data Flow

```mermaid
flowchart TD
  A["Data Sources<br/>raw scans Â· rasters Â· vectors"] --> B["ETL Process<br/>normalize Â· clean Â· export"]
  B --> C["Processed Data<br/>COGs Â· CSVs Â· JSONL"]
  C --> D["Checksum Generation<br/>sha256sum per file"]
  D --> E["Verification<br/>CI/CD Â· local validation"]
  E --> F["Catalog & Provenance<br/>STAC Â· MCP metadata"]
  F --> G["Public Access<br/>GitHub Â· Data Hub Â· Google Earth"]
<!-- END OF MERMAID -->
```

---

## ğŸ§¾ Change Log

| Version    | Date       | Type      | Description                                                 | Author     |
| :--------- | :--------- | :-------- | :---------------------------------------------------------- | :--------- |
| **v1.4.0** | 2025-10-12 | ğŸ”§ Update | Full revision with versioning, Mermaid, and best-practices. | Andy Barta |
| **v1.3.2** | 2025-09-28 | ğŸ§© Add    | Added checksum integration with STAC validation.            | KFM Core   |
| **v1.2.0** | 2025-08-15 | âœ¨ Feature | Introduced `_manifest_all.sha256` batch audits.             | Data Ops   |
| **v1.0.0** | 2025-06-01 | ğŸš€ Launch | Initial checksum repository for processed data.             | MCP Team   |

---

## ğŸ“– References

* ğŸ”— [GNU Coreutils SHA Utilities](https://www.gnu.org/software/coreutils/manual/html_node/sha2-utilities.html)
* ğŸŒ [STAC Specification 1.0](https://stacspec.org)
* ğŸ§© [JSON Schema](https://json-schema.org)
* ğŸ“˜ [Master Coder Protocol Standards](../../../docs/standards/)
* ğŸ§­ [Data Provenance in Open Science](https://www.nature.com/articles/s41597-019-0193-2)

---

<div align="center">

> â€œEvery verified checksum is a promise â€”
> Kansasâ€™s digital frontier remains consistent, transparent, and trusted.â€

**â€” Kansas Frontier Matrix Â· Integrity First**

</div>
