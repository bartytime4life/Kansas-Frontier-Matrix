<div align="center">

# ‚ö†Ô∏è Kansas Frontier Matrix ‚Äî Hazards Checksums  
`data/processed/checksums/hazards/`

**Mission:** Maintain and verify the **integrity, provenance, and reproducibility**  
of all processed natural hazard datasets ‚Äî including tornadoes, floods, wildfires, and drought layers ‚Äî  
within the Kansas Frontier Matrix data ecosystem.

[![Build & Deploy](https://github.com/bartytime4life/Kansas-Frontier-Matrix/actions/workflows/site.yml/badge.svg)](../../../../.github/workflows/site.yml)
[![Trivy Security](https://github.com/bartytime4life/Kansas-Frontier-Matrix/actions/workflows/trivy.yml/badge.svg)](../../../../.github/workflows/trivy.yml)
[![STAC Validate](https://github.com/bartytime4life/Kansas-Frontier-Matrix/actions/workflows/stac-validate.yml/badge.svg)](../../../../.github/workflows/stac-validate.yml)
[![Docs ¬∑ MCP](https://img.shields.io/badge/Docs-MCP-blue)](../../../../docs/)
[![License: Data](https://img.shields.io/badge/License-CC--BY%204.0-green)](../../../../LICENSE)

</div>

---

## üìö Overview

This directory stores **SHA-256 checksum files (`.sha256`)**  
for all processed **hazard datasets** in Kansas Frontier Matrix (KFM).  

Checksums serve as cryptographic fingerprints that ensure:
- **Data integrity** ‚Äî confirming no corruption or tampering has occurred.  
- **Reproducibility** ‚Äî verifying that ETL outputs are identical across runs.  
- **Provenance** ‚Äî linking each hazard dataset to its metadata and STAC record.  
- **Auditability** ‚Äî providing traceable verification within continuous integration workflows.  

---

## üóÇÔ∏è Directory Layout

```bash
data/processed/checksums/hazards/
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ tornado_tracks_1950_2024.geojson.sha256
‚îú‚îÄ‚îÄ flood_events_1900_2025.geojson.sha256
‚îú‚îÄ‚îÄ wildfire_perimeters_2000_2024.geojson.sha256
‚îî‚îÄ‚îÄ drought_index_2000_2025.tif.sha256
````

> **Note:** Each `.sha256` file corresponds directly to its dataset in
> `data/processed/hazards/` and is automatically validated in CI workflows via `sha256sum -c`.

---

## üîê Purpose of Checksums

| Objective                  | Description                                                              |
| :------------------------- | :----------------------------------------------------------------------- |
| **Integrity Verification** | Detects data corruption or modification post-processing.                 |
| **Reproducibility**        | Ensures ETL pipeline outputs are consistent and deterministic.           |
| **Provenance**             | Connects each dataset to its metadata, STAC entry, and source reference. |
| **CI Enforcement**         | Integrated into GitHub Actions; builds fail on checksum mismatches.      |

---

## üßÆ Example `.sha256` File

```bash
# File: tornado_tracks_1950_2024.geojson.sha256
8fb29cda3d0e44182f26c7bceff74b2c81b83e742d47d836b33151f871bb69d1  tornado_tracks_1950_2024.geojson
```

This checksum authenticates the dataset
`data/processed/hazards/tornado_tracks_1950_2024.geojson`.

---

## ‚öôÔ∏è Checksum Generation Workflow

Checksums are generated automatically as part of the hazards ETL process.

**Makefile target:**

```bash
make hazards-checksums
```

**Equivalent Python command:**

```bash
python src/utils/generate_checksums.py data/processed/hazards/ --algo sha256
```

**Steps:**

1. Identify all processed hazard files (`.geojson`, `.tif`, `.csv`).
2. Compute their SHA-256 hashes using the `hashlib` library.
3. Output `<filename>.sha256` files into this directory.
4. Validate these hashes as part of CI/CD pipelines.

---

## üß∞ CI/CD Validation

Checksum validation runs automatically in GitHub Actions workflows.

**Example validation command:**

```bash
sha256sum -c data/processed/checksums/hazards/*.sha256
```

If any hash mismatch is detected, the workflow fails ‚Äî halting commits or deploys
until the affected dataset is rebuilt and re-hashed.

---

## üß© Integration with Metadata & STAC

| Linked Component                            | Purpose                                                              |
| :------------------------------------------ | :------------------------------------------------------------------- |
| `data/processed/metadata/hazards/`          | STAC items reference checksum files for data verification            |
| `src/pipelines/hazards/hazards_pipeline.py` | Handles generation and validation of hashes                          |
| `.github/workflows/stac-validate.yml`       | CI job re-verifying all hashes automatically                         |
| `data/stac/hazards/`                        | STAC catalog embeds checksum references in each asset metadata entry |

---

## üß† MCP Compliance Summary

| MCP Principle           | Implementation                                                   |
| :---------------------- | :--------------------------------------------------------------- |
| **Documentation-first** | Each hazard dataset has an accompanying checksum and STAC record |
| **Reproducibility**     | Deterministic ETL outputs verified via hashes                    |
| **Open Standards**      | Uses SHA-256 (FIPS 180-4 compliant cryptographic algorithm)      |
| **Provenance**          | Checksum links connect source ‚Üí processed ‚Üí metadata             |
| **Auditability**        | CI pipelines verify every checksum during validation             |

---

## üìÖ Version History

| Version | Date       | Summary                                                                                    |
| :------ | :--------- | :----------------------------------------------------------------------------------------- |
| v1.0    | 2025-10-04 | Initial hazards checksum release ‚Äî includes tornado, flood, wildfire, and drought datasets |

---

<div align="center">

**Kansas Frontier Matrix** ‚Äî *‚ÄúEvery Storm Verified: Data Integrity for a Changing Kansas.‚Äù*
üìç [`data/processed/checksums/hazards/`](.) ¬∑ Linked to the **Hazards STAC Collection**

</div>
