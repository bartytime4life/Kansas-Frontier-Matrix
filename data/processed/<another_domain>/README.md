# ğŸ“¦ Processed Data â€” `<another_domain>` (KFM)

![Lifecycle](https://img.shields.io/badge/lifecycle-processed-blue)
![Evidence Stack](https://img.shields.io/badge/evidence-STAC%20%7C%20DCAT%20%7C%20PROV-success)
![Governance](https://img.shields.io/badge/policy-fail--closed-critical)
![Reproducible](https://img.shields.io/badge/pipelines-deterministic%20%26%20reproducible-informational)
![Ready for UI](https://img.shields.io/badge/serving-API%20%2B%20UI%20%2B%20Focus%20Mode-brightgreen)

> [!IMPORTANT]
> **Nothing in `data/processed/` is â€œhand-editedâ€.** Every byte in this folder must be **reproducible** from `data/raw/â€¦` inputs + pipeline code/config, and must be **discoverable + auditable** via the KFM evidence stack.

---

## ğŸ¯ Purpose

This directory stores **final, publishable artifacts** for the **`<another_domain>`** domain:
- âœ… cleaned / normalized / validated outputs  
- âœ… ready to be served by **API**, visualized in **UI**, and cited by **Focus Mode**  
- âœ… referenced by **STAC/DCAT** catalogs and **PROV** lineage  
- âœ… safe to distribute (respecting **FAIR + CARE**, sensitivity labels, and policy gates)

> Replace `<another_domain>` with your domain slug (recommended: `kebab-case`).

---

## âœ… What belongs here

**Put these here:**
- ğŸ§± **Authoritative processed datasets** (the ones weâ€™re willing to stand behind)
- ğŸ—ºï¸ **UI-ready geospatial artifacts**  
  - vectors (e.g., GeoParquet, GeoJSON for small sets)  
  - rasters (e.g., COGs)  
  - tiles / caches (e.g., PMTiles or precomputed tiles)  
- ğŸ“Œ **Sidecar integrity + audit files** next to the artifacts (checksums, run manifests)
- ğŸ§¾ **Stable, versioned releases** (date- or semver-based)

---

## ğŸš« What does *not* belong here

**Never put these here:**
- ğŸ§ª experimental outputs (keep in `data/work/â€¦` until promoted)
- ğŸ§± raw downloads â€œas-receivedâ€ (those live in `data/raw/â€¦`)
- âœï¸ manual â€œfix-upsâ€ (fix the pipeline/config instead)
- ğŸ§¨ orphan files not referenced by catalogs/provenance
- ğŸ”‘ secrets / tokens / credentials (policy should block these anyway)

---

## ğŸ”— The KFM Evidence Stack Contract (nonâ€‘negotiable)

For **every dataset version** promoted into `data/processed/<another_domain>/â€¦`, you must ensure the **catalog triplet** exists and stays consistent:

1) ğŸ—ºï¸ **STAC** (spatiotemporal + asset metadata)  
2) ğŸ§¾ **DCAT** (publication, license, distribution, governance fields)  
3) ğŸ§¬ **PROV** (lineage: inputs â†’ activity/run â†’ outputs)

**Optional but strongly recommended sidecars (per version):**
- ğŸ” `checksums.sha256` (or multihash file)
- ğŸ§¾ `run_manifest.json` (run_id, tool versions, inputs/outputs summary)
- ğŸ“š `data_dictionary.md` (field meaning, units, caveats)
- ğŸ§ª `validation_report.json` (schema/geometry/range checks)

> [!TIP]
> If a dataset canâ€™t be traced, it canâ€™t be trusted. If it canâ€™t be trusted, it canâ€™t be served.

---

## ğŸ—‚ï¸ Recommended Folder Structure

> This is the **recommended** structure inside `data/processed/<another_domain>/`.  
> Keep it boring and consistent. Future you will thank you. âœ…

```text
ğŸ“ data/processed/<another_domain>/
â”œâ”€ ğŸ“„ README.md                       # ğŸ‘ˆ you are here
â”œâ”€ ğŸ“ _schemas/                       # shared schema contracts (optional)
â”‚  â””â”€ ğŸ“„ <dataset_id>.schema.json
â”œâ”€ ğŸ“ _docs/                          # domain-specific notes (optional)
â”‚  â””â”€ ğŸ“„ glossary.md
â”œâ”€ ğŸ“ <dataset_id>/                   # one folder per canonical dataset
â”‚  â”œâ”€ ğŸ“ v<version>/                  # one folder per release
â”‚  â”‚  â”œâ”€ ğŸ§± <dataset_id>__v<version>.geoparquet
â”‚  â”‚  â”œâ”€ ğŸ—ºï¸ <dataset_id>__v<version>.pmtiles
â”‚  â”‚  â”œâ”€ ğŸ›°ï¸ <dataset_id>__v<version>.cog.tif
â”‚  â”‚  â”œâ”€ ğŸ” checksums.sha256
â”‚  â”‚  â”œâ”€ ğŸ§¾ run_manifest.json
â”‚  â”‚  â”œâ”€ âœ… validation_report.json
â”‚  â”‚  â””â”€ ğŸ“š data_dictionary.md
â”‚  â””â”€ ğŸ“„ LATEST                        # (optional) pointer file or note
â””â”€ ğŸ“ _inventory/
   â””â”€ ğŸ“„ inventory.csv                 # (optional) domain inventory export
```

---

## ğŸ†” Dataset IDs, Versions, and Naming

### Canonical dataset ID (`dataset_id`)
Use a **stable canonical ID** everywhere (catalogs + provenance + filenames).

**Suggested pattern (adapt to your repo conventions):**
- `kfm.ks.<another_domain>.<dataset_slug>`

Examples:
- `kfm.ks.hydrology.usgs_nwis_gauges`
- `kfm.ks.agriculture.usda_cropland_data_layer`

### Versioning
Pick one version strategy and stick to it per dataset:
- ğŸ“… date-based: `v2026-01-24`
- ğŸ§ª semver: `v1.3.0`
- ğŸ§Š content-hash builds (advanced): keep a human-friendly tag + recorded digest

### Filename convention
Use **dataset_id + version** in filenames to keep artifacts self-describing:
- `<dataset_id>__v<version>.<ext>`

---

## ğŸ” Promotion Workflow: Detect â†’ Validate â†’ Promote (GitOps)

```mermaid
flowchart LR
  A[ğŸ“¥ data/raw/<another_domain>/<dataset_id>/] --> B[ğŸ§ª Ingestion Gate<br/>checksum + sanity]
  B --> C[ğŸ§° data/work/<another_domain>/<dataset_id>/]
  C --> D[âš™ï¸ Deterministic ETL Pipeline<br/>(config-driven)]
  D --> E[ğŸ“¦ data/processed/<another_domain>/<dataset_id>/vX/]
  E --> F[ğŸ—ºï¸ STAC + ğŸ§¾ DCAT + ğŸ§¬ PROV updates]
  F --> G[ğŸ”— Graph import / index refresh]
  G --> H[ğŸŒ API + UI + ğŸ¤– Focus Mode consume]
```

### Automation (optional but encouraged) ğŸ¤–ğŸ¤âš™ï¸
- ğŸ•µï¸ **Watcher** detects upstream changes (new source version, new feed, etc.)
- ğŸ§  **Planner** generates a deterministic plan (what to ingest + which pipeline/config)
- ğŸ§° **Executor** runs the pipeline + opens a PR (never auto-merges)

---

## âœ… Validation & Policy Gates (Fail Closed)

Before anything in this folder is considered **live**, the following must pass:

- ğŸ§± **Schema validation** (fields, types, required columns)
- ğŸ—ºï¸ **Spatial validation** (CRS expectations, geometry validity, extents)
- ğŸ§¾ **License presence** (no unknown license)
- ğŸ·ï¸ **Sensitivity / classification correctness**  
  - classification must propagate correctly through transforms
- ğŸ§¬ **Provenance completeness** (inputs + processing steps declared)
- ğŸ” **Catalog completeness** (STAC/DCAT/PROV are present + consistent)

> [!NOTE]
> Policy gates should block merges when rules fail. Donâ€™t â€œfix it laterâ€ â€” later becomes never.

---

## ğŸ” Sensitivity, CARE, and Privacy (Donâ€™t Ship Harm)

If your domain touches **sensitive locations, vulnerable communities, PII, or culturally sensitive info**:

- ğŸ·ï¸ label the dataset correctly (classification + care/ethics signals)
- ğŸ§Š prefer **aggregation/generalization** over raw precision in public outputs
- ğŸŒ«ï¸ apply **blurring / zoom-based generalization** where appropriate (UI-safe artifacts)
- ğŸ§ª consider privacy-preserving techniques for releases (e.g., record-level privacy / noise for aggregates)

> [!IMPORTANT]
> If the dataset requires community/tribal approval or special handling, do **not** promote without the required review & governance record.

---

## ğŸš€ Publishing & Distribution Options

### Option A â€” Repo/Object Storage (standard)
- Store large artifacts in `data/processed/â€¦` or in object storage (S3/CDN).
- Ensure **every file is referenced** in metadata (no hidden/orphan blobs).
- Keep **checksums** next to the artifact and/or in catalogs.

### Option B â€” OCI Artifact Distribution (advanced) ğŸ“¦ğŸ”
Package datasets like â€œdata containersâ€:
- push artifacts (PMTiles / GeoParquet / COG) with **ORAS**
- sign with **Cosign**
- attach **PROV** + run manifest + SBOM/SLSA attestations as referrers

<details>
<summary>ğŸ§  Why OCI distribution is useful</summary>

- ğŸ” makes rollback easy (tags + digests)
- ğŸ§¾ encourages immutable, content-addressed releases
- ğŸ” improves supply-chain security (signatures + attestations)
- ğŸŒ enables federation (other regions can mirror/pull artifacts)

</details>

---

## ğŸ§ª Simulations & Models (Special Case)

Simulation/model outputs should follow a **sandbox â†’ promotion** pattern:

- ğŸ§° first run in `data/work/sims/â€¦` (or domain workbench)
- âœ… promote only after review + evidence stack artifacts are generated
- ğŸš« never point production UI/graph to `data/work/â€¦` outputs directly

---

## ğŸ§­ UI + API Consumption Notes (what to optimize for)

KFMâ€™s consumers generally expect:
- ğŸ—ºï¸ tile-friendly formats (fast pan/zoom)
- â³ time-awareness (timeline slider / temporal filtering)
- ğŸ§¾ â€œLayer Infoâ€ provenance (source, license, processing summary)
- ğŸ¤– Focus Mode citations: answers must point to datasets/assets that back claims

If youâ€™re unsure, prioritize:
- **GeoParquet** for analytics + joins
- **PMTiles** (or vector tiles) for interactive maps
- **COGs** for rasters + fast tiling

---

## ğŸ§‘â€ğŸ’» Add a New Dataset (Checklist)

> Print this checklist into your soul ğŸ˜„âœ…

### 1) Define the dataset
- [ ] Choose `dataset_id` (stable canonical ID)
- [ ] Choose versioning scheme
- [ ] Decide output formats (vector/raster/tiles)

### 2) Ingest raw evidence
- [ ] Add raw files to `data/raw/â€¦` (as-received)
- [ ] Generate `checksums.sha256`
- [ ] Record source + license + access constraints

### 3) Run deterministic pipeline
- [ ] Pipeline config committed
- [ ] Run produces artifacts into `data/processed/<another_domain>/<dataset_id>/v<version>/â€¦`
- [ ] Generate `run_manifest.json` + `validation_report.json`

### 4) Generate evidence stack artifacts
- [ ] STAC item/collection updated/created
- [ ] DCAT dataset/distributions updated/created
- [ ] PROV lineage generated/updated

### 5) Governance & safety
- [ ] classification propagated correctly (no â€œaccidental publicâ€)
- [ ] sensitive attributes handled (generalize/blur/aggregate if needed)

### 6) Open PR + pass CI gates
- [ ] PR contains processed artifacts + catalogs + provenance
- [ ] CI passes (policy + validation)
- [ ] Human review completed

---

## ğŸ“‹ Domain Inventory (Template)

> Keep this table updated (or auto-generate it from catalogs).

| Dataset ID | Title | Version | Formats | Temporal | License | Classification | Notes |
|---|---|---:|---|---|---|---|---|
| `kfm.ks.<another_domain>.example` | Example dataset | `v1.0.0` | GeoParquet, PMTiles | 1850â€“2025 | CC-BY-4.0 | public | demo row |

---

## ğŸ†˜ Troubleshooting (Common â€œWhy did CI fail?â€)

- âŒ **Missing license** â†’ add SPDX-like license value in DCAT and/or contract metadata  
- âŒ **STAC/DCAT/PROV mismatch** â†’ dataset_id/version must line up across all three  
- âŒ **Checksum mismatch** â†’ regenerate checksums after pipeline writes outputs  
- âŒ **Classification violation** â†’ outputs cannot be less restrictive than any input  
- âŒ **Orphan artifacts** â†’ every file must be referenced by metadata/catalogs  

---

## ğŸ“š Related Docs (in-repo pointers)

- ğŸ§­ `docs/architecture/â€¦` â€” system blueprint & evidence stack rules  
- ğŸ“¥ `docs/guides/pipelines/â€¦` â€” ingestion & pipeline conventions  
- ğŸ” `api/scripts/policy/README.md` â€” policy pack / governance checks  
- ğŸ—‚ï¸ `data/catalog/â€¦` â€” published STAC/DCAT  
- ğŸ§¬ `data/provenance/â€¦` â€” PROV records

---

## ğŸ§¾ Changelog (optional)

- **YYYY-MM-DD** â€” created `<another_domain>` processed domain README âœ…

