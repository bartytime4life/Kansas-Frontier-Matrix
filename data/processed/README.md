<!--
KANSAS FRONTIER MATRIX (KFM) â€” GOVERNED ARTIFACT
Path: data/processed/README.md
Zone: Processed (Publishable Truth)
-->

# ğŸ§ª data/processed/ â€” Processed Zone (Publishable Truth)

![KFM](https://img.shields.io/badge/KFM-governed-blue)
![Zone](https://img.shields.io/badge/zone-processed-2ea44f)
![Policy](https://img.shields.io/badge/policy-fail--closed-critical)

This directory contains **canonical, validated, provenance-linked** datasets that are allowed to power the KFM **API/UI/Stories/Focus Mode**.

> [!IMPORTANT]
> **Processed is the only publishable source of truth.**  
> Raw/work artifacts are *inputs* and *intermediates*â€”they are **not served directly** to users.

---

## âœ… What belongs here

- **Final, standardized outputs** produced by pipelines (repeatable + reproducible).
- Data that has passed **validation gates** (schema, geospatial, temporal, licensing, policy).
- Artifacts that have **checksums** and are fully represented in KFM catalogs (**DCAT/STAC/PROV**).

---

## ğŸš« What must NOT belong here

- â€œHand-editedâ€ one-off fixes that bypass pipelines.
- Any dataset with **unclear license**, missing attribution, or missing sensitivity classification.
- Sensitive/precise location data published without an approved **redaction/generalization** workflow and a distinct provenance chain.

> [!NOTE]
> If you discover an error: **fix the pipeline and publish a new version**. Donâ€™t mutate history.

---

## ğŸ§­ The KFM truth path (context)

```mermaid
flowchart LR
  RAW[Raw zone] --> WORK[Work zone]
  WORK --> PROC[Processed zone]
  PROC --> CATS[Catalogs: STAC / DCAT / PROV]
  CATS --> STORES[(Stores: object store / PostGIS / graph / search)]
  STORES --> API[Governed API + Policy]
  API --> UI[UI + Stories + Focus Mode]
```

---

## ğŸ“¦ Directory layout

KFM documentation has described both a **zone-first** and a **domain-first** layout. This README defines the **contract for the processed zone**, regardless of which layout you use.

### Option A â€” Zone-first (root processed)

```text
data/
â”œâ”€â”€ raw/                     # immutable source drops (read-only)
â”œâ”€â”€ work/                    # intermediate artifacts + QA outputs
â”œâ”€â”€ processed/               # âœ… THIS DIRECTORY
â”œâ”€â”€ stac/                    # STAC collections/items for published assets
â”œâ”€â”€ catalog/
â”‚   â””â”€â”€ dcat/                # DCAT dataset records (JSON-LD)
â””â”€â”€ prov/                    # PROV bundles (runs + lineage)
```

### Option B â€” Domain-first (processed per domain)

```text
data/
â””â”€â”€ <domain>/
    â”œâ”€â”€ raw/
    â”œâ”€â”€ work/
    â””â”€â”€ processed/           # same rules as this README
```

> [!TIP]
> If youâ€™re using Option B, copy this README into each domainâ€™s `processed/` folder (or link to it) to keep the contract consistent.

---

## ğŸ§¾ Dataset packaging contract (what â€œpublishableâ€ means)

Every dataset in `data/processed/` must be explainable and auditable without guesswork.

### Required artifacts (by policy)

| Artifact | Required | Where it lives | Purpose |
|---|---:|---|---|
| Processed data file(s) | âœ… | `data/processed/...` | The publishable, canonical output used by downstream components |
| Checksums (SHA-256) | âœ… | alongside artifacts | Integrity + reproducibility |
| DCAT dataset record | âœ… | `data/catalog/dcat/...` | License, attribution, coverage, distributions |
| PROV lineage bundle | âœ… | `data/prov/...` | Traceability: raw â†’ transforms â†’ processed |
| STAC collection/items | âœ…* | `data/stac/...` | Required for spatial assets (map/timeline) |
| Policy labels / sensitivity class | âœ… | (metadata / policy bundle) | Fail-closed access control + redaction rules |
| Validation report + run record | âœ… | typically `data/work/...` | Evidence that quality gates passed |

\* **STAC is required if the dataset is spatial** (raster/vector/assets).

---

## âœ… Promotion gate checklist (CI enforced)

A processed artifact is **not publishable** unless all gates can prove â€œallowâ€.

- [ ] **License present** and compatible with publication
- [ ] **Sensitivity classification present** (public / restricted / sensitive-location / etc.)
- [ ] **Schema checks pass** (types, required fields, constraints)
- [ ] **Geospatial checks pass** (geometry validity, bounds/extent sanity) *(if spatial)*
- [ ] **Temporal checks pass** (timestamps sane, ranges valid)
- [ ] **Checksums computed** (content integrity)
- [ ] **Catalog artifacts exist** and validate: DCAT (+ STAC if spatial + PROV always)
- [ ] **Cross-links resolve** (STAC â†” DCAT â†” PROV are navigable/resolvable)
- [ ] **Audit event recorded** (if your pipeline emits an audit ledger entry)
- [ ] **Human approval** required for sensitive datasets

> [!IMPORTANT]
> KFM is **fail-closed**: if policy/validation cannot prove it is safe and complete, promotion must be blocked.

---

## ğŸ” Sensitivity & redaction

Some data requires special handling (e.g., private ownership, precise archaeological sites, culturally restricted knowledge).

**Required pattern:**
1. Create a **generalized/redacted derivative** for broad audiences.
2. Store **precise data** under restricted access (policy-labeled) or in a restricted store (implementation-dependent).
3. Maintain **separate provenance chains** for the precise and generalized artifacts (including the redaction/generalization step).

> [!WARNING]
> Never â€œquietlyâ€ blur or drop sensitive fields without recording it in PROV. Redaction is a first-class transformation.

---

## ğŸ” Versioning rules (donâ€™t overwrite truth)

- Treat processed artifacts as **append-only by version**.
- When upstream changes or fixes are required:
  - publish a **new DatasetVersion**
  - keep the previous version for traceability and reproducibility

**Recommended on-disk pattern (example):**

```text
data/processed/
â””â”€â”€ <dataset_id>/
    â””â”€â”€ <version_id>/
        â”œâ”€â”€ artifact_1.parquet
        â”œâ”€â”€ artifact_2.geojson
        â”œâ”€â”€ checksums.sha256
        â””â”€â”€ README.md              # dataset-level notes (optional)
```

---

## ğŸ› ï¸ Adding a new dataset (thin-slice workflow)

1. **Intake**
   - Capture raw inputs (immutable) and record source, license, and retrieval details.
2. **Process**
   - Run pipeline to normalize/validate/enrich in the work zone.
3. **Validate**
   - Produce a validation report (schema/geo/time/profiling) and a run record.
4. **Publish**
   - Write outputs to `data/processed/` + compute checksums.
   - Emit/update catalogs: DCAT (always), STAC (if spatial), PROV (always).
5. **Promote via PR**
   - CI must run validators and block promotion if incomplete.
6. **Serve**
   - API/UI consume *catalogs* and *processed artifacts* through the policy boundary (trust membrane).

---

## ğŸ§¯ Common failure modes (and how to avoid them)

<details>
  <summary><strong>â€œWe fixed the data by hand; why is this a problem?â€</strong></summary>

Hand edits break reproducibility. KFM expects you to be able to re-run pipelines and regenerate the same processed outputs from raw inputs + pinned code. Fix the pipeline, then publish a new version.

</details>

<details>
  <summary><strong>â€œWhy do we need DCAT/STAC/PROV for everything?â€</strong></summary>

Because KFM treats datasets as governed artifacts: discovery (DCAT), geospatial browse/render (STAC), and lineage/auditability (PROV) are part of what makes KFM evidence-first and safe to serve.

</details>

<details>
  <summary><strong>â€œWhere do restricted datasets live?â€</strong></summary>

Implementation-dependent. Some teams keep restricted artifacts in the same processed zone but policy-label them strictly; others store restricted artifacts outside the repo/object store. Either way, publishable derivatives must be provenance-linked and policy-controlled.

</details>

---

## ğŸ”— Related (expected) references

> Paths may vary by repo version.

- `data/stac/` â€” STAC collections & items  
- `data/catalog/dcat/` â€” DCAT dataset records  
- `data/prov/` â€” PROV bundles  
- `src/pipelines/` â€” ETL + validation jobs  
- `schemas/` â€” JSON Schemas for catalogs, provenance, receipts, etc.

---

## âœ… Definition of Done (for a processed dataset PR)

- [ ] Processed artifacts added (no raw/work leaks)
- [ ] Checksums present and correct
- [ ] DCAT record present + validated
- [ ] STAC record present + validated (if spatial)
- [ ] PROV bundle present + validated
- [ ] Run record + validation report exist and reference outputs
- [ ] Sensitivity class + policy labels attached
- [ ] CI gates pass and promotion is â€œrealâ€