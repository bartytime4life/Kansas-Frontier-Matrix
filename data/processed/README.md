<div align="center">

# ğŸ“‚ Kansas Geo Timeline â€” Processed Data

This directory contains **derived, cleaned, and ready-for-use geospatial + historical datasets**.  

All files are **pipeline outputs** (ETL workflows, experiments, or transformations) and  
every artifact is referenced in the **STAC catalog** (`data/stac/items/`).  

[![Build & Deploy](https://github.com/bartytime4life/Kansas-Frontier-Matrix/actions/workflows/site.yml/badge.svg)](https://github.com/bartytime4life/Kansas-Frontier-Matrix/actions/workflows/site.yml)
[![STAC Validate](https://github.com/bartytime4life/Kansas-Frontier-Matrix/actions/workflows/stac-badges.yml/badge.svg)](https://github.com/bartytime4life/Kansas-Frontier-Matrix/actions/workflows/stac-badges.yml)
[![Pre-commit](https://github.com/bartytime4life/Kansas-Frontier-Matrix/actions/workflows/pre-commit.yml/badge.svg)](https://github.com/bartytime4life/Kansas-Frontier-Matrix/.pre-commit-config.yaml)

</div>

---

```mermaid
flowchart TD
  A["Raw data\n(data/raw/**)"] --> B["ETL / Cleaning\n(scripts, notebooks)"]
  B --> C["Processed outputs\n(data/processed/**)"]
  C --> D["Provenance sidecars\n(.sha256 Â· .meta.json)"]
  C --> E["STAC Items\n(data/stac/items/**)"]
  E --> F["Validate\n(stac-validate Â· schema)"]
  F --> G["Web viewer layers\n(web/config/**)"]

<!-- END OF MERMAID -->



â¸»

ğŸ§­ Principles
	â€¢	Immutable inputs, reproducible outputs
	â€¢	Raw data lives in data/raw/.
	â€¢	Outputs here must be reproducible from scripts + configs + GCPs.
	â€¢	Nothing in this directory is hand-edited.
	â€¢	STAC integration
	â€¢	Every file here links to a STAC Item in data/stac/items/.
	â€¢	Each item records: datetime, bbox, checksum, license, provenance.
	â€¢	Items are grouped into STAC Collections (data/stac/collections/).
	â€¢	Lightweight storage
	â€¢	Large rasters tracked with Git LFS or DVC.
	â€¢	Vectors & tables stored as GeoJSON, CSV, Parquet.
	â€¢	Holds only current, published outputs (not archives).
	â€¢	MCP reproducibility
	â€¢	Each dataset traces back to an experiment, config, or Makefile step.
	â€¢	Provenance sidecars (.sha256, .meta.json) required.
	â€¢	Outputs treated as experiment results with full lineage.

â¸»

ğŸ“‚ Typical contents

data/processed/
â”œâ”€â”€ towns_points.json          # Settlements (GeoJSON)
â”œâ”€â”€ ks_treaties.json           # Treaty boundaries (historic polygons)
â”œâ”€â”€ ks_railroads.json          # Railroad lines (historic)
â”œâ”€â”€ hydrology.json             # Rivers and waterbodies
â”œâ”€â”€ landcover_timeslices.json  # NLCD 1992â€“2021 snapshots
â”œâ”€â”€ dem/
â”‚   â”œâ”€â”€ ks_1m_hillshade.tif
â”‚   â”œâ”€â”€ ks_slope.tif
â”‚   â””â”€â”€ vectors/contours.json
â”œâ”€â”€ hydrology/                 # Subsets (Kansas River, floodplains)
â””â”€â”€ oral_histories.csv         # Oral history index (structured)

Formats
	â€¢	Vectors â†’ GeoJSON (*.json, *.geojson)
	â€¢	Tables â†’ CSV (*.csv), Parquet (*.parquet)
	â€¢	Rasters â†’ GeoTIFF/COG (*.tif)
	â€¢	Metadata â†’ JSON (*.meta.json)

Schemas align with web/config/layers.schema.json for seamless viewer integration.

â¸»

ğŸ”„ Workflow
	1.	Fetch raw data â†’ data/raw/

make fetch


	2.	Transform / clean â†’ scripts or notebooks (experiments/*/)
	â€¢	Reproject, clip to Kansas extent, normalize fields.
	3.	Save outputs â†’ data/processed/ in open formats.
	4.	Generate checksums

scripts/gen_sha256.sh data/processed/<file>


	5.	Update STAC Item â†’ data/stac/items/
	â€¢	Ensure href, checksum, and links are correct.
	6.	Validate â†’ schema + STAC

make stac-validate
pre-commit run --all-files



â¸»

ğŸ“‘ Example entries

Vector GeoJSON
	â€¢	File: data/processed/ks_treaties.json
	â€¢	STAC: data/stac/items/vectors/ks_treaties.json
	â€¢	Viewer: web/data/treaties.json

Raster COG
	â€¢	File: data/processed/dem/ks_1m_hillshade.tif
	â€¢	STAC: data/stac/items/topo/ks_1m_hillshade.json
	â€¢	Viewer: web/data/hillshade.json
	â€¢	Export: data/kml/ks_hillshade_2018_2020.kmz

â¸»

ğŸ“ Notes
	â€¢	âŒ Do not manually edit files here â€” regenerate via pipeline.
	â€¢	âœ… Document provenance (configs, scripts, GCPs).
	â€¢	ğŸ”— Keep filenames stable for references (STAC, Makefile, configs).
	â€¢	ğŸ—‚ï¸ Use collections (data/stac/collections/) to group datasets (e.g., treaties, DEM, landcover).
	â€¢	ğŸ“¦ Use make clean to clear rasters when rebuilding experiments.

â¸»

ğŸ“š See also
	â€¢	data/raw/ â€” raw acquisitions.
	â€¢	data/cogs/ â€” cloud-optimized mission-final rasters.
	â€¢	data/stac/ â€” STAC items & collections.
	â€¢	web/data/ â€” configs for the web viewer.
	â€¢	data/kml/ â€” KML/KMZ Earth exports.
	â€¢	experiments/ â€” MCP-style experiment logs.

â¸»

âœ… Mission-grade principle: Processed datasets must be consistent, versioned, STAC-compliant, and reproducible across pipelines, experiments, and web layers.

