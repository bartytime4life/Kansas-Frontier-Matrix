# ğŸ§ª `data/processed/` â€” Processed Data Zone

![Layer](https://img.shields.io/badge/data%20layer-processed-2b6cb0)
![Invariant](https://img.shields.io/badge/invariant-provenance%20required-16a34a)
![Invariant](https://img.shields.io/badge/invariant-deterministic%20outputs-16a34a)
![Policy](https://img.shields.io/badge/policy-no%20manual%20edits%20in%20place-f97316)
![Downstream](https://img.shields.io/badge/feeds-STAC%2FDCAT%2FPROV%20%E2%86%92%20stores%20%E2%86%92%20api%20%E2%86%92%20ui%2Fai-7c3aed)

Welcome to the **Processed** zone ğŸ­ â€” the **canonical cleaned data layer** of the Kansas Frontier Matrix truth path.

Processed exists so the system can rely on **trusted, standardized, reproducible artifacts** instead of one-off â€œfixedâ€ files.

**Goal:** turn disparate *raw* inputs into **analysis-ready, auditable, governable** artifacts that are:

- âœ… **reproducible** (same inputs + same pipeline + same config â†’ same outputs)
- âœ… **deterministic** (pipelines are non-interactive; no manual steps; stable outputs)
- âœ… **append-only** (no silent overwrites; versioned dataset bundles)
- âœ… **traceable** (â€œmap behind the mapâ€ â€” everything points back to sources)
- âœ… **ready to publish** (via required boundary artifacts: STAC/DCAT/PROV)
- âœ… **safe to serve** (only after governance gates; via the API layer)

---

## ğŸ§­ The Truth Path (Nonâ€‘Negotiable)

Processed is **not** the first step and not the last. KFM enforces staged outputs:

- **Raw**: immutable snapshots
- **Work**: intermediate transformations (regenerable)
- **Processed**: canonical outputs (authoritative for internal use)
- **Catalog + Lineage**: boundary artifacts required for publication (STAC/DCAT/PROV)
- **Runtime stores**: accelerators (regenerable from processed + metadata)
- **API**: the only access boundary (policy enforced)
- **UI/AI**: consumes only via API (evidence-first)

```mermaid
flowchart LR
  A[ğŸ“¥ data/raw/&lt;domain&gt;<br/>immutable source snapshots] --> W[ğŸ§ª data/work/&lt;domain&gt;<br/>intermediate transforms]
  W --> B[ğŸ­ data/processed/&lt;domain&gt;<br/>canonical cleaned artifacts]
  B --> C[ğŸ—‚ï¸ boundary artifacts<br/>data/stac Â· data/catalog/dcat Â· data/prov]
  C --> D[ğŸ—ƒï¸ runtime stores<br/>PostGIS Â· Neo4j Â· search/vector]
  D --> E[ğŸŒ API layer<br/>policy + redaction enforced]
  E --> F[ğŸ—ºï¸ UI & ğŸ¤– AI<br/>evidence-first outputs]
```

**No shortcuts.** Nothing jumps from raw/work/processed straight into databases, tiles, UI, or AI.

---

## âœ… What Belongs Here (and what doesnâ€™t)

### âœ… Belongs in `data/processed/`
Processed is for **canonical, standardized, validated outputs** that downstream systems can rely on:

- Cleaned + standardized **tables** (typed, normalized, unit-consistent)
- Spatially validated **vector layers** (valid geometry, declared CRS, stable IDs)
- Web-ready **raster assets** (e.g., COGs) produced from raw rasters
- Derived **analysis layers** (indices, classifications, change detection) with full lineage
- Companion **bundle artifacts** that make the dataset auditable (manifest/checksums/schema/QA)

### âŒ Does *not* belong in `data/processed/`
- Ad-hoc scratch files (use `data/work/â€¦` or `data/_tmp/â€¦`)
- Hand-edited â€œfinalâ€ datasets (**all changes must be in pipeline code/config**)
- Secrets, tokens, credentials ğŸ”’
- Unversioned overwrites (e.g., `latest.csv`) that destroy history

---

## ğŸ“ Canonical Folder Layout (Dataset Bundles)

KFMâ€™s staging convention is domain-scoped under the stage directory:

- `data/raw/<domain>/â€¦`
- `data/work/<domain>/â€¦`
- `data/processed/<domain>/â€¦`

Within `data/processed/<domain>/`, treat each dataset as a **versioned bundle**:

```text
data/processed/
  ğŸ“¦ <domain>/
    ğŸ“¦ <dataset_slug>/
      ğŸ·ï¸ <version>/                       # e.g., v1.0.0 or 2026-02-03
        ğŸ“„ README.md                      # optional: dataset notes + caveats
        ğŸ“„ manifest.json                  # required: authoritative inventory
        ğŸ“„ checksums.txt                  # required: sha256 for key artifacts
        ğŸ“„ schema.json                    # required for tabular; recommended for geo
        ğŸ“„ qa_report.json                 # required: validation summary + stats
        ğŸ“ data/
          ğŸ§¾ table.parquet
          ğŸ—ºï¸ layer.geoparquet
          ğŸ—ºï¸ layer.gpkg
          ğŸ›°ï¸ raster.cog.tif
          ğŸ§± tiles.pmtiles
        ğŸ“ meta/
          ğŸ§¾ fields.md                    # optional: human-friendly dictionary
          ğŸ—“ï¸ temporal.md                  # optional: time semantics + caveats
        ğŸ“ logs/
          ğŸ§° pipeline_run.log             # optional: keep when audit-useful
```

> **Important:** PROV/STAC/DCAT records are **not stored inside this bundle by default**.  
> They are **boundary artifacts** written to canonical locations (see below).

---

## ğŸ§± Bundle Contract (What a â€œProcessed Dataset Versionâ€ Must Contain)

### Required inside the processed bundle
| File | Required | Purpose |
|---|---:|---|
| `manifest.json` | âœ… | Authoritative inventory of artifacts + key stats (counts, hashes, bbox/time if applicable). |
| `checksums.txt` | âœ… | Quick integrity verification (sha256). |
| `qa_report.json` | âœ… | Proof itâ€™s valid: schema checks, null rates, duplicates, geometry/CRS checks, etc. |
| `schema.json` | âœ… (tabular) | Machine-readable contract for downstream loaders (DB/index/tiles). |

### Required boundary artifacts (outside `data/processed/`)
A dataset is **not publishable** until these exist:

| Artifact | Canonical location | Purpose |
|---|---|---|
| STAC Collection/Item(s) | `data/stac/collections/` + `data/stac/items/` | Spatial/temporal cataloging of assets (bbox/time/links/license/provider). |
| DCAT dataset entry | `data/catalog/dcat/` | Discovery + dataset-level metadata (title/description/license/distributions). |
| PROV lineage bundle | `data/prov/` | End-to-end lineage: raw â†’ work â†’ processed; activities; agents; timestamps; parameters. |

> These are the **publication boundary artifacts** and the interface to downstream stages (graph, API, UI).

---

## ğŸ§¾ Formats We Prefer (Interoperable + Web-Friendly)

### Tabular ğŸ“Š
- âœ… **Parquet** (preferred), optionally partitioned
- âœ… **CSV** only for *small/simple* exports (avoid for large/typed datasets)

### Vector ğŸ—ºï¸
- âœ… **GeoPackage (`.gpkg`)** for portable GIS interchange
- âœ… **GeoParquet** for analytics + fast IO
- âš ï¸ **GeoJSON** only for small layers

### Raster ğŸ›°ï¸
- âœ… **Cloud-Optimized GeoTIFF (COG)** for serving + partial reads
- âœ… **PMTiles / XYZ tile packages** when pre-tiling is required

---

## ğŸ·ï¸ Naming & Versioning Rules

### Dataset slugs
Keep dataset slugs:
- lowercase
- hyphenated
- stable over time

Example: `ks-counties`, `landsat-ndvi`, `historic-newspapers-index`

### Versions
Pick **one** strategy per dataset:
- `vMAJOR.MINOR.PATCH` for curated datasets
- `YYYY-MM-DD` for periodic refreshes
- `run_<timestamp>_<shortgitsha>` for experiments (avoid publishing)

ğŸš« Never overwrite an existing version folder.

---

## ğŸ” Determinism + â€œNo Manual Editsâ€ (Hard Rule)

Processed data is only trustworthy if itâ€™s re-creatable.

**Rules**
- No manual steps or interactive prompts in official pipelines.
- Pipelines must be idempotent where feasible (re-running shouldnâ€™t create duplicates if nothing changed).
- If outputs differ, they must be promoted as a **new version** with updated lineage.

> If you canâ€™t re-run it, itâ€™s not processed â€” itâ€™s a one-off.

---

## ğŸ§¬ Provenance & Publication Linkage

Processed bundles must be traceable to:
1) **Source** (raw snapshots and where they came from)
2) **Method** (pipeline name + version/commit)
3) **Parameters** (config used)
4) **Identity** (who/what ran it + when)
5) **Integrity** (hashes + counts + extents)

**Where provenance lives**
- Canonical lineage is recorded in **PROV** bundles under `data/prov/`.
- STAC/DCAT records must link to the processed artifacts and reference license/provider/source info.
- PROV must link raw â†’ work â†’ processed and identify the run/config/commit.

**Recommended: cross-link from the processed bundle**
In `manifest.json` (or a small `links.json`), include:
- `stac_items`: list of STAC item IDs/paths
- `stac_collection`: collection ID/path
- `dcat_entry`: DCAT JSON-LD path
- `prov_bundle`: PROV bundle path
This makes the processed bundle auditable *without* duplicating catalogs.

---

## âœ… QA / Validation Expectations

At minimum, every processed dataset version should have validation evidence in `qa_report.json`.

### General (all)
- âœ… schema validation (types, required fields)
- âœ… null-rate report per field
- âœ… duplicate key checks (if IDs exist)
- âœ… range checks for known numeric fields

### Spatial (if geometry exists)
- âœ… geometry validity
- âœ… CRS declared + consistent
- âœ… bounding box computed + stored
- âœ… topology rules documented or enforced (when relevant)

### Raster
- âœ… CRS/nodata/pixel size documented
- âœ… COG validity checks (tiling + overviews)
- âœ… stats/percentiles computed (optional but useful)

---

## ğŸ” How to Add or Refresh a Dataset (Governed Path)

1. **Ingest raw** into `data/raw/<domain>/...` ğŸ“¥  
2. **Transform in work** into `data/work/<domain>/...` ğŸ§ª  
3. **Write processed bundle** into `data/processed/<domain>/<dataset_slug>/<version>/...` ğŸ­  
4. **Generate bundle artifacts** (`manifest.json`, `schema.json`, `qa_report.json`, `checksums.txt`) ğŸ§¾  
5. **Generate boundary artifacts**:
   - STAC â†’ `data/stac/...`
   - DCAT â†’ `data/catalog/dcat/...`
   - PROV â†’ `data/prov/...`
6. **Pass policy + CI gates** (license + sensitivity + schema + provenance completeness) âœ…  
7. **Load runtime stores** via governed loaders ğŸ—ƒï¸  
8. **Serve only via API** (never direct DB access from UI/AI) ğŸŒ  

---

## ğŸ§¯ Common Mistakes (Avoid These)

- âŒ Editing a processed file manually â€œjust to fix one valueâ€
- âŒ Promoting outputs without STAC/DCAT/PROV boundary artifacts
- âŒ Missing license/sensitivity classification (policy will block publication)
- âŒ Missing schemas/QA evidence
- âŒ Publishing â€œlatestâ€ without pinning a version

---

## â“ FAQ

### â€œCan I store intermediate pipeline outputs here?â€
Prefer **no**. If itâ€™s intermediate or exploratory, it belongs in `data/work/â€¦` (or `data/_tmp/â€¦`).

### â€œDo we keep *all* processed versions forever?â€
Default is **yes** (append-only history). If storage becomes an issue, define an explicit retention policy per dataset.

### â€œAre runtime databases the source of truth?â€
No. Runtime stores are accelerators/caches built from processed + metadata. If needed, they can be regenerated from the truth path.

### â€œWhat about AI-generated artifacts?â€
Treat them as first-class datasets: store in processed, catalog in STAC/DCAT, and trace in PROV. They must still go through governed APIs.

---

## ğŸ§  Design Philosophy

Processed is where KFM becomes trustworthy:

- evidence-first âœ…  
- provenance-by-default âœ…  
- governed promotion âœ…  
- deterministic pipelines âœ…  
- fail-closed policy gates âœ…  

If youâ€™re unsure where something goes, follow the Truth Path diagram. ğŸ§­
