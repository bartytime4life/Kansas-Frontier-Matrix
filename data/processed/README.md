<div align="center">

# ğŸ“‚ Kansas-Frontier-Matrix â€” Processed Data  
`data/processed/`

[![Build & Deploy](https://github.com/bartytime4life/Kansas-Frontier-Matrix/actions/workflows/site.yml/badge.svg)](https://github.com/bartytime4life/Kansas-Frontier-Matrix/actions/workflows/site.yml)  
[![STAC Validate](https://github.com/bartytime4life/Kansas-Frontier-Matrix/actions/workflows/stac-validate.yml/badge.svg)](https://github.com/bartytime4life/Kansas-Frontier-Matrix/actions/workflows/stac-validate.yml)  
[![CodeQL](https://github.com/bartytime4life/Kansas-Frontier-Matrix/actions/workflows/codeql.yml/badge.svg)](https://github.com/bartytime4life/Kansas-Frontier-Matrix/actions/workflows/codeql.yml)  
[![Trivy](https://github.com/bartytime4life/Kansas-Frontier-Matrix/actions/workflows/trivy.yml/badge.svg)](https://github.com/bartytime4life/Kansas-Frontier-Matrix/actions/workflows/trivy.yml)

**Mission:** Hold **derived, cleaned, and ready-for-use geospatial + historical datasets**.  
All files here are **pipeline outputs** (ETL workflows, experiments, transformations)  
and every artifact is referenced in the **STAC catalog** (`data/stac/items/`).  

</div>

---

## ğŸ“ˆ Lifecycle

```mermaid
flowchart TD
  A["Raw data\n(data/raw/**)"] --> B["ETL / Cleaning\n(scripts, notebooks)"]
  B --> C["Processed outputs\n(data/processed/**)"]
  C --> D["Provenance sidecars\n(.sha256 Â· .meta.json)"]
  C --> E["STAC Items\n(data/stac/items/**)"]
  E --> F["Validate\n(make stac-validate Â· schema)"]
  F --> G["Web viewer layers\n(web/config/**)"]

<!-- END OF MERMAID -->



â¸»

ğŸ§­ Principles
	â€¢	Immutable inputs, reproducible outputs
	â€¢	Raw data lives in data/raw/
	â€¢	Outputs here must be reproducible from scripts + configs + GCPs
	â€¢	Nothing in this directory is hand-edited
	â€¢	STAC integration
	â€¢	Every file here links to a STAC Item in data/stac/items/
	â€¢	Each item records: datetime, bbox, checksum, license, provenance
	â€¢	Items are grouped into Collections (data/stac/collections/)
	â€¢	Lightweight storage
	â€¢	Large rasters tracked with Git LFS or DVC
	â€¢	Vectors & tables â†’ GeoJSON, CSV, Parquet
	â€¢	Holds only current, published outputs (not archives)
	â€¢	MCP reproducibility
	â€¢	Each dataset traces back to an experiment, config, or Makefile step
	â€¢	Provenance sidecars (.sha256, .meta.json) required
	â€¢	Outputs treated as experiment results with full lineage

â¸»

ğŸ“‚ Typical Contents

data/processed/
â”œâ”€â”€ towns_points.json          # Settlements (GeoJSON)
â”œâ”€â”€ ks_treaties.json           # Treaty boundaries (historic polygons)
â”œâ”€â”€ ks_railroads.json          # Railroad lines (historic)
â”œâ”€â”€ hydrology.json             # Rivers and waterbodies
â”œâ”€â”€ landcover_timeslices.json  # NLCD 1992â€“2021 snapshots
â”œâ”€â”€ dem/
â”‚   â”œâ”€â”€ ks_1m_hillshade.tif
â”‚   â”œâ”€â”€ ks_slope.tif
â”‚   â””â”€â”€ vectors/contours.json
â”œâ”€â”€ hydrology/                 # Subsets (Kansas River, floodplains)
â””â”€â”€ oral_histories.csv         # Oral history index (structured)

Formats:
	â€¢	Vectors â†’ GeoJSON (.json, .geojson)
	â€¢	Tables â†’ CSV (.csv), Parquet (.parquet)
	â€¢	Rasters â†’ GeoTIFF/COG (.tif)
	â€¢	Metadata â†’ JSON (.meta.json)

â¸»

ğŸ”„ Workflow
	1.	Fetch raw data â†’ data/raw/

make fetch


	2.	Transform / clean â†’ scripts or notebooks (experiments/*/)
	â€¢	Reproject, clip to Kansas extent, normalize fields
	3.	Save outputs â†’ data/processed/ in open formats
	4.	Generate checksums

scripts/gen_sha256.sh data/processed/<file>


	5.	Update STAC Item â†’ data/stac/items/
	â€¢	Ensure href, checksum, and links are correct
	6.	Validate â†’ schema + STAC

make stac-validate
pre-commit run --all-files



â¸»

ğŸ“‘ Example STAC Items

Vector (Treaty Boundaries, GeoJSON)

{
  "type": "Feature",
  "stac_version": "1.0.0",
  "id": "ks_treaties",
  "properties": {
    "title": "Kansas Treaty Boundaries",
    "description": "Historic treaty boundaries digitized for Kansas.",
    "datetime": "1854-01-01T00:00:00Z",
    "proj:epsg": 4326,
    "kfm:method": "Digitized from historic maps",
    "kfm:lineage": [
      "data/raw/treaties/treaty_maps_scan.tif",
      "data/processed/ks_treaties.json"
    ],
    "qa:status": "provisional"
  },
  "links": [
    {
      "rel": "collection",
      "href": "../../stac/collections/vectors.json"
    }
  ],
  "assets": {
    "geojson": {
      "href": "../../data/processed/ks_treaties.json",
      "title": "Treaty Boundaries (Kansas)",
      "type": "application/geo+json",
      "roles": ["data", "vector"]
    }
  }
}

Raster (DEM Hillshade, COG)

{
  "type": "Feature",
  "stac_version": "1.0.0",
  "id": "ks_1m_hillshade",
  "properties": {
    "title": "Kansas Hillshade (1m DEM)",
    "description": "Hillshade generated from Kansas 1m DEM mosaics.",
    "start_datetime": "2018-01-01T00:00:00Z",
    "end_datetime": "2020-12-31T23:59:59Z",
    "proj:epsg": 4326,
    "kfm:method": "GDAL hillshade",
    "kfm:lineage": [
      "data/cogs/dem/ks_1m_2018.tif",
      "data/cogs/dem/ks_1m_2020.tif"
    ],
    "qa:status": "verified"
  },
  "geometry": {
    "type": "Polygon",
    "coordinates": [[
      [-102.05, 36.99],
      [-102.05, 40.00],
      [-94.59, 40.00],
      [-94.59, 36.99],
      [-102.05, 36.99]
    ]]
  },
  "links": [
    {
      "rel": "collection",
      "href": "../../stac/collections/terrain.json"
    }
  ],
  "assets": {
    "cog": {
      "href": "../../data/processed/dem/ks_1m_hillshade.tif",
      "title": "Hillshade (Kansas DEM, 1m)",
      "type": "image/tiff; application=geotiff; profile=cloud-optimized",
      "roles": ["data", "raster"]
    }
  }
}


â¸»

ğŸ“ Notes
	â€¢	âŒ Do not manually edit files here â€” regenerate via pipeline
	â€¢	âœ… Document provenance (configs, scripts, GCPs)
	â€¢	ğŸ”— Keep filenames stable for references (STAC, Makefile, configs)
	â€¢	ğŸ—‚ï¸ Use collections (data/stac/collections/) to group datasets (treaties, DEM, landcover)
	â€¢	ğŸ“¦ Use make clean to clear rasters when rebuilding experiments

â¸»

ğŸ“š See Also
	â€¢	data/raw/ â†’ raw acquisitions
	â€¢	data/cogs/ â†’ cloud-optimized mission-final rasters
	â€¢	data/stac/ â†’ STAC items & collections
	â€¢	web/data/ â†’ configs for the web viewer
	â€¢	data/kml/ â†’ KML/KMZ Earth exports
	â€¢	experiments/ â†’ MCP-style experiment logs

â¸»

âœ… Mission Principle

Processed datasets must be consistent, versioned, STAC-compliant, and reproducible
across pipelines, experiments, and web layers.