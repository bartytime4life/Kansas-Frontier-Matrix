```markdown
---
title: "data/processed â€” Final Data Products"
path: "data/processed/README.md"
version: "v1.1.0"
last_updated: "2026-01-08"
status: "active"
doc_kind: "Data Runbook"
license: "mixed (see per-dataset manifests)"

# Protocol + contracts (KFM)
markdown_protocol_version: "1.0"
pipeline_contract_version: "v13"

# Governance (folder-level; per-dataset may override)
governance_ref: "docs/governance/ROOT_GOVERNANCE.md"
ethics_ref: "docs/governance/ETHICS.md"
sovereignty_ref: "docs/governance/SOVEREIGNTY.md"
security_ref: "SECURITY.md"
fair_category: "FAIR+CARE"
care_label: "mixed"
sensitivity: "mixed"
classification: "mixed"
jurisdiction: "US"

# Integrity
doc_uuid: "urn:kfm:doc:data:processed:readme:v1.1.0"
commit_sha: "TBD"
doc_integrity_checksum: "sha256:TBD"
---

<div align="center">

# ğŸ“¦ `data/processed/` â€” Final Data Products (KFM)

![stage](https://img.shields.io/badge/data%20stage-processed-success)
![metadata](https://img.shields.io/badge/metadata-STAC%20%7C%20DCAT%20%7C%20PROV-important)
![pipeline](https://img.shields.io/badge/pipeline-deterministic%20%26%20idempotent-informational)
![governance](https://img.shields.io/badge/governance-FAIR%20%2B%20CARE-blueviolet)
![integrity](https://img.shields.io/badge/integrity-manifests%20%2B%20checksums-purple)

_This folder holds **curated, ready-to-serve outputs** produced by **config-driven pipelines** â€” not scratch work._ ğŸ—ºï¸âš™ï¸

</div>

> [!IMPORTANT]
> In KFM, **processed** means **final** outputs that are intended to be reused, referenced, and served.
> A dataset is only considered **published** once the **boundary artifacts** exist:
> **STAC** + **DCAT** + **PROV** (and the dataset passes validation).

> [!CAUTION]
> **No secrets, credentials, private keys, PII, or restricted coordinates** belong in `data/processed/` (or anywhere in Git history).
> If you suspect sensitive exposure, follow `SECURITY.md` (private reporting).

---

## âš¡ Quick links

- ğŸ“¥ Raw inputs (if present) â†’ [`../raw/`](../raw/)
- ğŸ§ª Work / intermediate (if present) â†’ [`../work/`](../work/)
- ğŸ›°ï¸ STAC catalogs â†’ [`../stac/collections/`](../stac/collections/) Â· [`../stac/items/`](../stac/items/)
- ğŸ—‚ï¸ DCAT discovery metadata â†’ [`../catalog/dcat/`](../catalog/dcat/)
- ğŸ§¬ PROV lineage bundles â†’ [`../prov/`](../prov/)
- ğŸ•¸ï¸ Graph import artifacts (if used) â†’ [`../graph/`](../graph/)
- ğŸ§° Validation tooling (if present) â†’ [`../../tools/validation/`](../../tools/validation/)
- ğŸ§¾ External source manifests (recommended) â†’ [`../sources/`](../sources/)

---

<details>
<summary><strong>ğŸ§­ Table of contents</strong></summary>

- [ğŸ¯ What belongs here](#-what-belongs-here)
- [ğŸ§± Folder contract](#-folder-contract)
- [ğŸ” Lifecycle and canonical pipeline](#-lifecycle-and-canonical-pipeline)
- [ğŸ“ Expected layout](#-expected-layout)
- [ğŸ“¦ Publication bundle per dataset](#-publication-bundle-per-dataset)
- [ğŸ§¾ Manifests and checksums](#-manifests-and-checksums)
- [ğŸ—ºï¸ Format guidance (raster â€¢ vector â€¢ tables â€¢ tiles)](#ï¸-format-guidance-raster--vector--tables--tiles)
- [ğŸ§ª Validation and CI gates](#-validation-and-ci-gates)
- [ğŸ§  ML, analytics, and simulation outputs](#-ml-analytics-and-simulation-outputs)
- [ğŸ” Privacy, sensitivity, and CARE](#-privacy-sensitivity-and-care)
- [ğŸ§³ Large files and external storage](#-large-files-and-external-storage)
- [â• Add a new processed dataset](#-add-a-new-processed-dataset)
- [ğŸ“š Project reference shelf](#-project-reference-shelf)
- [ğŸ•°ï¸ Version history](#ï¸-version-history)
- [âœ… Definition of Done](#-definition-of-done)

</details>

---

## ğŸ¯ What belongs here

**`data/processed/` contains final data products** that are ready to be:

- queried (Parquet/CSV/DB exports),
- mapped (COGs, GeoJSON/GeoPackage, tiles),
- indexed into catalogs (STAC/DCAT) and lineage (PROV),
- referenced by the graph and served via the API â†’ UI.

âœ… Typical â€œprocessedâ€ artifacts youâ€™ll see in KFM:
- ğŸ›°ï¸ **Raster layers**: georeferenced, web-ready rasters (often GeoTIFF/COG), plus optional tile pyramids
- ğŸ§© **Vector layers**: cleaned boundaries/routes/places as GeoJSON/GeoPackage (and/or DB extracts)
- ğŸ—ƒï¸ **Tabular products**: curated tables (Parquet/CSV) with stable schemas
- ğŸ“š **Text corpora**: OCR outputs, extracted entities, structured JSON/Parquet for downstream linking
- ğŸ§  **Evidence artifacts**: model outputs, derived indices, simulations (treated like any other dataset)

ğŸš« Not for this folder:
- raw downloads / source dumps â†’ `data/raw/<domain>/â€¦`
- intermediate joins / scratch outputs â†’ `data/work/<domain>/â€¦`
- â€œmystery filesâ€ with no manifest, no provenance, no license/terms

---

## ğŸ§± Folder contract

This folder is intentionally opinionated. Every **processed dataset** should be:

1. **Deterministic**: same inputs + config + code revision â‡’ same outputs (as practical).
2. **Idempotent**: re-running does not duplicate, drift silently, or mutate history without a new version/run ID.
3. **Traceable**: every artifact has a manifest + checksums, and links to STAC/DCAT/PROV.
4. **Governed**: classification / sensitivity / CARE label are explicit and preserved end-to-end.
5. **Reviewable**: structure is consistent so diffs and reviews are possible.

> [!TIP]
> If you canâ€™t explain the lineage, itâ€™s not â€œprocessedâ€ â€” itâ€™s just a file.

---

## ğŸ” Lifecycle and canonical pipeline

KFMâ€™s data lifecycle is staged (domain-scoped):

- `data/raw/<domain>/` â†’ ingest as-is (or store pointers via `data/sources/`)
- `data/work/<domain>/` â†’ intermediate transforms / joins / scratch
- `data/processed/<domain>/` â†’ final outputs (this folder)

At publication time, the dataset emits boundary artifacts:

```mermaid
flowchart LR
  RAW["ğŸ“¥ data/raw/<domain>\n(as-ingested)"] --> WORK["ğŸ§ª data/work/<domain>\n(intermediate)"]
  WORK --> PROC["ğŸ“¦ data/processed/<domain>\n(final products)"]

  PROC --> STAC["ğŸ›°ï¸ data/stac/\n(items + collections)"]
  PROC --> DCAT["ğŸ—‚ï¸ data/catalog/dcat/\n(discovery datasets)"]
  PROC --> PROV["ğŸ§¬ data/prov/\n(lineage bundles)"]

  STAC --> GRAPH["ğŸ•¸ï¸ Graph (references back)"]
  DCAT --> GRAPH
  PROV --> GRAPH

  GRAPH --> API["ğŸ”Œ API boundary\n(contracts + redaction)"]
  API --> UI["ğŸ—ºï¸ UI / Apps\n(MapLibre/Leaflet/etc.)"]
  UI --> STORY["ğŸ“– Story Nodes"]
  STORY --> FOCUS["ğŸ¯ Focus Mode\n(provenance-linked only)"]
```

---

## ğŸ“ Expected layout

> Keep the top-level tidy: **domain â†’ dataset â†’ version/run**.

```text
ğŸ“ data/
â””â”€â”€ ğŸ“ processed/
    â”œâ”€â”€ ğŸ“„ README.md                      ğŸ‘ˆ you are here
    â””â”€â”€ ğŸ“ <domain>/
        â””â”€â”€ ğŸ“ <dataset_slug>/
            â”œâ”€â”€ ğŸ“„ README.md              # dataset card (human context)
            â””â”€â”€ ğŸ“ <version_or_run_id>/
                â”œâ”€â”€ ğŸ“„ MANIFEST.json      # required
                â”œâ”€â”€ ğŸ“„ checksums.sha256   # required
                â”œâ”€â”€ ğŸ“„ QA_REPORT.md       # recommended
                â”œâ”€â”€ ğŸ“„ dataset.schema.json# recommended (tabular)
                â”œâ”€â”€ ğŸ—ºï¸ vectors.*          # geo outputs (GeoJSON/GPKG/etc.)
                â”œâ”€â”€ ğŸ›°ï¸ rasters.*          # raster outputs (COG/GeoTIFF/etc.)
                â””â”€â”€ ğŸ–¼ï¸ quicklook.*        # preview (png/jpg)
```

> [!NOTE]
> If outputs are too large for Git:
> store the artifacts externally and keep **pointers + checksums + manifests** here.

---

## ğŸ“¦ Publication bundle per dataset

A processed dataset is **ready to publish** when it has:

| What | Required | Where it lives | Why it exists |
|---|:---:|---|---|
| Processed artifacts | âœ… | `data/processed/<domain>/<dataset>/<run>/` | Actual deliverables |
| Manifest | âœ… | `MANIFEST.json` | Repro + governance + pointers |
| Checksums | âœ… | `checksums.sha256` | Integrity + drift detection |
| STAC | âœ… | `data/stac/collections/` + `data/stac/items/` | Spatial/temporal indexing + assets |
| DCAT | âœ… | `data/catalog/dcat/` | Cross-domain discovery + harvesting |
| PROV | âœ… | `data/prov/` | Lineage (inputs â†’ activity â†’ outputs) |
| Validation outputs | âš ï¸ recommended | `QA_REPORT.md` or `qa.json` | Debuggable CI + reviewer confidence |

---

## ğŸ§¾ Manifests and checksums

### âœ… `MANIFEST.json` (required)

A manifest is a compact â€œhow to trust and reproduce this datasetâ€ record.

<details>
<summary><strong>ğŸ“„ Minimal MANIFEST shape (starter)</strong></summary>

```json
{
  "dataset_id": "kfm.<domain>.<dataset_slug>",
  "domain": "<domain>",
  "dataset_slug": "<dataset_slug>",
  "version_or_run_id": "<yyyy-mm-dd>_v1",
  "produced_at": "2026-01-08T00:00:00Z",

  "license": {
    "spdx": "TBD",
    "source_terms_url": "TBD"
  },

  "governance": {
    "classification": "mixed",
    "sensitivity": "mixed",
    "care_label": "mixed",
    "jurisdiction": "US"
  },

  "inputs": [
    {
      "source_manifest": "data/sources/<source>.json",
      "retrieved_at": "TBD",
      "hash_or_etag": "TBD"
    }
  ],

  "pipeline": {
    "name": "<pipeline_name>",
    "commit_sha": "TBD",
    "config_files": ["<path/to/config.yml>"],
    "container_image": "TBD",
    "runtime_versions": {
      "python": "3.12",
      "gdal": "TBD",
      "postgres": "TBD"
    },
    "random_seed": "TBD"
  },

  "spatial": {
    "crs": "EPSG:4326",
    "bbox": [-102.05, 36.99, -94.59, 40.00],
    "resolution": "TBD"
  },

  "temporal": {
    "start": "TBD",
    "end": "TBD"
  },

  "outputs": [
    { "path": "rasters/ndvi_cog.tif", "sha256": "TBD" },
    { "path": "vectors/places.geojson", "sha256": "TBD" }
  ],

  "catalog_links": {
    "stac_collection_id": "TBD",
    "stac_item_ids": ["TBD"],
    "dcat_dataset_id": "TBD",
    "prov_bundle_id": "TBD"
  },

  "notes": "TBD"
}
```

</details>

> [!TIP]
> Keep `MANIFEST.json` **small and boring**. The big stuff lives in STAC/DCAT/PROV; the manifest is the â€œjoin keyâ€ glue.

### âœ… `checksums.sha256` (required)

A single checksum file protects against accidental drift and makes review verifiable.

Example:

```text
<sha256>  rasters/ndvi_cog.tif
<sha256>  vectors/places.geojson
<sha256>  QA_REPORT.md
<sha256>  MANIFEST.json
```

---

## ğŸ—ºï¸ Format guidance (raster â€¢ vector â€¢ tables â€¢ tiles)

KFMâ€™s system design explicitly expects ingestion pipelines to normalize into standard geospatial formats, including COGs for rasters and GeoJSON/shapefiles for vectors, with tiles generated when needed for interactive use. âœ…

### Recommended â€œdefault formatsâ€ (practical, not dogma)

| Output type | Preferred formats | Why |
|---|---|---|
| Raster layers | **COG GeoTIFF** (+ overviews) | Fast HTTP range reads; map-friendly; works well with tiling |
| Vector layers | **GeoJSON** (web), **GeoPackage** (exchange), **GeoParquet** (scale) | Interop + performance |
| Tabular products | **Parquet** (scale), **CSV** (small) | Schema stability + analytics |
| Tiles | Vector tiles (MVT), raster tiles | UI performance, mobile/low bandwidth |
| Previews | PNG/JPEG quicklooks | Human review; docs/UI thumbnails |
| Text corpora | JSON/JSONL/Parquet | Search + extraction pipelines |
| Model artifacts | metrics JSON, model cards, serialized model (if allowed) | Reproducibility + governance |

> [!NOTE]
> For Google Earth Engine exports (or similar), capture **export metadata** (parameters, bands, time period) and **pyramiding policy** as part of the run record so the same asset can be regenerated consistently.

---

## ğŸ§ª Validation and CI gates

### What CI should enforce for `data/processed/**`

Minimum baseline (recommended):

- âœ… `MANIFEST.json` exists and is valid JSON
- âœ… `checksums.sha256` exists and matches the referenced files
- âœ… STAC/DCAT/PROV links in the manifest resolve (or are clearly marked `TBD` on draft PRs)
- âœ… Geospatial sanity checks:
  - CRS declared and consistent
  - bbox/geometry valid
  - raster has nodata + overviews (if applicable)
- âœ… Governance checks:
  - license present (or a documented exception)
  - classification/sensitivity not â€œdowngradedâ€ by accident
  - no restricted coordinates leaked
- âœ… Security checks:
  - no secrets, tokens, credentials, private keys
  - no plaintext DB connection strings in committed artifacts

> [!TIP]
> If your repo includes `tools/validation/catalog_qa/`, run it as an early warning system for broken links, missing required metadata, and license mismatches.

### Local quick checks (examples)

```bash
# JSON parse sanity
python -m json.tool data/processed/<domain>/<dataset>/<run>/MANIFEST.json > /dev/null

# Validate checksums (example; adjust path)
(cd data/processed/<domain>/<dataset>/<run> && sha256sum -c checksums.sha256)

# Optional: validate a raster quickly (if gdalinfo exists)
gdalinfo -stats data/processed/<domain>/<dataset>/<run>/rasters/*.tif | head

# Optional: validate GeoJSON quickly
python -c "import json; json.load(open('data/processed/<domain>/<dataset>/<run>/vectors/*.geojson'))"
```

> [!CAUTION]
> Avoid hard-coding credentials in scripts or notebooks (even for examples). Prefer env vars and `.env` excluded via `.gitignore`.

---

## ğŸ§  ML, analytics, and simulation outputs

KFM treats analysis outputs (â€œevidence artifactsâ€) as **first-class datasets**:

- store outputs in `data/processed/â€¦`
- catalog them (STAC/DCAT)
- trace them (PROV)
- make them explainable (metrics, configs, seeds, limitations)

### Recommended evidence bundle (add to the run folder)

- `METRICS.json` (core metrics + confidence intervals if available)
- `MODEL_CARD.md` (purpose, training data versions, limitations, intended use)
- `FEATURES.md` (feature list + engineering notes)
- `SPLITS.json` (train/val/test identifiers) or a deterministic split rule
- `SEED.txt` (or manifest field) for determinism
- `BIAS_CHECKS.md` (when outputs can affect people/communities or sensitive interpretation)

### Simulation outputs (when applicable)

If you generate simulations (climate surfaces, counterfactuals, or other modeled layers):
- include a short **verification + validation** note (what was checked, what was not)
- include **sensitivity analysis** summary (what parameters matter)
- include uncertainty summaries (intervals, ensembles, or qualitative limits)

---

## ğŸ” Privacy, sensitivity, and CARE

### 1) Sensitive locations and sovereignty
If something is protected (sacred sites, community-protected places, restricted infrastructure):
- **generalize geometry** (coarse bbox, blurred points, aggregated zones),
- label the handling requirements (`care_label`, `classification`, `sensitivity`),
- route publication through review (data steward / governance review).

### 2) Processed outputs can still leak
Even if raw data is private, **derived outputs** (models, aggregates, mined patterns) can disclose sensitive information.
When publishing high-risk outputs, consider:
- redaction/aggregation at the API layer,
- query auditing/inference controls (where applicable),
- avoiding â€œtoo granular to be safeâ€ exports.

> [!IMPORTANT]
> Treat â€œprivacyâ€ as an output property, not just an input property.

---

## ğŸ§³ Large files and external storage

This repo may intentionally avoid committing massive binaries.

Recommended pattern:
- keep **source manifests** in `data/sources/` (URLs, licenses, retrieval date, checksums/ETags)
- keep **processed pointers + checksums + manifests** in `data/processed/`
- store large artifacts in object storage (or DVC, if adopted)

Rule of thumb:
> Git holds **contracts + metadata + lineage + pointers**.  
> Object storage holds **the heavy bytes**.

---

## â• Add a new processed dataset

### Checklist (fast lane)

- [ ] Create/confirm a stable `dataset_id` and `dataset_slug`
- [ ] Put raw inputs in `data/raw/<domain>/â€¦` (or create `data/sources/<source>.json` pointers)
- [ ] Generate intermediates in `data/work/<domain>/â€¦`
- [ ] Write final artifacts into `data/processed/<domain>/<dataset>/<run>/â€¦`
- [ ] Create `MANIFEST.json` and `checksums.sha256`
- [ ] Emit STAC items + collection in `data/stac/â€¦`
- [ ] Emit DCAT dataset entry in `data/catalog/dcat/â€¦`
- [ ] Emit PROV bundle in `data/prov/â€¦`
- [ ] Run validators (catalog QA, geometry/raster sanity checks)
- [ ] Confirm governance labels (license + sensitivity + CARE label)
- [ ] Confirm no secrets/PII/restricted coordinates are committed

---

## ğŸ“š Project reference shelf

These files shaped the conventions in this runbook (architecture, metadata contracts, validation discipline, geospatial formats, modeling integrity, security, ethics).

> Paths below assume a common convention:
> - specs in `docs/specs/`
> - reference PDFs in `docs/library/`
>
> If your repo stores them elsewhere, update links accordingly.

<details>
<summary><strong>ğŸ—ï¸ Core KFM specs</strong></summary>

- `docs/specs/MARKDOWN_GUIDE_v13.md` (or `MARKDOWN_GUIDE_v13.md.gdoc`)
- `docs/specs/Kansas Frontier Matrix (KFM) â€“ Comprehensive Technical Documentation.docx`
- `docs/library/Kansas-Frontier-Matrix_ Open-Source Geospatial Historical Mapping Hub Design.pdf`
- `docs/library/Comprehensive Markdown Guide_ Syntax, Extensions, and Best Practices.docx`

</details>

<details>
<summary><strong>ğŸŒ GIS, mapping, cartography, and UI constraints</strong></summary>

- `docs/library/python-geospatial-analysis-cookbook.pdf`
- `docs/library/making-maps-a-visual-guide-to-map-design-for-gis.pdf`
- `docs/library/Mobile Mapping_ Space, Cartography and the Digital - 9789048535217.pdf`
- `docs/library/responsive-web-design-with-html5-and-css3.pdf`
- `docs/library/webgl-programming-guide-interactive-3d-graphics-programming-with-webgl.pdf`
- `docs/library/compressed-image-file-formats-jpeg-png-gif-xbm-bmp.pdf`

</details>

<details>
<summary><strong>ğŸ›°ï¸ Remote sensing and Earth Engine discipline</strong></summary>

- `docs/library/Cloud-Based Remote Sensing with Google Earth Engine-Fundamentals and Applications.pdf`

</details>

<details>
<summary><strong>ğŸ—„ï¸ Data systems and scale</strong></summary>

- `docs/library/PostgreSQL Notes for Professionals - PostgreSQLNotesForProfessionals.pdf`
- `docs/library/Scalable Data Management for Future Hardware.pdf`
- `docs/library/Data Spaces.pdf`

</details>

<details>
<summary><strong>ğŸ“Š Statistics, experiments, modeling, and uncertainty</strong></summary>

- `docs/library/Understanding Statistics & Experimental Design.pdf`
- `docs/library/regression-analysis-with-python.pdf`
- `docs/library/graphical-data-analysis-with-r.pdf`
- `docs/library/think-bayes-bayesian-statistics-in-python.pdf`
- `docs/library/Scientific Modeling and Simulation_ A Comprehensive NASA-Grade Guide.pdf`
- `docs/library/topology-optimization-theory-methods-and-applications.pdf`
- `docs/library/spectral-geometry-of-networks.pdf`

</details>

<details>
<summary><strong>ğŸ” Security, privacy, and defensive thinking</strong></summary>

- `docs/library/ethical-hacking-and-countermeasures-secure-network-infrastructures.pdf`
- `docs/library/Gray Hat Python - Python Programming for Hackers and Reverse Engineers (2009).pdf`
- `docs/library/Data Mining Concepts & applictions.pdf`

</details>

<details>
<summary><strong>âš–ï¸ Ethics, autonomy, and law</strong></summary>

- `docs/library/An_Introduction_to_Digital_Humanism.pdf`
- `docs/library/principles_of_Biological_Autonomy.pdf`
- `docs/library/ai_law_and_regulation_path.pdf`

</details>

<details>
<summary><strong>ğŸ¤– AI quality, bias, and research practice</strong></summary>

- `docs/library/BIAS_TESTING_LLMs.pdf`
- `docs/library/bubeck2025.pdf`
- `docs/library/Deep Learning for Coders with fastai and PyTorch.pdf` (if present)

</details>

<details>
<summary><strong>ğŸ“š Engineering â€œtoolbeltâ€ bundles (reference-only)</strong></summary>

- `docs/library/D-E programming Books.pdf`
- `docs/library/F-H programming Books.pdf`
- `docs/library/I-N programming Books.pdf`

</details>

---

## ğŸ•°ï¸ Version history

| Version | Date | Change |
|---|---|---|
| v1.0.0 | 2026-01-02 | Initial runbook scaffold |
| v1.1.0 | 2026-01-08 | Tighten processed-vs-published contract; add manifest/checksum standards; align to STAC/DCAT/PROV + validation + CARE guidance |

---

## âœ… Definition of Done

- [x] Front-matter present and updated
- [x] Clear definition of â€œprocessedâ€ vs â€œpublishedâ€
- [x] Canonical lifecycle and pipeline ordering documented
- [x] Expected folder layout + publication bundle defined
- [x] Manifest + checksum requirements included
- [x] Validation expectations stated (local + CI)
- [x] CARE / sensitivity guidance included
- [ ] Links verified in-repo (fix any path drift)
- [ ] Reviewed by a data steward / maintainer

> [!TIP]
> To fill `doc_integrity_checksum`, compute a sha256 of the final Markdown file content (tooling choice is yours).
```