<div align="center">

# üßÆ Kansas-Frontier-Matrix ‚Äî Processed Data (`data/processed/`)

**Mission:** Hold all **cleaned, transformed, and analysis-ready datasets** generated by the ETL pipeline ‚Äî  
the bridge between raw data ingestion (`data/raw/`) and derived geospatial or analytical products (`data/derivatives/`).

[![Build & Deploy](https://github.com/bartytime4life/Kansas-Frontier-Matrix/actions/workflows/site.yml/badge.svg)](../../.github/workflows/site.yml)
[![STAC Validate](https://github.com/bartytime4life/Kansas-Frontier-Matrix/actions/workflows/stac-validate.yml/badge.svg)](../../.github/workflows/stac-validate.yml)
[![CodeQL](https://github.com/bartytime4life/Kansas-Frontier-Matrix/actions/workflows/codeql.yml/badge.svg)](../../.github/workflows/codeql.yml)
[![Trivy](https://github.com/bartytime4life/Kansas-Frontier-Matrix/actions/workflows/trivy.yml/badge.svg)](../../.github/workflows/trivy.yml)
[![Pre-Commit](https://github.com/bartytime4life/Kansas-Frontier-Matrix/actions/workflows/pre-commit.yml/badge.svg)](../../.github/workflows/pre-commit.yml)
[![Docs ¬∑ MCP](https://img.shields.io/badge/Docs-MCP-blue)](../../docs/)
[![License: Data](https://img.shields.io/badge/License-CC--BY%204.0-green)](../../LICENSE)
[![License: Code](https://img.shields.io/badge/License-MIT-yellow)](../../LICENSE)

</div>

---

## üìö Table of Contents
- [Overview](#overview)
- [Directory Layout](#directory-layout)
- [Processing Standards](#processing-standards)
- [STAC Catalog Integration](#stac-catalog-integration)
- [Provenance & Versioning](#provenance--versioning)
- [Adding New Processed Data](#adding-new-processed-data)
- [References](#references)

---

## üß≠ Overview

The `data/processed/` directory contains **post-ETL datasets** that have been standardized, cleaned,  
and reprojected for consistent downstream analysis.  
These files are **intermediate outputs** ‚Äî ready for visualization, modeling, or derivative generation,  
but not raw or ephemeral.

Typical contents:
- Normalized spatial data (GeoJSON, COG GeoTIFF)
- Joined tabular data (CSV, Parquet)
- Validated text or temporal datasets
- Feature-extracted AI/NLP outputs (entity tables, location mentions, summaries)

Each dataset here is versioned, traceable, and reproducible using the project‚Äôs  
**Master Coder Protocol (MCP)** and **SpatioTemporal Asset Catalog (STAC)** documentation standards.

---

## üóÇÔ∏è Directory Layout

```bash
data/
‚îî‚îÄ‚îÄ processed/
    ‚îú‚îÄ‚îÄ terrain/                 # Clean DEMs, slope-ready rasters
    ‚îú‚îÄ‚îÄ hydrology/               # Flow accumulation, sink-filled DEMs
    ‚îú‚îÄ‚îÄ landcover/               # Pre-classified NDVI/landcover rasters
    ‚îú‚îÄ‚îÄ climate/                 # Aggregated temperature/precipitation grids
    ‚îú‚îÄ‚îÄ hazards/                 # Clean event frequency rasters, SPI datasets
    ‚îú‚îÄ‚îÄ tabular/                 # Processed CSV/Parquet data tables
    ‚îú‚îÄ‚îÄ text/                    # Parsed/cleaned NLP text corpora
    ‚îú‚îÄ‚îÄ metadata/                # JSON metadata for all processed datasets
    ‚îú‚îÄ‚îÄ checksums/               # SHA256 hashes for all processed files
    ‚îî‚îÄ‚îÄ README.md
````

Each subfolder mirrors the **derivatives directory structure**, maintaining
a clean lineage chain between raw, processed, and derived products.

---

## ‚öôÔ∏è Processing Standards

All data in this folder adheres to the following conventions:

| Type    | Format        | CRS / Encoding    | Description                               |
| ------- | ------------- | ----------------- | ----------------------------------------- |
| Raster  | GeoTIFF (COG) | EPSG:4326 (WGS84) | Reprojected, tiled, and cloud-optimized   |
| Vector  | GeoJSON       | EPSG:4326         | Simplified geometries, topology validated |
| Tabular | CSV / Parquet | UTF-8             | Cleaned, standardized column names        |
| Text    | JSON / TXT    | UTF-8             | Extracted from OCR or NLP pipelines       |

Processing pipelines perform:

* **Reprojection:** All geospatial data normalized to WGS84.
* **Compression:** Using LZW/Deflate for efficient web serving.
* **Validation:** Schema checks, missing value audits, and geometry repair.
* **Hashing:** `.sha256` checksums for provenance tracking.

---

## üåê STAC Catalog Integration

Each processed dataset is indexed in the project‚Äôs [STAC catalog](../stac/).
Metadata entries under `data/stac/items/processed_*` describe:

* `id`: dataset identifier
* `datetime`: processing date
* `derived_from`: reference to raw source(s)
* `processing:software`: e.g. GDAL, Python ETL, spaCy, rasterio
* `mcp_provenance`: checksum and lineage tracking hash

Example snippet:

```json
{
  "id": "precipitation_processed_2024",
  "type": "Feature",
  "properties": {
    "title": "Precipitation (Processed) 2024",
    "datetime": "2024-06-01T00:00:00Z",
    "processing:software": "Python + xarray + rasterio",
    "derived_from": ["data/raw/noaa_precip_2024.csv"],
    "mcp_provenance": "sha256:e43f91..."
  },
  "assets": {
    "data": {
      "href": "./climate/precipitation_2024.tif",
      "type": "image/tiff; application=geotiff; profile=cloud-optimized"
    }
  }
}
```

---

## üß¨ Provenance & Versioning

* **Checksums:** Every processed dataset has a matching `.sha256` file.
* **Version Control:** Only lightweight processed data is committed; large artifacts tracked via DVC or Git LFS.
* **Metadata:** JSON metadata files under `metadata/` store lineage and process parameters.
* **MCP Documentation:** Every dataset build step is logged in the pipeline‚Äôs provenance chain.

All changes are validated automatically in CI (see `.github/workflows/stac-validate.yml`).

---

## ‚ûï Adding New Processed Data

1. Place new output in the correct subfolder (e.g., `climate/` or `hydrology/`).
2. Generate a `.sha256` checksum file using:

   ```bash
   sha256sum my_dataset.tif > checksums/my_dataset.tif.sha256
   ```
3. Create a STAC metadata JSON file under `metadata/`.
4. Validate with:

   ```bash
   make validate-processed
   ```
5. Submit via Pull Request with:

   * Source reference
   * Processing description
   * License details

All PRs are verified by GitHub Actions for metadata and integrity.

---

## üìñ References

* **STAC Spec 1.0:** [https://stacspec.org](https://stacspec.org)
* **GeoJSON Standard:** [https://tools.ietf.org/html/rfc7946](https://tools.ietf.org/html/rfc7946)
* **COG Format:** [https://www.cogeo.org](https://www.cogeo.org)
* **GDAL & Rasterio Documentation:** [https://gdal.org](https://gdal.org)
* **DVC (Data Version Control):** [https://dvc.org](https://dvc.org)
* **Master Coder Protocol (MCP):** [`docs/standards/`](../../docs/standards/)

---

<div align="center">

*‚ÄúFrom raw to ready ‚Äî these processed layers form the backbone of Kansas‚Äôs digital frontier.‚Äù*

</div>
```
