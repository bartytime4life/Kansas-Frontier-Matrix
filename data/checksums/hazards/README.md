<div align="center">

# ‚ö†Ô∏è Kansas Frontier Matrix ‚Äî Hazards Checksums  
`data/checksums/hazards/`

**Mission:** Guarantee the **integrity, reproducibility, and provenance** of all processed natural hazard datasets ‚Äî  
including tornado tracks, floods, wildfires, and drought indices ‚Äî through SHA-256 verification and continuous validation  
within the Kansas Frontier Matrix (KFM) ecosystem.

[![Build & Deploy](https://github.com/bartytime4life/Kansas-Frontier-Matrix/actions/workflows/site.yml/badge.svg)](../../../.github/workflows/site.yml)
[![STAC Validate](https://github.com/bartytime4life/Kansas-Frontier-Matrix/actions/workflows/stac-validate.yml/badge.svg)](../../../.github/workflows/stac-validate.yml)
[![Trivy Security](https://github.com/bartytime4life/Kansas-Frontier-Matrix/actions/workflows/trivy.yml/badge.svg)](../../../.github/workflows/trivy.yml)
[![Docs ¬∑ MCP](https://img.shields.io/badge/Docs-MCP-blue)](../../../docs/)
[![License: Data](https://img.shields.io/badge/License-CC--BY%204.0-green)](../../../LICENSE)

</div>

---

## üìö Overview

This directory contains **SHA-256 checksum files (`.sha256`)** for all processed **hazard datasets**  
in the Kansas Frontier Matrix.  

Checksums ensure:
- **Integrity** ‚Äî Detect file corruption or unauthorized modification.  
- **Reproducibility** ‚Äî Confirm deterministic ETL outputs across builds.  
- **Provenance** ‚Äî Provide traceable linkage between datasets, metadata, and STAC catalog.  
- **Auditability** ‚Äî Enable automated validation within CI/CD pipelines.  

All checksum files are generated by the **hazards ETL pipeline (`make hazards`)**  
and verified continuously during build and deployment workflows.

---

## üóÇÔ∏è Directory Layout

```bash
data/checksums/hazards/
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ tornado_tracks_1950_2024.geojson.sha256
‚îú‚îÄ‚îÄ flood_events_1900_2025.geojson.sha256
‚îú‚îÄ‚îÄ wildfire_perimeters_2000_2024.geojson.sha256
‚îî‚îÄ‚îÄ drought_index_2000_2025.tif.sha256
````

> **Note:** Each `.sha256` file corresponds to a dataset in
> `data/processed/hazards/` and is validated automatically via `sha256sum -c` in CI workflows.

---

## üîê Purpose of Checksums

| Objective                  | Description                                                               |
| :------------------------- | :------------------------------------------------------------------------ |
| **Integrity Verification** | Detects data corruption or modification after ETL or during transfer.     |
| **Reproducibility**        | Ensures identical ETL outputs are produced from the same inputs.          |
| **Provenance**             | Links checksum records directly to STAC items and source metadata.        |
| **Automation**             | Integrated into CI/CD pipelines for continuous data integrity validation. |

---

## üßÆ Example `.sha256` File

```bash
# File: tornado_tracks_1950_2024.geojson.sha256
8fb29cda3d0e44182f26c7bceff74b2c81b83e742d47d836b33151f871bb69d1  tornado_tracks_1950_2024.geojson
```

This checksum validates
`data/processed/hazards/tornado_tracks_1950_2024.geojson`
and ensures it matches the reference hash in this repository.

---

## ‚öôÔ∏è Checksum Generation Workflow

Checksums are created automatically at the end of the ETL pipeline.

**Makefile target:**

```bash
make hazards-checksums
```

**Python command:**

```bash
python src/utils/generate_checksums.py data/processed/hazards/ --algo sha256
```

**Steps:**

1. Locate processed hazard datasets (`.geojson`, `.tif`, `.csv`).
2. Compute SHA-256 hashes using Python‚Äôs `hashlib`.
3. Save results as `<filename>.sha256` to this directory.
4. Validate all checksums in CI/CD to ensure reproducibility.

---

## üß∞ CI/CD Validation

Checksum validation is integrated into continuous integration workflows.

**Command used in CI:**

```bash
sha256sum -c data/checksums/hazards/*.sha256
```

**If a mismatch occurs:**

* The build stops immediately.
* The affected dataset must be reprocessed and its checksum regenerated.
* Validation logs are retained to maintain auditability.

---

## üß© Integration with Metadata & STAC

| Linked Component                            | Purpose                                                      |
| :------------------------------------------ | :----------------------------------------------------------- |
| `data/processed/metadata/hazards/`          | STAC metadata files reference checksum entries.              |
| `src/pipelines/hazards/hazards_pipeline.py` | Generates and validates hashes during ETL.                   |
| `.github/workflows/stac-validate.yml`       | Automates checksum and STAC validation in CI/CD.             |
| `data/stac/hazards/`                        | STAC catalog includes checksum references in asset metadata. |

---

## üß† MCP Compliance Summary

| MCP Principle           | Implementation                                          |
| :---------------------- | :------------------------------------------------------ |
| **Documentation-first** | Each dataset includes README + checksum record.         |
| **Reproducibility**     | Deterministic outputs verified via SHA-256 hashing.     |
| **Open Standards**      | SHA-256 (FIPS 180-4) cryptographic algorithm.           |
| **Provenance**          | Checksum records link data ‚Üí metadata ‚Üí STAC entries.   |
| **Auditability**        | CI/CD logs and checksum validation ensure transparency. |

---

## üìÖ Version History

| Version | Date       | Summary                                                                                           |
| :------ | :--------- | :------------------------------------------------------------------------------------------------ |
| v1.0    | 2025-10-04 | Initial hazards checksum documentation ‚Äî includes tornado, flood, wildfire, and drought datasets. |

---

<div align="center">

**Kansas Frontier Matrix** ‚Äî *‚ÄúEvery Storm Verified: Integrity Through Time.‚Äù*
üìç [`data/checksums/hazards/`](.) ¬∑ Linked to the **Hazards STAC Collection**

</div>
