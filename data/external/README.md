# ğŸŒ External Data (Thirdâ€‘Party Sources)

![Provenance](https://img.shields.io/badge/provenance-first-brightgreen)
![FAIR+CARE](https://img.shields.io/badge/FAIR%20%2B%20CARE-governed-blue)
![STAC](https://img.shields.io/badge/STAC-catalog-orange)
![DCAT](https://img.shields.io/badge/DCAT-discovery-purple)
![PROV](https://img.shields.io/badge/W3C%20PROV-lineage-informational)

This folder is the **â€œintake dockâ€** for datasets that originate **outside** the project (archives, agencies, open-data portals, vendors, research groups, etc.) and are brought into KFM as **evidence-backed** inputs.

> âœ… Goal: make it impossible for â€œmystery dataâ€ to slip into the platform.  
> Everything added here must be **traceable**, **license-aware**, and **pipeline-ready**.

---

## ğŸ§­ Contents

- [ğŸ§© What belongs here](#-what-belongs-here)
- [ğŸš« What must NOT be here](#-what-must-not-be-here)
- [ğŸ—‚ï¸ Directory layout](#ï¸-directory-layout)
- [ğŸ” Canonical flow](#-canonical-flow)
- [â• Add a new external dataset](#-add-a-new-external-dataset)
- [ğŸ§¾ Required â€œboundary artifactsâ€](#-required-boundary-artifacts)
- [ğŸ” Licensing, sensitivity, and governance](#-licensing-sensitivity-and-governance)
- [âœ… Quality checks](#-quality-checks)
- [ğŸ§° Templates](#-templates)
- [ğŸ” Tips for big / remote datasets](#-tips-for-big--remote-datasets)

---

## ğŸ§© What belongs here

Typical examples:

- ğŸ—ºï¸ **Historical maps** (scans, GeoTIFFs, PDFs) + any georeferencing control files
- ğŸ§¾ **Archival indexes** (CSV/JSON metadata pulled from institutions)
- ğŸ›°ï¸ **Remote sensing derivatives** (small subsets / derived rasters, not entire global catalogs)
- ğŸ§­ **Reference layers** (boundaries, hydrography, transportation, place names)
- ğŸ§ª **Evidence artifacts** generated from external sources (OCR outputs, extracted tables), *as long as provenance is captured*

---

## ğŸš« What must NOT be here

- ğŸ”‘ **Secrets** (API keys, tokens, passwords)
- ğŸ§ **PII / sensitive personal data** unless explicitly governed and approved
- ğŸ§¨ â€œRandomâ€ files with unknown origin (if you canâ€™t cite it, it doesnâ€™t ship)
- ğŸ§± Direct DB dumps meant to bypass the pipeline (no â€œshortcut importsâ€)

---

## ğŸ—‚ï¸ Directory layout

This domain is expected to follow the standard â€œstageâ€ structure (raw â†’ work â†’ processed).  
If your repo uses a different layout elsewhere, keep the **spirit** identical: isolate external sources, keep raw read-only, and emit governed outputs.

```text
ğŸ“ data/
â””â”€ğŸ“ external/
  â”œâ”€ğŸ“„ README.md                         ğŸ‘ˆ you are here
  â”œâ”€ğŸ“ raw/                              ğŸ§¾ immutable snapshots (read-only)
  â”‚  â””â”€ğŸ“ <dataset_slug>/
  â”‚     â”œâ”€ğŸ“„ SOURCE.yaml                 (where it came from + license + retrieval date)
  â”‚     â”œâ”€ğŸ“„ CHECKSUMS.sha256            (hashes for integrity)
  â”‚     â””â”€ğŸ“¦ <original_download>.*       (zip/csv/tif/pdf/etc)
  â”œâ”€ğŸ“ work/                             ğŸ§ª intermediates (throwaway / reproducible)
  â”‚  â””â”€ğŸ“ <dataset_slug>/
  â”œâ”€ğŸ“ processed/                        âœ… curated outputs (served downstream)
  â”‚  â””â”€ğŸ“ <dataset_slug>/
  â”‚     â”œâ”€ğŸ—ºï¸ <layer>.geojson|parquet|tif
  â”‚     â””â”€ğŸ“„ README.md                   (dataset-specific notes + known issues)
  â””â”€ğŸ“ mappings/                         ğŸ§­ optional helper docs for metadata linkage
     â””â”€ğŸ“ <dataset_slug>/
        â”œâ”€ğŸ“„ stac.plan.md
        â”œâ”€ğŸ“„ dcat.plan.md
        â””â”€ğŸ“„ prov.plan.md
```

---

## ğŸ” Canonical flow

**No skipping steps.** External data must move through the same governed sequence as everything else:

```text
RAW â†’ WORK â†’ PROCESSED â†’ (STAC/DCAT/PROV) â†’ DATABASE â†’ API â†’ UI
```

> If a feature proposal â€œinjectsâ€ data directly into the UI, DB, or graph *without* catalogs/provenance, itâ€™s not considered a valid approach in this project.

---

## â• Add a new external dataset

### 1) Create a dataset slug ğŸ·ï¸
Use a stable, readable ID:

- âœ… `usgs_wbd_huc8_ks_2024`
- âœ… `kshs_railroads_index_1890s`
- âŒ `newdata-final-v2-FORREAL`

### 2) Drop the immutable raw snapshot ğŸ§¾
Put original inputs into:

- `data/external/raw/<dataset_slug>/`

Rules:
- Raw is **write-once** (treat as evidence).
- Prefer storing the *original container* (zip/tar) + a `CHECKSUMS.sha256`.

### 3) Document origin + license ğŸ“œ
Add `SOURCE.yaml` (template below).  
If the license is unclear, **stop** and route to governance review.

### 4) Build a deterministic pipeline ğŸ§°
Pipelines must:
- read from `data/external/raw/<dataset_slug>/`
- write intermediates to `data/external/work/<dataset_slug>/`
- write final outputs to `data/external/processed/<dataset_slug>/`
- run end-to-end without prompts (no â€œclick to continueâ€)

> ğŸ“Œ Put pipeline code where the repo standard expects it (commonly `src/pipelines/â€¦` or `pipelines/â€¦`). The key is *repeatability*.

### 5) Emit the required metadata artifacts ğŸ§¾
Every dataset must have:
- STAC Collection + Item(s)
- DCAT Dataset entry
- PROV activity bundle

These usually live in canonical catalog locations like:
- `data/stac/â€¦`
- `data/catalog/dcat/â€¦`
- `data/prov/â€¦` (or `data/provenance/â€¦` depending on repo layout)

### 6) Validate + open PR âœ…
Before a PR is considered â€œreadyâ€:
- metadata schema validation must pass
- secret scanning must be clean
- any sensitivity/classification constraints must be satisfied

---

## ğŸ§¾ Required boundary artifacts

These are the â€œinterfacesâ€ from external data into the rest of the system:

### ğŸ—ºï¸ STAC
- Points to the processed asset files (or stable object storage)
- Captures spatial/temporal footprint where applicable
- Includes source + license references

### ğŸ§¾ DCAT
- Makes the dataset discoverable in the projectâ€™s catalog
- Includes title/description/keywords/license/distributions

### ğŸ§¬ PROV
- Captures lineage end-to-end: **raw inputs â†’ work intermediates â†’ processed outputs**
- Records **who/what** ran the pipeline and **when**
- Prefer recording pipeline commit hash / run id for auditability

---

## ğŸ” Licensing, sensitivity, and governance

External data is not â€œfree by default.â€ Treat license and sensitivity as **first-class fields**.

### License rules ğŸ“œ
- Include the license identifier (or full text if needed).
- Record any constraints: attribution, non-commercial, share-alike, embargo, etc.
- If â€œTerms of Useâ€ are webpage-only, capture the URL + retrieval date in `SOURCE.yaml`.

### Sensitivity rules ğŸ§­
- If the dataset includes sensitive locations or culturally sensitive content:
  - do not publish precise coordinates publicly
  - document any redaction/generalization performed
  - ensure the API/UI layers enforce access controls (never rely on â€œthe UI hides itâ€)

### Governance triggers âš–ï¸
Expect manual review when:
- adding a **new external provider/source**
- introducing **restricted / sensitive** layers
- changing dataset classification or access scope

---

## âœ… Quality checks

Recommended minimum checks (prefer automated):

- âœ… record counts / null-rate summaries
- âœ… geometry validity (for vector)
- âœ… CRS normalization (document CRS transforms)
- âœ… plausible range checks (years, coordinates, units)
- âœ… duplicate detection (hash-based)

> Tip: store a tiny QA report (Markdown or JSON) in `processed/<dataset_slug>/README.md` so reviewers can verify quickly.

---

## ğŸ§° Templates

### `SOURCE.yaml` (required)

```yaml
dataset_id: "<dataset_slug>"
title: "<human readable title>"
provider: "<organization / archive / portal>"
source_url: "<canonical landing page or download endpoint>"
retrieved_at: "YYYY-MM-DD"
license:
  name: "<e.g., Public Domain | CC-BY-4.0 | custom>"
  url: "<license/terms page>"
attribution:
  required: true
  text: "<preferred attribution statement>"
sensitivity:
  classification: "public | restricted | confidential"
  notes: "<why / what to watch for>"
contents:
  - path: "raw/<file_or_bundle_name>"
    description: "<what it is>"
    sha256: "<fill from CHECKSUMS.sha256>"
processing:
  expected_outputs:
    - "processed/<dataset_slug>/<output_file>"
  pipeline_ref: "<path/to/pipeline or module name>"
```

### `CHECKSUMS.sha256` (recommended)

```text
<sha256_hash>  <filename>
<sha256_hash>  <filename>
```

### Dataset README (recommended)

Create: `data/external/processed/<dataset_slug>/README.md`

Include:
- source summary + license
- processing summary
- known limitations / uncertainty
- links to STAC/DCAT/PROV artifact IDs

---

## ğŸ” Tips for big / remote datasets

When the upstream dataset is huge (e.g., national rasters, satellite archives):

- âœ… Prefer **derived products** (clipped to Kansas AOI, summarized, or tiled)
- âœ… Store a **manifest + checksums** rather than committing multiâ€‘GB blobs
- âœ… Ensure STAC assets point to **stable storage** (release bundle, object store, or controlled mirror)
- âœ… Keep raw â€œevidenceâ€ verifiable: capture retrieval date + exact query parameters + hashes

---

### âœ… PR checklist (copy/paste)

- [ ] Raw files added under `data/external/raw/<dataset_slug>/` (or manifest if too large)
- [ ] `SOURCE.yaml` present + license documented
- [ ] Checksums recorded (`CHECKSUMS.sha256`)
- [ ] Pipeline added/updated and is deterministic
- [ ] Outputs written to `data/external/processed/<dataset_slug>/`
- [ ] STAC Collection + Item(s) created/updated
- [ ] DCAT Dataset entry created/updated
- [ ] PROV bundle created/updated (rawâ†’workâ†’processed chain)
- [ ] Any sensitivity/redaction documented (if applicable)

---

