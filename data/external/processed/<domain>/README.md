# ğŸŒ External Processed Data â€” `<domain>`  
![Stage](https://img.shields.io/badge/stage-processed-blue) ![Scope](https://img.shields.io/badge/scope-external-informational) ![Governance](https://img.shields.io/badge/governance-provenance--first-success) ![Build](https://img.shields.io/badge/pipeline-deterministic-important)

> [!IMPORTANT]
> This directory contains **final, curated, â€œready-to-serveâ€** datasets for the **external** domain **`<domain>`**.  
> These outputs are treated as **authoritative** (consumed by downstream indexing + APIs) and must be reproducible from raw sources via governed pipelines.

---

## ğŸš€ Quick links
- ğŸ“¥ **Raw inputs:** `../../raw/<domain>/`
- ğŸ§ª **Work / intermediates:** `../../work/<domain>/`
- âœ… **Processed outputs (you are here):** `./`
- ğŸ§¾ **STAC metadata:** `../../../stac/collections/` + `../../../stac/items/`
- ğŸ§­ **DCAT catalog:** `../../../catalog/dcat/`
- ğŸ§¬ **PROV lineage:** `../../../prov/` *(or legacy: `../../../provenance/`)*

---

## ğŸ§­ Canonical flow (nonâ€‘negotiable)
```mermaid
flowchart LR
  A["ğŸ“¥ External Raw<br/>data/external/raw/&lt;domain&gt;/"] --> B["ğŸ§° ETL + Normalization<br/>(deterministic, idempotent)"]
  B --> C["âœ… Processed Outputs<br/>data/external/processed/&lt;domain&gt;/"]
  C --> D["ğŸ§¾ Boundary Artifacts<br/>STAC + DCAT + PROV"]
  D --> E["ğŸ—ƒï¸ DB/Graph Load<br/>PostGIS Â· Neo4j"]
  E --> F["ğŸ›¡ï¸ Governed API Layer"]
  F --> G["ğŸ—ºï¸ UI / Story Nodes<br/>evidence-first"]
```

**Principles baked into this folder:**
- ğŸ§¾ **Provenance-first:** nothing is â€œpublishedâ€ without STAC/DCAT/PROV.
- ğŸ§© **Contract-first:** schemas are first-class artifacts; changes require versioning.
- ğŸ” **Deterministic ETL:** same inputs/config â‡’ same outputs; reruns are safe.

---

## âœ… Definition of Done (DoD) for any file in this folder
A dataset is allowed to land in `data/external/processed/<domain>/` only when ALL are true:

- [ ] **Output(s) present** in an approved format (see â€œFormatsâ€ below)
- [ ] **Schema/contract** exists and validates (JSON Schema / table schema / raster profile)
- [ ] **STAC Collection + Item(s)** created and link to the asset(s)
- [ ] **DCAT Dataset entry** created (license, keywords, distributions)
- [ ] **PROV bundle** created (raw â†’ work â†’ processed lineage, run info, params)
- [ ] **Validation report** stored/linked (row counts, geometry checks, QA metrics)
- [ ] **License & attribution** documented (source terms complied with)
- [ ] **Sensitivity/classification** propagated + redactions applied (if needed)
- [ ] **Large-file policy** followed (Git LFS / pointer / checksum manifest)

> [!TIP]
> If you canâ€™t point to the pipeline config + PROV + catalog records, the data isnâ€™t â€œdoneâ€ yet.

---

## ğŸ“¦ What belongs here (and what doesnâ€™t)
### âœ… Belongs âœ…
- Final, curated outputs **ready for indexing** and **API/UI consumption**
- Standardized schemas, clean attributes, normalized units
- Stable filenames/IDs, versioned datasets, checksums (for integrity)

### âŒ Does NOT belong âŒ
- â€œOne-offâ€ manual edits (fix the pipeline instead)
- Scratch files, notebooks, ad-hoc exports, temporary caches
- Secrets/credentials/tokens (ever ğŸ”¥)
- Unlicensed or unclear-rights third-party data

---

## ğŸ—‚ï¸ Directory layout (recommended)
```text
data/
â””â”€â”€ external/
    â”œâ”€â”€ raw/
    â”‚   â””â”€â”€ <domain>/            ğŸ“¥ write-once source artifacts (read-only)
    â”œâ”€â”€ work/
    â”‚   â””â”€â”€ <domain>/            ğŸ§ª intermediate outputs (rebuildable)
    â””â”€â”€ processed/
        â””â”€â”€ <domain>/            âœ… final curated outputs (this folder)
            â”œâ”€â”€ README.md        ğŸ“˜ this runbook
            â”œâ”€â”€ manifests/       ğŸ§¾ checksums, inventories, size reports (optional)
            â”œâ”€â”€ qa/              ğŸ§ª validation outputs (optional)
            â””â”€â”€ <dataset filesâ€¦> ğŸ—ƒï¸ authoritative assets
```

---

## ğŸ§¾ Dataset inventory
Keep this table current. It is the **human-friendly index** to whatâ€™s in this folder.

| Dataset ID | What it is ğŸ§  | Files ğŸ—ƒï¸ | Format | Spatial â›°ï¸ | Temporal ğŸ•°ï¸ | Schema ğŸ§© | STAC/DCAT/PROV ğŸ§¾ |
|---|---|---:|---|---|---|---|---|
| `<dataset_id>` | `<short description>` | `N` | `GeoParquet/GeoJSON/GeoTIFF/CSV` | `EPSG:4326` | `<range>` | `schemas/...` | `stac/items/...` |

> [!NOTE]
> If a dataset is updated/reprocessed, **add a new version** (donâ€™t overwrite silently). Link revisions in DCAT/PROV.

---

## ğŸ·ï¸ Naming & versioning conventions
### Recommended filename pattern
Use stable naming to support diffs, provenance, and caching:

- **Vectors / tables:**  
  `"<dataset_id>__v<semver>__<yyyymmdd>.parquet"`  
  `"<dataset_id>__v<semver>__<yyyymmdd>.csv"`

- **Rasters:**  
  `"<dataset_id>__v<semver>__<yyyymmdd>.tif"`

- **Manifests:**  
  `manifests/<dataset_id>__v<semver>__<yyyymmdd>.sha256`

### Dataset version rules
- Patch (`x.y.Z`) â†’ bugfix/cleaning, no schema breaking changes  
- Minor (`x.Y.z`) â†’ additive fields, new layers, backward compatible  
- Major (`X.y.z`) â†’ schema breaking change or semantic redefinition

---

## ğŸ§© Data contracts & schema expectations
Every dataset in this folder must have a **machine-checkable contract**:

- ğŸ“„ **Tabular**: schema file (JSON Schema / parquet schema export / data dictionary)
- ğŸ§­ **Vector**: geometry type + CRS + required properties + units
- ğŸ›°ï¸ **Raster**: CRS, resolution, nodata, band meanings, pixel type

> [!IMPORTANT]
> Contract-first means: **change the schema first**, then update pipeline + data + validations.

---

## ğŸŒ Metadata: STAC + DCAT (required)
### STAC (asset indexing)
- Create/maintain a **STAC Collection** for `<domain>`
- Create **STAC Item(s)** for each asset or logical slice
- Ensure Items link to:
  - the processed file(s) here
  - license + attribution
  - the matching PROV bundle

### DCAT (dataset discovery)
- Create **one DCAT dataset entry per published dataset version**
- Include:
  - title/description/keywords
  - license + usage terms
  - distribution links (STAC Item(s) and/or direct downloads)

---

## ğŸ§¬ Provenance: PROV (required)
A PROV bundle must link the whole chain:

- `raw inputs` â†’ `work intermediates` â†’ `processed outputs`
- Must record:
  - pipeline name + version (or git commit)
  - run timestamp + parameters/config
  - checksums (or stable IDs) for inputs/outputs
  - agent (person/service) responsible

---

## ğŸ§ª Validation & QA/QC
### Minimum checks (recommended)
- âœ… Schema validation passes (required fields + types)
- âœ… Row/feature counts stable (or justified deltas)
- âœ… Geometry validity (for vectors), bounding boxes plausible
- âœ… CRS is explicit (prefer WGS84/EPSG:4326 unless justified)
- âœ… Units standardized + documented
- âœ… Null rate / missingness reported for critical fields

### Where to store QA
- `qa/<dataset_id>/validation_report.json`
- `qa/<dataset_id>/summary.md` (human-readable notes)

---

## ğŸ“¦ Formats (preferred)
Choose formats that are open, diff-friendly, and performant:

- ğŸ§Š **GeoParquet / Parquet** (preferred for large vectors/tables)
- ğŸ—ºï¸ **GeoJSON** (small/moderate vectors; great for demos)
- ğŸ›°ï¸ **GeoTIFF** (rasters)
- ğŸ“„ **CSV** (small tabular; avoid for huge data unless necessary)

---

## ğŸ§± Large-file policy
If a processed asset is â€œtoo bigâ€ for normal Git:
- Use **Git LFS** *or*
- Store a **pointer + checksum** and provide a fetch script

**Rule of thumb:** anything > **~100MB** should be treated as â€œlargeâ€ and handled deliberately.

---

## âš–ï¸ Governance, licensing, and sensitivity ğŸ”
### External source hygiene
For **new external sources**, verify:
- âœ… license compatibility / redistribution permissions
- âœ… attribution requirements
- âœ… update cadence and stability
- âœ… provenance quality (what is the authoritative upstream?)

### Sensitivity propagation
- No derivative output can be **less restricted** than its inputs.
- If redaction/generalization is needed, apply it:
  - in processed outputs
  - in STAC/DCAT metadata (flagging redaction)
  - in API enforcement and UI presentation

> [!WARNING]
> If this domain includes culturally sensitive locations (e.g., archaeology), route through governance review before publishing.

---

## ğŸ” Rebuild / update workflow (template)
<details>
<summary><strong>ğŸ› ï¸ Example workflow</strong> (click to expand)</summary>

```bash
# 1) Fetch/refresh raw inputs (external)
python -m src.pipelines.<domain>.fetch --config configs/<dataset_id>.yaml

# 2) Transform â†’ work â†’ processed (deterministic build)
python -m src.pipelines.<domain>.build --config configs/<dataset_id>.yaml

# 3) Validate outputs (schema + QA)
python -m tools.validate_dataset --dataset <dataset_id> --domain <domain>

# 4) Emit metadata + provenance boundary artifacts
python -m src.pipelines.<domain>.catalog --dataset <dataset_id>

# 5) (Optional) Load into DB/Graph for serving
python -m src.pipelines.<domain>.load --dataset <dataset_id>
```

</details>

---

## ğŸ¤ Contributing checklist (PR-ready)
- [ ] Update **Dataset inventory table**
- [ ] Add/Update **schema contract**
- [ ] Add/Update **STAC/DCAT/PROV**
- [ ] Add/Update **QA outputs**
- [ ] Confirm **license + attribution**
- [ ] Confirm **sensitivity classification + redactions**
- [ ] Ensure **deterministic rebuild** works from raw

---

## ğŸ§¾ Changelog (keep short)
- `YYYY-MM-DD` â€” `<dataset_id>` v`x.y.z` â€” `<what changed>`
- `YYYY-MM-DD` â€” `<dataset_id>` v`x.y.z` â€” `<what changed>`

---

## ğŸ“Œ Maintainers / stewards
- **Domain steward:** `<name/handle>`
- **Pipeline owner:** `<name/handle>`
- **Governance reviewer (if sensitive):** `<name/handle>`

