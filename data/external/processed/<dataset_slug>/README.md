# ğŸ“¦ `<dataset_slug>` â€” External *Processed* Dataset

![Stage](https://img.shields.io/badge/stage-processed-blue)
![Scope](https://img.shields.io/badge/scope-external-lightgrey)
![Metadata](https://img.shields.io/badge/metadata-STAC%20%7C%20DCAT%20%7C%20PROV-brightgreen)
![Version](https://img.shields.io/badge/dataset%20version-TODO-orange)

**Path:** `data/external/processed/<dataset_slug>/` âœ…

> [!IMPORTANT]
> In this system, datasets are expected to move **Raw â†’ Processed â†’ Catalog/Prov â†’ Database â†’ API â†’ UI**.  
> Anything that shortcuts this flow (e.g., â€œjust drop a file into the UIâ€) is treated as flawed unless explicitly justified. :contentReference[oaicite:0]{index=0}

---

## âœ¨ What this folder is for

This folder contains the **authoritative processed outputs** for the dataset **`<dataset_slug>`**:
- cleaned/standardized files ready for analysis and/or DB ingestion
- stable asset paths that metadata (STAC/DCAT/PROV) can safely point to
- versioned artifacts (so changes can be reviewed like code):contentReference[oaicite:1]{index=1}

> [!WARNING]
> **Do not â€œhand-editâ€ processed outputs.**  
> Update the upstream pipeline and regenerate outputs so provenance stays correct and reproducible. (Pipelines are intended to be deterministic and run end-to-end without manual steps.):contentReference[oaicite:2]{index=2}

---

<details>
<summary>ğŸ§­ Table of Contents</summary>

- [ğŸ“ Dataset card](#-dataset-card)
- [ğŸ—‚ï¸ Folder layout](#ï¸-folder-layout)
- [ğŸ”— Upstream sources](#-upstream-sources)
- [âš™ï¸ Processing and reproducibility](#ï¸-processing-and-reproducibility)
- [ğŸ“¦ Output inventory](#-output-inventory)
- [ğŸ§¾ Schema and data dictionary](#-schema-and-data-dictionary)
- [ğŸ§­ Spatial and temporal reference](#-spatial-and-temporal-reference)
- [âœ… Validation and QA](#-validation-and-qa)
- [ğŸ§¬ Metadata and provenance](#-metadata-and-provenance)
- [ğŸ§· Versioning and changelog](#-versioning-and-changelog)
- [ğŸ§© System integration](#-system-integration)
- [ğŸ” Licensing and governance](#-licensing-and-governance)
- [ğŸ¤ How to update this dataset](#-how-to-update-this-dataset)
- [ğŸ“‡ Maintainers](#-maintainers)
- [ğŸ“š Project standards references](#-project-standards-references)

</details>

---

## ğŸ“ Dataset card

| Field | Value |
|---|---|
| **Dataset slug** | `<dataset_slug>` |
| **Stage** | `external â†’ processed` |
| **Short description** | TODO (1â€“2 sentences) |
| **Primary entity** | TODO (e.g., parcels / events / imagery / boundaries / stations) |
| **Spatial?** | âœ…/âŒ (vector / raster / tabular-with-geo) |
| **Geometry type** | TODO (Point / LineString / Polygon / Multi* / Raster) |
| **CRS** | TODO (e.g., EPSG:4326) |
| **Temporal coverage** | TODO (startâ€“end) |
| **Spatial coverage** | TODO (bbox / region) |
| **Record count** | TODO |
| **Granularity** | TODO (daily / yearly / event-based / etc.) |
| **License (derived)** | TODO (SPDX identifier if possible) |
| **Upstream source(s)** | TODO (publisher + dataset title + release date) |
| **Pipeline entrypoint** | TODO (e.g., `pipelines/<dataset_slug>/...`) |
| **Provenance bundle** | TODO (link to PROV file path) |

---

## ğŸ—‚ï¸ Folder layout

Recommended structure inside `data/external/processed/<dataset_slug>/`:

```text
ğŸ“ data/external/processed/<dataset_slug>/
â”œâ”€ ğŸ“„ README.md                ğŸ‘ˆ you are here
â”œâ”€ ğŸ“„ CHANGELOG.md             (optional but recommended)
â”œâ”€ ğŸ“„ checksums.sha256         (recommended for large/binary assets)
â”œâ”€ ğŸ“ data/                    (processed outputs)
â”‚  â”œâ”€ <dataset_slug>.parquet
â”‚  â”œâ”€ <dataset_slug>.geojson
â”‚  â””â”€ ...
â”œâ”€ ğŸ“ schema/                  (data dictionary + schema artifacts)
â”‚  â”œâ”€ schema.json
â”‚  â””â”€ data_dictionary.md
â””â”€ ğŸ“ validation/              (pipeline-produced reports)
   â”œâ”€ row_counts.json
   â”œâ”€ expectations.md
   â””â”€ validation_report.md
```

> [!TIP]
> If an output is **large**, consider Git LFS or a â€œfetch by checksumâ€ strategy; the key expectation is that the **identity and existence** of every processed dataset is tracked, even when storage is externalized. :contentReference[oaicite:3]{index=3}

---

## ğŸ”— Upstream sources

### ğŸ“¥ Raw inputs live elsewhere
This processed dataset should trace back to one (or more) raw source artifacts. Typical convention:

- `data/external/raw/<dataset_slug>/...` *(or equivalent)*  
- Raw inputs are treated as **read-only snapshots** and should not be modified by pipelines.:contentReference[oaicite:4]{index=4}

### ğŸ“Œ Source list
Fill this in with concrete citations/links appropriate for your repo:

- **Source 1:** TODO  
  - Publisher: TODO  
  - Dataset/product: TODO  
  - Access date: TODO  
  - License/terms: TODO  
  - Notes: TODO (rate limits, API keys, special conditions)

---

## âš™ï¸ Processing and reproducibility

### ğŸ§ª Deterministic pipelines (required)
Pipelines should:
- produce the **same outputs** given the same inputs + config
- avoid manual/interactive steps
- avoid duplicating datasets when nothing changed (use checksums/version checks):contentReference[oaicite:5]{index=5}

### ğŸ§° Pipeline entrypoint
- `pipelines/<dataset_slug>/...` *(preferred)*  
  **or**
- `pipelines/import_<dataset_slug>.py` *(simple pattern)*

> Example patterns from project docs include scripts like `pipelines/import_census.py` and per-source raw folders like `data/raw/census_1900/...` (adapt to your `data/external/raw/...` layout).:contentReference[oaicite:6]{index=6}

### ğŸ§© Optional: plugin-style pipeline packaging
If youâ€™re using a plugin/orchestrator pattern, document it here:

- `pipelines/plugins/<dataset_slug>/pipeline.yml`
- `pipelines/plugins/<dataset_slug>/run.py`

This lets the orchestrator â€œdiscoverâ€ pipelines by convention and run on schedules/configs.:contentReference[oaicite:7]{index=7}

### â–¶ï¸ Rebuild command
> Replace these placeholders with the real command(s) used in this repo.

```bash
# Example: run the dataset pipeline
python -m pipelines.<dataset_slug>.run \
  --config pipelines/<dataset_slug>/config.yml \
  --output data/external/processed/<dataset_slug>/
```

### ğŸ§¾ Required outputs from the pipeline (contract artifacts)
Each dataset pipeline is expected to emit:
- processed data files in this folder
- a **STAC** record(s)
- a **DCAT** dataset record
- a **PROV** lineage bundle (raw â†’ work â†’ processed)
- (recommended) validation reports & summary stats:contentReference[oaicite:8]{index=8}

If these are missing, CI is expected to flag/reject the contribution (â€œno data enters without documentationâ€).:contentReference[oaicite:9]{index=9}

---

## ğŸ“¦ Output inventory

List every â€œblessedâ€ output file that downstream components depend on.

| File | Format | Purpose | Row/feature count | Notes |
|---|---:|---|---:|---|
| `data/<dataset_slug>.parquet` | Parquet | TODO | TODO | TODO |
| `data/<dataset_slug>.geojson` | GeoJSON | TODO | TODO | TODO |
| `data/<dataset_slug>.tif` | GeoTIFF | TODO | n/a | TODO |

> [!NOTE]
> Processed datasets should be **consistent**: cleaned attributes, standardized units, and a deliberate CRS choice (often a common one like WGS84 unless thereâ€™s reason otherwise).:contentReference[oaicite:10]{index=10}

---

## ğŸ§¾ Schema and data dictionary

### ğŸ”‘ Column-level documentation
Include:
- field name
- type
- units
- allowed values / enumerations
- nullability
- how it was derived (if computed)

| Field | Type | Units | Nullable | Description |
|---|---|---:|:---:|---|
| `id` | string | â€” | âŒ | TODO |
| `name` | string | â€” | âœ… | TODO |
| `value` | float | TODO | âœ… | TODO |
| `geometry` | geometry | CRS units | âŒ | TODO |

### ğŸ§  Metadata best-practice checklist (geo + non-geo)
For dependable GIS datasets, metadata commonly includes: identification, quality, spatial reference (CRS/projection), entity & attribute info, distribution, citation, temporal info, and contact info.:contentReference[oaicite:11]{index=11}

---

## ğŸ§­ Spatial and temporal reference

### ğŸ—ºï¸ CRS / Projection
- **CRS:** `EPSG:TODO`
- **Why this CRS:** TODO  
- **Reprojection rules:** TODO

> Pipelines often include coordinate conversion as part of the â€œprocess and cleanâ€ step, and projects may standardize on EPSG:4326 or a Kansas-specific projection depending on the dataset needs.:contentReference[oaicite:12]{index=12}

### ğŸ§­ Coverage
- **Spatial extent (bbox):** TODO (minx, miny, maxx, maxy)
- **Temporal extent:** TODO (startâ€“end)
- **Resolution / scale:** TODO (if raster)

---

## âœ… Validation and QA

### âœ… Required validations
- [ ] schema matches `schema/schema.json`
- [ ] geometry validity checks (if vector)
- [ ] expected column set present
- [ ] plausible ranges (min/max) verified
- [ ] null-rate thresholds enforced
- [ ] key uniqueness constraints (if applicable)

> Pipelines may produce **validation reports** and summary statistics (row counts, min/max, etc.) to make review easier.:contentReference[oaicite:13]{index=13}

### ğŸ“Š Recommended validation artifacts
- `validation/validation_report.md`
- `validation/row_counts.json`
- `validation/minmax.json`

---

## ğŸ§¬ Metadata and provenance

### ğŸŒ Required: STAC + DCAT + PROV (alignment policy)
Every dataset/evidence artifact is expected to have:
- **STAC collection + item(s)** (even non-spatial datasets often get a STAC collection for consistency)
- **DCAT dataset entry** (title, description, license, keywords, distribution links)
- **PROV activity bundle** (raw sources, steps, responsible agents, timestamps, configs):contentReference[oaicite:14]{index=14}

### ğŸ”— Cross-layer linkage expectations (keep everything in sync)
- STAC Items **must point to** actual assets in processed storage (this folder qualifies as â€œprocessed storageâ€) and carry source/license attribution.
- DCAT should link to STAC and/or direct downloads.
- PROV should connect raw â†’ intermediate work â†’ processed outputs and include run IDs/commit hashes.
- The knowledge graph should reference catalog IDs (STAC/DOI/etc.), not duplicate payloads.:contentReference[oaicite:15]{index=15}

### ğŸ“Œ Put links here (update paths to match your repo)
- **STAC:** `data/catalog/.../<dataset_slug>...`
- **DCAT:** `data/catalog/.../<dataset_slug>...`
- **PROV:** `data/provenance/<dataset_slug>.prov.json` *(example pattern)*:contentReference[oaicite:16]{index=16}

---

## ğŸ§· Versioning and changelog

### ğŸ“Œ Dataset versioning rules
When updating/reprocessing:
- create a new dataset version and link it to the previous version in **DCAT** and **PROV** (e.g., `prov:wasRevisionOf`)
- ideally assign a persistent identifier (DOI/ARK) for published versions:contentReference[oaicite:17]{index=17}

### ğŸ“ Changelog (fill this in)
- **vX.Y.Z** â€” YYYY-MM-DD  
  - Added: TODO  
  - Changed: TODO  
  - Fixed: TODO  
  - Notes: TODO

---

## ğŸ§© System integration

### ğŸ—ƒï¸ Database loading
Document target tables, schemas, or indexes here.

- **PostGIS table:** `TODO.schema.todo_table`
- **Primary key:** `TODO`
- **Geometry column:** `geom` (or `geometry`)  
- **Indexing:** TODO (GiST / SP-GiST / BRIN / etc.)

> Processed datasets are considered authoritative; updates can trigger re-indexing or reload into DBs as needed (e.g., a changed GeoJSON may cause a backend re-index).:contentReference[oaicite:18]{index=18}

### ğŸ”Œ API + UI touchpoints
- **API endpoint(s):** TODO
- **UI layer name:** TODO
- **Graph node IDs (if applicable):** TODO (prefer catalog references)

---

## ğŸ” Licensing and governance

### ğŸ“œ Licensing
- **Upstream license:** TODO  
- **Derived dataset license:** TODO  
- **Attribution text:** TODO (copy/paste ready)

### ğŸ§­ Ethics, access, and community controls
If the dataset contains sensitive content (locations of cultural sites, personal data, etc.), document:
- redaction rules
- tiered access strategy
- governance/approval requirements

> The project blueprint emphasizes embedding FAIR and CARE principles, including tiered data access and honoring community control over information.:contentReference[oaicite:19]{index=19}

---

## ğŸ¤ How to update this dataset

### âœ… â€œReady to mergeâ€ checklist
- [ ] Raw inputs stored/linked under `data/external/raw/<dataset_slug>/` (or equivalent)
- [ ] Pipeline is deterministic and non-interactive
- [ ] Processed outputs updated in `data/external/processed/<dataset_slug>/data/`
- [ ] STAC updated/created
- [ ] DCAT updated/created
- [ ] PROV bundle updated/created (includes run ID / commit hash)
- [ ] Validation reports updated
- [ ] This README updated (schema, counts, coverage, version)

---

## ğŸ“‡ Maintainers

- **Owner:** TODO (@handle)
- **Domain reviewer:** TODO
- **Last reviewed:** YYYY-MM-DD
- **Contact:** TODO

---

## ğŸ“š Project standards references

<details>
<summary>ğŸ“Œ Why the rules above exist (quick citations)</summary>

- Canonical pipeline sequence: Raw â†’ Processed â†’ Catalog/Prov â†’ Database â†’ API â†’ UI:contentReference[oaicite:20]{index=20}
- Processed data tracked as authoritative, with options for Git LFS / checksums for large assets:contentReference[oaicite:21]{index=21}
- Pipelines should be deterministic, reproducible, and should not modify raw inputs:contentReference[oaicite:22]{index=22}
- Required metadata alignment: STAC + DCAT + PROV, with cross-linking expectations:contentReference[oaicite:23]{index=23}
- Dataset versioning: link new versions to predecessors via DCAT/PROV (`prov:wasRevisionOf`), prefer persistent identifiers:contentReference[oaicite:24]{index=24}
- Metadata categories commonly needed for dependable GIS data (identification, quality, spatial reference, etc.):contentReference[oaicite:25]{index=25}

</details>

