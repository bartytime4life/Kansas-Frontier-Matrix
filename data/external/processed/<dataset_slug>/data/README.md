# `<dataset_slug>` â€” Processed Dataset ğŸ“¦ğŸ—ºï¸

![Stage](https://img.shields.io/badge/stage-processed-blue)
![Domain](https://img.shields.io/badge/domain-external-lightgrey)
![Status](https://img.shields.io/badge/status-TBD-orange)
![License](https://img.shields.io/badge/license-TBD-red)

> âœ… This folder contains the **processed (analysis-ready)** outputs for the external dataset **`<dataset_slug>`**.  
> ğŸ”’ Raw inputs should remain immutable; processed outputs live here for downstream use (catalogs â†’ graph â†’ API/UI).

---

## ğŸ“Œ Quick facts

| Field | Value |
|---|---|
| Dataset slug | `<dataset_slug>` |
| Domain | `external` |
| Lifecycle stage | `processed` |
| Dataset version | `vX.Y.Z` (or `YYYY-MM-DD`) |
| Date last processed | `YYYY-MM-DD` |
| Pipeline | `pipelines/<pipeline_name>.py` (or `.ipynb`) |
| Primary formats | `Parquet / GeoJSON / GPKG / GeoTIFF / CSV` (pick) |
| Geometry type | `Point / LineString / Polygon / Raster / None` |
| CRS / Projection | `EPSG:4326` (or `TBD`) |
| Spatial coverage | `TBD` (bbox, region) |
| Temporal coverage | `TBD` (startâ€“end) |
| License (upstream) | `TBD` |
| Citation / DOI | `TBD` |

---

## ğŸ§­ Where this sits in the KFM pipeline

```mermaid
flowchart LR
  raw["ğŸ“¥ Raw evidence<br/>data/external/raw/<dataset_slug>/"] --> work["ğŸ§ª Work / staging<br/>data/external/work/<dataset_slug>/"]
  work --> proc["âœ… Processed outputs<br/>data/external/processed/<dataset_slug>/data/"]
  proc --> stac["ğŸ›°ï¸ STAC<br/>data/stac/..."]
  proc --> dcat["ğŸ“š DCAT<br/>data/catalog/dcat/..."]
  proc --> prov["ğŸ§¾ PROV<br/>data/prov/..."]
  stac --> graph["ğŸ•¸ï¸ Graph (Neo4j refs)"]
  dcat --> graph
  prov --> graph
  graph --> api["âš™ï¸ API"]
  api --> ui["ğŸ§­ UI / Story Nodes"]
```

---

## ğŸ—‚ï¸ Folder layout

> You are here: `data/external/processed/<dataset_slug>/data/README.md`

```text
data/external/processed/<dataset_slug>/
â”œâ”€ data/                              # âœ… analysis-ready outputs (this folder)
â”‚  â”œâ”€ README.md                       # â¬… you are here
â”‚  â”œâ”€ <dataset_slug>.<ext>            # primary artifact (TBD)
â”‚  â”œâ”€ <dataset_slug>_assets/          # optional: tiles, sidecars, thumbnails
â”‚  â””â”€ checksums.sha256                # optional but recommended
â”œâ”€ metadata/                          # optional local helpers (schema + QA)
â”‚  â”œâ”€ schema.json                     # optional JSON Schema / Parquet schema export
â”‚  â”œâ”€ data_dictionary.md              # optional human-readable dictionary
â”‚  â””â”€ validation_report.md            # optional QA notes/results
â””â”€ logs/                              # optional pipeline logs (run summaries)
   â””â”€ <run_id>.log
```

---

## ğŸ“¦ Whatâ€™s in this `data/` directory

### 1) Primary outputs (required)
List the â€œcontract outputsâ€ that downstream systems should rely on.

- **`<dataset_slug>.<ext>`** â€” `TBD`: one-line description  
  - Rows/features: `TBD`  
  - Geometry: `TBD`  
  - CRS: `TBD`  
  - Expected consumer(s): `Graph ingest / API / UI / Analysis`

### 2) Supporting outputs (optional)
- `checksums.sha256` â€” integrity checks for reproducibility
- `<dataset_slug>_preview.png` â€” quick visual sanity check
- `<dataset_slug>_sample.<ext>` â€” small sample for tests/demos

---

## ğŸ§ª How to use / load the data

### Python ğŸ
```python
# Example: pick the right reader for your format
from pathlib import Path

p = Path("data/external/processed/<dataset_slug>/data/<dataset_slug>.<ext>")

# GeoPackage / GeoJSON
# import geopandas as gpd
# gdf = gpd.read_file(p)

# Parquet
# import pandas as pd
# df = pd.read_parquet(p)

print(p)
```

### R ğŸ“Š
```r
# sf for vectors, arrow for parquet, readr for csv
# library(sf)
# x <- st_read("data/external/processed/<dataset_slug>/data/<dataset_slug>.<ext>")
```

### CLI sanity checks ğŸ§°
```bash
# File size + hash
ls -lah data/external/processed/<dataset_slug>/data/
sha256sum data/external/processed/<dataset_slug>/data/<dataset_slug>.<ext> | head

# If vector:
# ogrinfo -so data/external/processed/<dataset_slug>/data/<dataset_slug>.gpkg <layer_name>
# If raster:
# gdalinfo data/external/processed/<dataset_slug>/data/<dataset_slug>.tif | head
```

---

## ğŸ§¾ Schema, semantics, and field definitions

> Treat this as the datasetâ€™s **contract**. Keep it stable; if you must break it, create a new version.

### Data dictionary (fill this in âœ…)

| Field | Type | Units | Description | Allowed values | Nullable | Notes |
|---|---|---:|---|---|:---:|---|
| `id` | string |  | Stable record identifier |  | âŒ | Prefer deterministic IDs |
| `source_id` | string |  | Upstream identifier |  | âœ… | Linkable to raw evidence |
| `name` | string |  | Human label |  | âœ… |  |
| `date_start` | date |  | Start date |  | âœ… | ISO-8601 |
| `date_end` | date |  | End date |  | âœ… | ISO-8601 |
| `geom` | geometry |  | Geometry (if spatial) |  | âœ… | If present, specify type/CRS |
| `confidence` | number |  | Uncertainty/confidence | 0â€“1 | âœ… | Explain scoring |

### Spatial specifics (if applicable)
- **Geometry type**: `TBD`
- **CRS**: `TBD`
- **Validity rules**: `TBD` (e.g., polygons must be valid; no self-intersections)
- **Precision / rounding**: `TBD`

### Temporal specifics (if applicable)
- **Time zone**: `UTC / local / TBD`
- **Granularity**: `year / month / day / timestamp`
- **Rules**: `date_end >= date_start`, missingness semantics, etc.

---

## ğŸ”— Required metadata â€œboundary artifactsâ€ (STAC / DCAT / PROV)

> These records are what make the dataset discoverable, traceable, and reproducible.

| Artifact | Status | Expected location (relative) | Notes |
|---|:---:|---|---|
| ğŸ›°ï¸ STAC Collection | â¬œ | `../../../../stac/collections/<dataset_slug>/collection.json` | Required even if â€œmostly non-spatialâ€ |
| ğŸ›°ï¸ STAC Item(s) | â¬œ | `../../../../stac/items/<dataset_slug>/` | Item assets must link to the files in this folder |
| ğŸ“š DCAT Dataset | â¬œ | `../../../../catalog/dcat/<dataset_slug>.jsonld` | Include license + distributions |
| ğŸ§¾ PROV bundle | â¬œ | `../../../../prov/<dataset_slug>/prov.json` | Must link raw â†’ work â†’ processed + run info |

âœ… **When updating this dataset**, update these artifacts in the same PR.

---

## ğŸ§¬ Provenance & processing notes

### Upstream sources (raw evidence)
- Source name: `TBD`
- Source URL(s): `TBD`
- Retrieval date(s): `TBD`
- Raw location: `data/external/raw/<dataset_slug>/...`

### Transformation summary (what changed from raw â†’ processed)
Keep this concise but explicit:

1. **Ingest**: `TBD` (download/unzip/parse)  
2. **Normalize**: `TBD` (types, columns, units, CRS)  
3. **Clean**: `TBD` (dedupe, missing values, snapping/repair)  
4. **Enrich**: `TBD` (joins, geocoding, derived attributes)  
5. **Export**: `TBD` (format + partitioning)  
6. **Catalog/Lineage**: updated STAC/DCAT/PROV (required)

### Reproducible run command
```bash
# Example patterns (pick one that matches your repo)
# python pipelines/<pipeline_name>.py --config pipelines/config/<dataset_slug>.yml
# make data-external-<dataset_slug>
# docker-compose exec api python pipelines/<pipeline_name>.py --config ...
```

---

## âœ… Validation & QA checklist

> Make it easy for future-you to verify quality quickly.

### Automated checks (recommended)
- [ ] Schema validation passes (`metadata/schema.json` or equivalent)
- [ ] Geometry validity checks (if spatial)
- [ ] Row counts match expectation (document deltas if not)
- [ ] No duplicate primary keys
- [ ] Checksums updated (`checksums.sha256`)
- [ ] STAC/DCAT/PROV updated and cross-linked

### Manual spot checks (recommended)
- [ ] Visual inspection in QGIS/Kepler/Notebook
- [ ] Outlier scan (bounds, null rates, crazy values)
- [ ] Compare sample against raw evidence for fidelity

---

## ğŸ§· Versioning & change policy

- **Version format**: `vX.Y.Z` or `YYYY-MM-DD` (choose and keep consistent)
- **When version changes**:
  - Patch: metadata fix / non-semantic change  
  - Minor: additive columns / additive features (backwards compatible)  
  - Major: breaking schema changes

> ğŸ§  Tip: if you reprocess or update, record linkage in **DCAT + PROV** so newer versions point back to older ones.

---

## âš–ï¸ License, attribution, and citation

### License
- Upstream license: `TBD`
- Redistribution allowed? `TBD`
- Required attribution text:  
  > `TBD`

### How to cite
```bibtex
@dataset{<dataset_slug>,
  title        = {TBD},
  author       = {TBD},
  year         = {TBD},
  version      = {TBD},
  publisher    = {Kansas Frontier Matrix (KFM)},
  note         = {Processed dataset + provenance in repository},
  url          = {TBD}
}
```

---

## ğŸ§­ Governance, FAIR/CARE & sensitivity (donâ€™t skip) ğŸŒ±

- **FAIR** (Findable, Accessible, Interoperable, Reusable): `TBD`
- **CARE / sovereignty considerations** (if applicable): `TBD`
- Contains personal/sensitive data? `Yes/No`  
  - If yes: document redaction/aggregation rules + access constraints.
- Indigenous data present or adjacent? `Yes/No`  
  - If yes: add governance notes and constraints clearly.

---

## ğŸ› Known issues / limitations

- `TBD` (e.g., gaps, uncertain geocodes, boundary mismatches, missing years)
- `TBD`

---

## ğŸ§¾ Changelog

| Date | Version | Summary | Author |
|---|---|---|---|
| YYYY-MM-DD | v0.1.0 | Initial processed release | @you |
| YYYY-MM-DD | v0.1.1 | Fix metadata / minor cleanup | @you |

---

## ğŸ‘¥ Maintainers

- Primary: `@TBD`
- Reviewer: `@TBD`
- Domain owner: `@TBD`

---

## ğŸ”— Related docs

- Dataset pipeline: `../../../../../pipelines/<pipeline_name>.py`
- Raw evidence: `../../../../raw/<dataset_slug>/`
- Work products: `../../../../work/<dataset_slug>/`
- STAC/DCAT/PROV profiles: `KFM_STAC_PROFILE.md`, `KFM_DCAT_PROFILE.md`, `KFM_PROV_PROFILE.md` (location may vary)

