---
title: "Validation Report â€” <dataset_slug>"
doc_kind: "dataset_validation_report"
dataset_slug: "<dataset_slug>"
dataset_title: "<Human-friendly dataset name>"
dataset_version: "<YYYY-MM-DD | semver | git tag>"
stage: "data/external/processed"
validation_status: "UNKNOWN" # PASS | WARN | FAIL
generated_at: "<YYYY-MM-DDTHH:MM:SSZ>"
generated_by: "<tool/script name + version>"
pipeline_ref: "<path/to/pipeline_or_notebook_or_make_target>"
git_commit: "<sha>"
license: "<SPDX | file ref>"
sensitivity: "<public|internal|confidential|restricted>"
---

# âœ… Validation Report â€” `<dataset_slug>` ğŸ§ª

![stage](https://img.shields.io/badge/stage-processed-blue)
![validation](https://img.shields.io/badge/validation-UNKNOWN-lightgrey)
![dataset](https://img.shields.io/badge/dataset-<dataset_slug>-informational)
![metadata](https://img.shields.io/badge/metadata-STAC%20%7C%20DCAT%20%7C%20PROV-lightgrey)

> [!IMPORTANT]
> This report is the **dataset-level validation contract artifact** for `<dataset_slug>`.  
> It is intended to be reviewable by humans âœ… and parseable by automation ğŸ¤–.

---

## ğŸ“Œ Quick Links

- ğŸ“¦ **Processed dataset folder:** `data/external/processed/<dataset_slug>/`
- ğŸ§¾ **This report:** `data/external/processed/<dataset_slug>/validation/validation_report.md`
- ğŸ§ª **Validation outputs:** `data/external/processed/<dataset_slug>/validation/`
- ğŸ—‚ï¸ **Metadata boundary artifacts (expected):**
  - ğŸŒ STAC: `data/stac/...`
  - ğŸ§­ DCAT: `data/catalog/dcat/...`
  - ğŸ§¬ PROV: `data/prov/...` (or `data/provenance/...` depending on repo convention)

---

## ğŸ§­ Directory Layout (what should exist) ğŸ—‚ï¸

```text
ğŸ“ data/
  ğŸ“ external/
    ğŸ“ processed/
      ğŸ“ <dataset_slug>/
        ğŸ“„ README.md                     ğŸ“ dataset runbook (recommended)
        ğŸ“„ <dataset_slug>.<ext>          ğŸ“¦ primary output (csv/parquet/geojson/geopackage/tif/etc)
        ğŸ“ validation/
          ğŸ“„ validation_report.md        âœ… you are here
          ğŸ“„ validation_summary.json     ğŸ¤– machine-readable results (recommended)
          ğŸ“„ validation_log.txt          ğŸªµ raw validator output (recommended)
          ğŸ“„ checksums.sha256            ğŸ” hashes for key outputs (recommended)
```

---

## ğŸ—ºï¸ Validation Flow (concept) ğŸ”

```mermaid
flowchart LR
  RAW[ğŸ“¥ Raw Inputs<br/>data/raw/...]
  WORK[ğŸ§° Work / Intermediate<br/>data/work/...]
  PROC[ğŸ“¦ Processed Output<br/>data/external/processed/&lt;dataset_slug&gt;]
  VAL[ğŸ§ª Validation Artifacts<br/>validation_report.md + summary.json]
  STAC[ğŸŒ STAC Records<br/>data/stac/...]
  DCAT[ğŸ§­ DCAT Record<br/>data/catalog/dcat/...]
  PROV[ğŸ§¬ PROV Lineage<br/>data/prov/...]
  CI[âœ… CI / Policy Gates]
  DOWN[ğŸš€ Downstream<br/>DB â€¢ Graph â€¢ API â€¢ UI]

  RAW --> WORK --> PROC --> VAL
  PROC --> STAC --> CI
  PROC --> DCAT --> CI
  PROC --> PROV --> CI
  CI --> DOWN
```

---

## ğŸ§¾ Dataset Identity

| Field | Value |
|---|---|
| **Dataset slug** | `<dataset_slug>` |
| **Title** | `<Human-friendly dataset name>` |
| **Domain / Module** | `<e.g., treaties | census | railroads | ecology | archaeology>` |
| **Version** | `<YYYY-MM-DD or semver>` |
| **Primary steward** | `<team/person>` |
| **License** | `<SPDX or ref>` |
| **Sensitivity** | `<public|internal|confidential|restricted>` |
| **Intended downstream use** | `<graph load | API exposure | UI layer | analysis only>` |

---

## âœ… Definition of Done Checklist (merge-ready)

> [!TIP]
> Treat this as the â€œgreen lightâ€ checklist before promotion to DB/Graph/API/UI.

- [ ] **Processed output(s) exist** under `data/external/processed/<dataset_slug>/`
- [ ] **Validation completed** and `validation_status` is **PASS** (or WARN with approved waiver)
- [ ] **STAC record(s)** exist and link to the output asset(s)
- [ ] **DCAT entry** exists (with license + distribution links)
- [ ] **PROV bundle** exists and traces inputs â†’ transforms â†’ outputs
- [ ] **Policy checks** satisfied (license, required metadata fields, sensitivity rules)
- [ ] **Repro steps** documented (how to regenerate the dataset)

---

## ğŸ§ª Validation Summary

| Category | Result | Notes |
|---|---:|---|
| Overall status | `UNKNOWN` | `PASS / WARN / FAIL` |
| Run timestamp | `<YYYY-MM-DDTHH:MM:SSZ>` |  |
| Validator | `<tool/script + version>` |  |
| Pipeline ref | `<path>` |  |
| Git commit | `<sha>` |  |

### ğŸ” High-level Metrics (fill in)

| Metric | Value |
|---|---:|
| Record count | `<n>` |
| File size | `<n MB>` |
| Primary key field | `<field_name>` |
| Duplicate primary keys | `<n>` |
| Null rate (key fields) | `<%>` |
| Date/time coverage | `<start â†’ end>` |
| Geometry type(s) | `<Point/LineString/Polygon/... or N/A>` |
| CRS / EPSG | `<EPSG:4326 etc or N/A>` |
| Spatial extent (bbox) | `<minx, miny, maxx, maxy or N/A>` |

---

## ğŸ“¦ Artifact Inventory

### 1) Data outputs (processed)

List the authoritative output assets produced by the pipeline.

- ğŸ“„ `data/external/processed/<dataset_slug>/<file_1>`
- ğŸ“„ `data/external/processed/<dataset_slug>/<file_2>`
- ğŸ§¾ (optional) `data/external/processed/<dataset_slug>/<data_dictionary_or_schema_doc>`

### 2) Validation artifacts

- âœ… `data/external/processed/<dataset_slug>/validation/validation_report.md`
- ğŸ¤– `data/external/processed/<dataset_slug>/validation/validation_summary.json`
- ğŸªµ `data/external/processed/<dataset_slug>/validation/validation_log.txt`
- ğŸ” `data/external/processed/<dataset_slug>/validation/checksums.sha256`

### 3) Metadata â€œboundary artifactsâ€ (expected)

> [!NOTE]
> If these are missing, **treat as FAIL** unless a maintainer-approved waiver exists.

- ğŸŒ STAC Collection: `data/stac/collections/<collection_id>.json`
- ğŸŒ STAC Item(s): `data/stac/items/<item_id>.json` (or equivalent structure)
- ğŸ§­ DCAT Dataset: `data/catalog/dcat/<dataset_id>.{json|ttl}`
- ğŸ§¬ PROV: `data/prov/<dataset_slug>.prov.{json|ttl}` (or repo-defined location)

---

## ğŸ§± Validation Checks Matrix

> [!IMPORTANT]
> Mark each check as âœ… PASS, âš ï¸ WARN, or âŒ FAIL. Add evidence paths for reproducibility.

### A) File & Format Integrity ğŸ§°

| Check | Status | Evidence | Notes |
|---|---|---|---|
| Files exist & readable | â³ | `validation_log.txt` |  |
| Non-empty dataset | â³ |  |  |
| Encoding / delimiter sane (tabular) | â³ |  |  |
| GeoJSON/GeoPackage parses (spatial) | â³ |  |  |
| Raster opens (raster) | â³ |  |  |
| Checksums recorded | â³ | `checksums.sha256` |  |

### B) Schema & Contract Validation ğŸ§¾

| Check | Status | Evidence | Notes |
|---|---|---|---|
| Schema file present (if used) | â³ | `<path/to/schema>` |  |
| Column presence matches contract | â³ |  |  |
| Data types valid / coercible | â³ |  |  |
| Required fields non-null | â³ |  |  |
| Enumerations / controlled vocab valid | â³ |  |  |

### C) Record-Level Quality ğŸ¯

| Check | Status | Evidence | Notes |
|---|---|---|---|
| Primary key uniqueness | â³ |  |  |
| Duplicate record detection | â³ |  |  |
| Range checks (numeric) | â³ |  | `<min/max thresholds>` |
| Pattern checks (IDs, codes) | â³ |  |  |
| Outlier scan (basic) | â³ |  |  |

### D) Temporal Validity â±ï¸

| Check | Status | Evidence | Notes |
|---|---|---|---|
| Date fields parse correctly | â³ |  |  |
| Time range plausible for domain | â³ |  |  |
| No impossible intervals (end < start) | â³ |  |  |
| Timezone handling documented | â³ |  |  |

### E) Spatial Validity ğŸŒ (skip if non-spatial)

| Check | Status | Evidence | Notes |
|---|---|---|---|
| CRS recorded and consistent | â³ |  |  |
| Geometry validity (self-intersections, etc.) | â³ |  |  |
| Coordinates within expected bounds | â³ |  | e.g., Kansas bbox |
| No empty / null geometries (if required) | â³ |  |  |
| Topology sanity (if polygons/lines) | â³ |  | overlaps, slivers, dangles |

### F) Referential Integrity ğŸ”— (if applicable)

| Check | Status | Evidence | Notes |
|---|---|---|---|
| Foreign keys resolve | â³ |  |  |
| Joins to canonical IDs work | â³ |  |  |
| Controlled vocab references valid | â³ |  |  |

---

## ğŸŒ Metadata & Cross-Layer Integrity

> [!IMPORTANT]
> This section ensures catalogs, provenance, and downstream representations remain synchronized.

### 1) STAC âœ…

- [ ] STAC Collection exists
- [ ] STAC Items exist (for each asset / partition)
- [ ] Each STAC Item links to the **actual** processed asset path
- [ ] Spatial/temporal extent present and matches data

**Evidence:**  
- STAC Collection: `<path>`  
- STAC Items: `<path(s)>`

### 2) DCAT âœ…

- [ ] DCAT Dataset entry exists
- [ ] License field present and valid
- [ ] Distribution links point to STAC and/or direct asset download

**Evidence:**  
- DCAT record: `<path>`

### 3) PROV âœ…

- [ ] PROV bundle exists
- [ ] Lineage includes **raw inputs â†’ work steps â†’ processed outputs**
- [ ] Includes pipeline identity (run id / config / commit)

**Evidence:**  
- PROV bundle: `<path>`

### 4) Policy / Governance Gate âš–ï¸ (FAIR+CARE)

- [ ] License present (no â€œunknownâ€)
- [ ] Citation/source attribution present
- [ ] Sensitivity classification is set correctly
- [ ] Any restricted fields are redacted/aggregated as required

**Evidence:**  
- Policy outputs / CI logs: `<path or link>`

---

## ğŸš¦CI Gate Summary (what should be green)

| Gate | Status | Notes |
|---|---|---|
| Lint / formatting (if applied) | â³ |  |
| Policy checks (license/metadata) | â³ |  |
| Metadata schema validation (STAC/DCAT/PROV) | â³ |  |
| Data validation (this report) | â³ |  |
| Link integrity (no broken refs) | â³ |  |

---

## ğŸ§¯ Findings & Issues Log

> [!TIP]
> Keep the log crisp: one issue per bullet, include impact + fix.

### âŒ Blockers (must fix before merge)

- `[#] <title>` â€” **Impact:** <what breaks> â€” **Fix:** <what to do> â€” **Evidence:** <path/log>

### âš ï¸ Warnings (allowed only with waiver)

- `[#] <title>` â€” **Risk:** <risk> â€” **Mitigation:** <mitigation> â€” **Evidence:** <path/log>

### â„¹ï¸ Notes (informational)

- `[#] <note>`

---

## ğŸ› ï¸ Remediation Plan (Action Items)

- [ ] Fix blockers
- [ ] Regenerate outputs
- [ ] Re-run validation (update `generated_at`, `git_commit`, `validation_status`)
- [ ] Confirm STAC/DCAT/PROV alignment updated
- [ ] Re-check policy gates (license + sensitivity)

---

## â™»ï¸ Reproducibility (How to regenerate)

### Environment

- OS: `<...>`
- Python/R/Node: `<...>`
- Key deps: `<...>`
- Docker/Compose: `<...>` (if used)

### Commands (examples â€” replace with real ones)

```bash
# 1) Run pipeline
python <path/to/pipeline>.py --dataset <dataset_slug> --out data/external/processed/<dataset_slug>/

# 2) Run validation
python <path/to/validator>.py --dataset <dataset_slug> --report data/external/processed/<dataset_slug>/validation/

# 3) (Optional) Validate STAC/DCAT/PROV
python <path/to/metadata_validator>.py --dataset <dataset_slug>
```

### Determinism notes ğŸ§Š

- Inputs pinned: `<checksums / source version / download URL>`
- Non-deterministic steps (if any): `<describe + how handled>`
- Known diffs allowed: `<explain>`

---

## ğŸ“ Appendix A â€” Profiling Snapshot (optional)

<details>
<summary>Click to expand ğŸ“Š</summary>

### Column-level stats (tabular)

- `<field>`: min `<...>`, max `<...>`, null `<...%>`, distinct `<...>`

### Geometry stats (spatial)

- Valid: `<n>` / Invalid: `<n>`
- Empty geometries: `<n>`
- Geometry types distribution: `<...>`

</details>

---

## ğŸ“ Appendix B â€” Checksums (optional) ğŸ”

> Keep this aligned with `checksums.sha256`.

```text
<sha256>  data/external/processed/<dataset_slug>/<file_1>
<sha256>  data/external/processed/<dataset_slug>/<file_2>
```

---

## âœï¸ Sign-off âœ…

| Role | Name | Date | Decision |
|---|---|---:|---|
| Data Steward | `<name>` | `<YYYY-MM-DD>` | âœ… Approve / âŒ Reject |
| Maintainer | `<name>` | `<YYYY-MM-DD>` | âœ… Approve / âŒ Reject |
| Governance (if needed) | `<name>` | `<YYYY-MM-DD>` | âœ… Approve / âŒ Reject |

---

## ğŸ•°ï¸ Report Changelog

- `<YYYY-MM-DD>` â€” Created report template for `<dataset_slug>` (status: `UNKNOWN`)
- `<YYYY-MM-DD>` â€” Updated after validation run (status: `PASS/WARN/FAIL`)

