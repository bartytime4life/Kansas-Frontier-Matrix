# ğŸ“’ CHANGELOG â€” `<dataset_slug>`

![dataset](https://img.shields.io/badge/dataset-%3Cdataset_slug%3E-blue)
![stage](https://img.shields.io/badge/stage-processed-success)
![versioning](https://img.shields.io/badge/versioning-SemVer-informational)
![provenance](https://img.shields.io/badge/provenance-PROV%20required-important)
![license](https://img.shields.io/badge/license-TBD-lightgrey)

> âœ… This file records **what changed, when, and why** for the processed dataset in this folder.  
> ğŸ” Goal: anyone can **reproduce, audit, and diff** the exact outputs for any version.

---

## ğŸ“˜ Overview

| Item | Value |
|---|---|
| Dataset name | `<dataset_name>` |
| Dataset slug | `<dataset_slug>` |
| Domain | `<domain>` |
| Dataset type | `<source / derived / AI-evidence>` |
| Primary maintainer | `<name / team>` |
| Source(s) | `<url(s) / citation(s) / internal reference IDs>` |
| Output format(s) | `<GeoJSON / Parquet / CSV / GPKG / TIFF / etc.>` |
| CRS / projection | `<EPSG:####>` |
| Spatial extent | `<bbox / region>` |
| Temporal extent | `<YYYY-MM-DD â€¦ YYYY-MM-DD>` |
| PII / sensitivity | `<public / restricted / tribal sensitive / etc.>` |

---

## ğŸ—‚ï¸ Directory + artifact expectations

This dataset lives in:

- `data/external/processed/<dataset_slug>/` âœ… (this folder)

Recommended staging mirrors the standard pipeline layout (raw â†’ work â†’ processed):

```text
data/
  external/
    raw/<dataset_slug>/            # immutable source drops (never overwrite)
    work/<dataset_slug>/           # intermediate transforms / scratch
    processed/<dataset_slug>/      # publish-ready outputs âœ…
      CHANGELOG.md
      README.md                    # dataset runbook + field dictionary
      outputs/                     # final data artifacts (files)
      manifests/                   # optional: checksums, row counts, schema snapshots
```

### ğŸ§­ â€œTruth pathâ€ (how this dataset flows downstream)

```mermaid
flowchart LR
  A["Raw\n(data/external/raw/<dataset_slug>)"] --> B["Work\n(data/external/work/<dataset_slug>)"]
  B --> C["Processed\n(data/external/processed/<dataset_slug>)"]
  C --> D["Boundary artifacts\n(STAC + DCAT + PROV)"]
  D --> E["DB / Graph ingest\n(PostGIS / Neo4j)"]
  E --> F["API layer\n(contracts + redaction)"]
  F --> G["UI / Story nodes\n(Map + narrative)"]
```

### ğŸ“¦ Boundary artifacts (publish gate)

Before a version is considered â€œpublishedâ€, ensure metadata + lineage are updated:

- `data/stac/collections/â€¦` and `data/stac/items/â€¦`
- `data/catalog/dcat/â€¦`
- `data/prov/â€¦` (or `data/provenance/â€¦` depending on repo convention)

> ğŸ§© If this dataset is AI/analysis-generated, it is still treated as a **first-class** dataset: store outputs in `processed/â€¦` and include full provenance.

---

## ğŸ§¾ Changelog rules

Create a new entry whenever any of these change:

- âœ… **Inputs**: new/updated raw source, upstream revision, new download date
- âœ… **Transform code**: ETL logic, cleaning rules, joins, filters, deduping, geocoding
- âœ… **Schema**: columns/fields, types, units, enumerations, IDs, geometry type
- âœ… **Semantics**: meaning of a field, interpretation rules, thresholds/classifications
- âœ… **Outputs**: file set, formats, compression, tiling, partitioning, filenames
- âœ… **Quality**: missing/wrong values fixed, geometry repairs, outlier handling
- âœ… **Provenance/metadata**: STAC/DCAT/PROV updates, licensing, sensitivity flags
- âœ… **Delivery**: moved to Git LFS, external pointer + checksum, fetch script changes

### ğŸ”– Versioning policy (SemVer)

- **MAJOR** (`X.0.0`): breaking change for consumers (schema/IDs/units/CRS/meaning)
- **MINOR** (`0.Y.0`): backward-compatible additions (new optional fields, new coverage)
- **PATCH** (`0.0.Z`): corrections that donâ€™t break schema (fix values, typos, duplicates)

---

## âœ… Release checklist

- [ ] Outputs updated in `outputs/` (or pointers + checksums if stored externally)
- [ ] Dataset **README / data dictionary** updated
- [ ] **STAC** collection/items updated
- [ ] **DCAT** record updated
- [ ] **PROV** lineage bundle updated
- [ ] Row/feature counts recorded
- [ ] Checksums recorded (or regenerated)
- [ ] Pipeline script + commit SHA + run ID recorded
- [ ] Consumer impact assessed (breaking vs non-breaking)

---

## [Unreleased]

### Added
- _TBD_

### Changed
- _TBD_

### Fixed
- _TBD_

### Removed
- _TBD_

---

## [0.0.1] - 2026-01-29

### Added
- ğŸ§± Created dataset-level `CHANGELOG.md` template (this file).

---

<details>
<summary>ğŸ§© Changelog entry template (copy/paste)</summary>

```markdown
## [<version>] - <YYYY-MM-DD>

### ğŸ”— Reproducibility
- Pipeline: `<path/to/pipeline_script.py>` (commit: `<sha>`)
- Run ID / job URL: `<id or link>`
- Inputs: `<raw drop name + date + checksum>`
- Outputs: `<outputs list + checksums>`

### Added
- ...

### Changed
- ...

### Fixed
- ...

### Removed
- ...

### ğŸ“Š Quick stats (recommended)
| Metric | Before | After |
|---|---:|---:|
| Rows / features | `<n>` | `<n>` |
| Unique IDs | `<n>` | `<n>` |
| Missing geometry | `<n>` | `<n>` |
| CRS | `<epsg>` | `<epsg>` |

### ğŸ§¾ Notes
- Backwards compatibility: `<yes/no>`
- Breaking changes: `<yes/no>`
- Known issues: `<...>`
```

</details>

