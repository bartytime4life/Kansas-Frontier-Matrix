# `data/external/processed/` ğŸ§³ğŸ§ªğŸ“¦

![data stage](https://img.shields.io/badge/data%20stage-external%2Fprocessed-blue)
![status](https://img.shields.io/badge/status-staging%20%2F%20pre--publish-yellow)
![pipeline](https://img.shields.io/badge/pipeline-deterministic%20ETL-success)
![metadata](https://img.shields.io/badge/metadata-STAC%20%7C%20DCAT%20%7C%20PROV-informational)
![governance](https://img.shields.io/badge/governance-FAIR%20%2B%20CARE%20aware-purple)

> [!NOTE]
> This folder is a **staging area** for **externally sourced datasets that have already been processed** (cleaned/normalized), but are **not yet â€œpublishedâ€ as canonical KFM datasets**.
>
> âœ… Use it for: evaluation, restricted-license, too-large-to-commit, or â€œnot-ready-to-promoteâ€ outputs.  
> ğŸš€ When a dataset is ready to power **graph â†’ API â†’ UI**, it should be promoted to the canonical pipeline outputs and metadata.

---

<details>
  <summary><strong>ğŸ“š Table of Contents</strong></summary>

- [What belongs here](#what-belongs-here-)
- [What does not belong here](#what-does-not-belong-here-)
- [How this fits into the KFM pipeline](#how-this-fits-into-the-kfm-pipeline-)
- [Recommended folder layout](#recommended-folder-layout-)
- [File formats & conventions](#file-formats--conventions-)
- [Reproducibility rules](#reproducibility-rules-)
- [Sidecar metadata (required for anything non-trivial)](#sidecar-metadata-required-for-anything-non-trivial-)
- [Large / restricted datasets](#large--restricted-datasets-)
- [Promotion checklist](#promotion-checklist-)
- [Related docs](#related-docs-)
</details>

---

## What belongs here âœ…

Store **processed external outputs** that are:

- ğŸ§¼ **Cleaned + standardized** (schemas normalized, columns named, units consistent, CRS documented)
- ğŸ§ª **Experimental** outputs used during exploration/validation
- ğŸ”’ **License-restricted** or **sensitive** derivatives that canâ€™t be published directly
- ğŸ§± **Large** outputs that shouldnâ€™t be committed as raw binaries (use pointers + checksums)

Examples:
- A â€œtrialâ€ GeoParquet built from a third-party shapefile
- A temporary raster mosaic built for QA
- A cleaned CSV extract youâ€™re validating before formal ingestion
- A pre-redaction version used only by maintainers

---

## What does not belong here âŒ

- ğŸ§Š **Raw downloads / originalsWS / originals** â†’ `data/external/raw/` (or canonical `data/raw/<domain>/`)
- ğŸ§° **Intermediate scratch outputs** â†’ `data/external/work/` (or canonical `data/work/<domain>/`)
- âœ… **Canonical published outputs** consumed by KFM catalogs/graph/UI â†’ `data/processed/<domain>/`
- ğŸ”‘ Secrets, tokens, credentials (ever) ğŸ˜…

> [!IMPORTANT]
> Anything referenced by the **catalogs (STAC/DCAT/PROV)** should point to **stable outputs** (typically `data/processed/**` or equivalent stable storage), not ephemeral experiments.

---

## How this fits into the KFM pipeline ğŸ§­

```mermaid
flowchart LR
  subgraph EXT["ğŸ§³ External staging (local/restricted/experimental)"]
    ER["data/external/raw/"] --> EW["data/external/work/"]
    EW --> EP["data/external/processed/"]
  end

  EP -->|âœ… Promote when ready| CP["data/processed/<domain>/"]

  CP --> STAC["data/stac/ (Collections + Items)"]
  CP --> DCAT["data/catalog/dcat/"]
  CP --> PROV["data/prov/"]

  STAC --> G["Neo4j Graph (refs catalogs)"]
  DCAT --> G
  PROV --> G
  G --> API["API Layer (contracts + redaction)"]
  API --> UI["Map UI + Story Nodes"]
```

**Rule of thumb:**  
If you want it to become **evidence** that can power **graph/API/UI**, it needs to graduate out of `data/external/processed/` and into the canonical publishing flow.

---

## Recommended folder layout ğŸ“

Keep the structure predictable so future you (and CI) can reason about it:

```text
ğŸ“ data/external/processed/
  ğŸ“ <domain>/                     # e.g., historical, environmental, transportation
    ğŸ“ <dataset_slug>/              # short + stable identifier (snake_case)
      ğŸ“ vYYYYMMDD/                 # date-based version OR vX.Y.Z
        ğŸ“„ <dataset_slug>.<ext>     # primary artifact
        ğŸ“„ checksums.sha256         # sha256 for every artifact (incl. pointers)
        ğŸ“„ dataset.meta.yaml        # required sidecar (see template below)
        ğŸ“„ README.md                # optional dataset-level runbook
```

**Versioning tips ğŸ·ï¸**
- Prefer `vYYYYMMDD` for â€œsnapshotâ€ data pulls.
- Prefer semantic `vX.Y.Z` for curated releases you intend to promote.

---

## File formats & conventions ğŸ§¾

Use formats that support **reproducibility + long-term stability**:

### âœ… Preferred
- **Vector**: GeoPackage (`.gpkg`), GeoParquet (`.parquet`), FlatGeobuf (`.fgb`)
- **Raster**: Cloud-Optimized GeoTIFF (`.tif` / COG), GeoTIFF
- **Tabular**: Parquet (`.parquet`), CSV (`.csv`) for small/portable exports
- **Docs**: Markdown (`.md`), JSON (`.json`), JSON-LD (`.jsonld`), YAML (`.yml/.yaml`)

### âš ï¸ Allowed, but avoid if you can
- Shapefile (legacy multi-file complexity)
- GeoJSON for large vectors (size + performance)

### ğŸš« Avoid
- Pickle/joblib blobs as â€œdataâ€ (not portable, security risk)
- Undocumented proprietary formats

**Naming conventions**
- `snake_case` file names
- No spaces
- If multiple outputs: suffix with intent (e.g., `_qa`, `_clipped`, `_redacted`, `_tileindex`)

---

## Reproducibility rules â™»ï¸

Treat processed outputs as *regenerable artifacts*, not hand-crafted files:

- ğŸ” **Idempotent builds**: re-running the pipeline with the same inputs/config should yield the same outputs
- ğŸ§¾ **Fully logged**: record inputs, parameters, tool versions, and commit hash (in `dataset.meta.yaml`)
- âœ‹ **No manual edits** of processed outputs â€” fix the pipeline, then rebuild
- ğŸ§  **AI/analysis outputs** should be treated as evidence artifacts with full provenance if promoted (not â€œmagic filesâ€)

---

## Sidecar metadata (required for anything non-trivial) ğŸ—‚ï¸

Every dataset folder should include a lightweight **sidecar metadata** file:

**`dataset.meta.yaml` (template)**
```yaml
id: <dataset_slug>
title: "<Human friendly title>"
domain: <domain>

source:
  name: "<Provider / archive / agency>"
  url: "<Where it came from>"
  retrieved_at: "YYYY-MM-DD"
  license: "<SPDX id or human-readable license>"
  notes: "<Any restrictions or attribution requirements>"

processing:
  stage: "external/processed"
  pipeline:
    entrypoint: "pipelines/<script_or_notebook>"
    config: "configs/<config_file_if_any>"
    git_commit: "<commit_sha>"
  inputs:
    - "data/external/raw/<domain>/<...>"
  outputs:
    - "data/external/processed/<domain>/<dataset_slug>/<version>/<file>"
  checksums_file: "checksums.sha256"

data_characteristics:
  type: "vector|raster|tabular|document"
  crs: "EPSG:4326"       # or specify actual CRS + why
  temporal: "<time range or null>"
  spatial: "<bbox or description>"
  sensitivity: "public|internal|restricted"
  redaction:
    applied: false
    method: null
    notes: null

quality:
  validation:
    - "<what was checked>"
  known_issues:
    - "<edge cases, missing fields, etc.>"
```

> [!TIP]
> Even if you *donâ€™t* publish this dataset, the sidecar metadata makes the work auditable and prevents â€œmystery filesâ€ from piling up.

---

## Large / restricted datasets ğŸ§±ğŸ”’

If the dataset is too large to commit or license-restricted:

- âœ… Commit **metadata + checksums + pointer**, not the full binary
- âœ… Include a small **`FETCHING.md`** or **`SOURCE.txt`** describing how maintainers can obtain it
- âœ… Prefer stable external storage (object store, release artifact, institutional archive)
- âœ… Ensure the pointer references an **immutable version** (hash, DOI, or versioned URL)

**Example pointer file ideas**
- `external.uri.json` (contains URL + checksum + expected filename)
- `FETCH.sh` (simple fetch script; no secrets; deterministic)
- Git LFS pointer (when allowed)

---

## Promotion checklist ğŸš€

Before moving anything into canonical `data/processed/<domain>/` and exposing via catalogs:

- [ ] âœ… License confirmed and compatible (and recorded)
- [ ] âœ… Sensitivity reviewed (FAIR+CARE concerns considered)
- [ ] âœ… Output schema stable + documented
- [ ] âœ… CRS and units documented and standardized
- [ ] âœ… Deterministic build (re-run produces same checksums)
- [ ] âœ… STAC + DCAT + PROV created/updated
- [ ] âœ… Domain runbook updated (`docs/data/<domain>/README.md`)
- [ ] âœ… If data could be sensitive: redaction/generalization applied **before** publish

> [!WARNING]
> If a dataset includes sensitive locations (e.g., archaeological sites) or sovereignty-sensitive data, treat it as **restricted by default** and route through governance + redaction rules before publication.

---

## Related docs ğŸ“šğŸ”—

- ğŸ“˜ `docs/MASTER_GUIDE_v13.md` â€” canonical pipeline ordering + contracts
- ğŸ§© `docs/standards/` â€” KFM profiles (STAC/DCAT/PROV) + validation expectations
- ğŸ—ºï¸ `docs/data/<domain>/README.md` â€” domain-level ETL runbook (sources, transforms, gotchas)
- ğŸ§ª `src/pipelines/` or `pipelines/` â€” dataset import + normalization entrypoints (project-dependent)

---

ğŸ§  **Design intent (in one sentence):**  
`data/external/processed/` exists so we can iterate quickly on external inputs while keeping the canonical, provenance-driven KFM publication flow clean and governed. âœ…

