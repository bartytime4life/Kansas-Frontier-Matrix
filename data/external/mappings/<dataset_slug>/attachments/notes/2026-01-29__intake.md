---
title: "ğŸ“¥ Intake â€” External Dataset Mapping: <dataset_slug>"
date: "2026-01-29"
last_updated: "2026-01-29"
status: "draft"
stage: "intake"
dataset_slug: "<dataset_slug>"
domain: "<domain>"
owners:
  - "<name_or_handle>"
source:
  name: "<publisher / archive / institution>"
  type: "<raster | vector | tabular | mixed>"
  access: "<download | api | scan | scrape | manual>"
  url: "<source_url>"
license:
  name: "<license_name>"
  url: "<license_url_or_terms>"
  redistribution: "<allowed | restricted | unknown>"
sensitivity:
  pii: "<none | possible | confirmed>"
  redaction_required: "<yes | no | unknown>"
kfm:
  canonical_entities: ["<entity_type_1>", "<entity_type_2>"]
  target_outputs: ["<GeoJSON | COG | Parquet | CSV | other>"]
---

# ğŸ“¥ Intake â€” External Dataset Mapping: `<dataset_slug>`

![Status](https://img.shields.io/badge/status-draft-yellow)
![Stage](https://img.shields.io/badge/pipeline-intake-blue)
![Date](https://img.shields.io/badge/date-2026--01--29-6c757d)

> [!NOTE]
> This intake note lives at:
>
> `data/external/mappings/<dataset_slug>/attachments/notes/2026-01-29__intake.md`
>
> The goal: capture **source facts**, **rights/licensing**, **mapping decisions**, and a **publishable plan** (so we can move cleanly through Raw â†’ Processed â†’ Catalog/Prov â†’ DB â†’ API â†’ UI). âœ…

---

## ğŸ§­ TL;DR (fill this first)

| Field | Value |
|---|---|
| Dataset (human name) | `<dataset_name>` |
| Slug | `<dataset_slug>` |
| Domain (where it lands) | `<domain>` |
| Coverage | `<Kansas statewide | county | corridor | site | multi-state>` |
| Time range | `<YYYYâ€“YYYY>` |
| Data type | `<vector/raster/tabular/mixed>` |
| Geometry | `<points/lines/polygons/raster>` |
| License | `<license_name>` |
| Intended use in KFM | `<map layer | story node evidence | reference table | graph enrichment>` |

---

## ğŸ¯ Intake Goals

- [ ] Identify **authoritative source** and retrieval method ğŸ§¾
- [ ] Confirm **license/rights** and redistribution permission âš–ï¸
- [ ] Capture **spatial + temporal characteristics** ğŸ—ºï¸â³
- [ ] Define **schema/field mapping** to canonical entities ğŸ§¬
- [ ] Specify **deterministic pipeline plan** (no manual steps in â€œofficialâ€ runs) ğŸ¤–
- [ ] List required **boundary artifacts** (catalog + provenance) ğŸ“šğŸ§¾

---

## ğŸ“¦ Expected Repo Layout (attachments + lifecycle)

### ğŸ“ Mapping Workspace (this dataset)
```text
ğŸ“ data/
  ğŸ“ external/
    ğŸ“ mappings/
      ğŸ“ <dataset_slug>/
        ğŸ“ attachments/
          ğŸ“ raw/                 # original downloads / scans (as-received)
          ğŸ“ license/             # license text, ToS screenshots, emails
          ğŸ“ notes/               # intake + decision logs (this file lives here)
          ğŸ“ qa/                  # QA exports, screenshots, validation outputs
          ğŸ“ correspondence/      # optional: emails/letters granting permission
          ğŸ“ excerpts/            # optional: small excerpts used for testing
          ğŸ“„ README.md            # optional: mapping-specific readme
        ğŸ“„ mapping.yaml           # optional: formal field mapping spec
        ğŸ“„ transform.md           # optional: transform rules + edge cases
```

### ğŸ—ï¸ Data Lifecycle Targets (where it eventually goes)
```text
ğŸ“ data/
  ğŸ“ raw/<domain>/                # immutable source snapshots
  ğŸ“ work/<domain>/               # intermediate processing
  ğŸ“ processed/<domain>/          # publish-ready outputs
  ğŸ“ stac/collections/            # STAC collections (if used here)
  ğŸ“ stac/items/                  # STAC items (if used here)
  ğŸ“ catalog/dcat/                # DCAT dataset entries (if used here)
  ğŸ“ prov/                        # PROV lineage bundles (if used here)
```

---

## ğŸ” Source & Acquisition

### Source identity
- **Publisher / owner**: `<publisher>`
- **Citation (preferred)**: `<formal citation string>`
- **Primary URL**: `<source_url>`
- **Secondary mirrors**: `<mirror_urls_or_none>`

### Acquisition log
- **Method**: `<download | API | scrape | scan | manual request>`
- **Retrieved by**: `<name_or_handle>`
- **Retrieved on**: `2026-01-29`
- **Auth required**: `<yes/no>`
- **Checksum strategy**: `<sha256 for each raw file | signed archive | none>`

> [!TIP]
> Attach the **exact raw artifact** you downloaded/received (zip, pdf, tif, csv) under `attachments/raw/` and do not modify it. Any edits happen downstream.

---

## âš–ï¸ License, Rights, & Ethics (FAIR + CARE)

### License summary
- **License name**: `<license>`
- **License URL / terms**: `<url_or_path_in_attachments/license/>`
- **Redistribution**: `<allowed | restricted | unknown>`
- **Attribution requirements**: `<yes/no/details>`
- **Share-alike / copyleft**: `<yes/no/details>`
- **Commercial use**: `<allowed/restricted/unknown>`

### Sensitive content check
- **PII present?** `<none | possible | confirmed>`
- **Indigenous/tribal or culturally sensitive data?** `<yes/no/unknown>`
- **Redaction required before publishing?** `<yes/no/unknown>`
- **Notes**: `<constraints, community requests, access limits>`

> [!IMPORTANT]
> If licensing is unclear, this dataset must not advance beyond â€œraw + notesâ€ until clarified. Add evidence (screenshots / emails) into `attachments/license/`.

---

## ğŸ—ºï¸ Spatial Characteristics

| Field | Value |
|---|---|
| Geometry type | `<point/line/polygon/raster>` |
| CRS as delivered | `<EPSG:#### | WKT | â€œunknownâ€>` |
| Spatial extent | `<bbox or description>` |
| Resolution / scale | `<e.g., 30m; 1:24,000; parcel-level; etc>` |
| Positional accuracy | `<stated accuracy or inferred>` |
| Topology expectations | `<must not self-intersect; boundaries snap; etc>` |

### CRS / projection notes
- Metadata file(s) that specify CRS: `<path(s) under attachments/raw/>`
- If unknown: plan to infer via `<prj file | accompanying documentation | control points | provider contact>`

### Recommended target CRS (for publish-ready outputs)
- **Web-ready**: `<WGS84 lon/lat for GeoJSON or API outputs>`
- **Analysis-ready**: `<Kansas-appropriate projected CRS if needed>`

---

## â³ Temporal Characteristics

| Field | Value |
|---|---|
| Time coverage | `<YYYYâ€“YYYY>` |
| Temporal granularity | `<day/month/year/era/unknown>` |
| Time field(s) | `<source field names>` |
| Calendar / dating quirks | `<Julian/Gregorian, â€œcircaâ€, ambiguous years, etc>` |
| Uncertainty model | `<exact | range | fuzzy | needs design>` |

---

## ğŸ§¬ Canonical Mapping Plan (Source â†’ KFM)

### 1) What does this dataset represent?
- **Primary entities**: `<e.g., LandParcel, HistoricalEvent, SurveyRecord, RouteSegment, TreatyBoundary>`
- **Relationships**: `<e.g., eventâ†’place; parcelâ†’owner; routeâ†’town>`
- **Evidence artifacts**: `<scanned map, transcription, OCR table, etc>`

### 2) Field mapping (draft)

<details>
<summary>ğŸ§¾ Draft Field Mapping Table (expand)</summary>

| Source field | Type | Example | Canonical field | Transform | Notes |
|---|---:|---|---|---|---|
| `<src_field_1>` | `<str/int/float/date>` | `<example>` | `<canon_field_1>` | `<trim/parse/join/reproject>` | `<edge cases>` |
| `<src_field_2>` |  |  |  |  |  |
| `<src_field_3>` |  |  |  |  |  |

</details>

### 3) ID strategy (must be stable)
- **Source primary key**: `<field or none>`
- **Canonical ID**: `<uuidv5(namespace, source_pk) | hash | other>`
- **Dedup rules**: `<by geometry + name + date window, etc>`

### 4) Normalization rules
- Names: `<casefolding, punctuation, alias tables>`
- Units: `<milesâ†’meters, acresâ†’hectares, etc>`
- Dates: `<YYYY vs YYYY-MM-DD; approximate dates>`
- Geometry: `<snap tolerance; simplify; dissolve; multipart handling>`

---

## ğŸ” Pipeline Plan (deterministic + reviewable)

```mermaid
flowchart LR
  A["ğŸ“¦ Raw (immutable)"] --> B["ğŸ§¹ Work (clean/normalize)"]
  B --> C["âœ… Processed (publish-ready)"]
  C --> D["ğŸ“š Catalog (STAC/DCAT)"]
  C --> E["ğŸ§¾ Provenance (PROV)"]
  D --> F["ğŸ—„ï¸ DB / Graph"]
  E --> F
  F --> G["ğŸ§ª API"]
  G --> H["ğŸ—ºï¸ UI / Story Nodes"]
```

### Planned artifacts (checklist)
- [ ] Raw files stored (as received) in `data/raw/<domain>/â€¦` **or** `attachments/raw/â€¦` (if interim holding)
- [ ] Work/intermediate outputs in `data/work/<domain>/â€¦`
- [ ] Final processed outputs in `data/processed/<domain>/â€¦`
- [ ] Catalog records created/updated (STAC/DCAT as applicable)
- [ ] PROV lineage bundle created/updated
- [ ] Dataset appears through API contracts (and respects redaction/classification)
- [ ] UI layer/story integration references catalog + provenance (no raw-direct shortcuts)

### Determinism guardrails
- Pipeline must be runnable end-to-end with **no interactive prompts**
- Re-running on identical raw inputs should not create duplicate outputs
- Output files should be stable (ordering, formatting, projection, schema)

---

## âœ… QA / Validation Plan

### Data validation
- [ ] Schema validation (required columns, types, null rules)
- [ ] Geometry validation (valid geometries, no insane coordinates)
- [ ] CRS sanity check (bounds look like Kansas / expected AOI)
- [ ] Duplicate detection (IDs + near-duplicate geometries)
- [ ] Spot checks against source (N = `<10/25/50>` features)

### Visual validation (maps)
- [ ] Quick render in `<QGIS | MapLibre | Kepler | other>`
- [ ] Compare overlay against reference layers (counties, rivers, towns)
- [ ] Screenshot evidence stored in `attachments/qa/`

---

## ğŸ“š Catalog + Provenance (publication boundary)

### Catalog records (fill once planned filenames exist)
- STAC Collection: `<path>`
- STAC Item(s): `<path(s)>`
- DCAT Dataset entry: `<path>`
- Notes: `<linkage strategy: itemâ†’collection; DCATâ†’STAC; etc>`

### Provenance bundle
- PROV file: `<path>`
- Inputs listed: `<raw filenames + checksums>`
- Activity listed: `<pipeline script + version>`
- Agents listed: `<human + software agent>`
- Output entities listed: `<processed filenames + checksums>`

---

## ğŸ§± Integration Notes (DB / API / UI)

### PostGIS / spatial DB
- Target table(s): `<schema.table>`
- Indexing needs: `<GiST on geom; btree on date; etc>`
- Partitioning needs (if time series): `<yes/no>`

### Graph (Neo4j or equivalent)
- Node types: `<Entity>`
- Relationship types: `<RELATES_TO>`
- Provenance linkage: `<entityâ†”provâ†”source>`

### API
- Endpoint(s): `<GET /datasets | /features | /search>`
- Access controls: `<public | gated | redacted>`

### UI
- Layer name: `<human-friendly>`
- Default styling: `<line weight, opacity, palette>`
- Legend text: `<short>`
- Story node references: `<if applicable>`

---

## ğŸ§© Open Questions / Risks

- [ ] CRS is unclear: `<plan to resolve>`
- [ ] License is unclear: `<plan to resolve>`
- [ ] Temporal ambiguity: `<plan>`
- [ ] Geometry quality issues: `<plan>`
- [ ] Join keys missing: `<plan>`

---

## ğŸ—’ï¸ Changelog

- **2026-01-29** â€” Created intake note (draft). Next: fill source/license/CRS + draft mapping table.

---

## âœ… Next Actions (1â€“3 hour wins)

- [ ] Drop raw download(s) into `attachments/raw/` (and/or `data/raw/<domain>/`)
- [ ] Save license/terms evidence into `attachments/license/`
- [ ] Fill the TL;DR table + Spatial/Temporal sections
- [ ] Draft the field mapping table (even if partial)
- [ ] Create `mapping.yaml` skeleton if we want machine-readable mappings
- [ ] Add 3â€“5 QA screenshots into `attachments/qa/`
