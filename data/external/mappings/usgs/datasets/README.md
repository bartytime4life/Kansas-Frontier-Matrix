# ğŸ—ºï¸ USGS Datasets â€” External Intake, Manifests & Mapping Layer

![Source](https://img.shields.io/badge/source-USGS-blue)
![Stage](https://img.shields.io/badge/stage-data%2Fexternal-orange)
![Catalogs](https://img.shields.io/badge/metadata-STAC%20%7C%20DCAT%20%7C%20PROV-purple)
![Policy](https://img.shields.io/badge/pipeline-ordering-non--negotiable-black)

> ğŸ“Œ **You are here:** `data/external/mappings/usgs/datasets/README.md`  
> ğŸ¯ **Goal:** Make USGS-sourced datasets **repeatably ingestible** (download â†’ verify â†’ process â†’ publish) with **manifests + provenance-first metadata**, while keeping big binaries manageable.

---

## ğŸ§­ What this folder is

This folder is the **USGS intake + mapping control-plane** for Kansas Frontier Matrix (KFM):

- ğŸ“¥ **External staging**: large USGS downloads can land here first (often `.gitignore`â€™d / cache-like).
- ğŸ§¾ **Manifests & checksums**: tracked â€œsource-of-truthâ€ records for what we downloaded and when.
- ğŸ§© **Mapping configs**: how a given USGS product becomes a KFM dataset (formats, CRS policy, tiling, QA gates).
- ğŸ§¬ **Provenance hooks**: everything needed to generate **STAC + DCAT + PROV** â€œboundary artifactsâ€ during publish.

> âœ… **Rule of thumb:**  
> **Big files live like data.**  
> **Manifests + metadata live like code.**  
> **Nothing reaches UI/graph without catalogs + provenance.**

---

## ğŸš« What this folder is NOT

- âŒ Not the canonical â€œpublishedâ€ catalog home  
  (those live in KFM canonical locations like `data/stac/`, `data/catalog/dcat/`, `data/prov/`).
- âŒ Not a random dumping ground of rasters/vectors with no lineage.
- âŒ Not a place for ad-hoc manual edits to â€œfixâ€ data without documenting the transform.

---

## ğŸ—‚ï¸ Directory layout

> ğŸ§© This is a **pattern**. Your repo may add/omit subfolders, but keep the intent.

```text
ğŸ“ data/
â””â”€ ğŸ“ external/
   â””â”€ ğŸ“ mappings/
      â””â”€ ğŸ“¦ usgs/                                         ğŸ›°ï¸ USGS mapping packs + dataset packages
         â””â”€ ğŸ“ datasets/
            â”œâ”€ ğŸ“„ README.md                                ğŸ‘ˆ you are here (how this registry works)
            â”œâ”€ ğŸ§¾ registry.usgs.yml                        âœ… dataset roster (tracked inventory)
            â”œâ”€ ğŸ“ templates/                               ğŸ§¬ reusable config stubs (starter kits)
            â””â”€ ğŸ“ usgs_historical_topo/                    ğŸ—ºï¸ example dataset package (reference implementation)
               â”œâ”€ ğŸ“„ README.md                              ğŸ“˜ dataset runbook + scope + outputs
               â”œâ”€ ğŸ§¾ source.yml                             ğŸ”— upstream source info (who/where/what license)
               â”œâ”€ ğŸ§© ingest.yml                             âš™ï¸ fetch + process plan (deterministic; no secrets)
               â”œâ”€ ğŸ“ manifest/                              ğŸ§¾ what we downloaded (git-friendly)
               â”‚  â”œâ”€ ğŸ“„ files.jsonl                          ğŸ§¾ one line per file (url, size, hash, timestamp)
               â”‚  â””â”€ ğŸ” checksums.sha256                     âœ… integrity hashes (rollup)
               â”œâ”€ ğŸ“ cache/                                 ğŸ“¥ big downloads (usually gitignored)
               â”œâ”€ ğŸ“ qa/                                    ğŸ§ª validation reports (small + tracked)
               â””â”€ ğŸ“ notes/                                 ğŸ“ edge cases, decisions, tickets/links
```

---

## ğŸ§  How this fits the KFM pipeline (mental model)

```mermaid
flowchart LR
  A[External Sources: USGS] --> B[Fetch/Stage: data/external/...]
  B --> C[Raw Snapshot: data/raw/usgs/...]
  C --> D[Work: data/work/usgs/...]
  D --> E[Processed: data/processed/usgs/...]
  E --> F[Catalogs: STAC/DCAT/PROV]
  F --> G[Graph]
  G --> H[API]
  H --> I[UI]
```

ğŸ”’ **Non-negotiable:** anything exposed downstream must be **cataloged + provenanced** first.

---

## ğŸ“š USGS dataset registry

Keep a lightweight registry so we always know whatâ€™s â€œin play.â€

> âœ… Track *IDs + intent* here (small & reviewable).  
> ğŸ§Š Donâ€™t try to list every single tile/quad here â€” that belongs in per-dataset manifests.

| Dataset ID | Theme | Type | Typical Outputs | Status |
|---|---|---:|---|---|
| `usgs_historical_topo` | Historical maps | Raster | COG (+ optional tiles) | ğŸ§ª pilot |
| `usgs_3dep_dem` | Elevation / terrain | Raster | COG + hillshade derivatives | ğŸ§© planned |
| `usgs_nhd` | Hydrography | Vector | GPKG/FGb + API-ready layers | ğŸ§© planned |
| `usgs_gnis` | Gazetteer / names | Table/Vector | parquet/csv + joinable lookups | ğŸ§© planned |

> ğŸ’¡ Tip: keep dataset IDs **stable**, even if products/editions evolve.

---

## âœ… Definition of Done (DoD) for a USGS dataset package

A dataset in this folder is â€œreadyâ€ when:

- [ ] ğŸ“„ **`source.yml` exists** (source, license, attribution, retrieval method)
- [ ] âš™ï¸ **`ingest.yml` exists** (reproducible steps, params, target CRS policy)
- [ ] ğŸ§¾ **manifest exists** (`manifest/files.jsonl` + `checksums.sha256`)
- [ ] ğŸ§ª **QA checks recorded** (`qa/` has at least one run report)
- [ ] ğŸ“¦ **processed outputs land in** `data/processed/usgs/<dataset_id>/...`
- [ ] ğŸŒ **STAC + DCAT + PROV** are produced on publish
- [ ] ğŸ” **governance tags** are defined (sensitivity/classification) and propagate

---

## â• Add a new USGS dataset (recipe)

### 1) Create a dataset package folder ğŸ“
```bash
mkdir -p data/external/mappings/usgs/datasets/usgs_<dataset_id>/{manifest,qa,notes,cache}
touch data/external/mappings/usgs/datasets/usgs_<dataset_id>/{README.md,source.yml,ingest.yml}
```

### 2) Fill `source.yml` ğŸ”— (minimum fields)
**Keep this file human-readable and diff-friendly.**

```yaml
id: usgs_<dataset_id>
title: "USGS <Product Name>"
source_org: "U.S. Geological Survey (USGS)"
source_kind: "download|api|wms|wmts"
license: "PUBLIC_DOMAIN|USGS_TERMS|<other>"
attribution: "USGS"
retrieval:
  method: "manual|scripted"
  uri: "<stable landing page or API endpoint>"
  retrieved_at: "YYYY-MM-DD"
coverage:
  spatial: "Kansas|CONUS|<bbox>"
  temporal: "1880-2000|YYYY-YYYY|present"
sensitivity:
  classification: "public|restricted|sensitive"
  notes: "If sensitive, explain why + required handling."
```

### 3) Fill `ingest.yml` âš™ï¸ (make it deterministic)
```yaml
fetch:
  tool: "python|curl|aria2|custom"
  params:
    area_of_interest: "KS"
    filters:
      - "edition: 1950-1959"
      - "scale: 1:24000"
verify:
  checksums: true
  schema: false
process:
  raster:
    to_cog: true
    build_overviews: true
    nodata: "auto"
    target_crs_policy: "preserve_source + publish_wgs84_geom"
  vector:
    target_format: "GPKG"
    make_valid: true
publish:
  catalogs:
    stac: true
    dcat: true
    prov: true
  outputs:
    processed_root: "data/processed/usgs/usgs_<dataset_id>/"
```

### 4) Populate `manifest/` ğŸ§¾
A manifest is the â€œreceiptâ€ for what you downloaded.

**Recommended manifest fields** (JSONL):
- `path` (relative)
- `sha256`
- `size_bytes`
- `retrieved_at`
- `source_uri`
- `content_type` (raster/vector/pdf/etc)
- `crs` (if known)
- `bbox_wgs84` (if known)

Example line:
```json
{"path":"cache/1957/quad_foo.tif","sha256":"...","size_bytes":1234567,"retrieved_at":"1957-01-01","source_uri":"...","content_type":"raster","crs":"EPSG:26914"}
```

### 5) Run QA gates ğŸ§ª
Store results in `qa/` as small artifacts (text + summary JSON).

**Raster QA ideas**
- `gdalinfo` snapshot
- COG validity check (driver, internal tiling, overviews)
- nodata + stats sanity

**Vector QA ideas**
- geometry validity
- attribute schema checks (required columns)
- bbox sanity

### 6) Promote to canonical raw/work/processed ğŸ§±
If `data/external/...` is a cache/staging area, then:
- copy/sync â€œfrozen inputsâ€ into `data/raw/usgs/<dataset_id>/...`
- write intermediates to `data/work/usgs/<dataset_id>/...`
- publish analysis-ready outputs to `data/processed/usgs/<dataset_id>/...`

### 7) Publish catalogs ğŸŒ
Publishing creates the â€œboundary artifactsâ€:
- ğŸ—ºï¸ STAC Collection + Item(s)
- ğŸ§¾ DCAT dataset entry
- ğŸ§¬ PROV lineage bundle

> ğŸ§· Store these in canonical paths (project standard), not here:
> - `data/stac/collections/â€¦`
> - `data/stac/items/â€¦`
> - `data/catalog/dcat/â€¦`
> - `data/prov/â€¦`

---

## ğŸ§­ CRS & projection policy (practical rules)

> ğŸ“ **Never do distance/area work in degrees** (EPSG:4326).  
> Use a **projected CRS in meters** for rasterization/resampling and for accurate measurement.

### âœ… Required metadata behaviors
- Always **record the source CRS** (even if you reproject later).
- Always **publish WGS84 geometry** for discovery/extent (bbox + footprints).
- If Kansas spans multiple zones/regions: prefer a **project-level analysis CRS** to avoid mixed-zone headaches.

### Suggested strategy (works well in most stacks)
- **Discovery/metadata footprints:** EPSG:4326
- **Web map rendering tiles:** EPSG:3857
- **Analysis/measurements:** project standard (choose once; document globally)

> ğŸ§© If a dataset arrives in an unexpected CRS (common with historic scans), document it in `notes/` and preserve a clear PROV step describing the reprojection.

---

## ğŸ§ª Data product standards (what â€œgoodâ€ looks like)

### ğŸŸ¦ Raster
- âœ… Prefer **COG** for distribution (Cloud Optimized GeoTIFF)
- âœ… Add internal tiling + overviews
- âœ… Explicit nodata
- âœ… Compression appropriate to content (lossless for categorical; careful with historical scans)

**COG conversion example**
```bash
gdal_translate input.tif output.cog.tif \
  -of COG \
  -co COMPRESS=DEFLATE \
  -co BIGTIFF=IF_SAFER \
  -co RESAMPLING=NEAREST
```

### ğŸŸ© Vector
- âœ… Prefer **GeoPackage (GPKG)** or **FlatGeobuf (FGb)** for â€œrealâ€ datasets
- âš ï¸ Shapefiles only when unavoidable (many limits; easy to corrupt)
- âœ… Run `make_valid` / topology checks when needed
- âœ… Normalize encoding to UTF-8

**Vector conversion example**
```bash
ogr2ogr -f GPKG output.gpkg input.shp -lco SPATIAL_INDEX=YES
```

---

## ğŸ” Governance & sensitivity (CARE/FAIR aligned)

KFM treats governance as a **hard constraint**, not a suggestion:

- ğŸ›¡ï¸ **Classification propagation:** outputs cannot be less restricted than inputs.
- ğŸ§­ **Sensitive locations:** ensure redaction/generalization is applied consistently across:
  - processed data
  - STAC/DCAT metadata
  - API behavior
  - UI presentation

> âœ… If a layer could reveal protected sites (archaeology, culturally sensitive areas, etc.), treat it as **restricted** unless explicitly cleared.

---

## ğŸ§¾ Dataset package templates (copy/paste starters)

<details>
<summary>ğŸ“„ Template: <code>README.md</code> (per-dataset)</summary>

```markdown
# ğŸ§© <Dataset Title>

## Overview
- **ID:** usgs_<dataset_id>
- **Source:** USGS
- **Theme:** <e.g., topo / hydro / dem>
- **Coverage:** <KS/CONUS/bbox>
- **Temporal:** <range>

## Inputs
- Where it comes from (URL/API)
- License/attribution

## Processing
- Steps (deterministic)
- CRS decisions
- Output formats

## Outputs
- data/raw/usgs/...
- data/processed/usgs/...
- STAC/DCAT/PROV IDs (once published)

## QA
- Checks performed
- Known caveats

## Notes
- Edge cases, anomalies, known issues
```
</details>

<details>
<summary>ğŸ§¾ Template: <code>manifest/files.jsonl</code></summary>

```json
{"path":"cache/<...>","sha256":"<...>","size_bytes":0,"retrieved_at":"YYYY-MM-DD","source_uri":"<...>","content_type":"raster|vector|doc","crs":"EPSG:xxxx","bbox_wgs84":[minx,miny,maxx,maxy]}
```
</details>

---

## ğŸ§© Troubleshooting (common pain points)

- ğŸ§¨ **â€œ.prj missing or wrongâ€**  
  â†’ treat as an ingest bug; document and fix deterministically (donâ€™t handwave CRS).

- ğŸ§¨ **Historic scans donâ€™t line up**  
  â†’ georeferencing is a processing step, not a metadata tweak. Capture control points + method in PROV.

- ğŸ§¨ **Mixed resolutions / inconsistent tile grids**  
  â†’ normalize into a standard tiling strategy at publish, and keep the source as-is in raw.

---

## ğŸ“Œ TODOs (nice upgrades)
- [ ] Add a lightweight `registry.usgs.yml` validator
- [ ] Add `make usgs-validate` target to run raster/vector QA gates
- [ ] Add a small â€œgolden sampleâ€ tile set for CI smoke tests
- [ ] Add dataset-specific SOPs under `docs/sops/` (georef, COG, tiling, etc.)

---

## ğŸ·ï¸ Legend
- ğŸ“¥ download/cache
- ğŸ§¾ manifest/checksums
- âš™ï¸ reproducible config
- ğŸ§ª QA gate
- ğŸŒ publish catalogs
- ğŸ§¬ provenance/lineage
- ğŸ” governance/sensitivity

