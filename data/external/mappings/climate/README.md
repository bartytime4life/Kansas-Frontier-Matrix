# ğŸŒ¦ï¸ Climate â€” External Dataset Mappings (KFM)

![Status](https://img.shields.io/badge/status-active-success)
![Scope](https://img.shields.io/badge/scope-external%20climate-blue)
![Metadata](https://img.shields.io/badge/metadata-STAC%20%7C%20DCAT%20%7C%20PROV-purple)
![Governance](https://img.shields.io/badge/governance-FAIR%20%2B%20CARE-orange)

> **Purpose:** This folder is the *mapping layer* ğŸ§­ for external climate datasetsâ€”how we describe, validate, and connect them to **KFMâ€™s provenance-first â€œtruth pathâ€** (Raw â†’ Processed â†’ Catalog/Prov â†’ Database â†’ API â†’ UI).

---

## ğŸ“Œ What lives here (and what does not)

### âœ… Lives here
- ğŸ“„ **Dataset mapping docs** (dataset â†’ STAC/DCAT/PROV)
- ğŸ—‚ï¸ **Manifests / indices** for climate sources & variables
- ğŸ§¾ **Units + CRS + temporal semantics** decisions (the â€œrules of the roadâ€)
- ğŸ§ª **Validation notes** (quality checks, edge cases, known issues)
- ğŸ” **License + attribution** requirements (fail-closed by default)

### âŒ Does *not* live here
- ğŸŒ§ï¸ Raw rasters / NetCDF / CSV dumps (those belong in `data/external/raw/...`)
- ğŸ§± Processed outputs (those belong in `data/external/processed/...`)
- ğŸ§  Derived analysis products (those belong in `data/derived/...` or a domain-specific derived folder)

---

## ğŸ§¬ Canonical â€œTruth Pathâ€ for Climate Data

```mermaid
flowchart LR
  A["ğŸ“¥ Raw climate inputs â€” data/external/raw/..."] --> B["ğŸ§¹ ETL + standardization â€” src/pipelines/..."]
  B --> C["ğŸ“¦ Processed outputs â€” data/external/processed/..."]
  B --> D["ğŸ—ºï¸ Catalog metadata (STAC/DCAT) â€” data/catalog/..."]
  B --> E["ğŸ§¾ Provenance logs (PROV) â€” data/provenance/..."]
  C --> F["ğŸ›°ï¸ PostGIS / ğŸ•¸ï¸ Neo4j"]
  D --> F
  E --> F
  F --> G["ğŸ”Œ API"]
  G --> H["ğŸ–¥ï¸ UI"]
```

> **Rule:** No skipping steps. The UI should never â€œtouchâ€ raw climate files directly.

---

## ğŸ“ Expected structure

> This repo can be organized several ways; the **pattern** below is the one this README assumes.

```text
ğŸ“ data/
â””â”€â”€ ğŸ“ external/
    â”œâ”€â”€ ğŸ“ raw/
    â”‚   â””â”€â”€ ğŸ“ climate/                 # immutable source drops
    â”œâ”€â”€ ğŸ“ processed/
    â”‚   â””â”€â”€ ğŸ“ climate/                 # standardized outputs for loading/serving
    â”œâ”€â”€ ğŸ“ mappings/
    â”‚   â””â”€â”€ ğŸ“ climate/                 # âœ… you are here
    â”‚       â”œâ”€â”€ ğŸ“„ README.md
    â”‚       â”œâ”€â”€ ğŸ“ manifests/           # dataset registry, variable specs
    â”‚       â”œâ”€â”€ ğŸ“ stac/                # STAC collection/item templates or generated examples
    â”‚       â”œâ”€â”€ ğŸ“ dcat/                # DCAT dataset templates or generated examples
    â”‚       â”œâ”€â”€ ğŸ“ prov/                # PROV templates or lineage conventions
    â”‚       â””â”€â”€ ğŸ“ policies/            # optional: OPA policy notes for access/licensing
    â””â”€â”€ ğŸ“ cache/
        â””â”€â”€ ğŸ“ climate/                 # optional: downloads, tiles, temp exports (gitignored)
```

---

## ğŸ§© Mapping artifacts (what you should create)

### 1) ğŸ“‹ Dataset manifest(s)
A machine-readable â€œsource of truthâ€ for:
- dataset ID
- provider / source
- variables & units
- spatial & temporal coverage
- license
- ingest method (API / manual / Earth Engine export)
- update cadence
- checksums / hashes (when applicable)

**Recommended format:** YAML or JSON.

<details>
<summary>ğŸ§¾ Minimal manifest example (YAML)</summary>

```yaml
id: gridmet_daily
title: "GridMET Daily Climate"
source:
  provider: "University of Idaho / gridMET"
  access: "earthengine|http|manual"
license:
  spdx: "CC-BY-4.0"     # or "unknown" (but unknown should fail CI)
variables:
  - name: tmmx
    title: "Daily max temperature"
    units: "K"
    temporal_resolution: "daily"
  - name: pr
    title: "Daily precipitation"
    units: "mm"
    temporal_resolution: "daily"
spatial:
  crs: "EPSG:4326"
  resolution: "0.0416667deg"
temporal:
  start: "1979-01-01"
  end: null              # null = still updating
provenance:
  captured_by: "src/pipelines/climate/..."
  hash_method: "sha256"
```

</details>

---

### 2) ğŸ—ºï¸ STAC mapping
Climate rasters/time series should be discoverable via STAC:
- one **Collection** per dataset (or per variable family)
- one **Item** per time slice (daily/monthly) *or* per chunk (yearly tiles), depending on scale

**Goal:** Make climate layers *findable + interoperable*.

<details>
<summary>ğŸ§± STAC â€œrequired fieldsâ€ checklist</summary>

- `id`, `title`, `description`
- `extent.spatial.bbox`
- `extent.temporal.interval`
- `license`
- `providers`
- `keywords`
- `assets` (COG/NetCDF/Parquet + thumbnails/metadata sidecars if used)
- **Projection info** (CRS, transform, shape) for raster assets
</details>

---

### 3) ğŸ§¾ DCAT mapping
DCAT is for â€œdataset catalogâ€ interoperability (portals, cross-org sharing):
- dataset-level description
- distribution formats
- licensing + attribution
- contact/maintainer (if applicable)

---

### 4) ğŸ§¬ PROV mapping
Provenance should answer:
- Where did the data come from?
- What did we do to it?
- With what parameters?
- When?
- What outputs were produced?

**Minimum lineage fields to capture**
- input identifier(s) + version/date retrieved
- processing script path + git commit hash
- parameter set (CRS, resampling, aggregation, AOI)
- output checksums + row/pixel counts
- validation report pointer

---

## ğŸ·ï¸ Naming conventions (files + dataset IDs)

### Dataset IDs
Use **lowercase, hyphen/underscore**, stable identifiers:
- âœ… `gridmet_daily`
- âœ… `prism_monthly_normals`
- âœ… `era5_land_hourly`
- âŒ `GridMET Daily (v2)`

### Files
Use a consistent, sortable pattern:

`<dataset>__<variable>__<temporal>__<spatial>__<crs>__<time-range>__v<version>.<ext>`

Examples:
- `gridmet_daily__tmmx__1d__4km__epsg4326__2001-2020__v1.parquet`
- `prism_monthly__ppt__1mo__800m__epsg4326__1990-2020__v1.tif`

---

## ğŸŒ CRS, projections, and map-readiness

**Principle:** preserve the original CRS in raw, standardize in processed.

Suggested defaults:
- ğŸŒ **Raw:** keep native CRS (often EPSG:4326)
- ğŸ“ **Analysis:** prefer equal-area or domain-standard CRS (project decision)
- ğŸ§­ **Web tiles:** EPSG:3857 (only for display)

âœ… Always record:
- CRS (`EPSG:####`)
- pixel size / resolution
- resampling method (nearest/bilinear/cubic/avg)
- nodata value

---

## ğŸ•°ï¸ Time semantics (climate is tricky)

Climate data is time-oriented by natureâ€”avoid ambiguity by storing **both**:
- **Observation time** (what date/hour the measurement represents)
- **Ingest/updated time** (when we fetched/processed it)

Checklist:
- ISO-8601 dates (UTC by default)
- explicit calendar handling (leap days, missing days)
- explicit aggregation rules (e.g., daily â†’ monthly mean vs sum)
- no â€œfloatingâ€ local-time timestamps

---

## â˜ï¸ Google Earth Engine export playbook (when used)

When Earth Engine is part of the pipeline:
- batch exports by **time windows** (e.g., yearly chunks)
- export only required columns/fields (avoid massive `.geo` payloads)
- keep a stable **join key** (e.g., GEOID) so geometry can be rejoined downstream

<details>
<summary>ğŸ“‰ Scaling tip: â€œSelectorsâ€ to avoid gigantic CSVs</summary>

**Why:** exporting geometry for every row explodes file size.

**Pattern:** export only `id + date + bands` and join geometry later (PostGIS / vector layer).
</details>

---

## ğŸ§ª Validation gates (fail-closed âœ…)

Before a climate dataset is â€œservableâ€:
- âœ… license present + compatible
- âœ… manifest entry exists
- âœ… STAC/DCAT records generated (or updated)
- âœ… PROV lineage generated
- âœ… basic QA checks pass:
  - missing dates (%)
  - min/max sanity (units)
  - nodata proportion
  - CRS + bounds match expectations
  - reproducible checksum

> If any check fails, the contribution should **not** merge.

---

## ğŸ—ºï¸ Visualization notes (so maps donâ€™t lie)

When designing climate layers for UI:
- prefer perceptually uniform ramps for continuous variables ğŸŒˆ
- show units prominently (Â°C vs K, mm vs inches)
- include temporal context (baseline period, anomaly definition)
- offer â€œthen/nowâ€ comparisons + time slider support

---

## ğŸ§‘â€ğŸ¤â€ğŸ§‘ Adding a new climate dataset (workflow)

1) ğŸ“¥ Put source material in `data/external/raw/climate/<dataset>/` *(or document the API/GEE source)*  
2) ğŸ§¾ Add/extend a manifest in `manifests/`  
3) ğŸ§¹ Implement or update pipeline in `src/pipelines/...`  
4) ğŸ—ºï¸ Generate/validate STAC + DCAT + PROV artifacts  
5) âœ… Run validation gates (CI/local)  
6) ğŸ”€ Open PR with:
   - mapping docs
   - metadata/prov outputs (or reproducible generation steps)
   - notes about known limitations

---

## ğŸ†˜ Troubleshooting

- **CRS mismatch in UI** â†’ confirm processed assets are in the â€œUI CRSâ€ (or have tiles generated)  
- **Huge exports** â†’ chunk by time/space; drop unused columns; avoid embedding geometries per record  
- **Temporal gaps** â†’ verify source cadence; handle missing days explicitly; donâ€™t silently interpolate  
- **Unit confusion** â†’ standardize early; record conversions in PROV

---

## ğŸ“š Project library (recommended reading)

- ğŸ“˜ *Kansas Frontier Matrix (KFM) â€“ Comprehensive Technical Blueprint* (architecture, truth path, governance)
- ğŸ“— *Cloud-Based Remote Sensing with Google Earth Engine* (scalable climate time series extraction & exports)
- ğŸ“™ *Visualization of Time-Oriented Data* (temporal QA + visualization pitfalls)
- ğŸ“• *Data Spaces* (machine-readable metadata, licensing, provenance as first-class requirements)

---

## âœ… Status

- [ ] Add `manifests/` starter files (dataset index + variable registry)
- [ ] Add mapping templates for STAC/DCAT/PROV
- [ ] Wire validation gates for license + metadata completeness
- [ ] Document â€œofficialâ€ CRS/time/unit standards for KFM climate layers

