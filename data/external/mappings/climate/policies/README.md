# ğŸŒ¦ï¸ Climate Policy Mappings (External)  
`data/external/mappings/climate/policies/`

![KFM](https://img.shields.io/badge/KFM-provenance--first-1f6feb)
![Governance](https://img.shields.io/badge/FAIR%2BCARE-governed-orange)
![Metadata](https://img.shields.io/badge/STAC%20%7C%20DCAT%20%7C%20PROV-aligned-brightgreen)
![Scope](https://img.shields.io/badge/scope-external%20climate%20policies-6e40c9)
![Data](https://img.shields.io/badge/data-mappings%20only-555)

> [!NOTE]  
> This folder stores **mapping specs + documentation** that translate *external climate-policy datasets* into **KFM-ready, provenance-backed artifacts**.  
> ğŸ“¦ Data files themselves belong in `data/external/raw/â€¦` â†’ `data/external/processed/â€¦` (via ETL). This folder documents **how** that translation happens.

---

## ğŸ¯ What this folder is for

Climate policy data is messy: itâ€™s jurisdictional, time-bounded, amended, and often published in formats that donâ€™t behave like â€œclean GIS.â€ This directory exists to:

- ğŸ§­ Define **repeatable mappings** from a policy dataset â†’ **canonical KFM schema** (fields + semantics).
- ğŸ—ºï¸ Specify **geometry rules** (what the boundaries mean, how theyâ€™re built/normalized, CRS expectations).
- ğŸ•°ï¸ Capture **time validity** (effective dates, sunset dates, amendment history) in a consistent way.
- ğŸ“‡ Ensure every policy layer can be published as **STAC/DCAT/PROV boundary artifacts** (discoverable + auditable).
- âš–ï¸ Encode **governance + access classification** decisions for policy layers and their derivatives.

---

## ğŸš« What does *not* belong here

- âŒ Raw PDFs, shapefiles, spreadsheets, or scraped HTML dumps  
  â†’ put those in `data/external/raw/climate/policies/...`
- âŒ Final outputs (GeoParquet/GeoJSON/COG)  
  â†’ those are `data/external/processed/climate/policies/...`
- âŒ Opinionated legal interpretation / â€œwhat the law meansâ€  
  â†’ policy layers should stay **descriptive**, not advisory
- âŒ One-off manual edits without provenance  
  â†’ mapping changes must be reproducible

---

## ğŸ—‚ï¸ Suggested directory layout

This directory is designed to scale across many policy datasets and versions.

```text
data/external/mappings/climate/policies/
â”œâ”€â”€ ğŸ“„ README.md                        # you are here ğŸ™‚
â”œâ”€â”€ ğŸ“„ dataset_registry.yml             # optional: index of all policy datasets in this module
â”œâ”€â”€ ğŸ“ templates/                       # starter templates for new mappings
â”‚   â”œâ”€â”€ ğŸ“„ mapping.template.yml
â”‚   â”œâ”€â”€ ğŸ“„ fields.template.csv
â”‚   â””â”€â”€ ğŸ“„ notes.template.md
â””â”€â”€ ğŸ“ datasets/
    â””â”€â”€ ğŸ“ <dataset_slug>/              # one per dataset (or per dataset+publisher)
        â”œâ”€â”€ ğŸ“„ source.yml               # where it came from + license + update cadence
        â”œâ”€â”€ ğŸ“„ mapping.yml              # field + semantic mapping into KFM canonical model
        â”œâ”€â”€ ğŸ“„ fields.csv               # sourceâ†’target field crosswalk (machine-friendly)
        â”œâ”€â”€ ğŸ“„ geometry.md              # geometry meaning, dissolves,ï¸, joins, QA rules
        â”œâ”€â”€ ğŸ“„ temporal.md              # effective/valid/transaction time rules
        â”œâ”€â”€ ğŸ“„ governance.md            # classification + redaction requirements
        â””â”€â”€ ğŸ“„ notes.md                 # assumptions, quirks, gotchas, edge cases
```

> [!TIP]  
> If you donâ€™t need all files, start with **`source.yml` + `mapping.yml` + `fields.csv` + `notes.md`** and expand when the dataset gets complex.

---

## ğŸ§© Mapping contract (what every dataset mapping should contain)

### 1) ğŸ“Œ Source + licensing (`source.yml`)
Capture the â€œmap behind the mapâ€:

- dataset title + publisher/agency
- original access URL(s) / identifiers
- license + attribution requirements
- update cadence (static / annual / monthly / ad hoc)
- geographic coverage (Kansas-only? multi-state? national?)
- primary format (vector/raster/table/text) and how itâ€™s obtained

### 2) ğŸ§¾ Field crosswalk (`fields.csv`)
A minimal machine-friendly table:

| source_field | target_field | type | transform | notes |
|---|---|---:|---|---|
| `program` | `policy_program` | string | normalize_case | official program name |
| `start_dt` | `effective_start` | date | parse_iso | dataset uses yyyy-mm-dd |
| `end_dt` | `effective_end` | date | parse_iso_or_null | null = still active |

### 3) ğŸ§  Semantic mapping (`mapping.yml`)
Explain meaning beyond the crosswalk:

- controlled vocabularies (policy_type, instrument_type, sector)
- jurisdiction rules (state/county/city/tribal/federal)
- â€œwhat does the geometry representâ€ (legal jurisdiction vs administrative boundary vs service area)

### 4) ğŸ—ºï¸ Geometry rules (`geometry.md`)
Policy layers often need clarification:

- geometry type: polygon / multipolygon / line / point / raster mask  
- how boundaries are built: dissolve by jurisdiction, union by program, clip to Kansas
- CRS requirements and reprojection decisions
- edge-case handling: overlaps, holes, slivers, invalid geometries

### 5) ğŸ•°ï¸ Temporal rules (`temporal.md`)
Policy data is **time-oriented** and often **bi-temporal** in practice (valid time vs publication/update time).

Minimum fields to standardize:

- `enacted_date` (if available)
- `effective_start` / `effective_end` (policy validity)
- `last_verified` (when *we* confirmed the dataset)
- `source_last_updated` (if publisher provides)

### 6) âš–ï¸ Governance (`governance.md`)
Even policy datasets can contain sensitive context (e.g., program locations tied to vulnerable infrastructure).

Include:

- access classification (Public / Internal / Restricted)
- redaction/generalization rules (if any)
- allowed uses + disallowed uses
- owner/steward contacts (internal roles)

---

## ğŸŒ How mappings relate to STAC / DCAT / PROV

Mappings in this folder support creation of â€œboundary artifactsâ€ in canonical catalog locations:

- ğŸ§­ **STAC**: spatial/temporal indexing for assets (collections + items)
- ğŸ—ƒï¸ **DCAT**: discoverability (dataset-level description + distributions)
- ğŸ” **PROV**: lineage chain (inputs â†’ transformations â†’ outputs + agents)

> [!IMPORTANT]  
> In KFM, **published datasets must be provenance-backed** before they can be used downstream (graph/API/UI/story layers).  
> Mapping docs help keep this deterministic, reviewable, and repeatable.

---

## ğŸ” Pipeline snapshot (why weâ€™re strict about this)

```mermaid
flowchart LR
  A["ğŸ“¥ External policy sources<br/>PDF â€¢ CSV â€¢ API â€¢ Shapefile"] --> B["ğŸ§ª ETL + Normalization<br/>(deterministic)"]
  B --> C["ğŸ—ºï¸ Processed outputs<br/>GeoParquet/GeoJSON/COG"]
  C --> D["ğŸ“‡ STAC + DCAT<br/>catalog entries"]
  C --> E["ğŸ§¾ PROV<br/>lineage bundle"]
  D --> F["ğŸ§  Graph build (refs to catalogs)"]
  F --> G["ğŸ”Œ API layer (contracts + redaction)"]
  G --> H["ğŸ—ºï¸ Map UI + timelines"]
```

---

## âœ… Definition of Done for a new policy mapping

A mapping PR is â€œdoneâ€ when:

- âœ… Dataset has a clear `source.yml` with license + attribution
- âœ… Field crosswalk exists (`fields.csv`) and is understandable
- âœ… Time semantics are explicit (even if â€œunknownâ€ is the answer)
- âœ… Geometry meaning is documented (what does the polygon *mean*?)
- âœ… Governance is explicit (classification + any required redaction)
- âœ… Notes capture assumptions + known limitations
- âœ… Mapping is reproducible (no mystery manual steps)

---

## ğŸ§ª Validation checklist (human + automated)

### Human review (fast)
- ğŸ” Does the mapping **match the real-world meaning** of the policy?
- ğŸ§¾ Are **citations and attribution** clearly specified?
- ğŸ•°ï¸ Are effective dates consistent (no impossible ranges)?
- ğŸ—ºï¸ Are boundaries plausible (no huge gaps/overlaps without explanation)?

### Automated gates (recommended)
- âœ… Schema validation for STAC/DCAT/PROV outputs
- âœ… Link/reference checks (no dead internal links)
- âœ… Geometry QA: validity, winding, SRID, area thresholds
- âœ… Classification propagation checks (no â€œdowngradingâ€ outputs)

---

## ğŸ§° Quickstart: add a new dataset mapping

1) Create a folder:
```bash
mkdir -p data/external/mappings/climate/policies/datasets/<dataset_slug>
```

2) Add the minimum files:
- `source.yml`
- `mapping.yml`
- `fields.csv`
- `notes.md`

3) If the dataset is time-heavy or legally subtle:
- add `temporal.md` and `geometry.md`

4) Open a PR with:
- dataset description
- why it matters (use cases)
- what changed
- any governance flags (classification / redaction)

---

## ğŸ“š Related KFM standards & docs (repo links)

- ğŸ“ `docs/standards/` â€” STAC/DCAT/PROV profiles + governance standards  
- ğŸ§­ `docs/governance/` â€” ethics, sovereignty, review gates  
- ğŸ§ª `src/pipelines/` â€” ETL jobs that should implement these mappings  
- ğŸ—ºï¸ `data/stac/` + `data/catalog/dcat/` + `data/prov/` â€” published boundary artifacts

---

## ğŸ§  Mini glossary (policy mapping terms)

- **Jurisdiction**: the governing unit (state/county/city/tribal/federal) that the policy applies to
- **Instrument**: the policy mechanism (incentive, regulation, mandate, cap, subsidy, standard)
- **Valid time**: when the policy is actually in force
- **Transaction time**: when the dataset was published/updated/ingested

---

## ğŸ§© Future upgrades (nice-to-have)

- ğŸ§  Add an ontology-backed `policy_type` vocabulary shared across domains
- ğŸ•°ï¸ Add bitemporal indexing support in catalogs for â€œas-ofâ€ queries
- ğŸ” Add a lint rule that blocks mappings missing license/attribution
- ğŸ§¯ Add a â€œsensitivity scannerâ€ for policy layers tied to protected locations

