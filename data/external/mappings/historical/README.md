# ğŸ—ºï¸ Historical External Mapping Manifests (KFM)

![Status](https://img.shields.io/badge/status-active%20runbook-brightgreen)
![Scope](https://img.shields.io/badge/scope-historical%20maps%20%26%20layers-blue)
![Data](https://img.shields.io/badge/data-provenance--first-purple)
![Standards](https://img.shields.io/badge/metadata-STAC%20%7C%20DCAT%20%7C%20PROV-orange)
![Ethics](https://img.shields.io/badge/principles-FAIR%20%2B%20CARE-black)

> [!NOTE]
> This directory is **text-first** by design âœ…  
> It holds **manifests + mapping docs** that describe *external* historical mapping sources and how they flow into KFMâ€”**not** the bulky map files themselves.

---

## ğŸ“Œ What lives here?

This folder is the â€œ**map behind the map**â€ layer for **historical** sources:
- ğŸ§¾ **Source manifests**: where the data came from, licensing, access method, checksums, expected artifacts.
- ğŸ§­ **Schema & field mappings**: how attributes map into KFMâ€™s canonical fields (names, dates, geometry, provenance hooks).
- ğŸ§± **Georeferencing run logs**: how a scanned historical map was rectified (GCPs, residuals, CRS, warp method).
- ğŸ§ª **Validation notes**: what was checked, what failed, and what remains uncertain.

If youâ€™re looking for:
- ğŸ“¥ raw downloads / scans â†’ `../../../../data/raw/historical/`
- ğŸ§° intermediate artifacts â†’ `../../../../data/work/historical/`
- âœ… final, load-ready outputs â†’ `../../../../data/processed/historical/`
- ğŸ—‚ï¸ published metadata â†’ `../../../../data/stac/`, `../../../../data/catalog/dcat/`, `../../../../data/prov/`

---

## ğŸ” Canonical KFM flow (why these mappings matter)

KFM is built so that **no dataset becomes â€œrealâ€ until it has lineage + catalog records**. These mapping docs exist to make that repeatable and reviewable.

```mermaid
flowchart LR
  A[ğŸŒ External Historical Source] --> B[ğŸ§¾ Manifest + Mapping Docs]
  B --> C[âš™ï¸ Deterministic ETL Pipeline]
  C --> D[âœ… data/processed/historical/*]
  D --> E[ğŸ—‚ï¸ STAC + DCAT + PROV Boundary Artifacts]
  E --> F[ğŸ§  Graph / API Layer]
  F --> G[ğŸ—ºï¸ Map UI + ğŸ§¾ Story Nodes]
```

---

## ğŸ§° Recommended layout in this folder

```text
ğŸ“ data/external/mappings/historical/
â”œâ”€â”€ ğŸ“„ README.md
â”œâ”€â”€ ğŸ“ templates/
â”‚   â”œâ”€â”€ ğŸ“„ source_manifest.template.yml
â”‚   â”œâ”€â”€ ğŸ“„ dataset_mapping.template.yml
â”‚   â””â”€â”€ ğŸ“„ georeference_log.template.md
â””â”€â”€ ğŸ“ <source_slug>/                       # one folder per external source
    â”œâ”€â”€ ğŸ“„ manifest.yml                      # where to fetch + license + checksums
    â”œâ”€â”€ ğŸ“„ mapping.yml                       # how fields map into KFM canonical schema
    â”œâ”€â”€ ğŸ“„ georeference.md                   # only if raster scans are rectified
    â”œâ”€â”€ ğŸ“„ assets.csv                        # optional: file list, years, sheet ids, urls, checksums
    â””â”€â”€ ğŸ“„ notes.md                          # optional: messy realities, edge cases, citations
```

> [!TIP]
> Keep â€œexternalâ€ **lightweight**: prefer manifests, checksums, and URLs over committing large rasters.  
> If you *must* store large binaries, use the repoâ€™s large-file strategy (e.g., pointers/LFS) and document it clearly. ğŸ“¦

---

## âœ… Adding a new historical source (checklist)

1. **Create a new folder**:
   - `data/external/mappings/historical/<source_slug>/`

2. **Add a manifest**:  
   - `manifest.yml` (where it lives, how we fetch it, licensing/terms, checksums)

3. **Add a mapping**:  
   - `mapping.yml` (how raw fields become KFM canonical fields + which pipeline consumes it)

4. **If scanned raster maps:** add `georeference.md`  
   - record GCPs approach, RMS error, CRS, resampling method, and output plan.

5. **Wire to a deterministic pipeline**:
   - Pipeline code typically lives in `../../../../src/pipelines/â€¦`
   - Pipelines must be **non-interactive** and **re-runnable** (same inputs â†’ same outputs).

6. **Publish boundary artifacts**:
   - STAC item/collection (spatiotemporal discovery) ğŸ—‚ï¸
   - DCAT dataset entry (catalog view) ğŸ§¾
   - PROV lineage (how we produced it) ğŸ”—

7. **Open a PR**:
   - reviewers should be able to answer: *â€œWhere did this come from? What changed? Can I reproduce it?â€* ğŸ§ 

---

## ğŸ§¾ Manifest spec (minimal but strict)

Create `manifest.yml` using this baseline:

```yaml
id: "historical.<source_slug>"
title: "Human-readable dataset title"
description: "What it is, why it matters, and what it covers (Kansas-relevant scope)."
provider:
  name: "Institution / archive / agency"
  homepage: "https://example.org"
license:
  spdx: "CC-BY-4.0"         # or "NONE" if unclear (but then add notes + restrictions)
  text: "Short human summary of reuse terms"
access:
  type: "download|api|wms|wmts|manual_request|onsite_only"
  url: "https://example.org/download"
  auth: "none|api_key|oauth|unknown"
  notes: "Rate limits, request steps, archival quirks"
temporal:
  coverage: ["1854-01-01", "1900-12-31"]    # coverage period
  map_dates_field: "publication_year"       # if a per-sheet date exists
spatial:
  bbox_wgs84: [-102.05, 36.99, -94.59, 40.00]  # Kansas bbox default; tighten per dataset if known
  crs_raw: "unknown|EPSG:xxxx"
artifacts:
  expected_raw:
    - type: "raster_scan|vector|table|tile_index"
      format: "tif|jpg|pdf|shp|geojson|csv"
  expected_processed:
    - type: "cog"        # Cloud-Optimized GeoTIFF
      format: "tif"
    - type: "geojson"
      format: "geojson"
pipelines:
  primary: "src/pipelines/historical/<pipeline_name>.py"
  params:
    default_output_crs: "EPSG:4326"
integrity:
  checksum_type: "sha256"
  checksums_file: "assets.csv"              # optional but encouraged
provenance:
  citation_bib: "notes.md#citations"        # anchor or file reference
  prov_template: "data/prov/templates/<template>.json"
```

---

## ğŸ§© Dataset mapping spec (field-level mapping)

Create `mapping.yml` to document how raw fields become canonical fields.

```yaml
dataset: "historical.<source_slug>"
geometry:
  type: "raster|vector"
  expected_geom: "Polygon|LineString|Point|Raster"
  output_crs: "EPSG:4326"
fields:
  - raw: "NAME"
    canonical: "feature_name"
    type: "string"
    rules: ["trim", "title_case"]
  - raw: "DATE"
    canonical: "event_date"
    type: "date"
    rules: ["parse_year_if_needed"]
  - raw: "SOURCE"
    canonical: "source_ref"
    type: "string"
temporal_model:
  note: "Track BOTH map publication date and represented time if they differ."
joins:
  - name: "gazetteer_link"
    key: "feature_name"
    method: "fuzzy"
qa:
  required:
    - "valid_geometry"
    - "non_empty_required_fields"
    - "temporal_range_present"
  optional:
    - "topology_checks"
    - "duplicate_detection"
outputs:
  processed_paths:
    - "data/processed/historical/<source_slug>/<dataset>__v1.geojson"
  metadata_paths:
    stac_item: "data/stac/items/historical/<dataset>.json"
    dcat: "data/catalog/dcat/<dataset>.jsonld"
    prov: "data/prov/<dataset>__v1.prov.json"
```

---

## ğŸ§­ Georeferencing run log (for scanned maps)

If you rectify rasters, create `georeference.md`:

```md
# ğŸ§­ Georeferencing Log â€” <source_slug>

## Source
- Scan filename(s):
- Archive / call number:
- Original map year (printed):
- Region / sheet name:
- Notes on scan quality (folds, warping, stains):

## Target spatial reference
- Target CRS: EPSG:4326 (WGS84) or: __________
- Reference layer used (modern basemap / control layer):
- GCP strategy:
  - Count:
  - Feature types used (road intersections, river confluences, section corners, etc.)
  - Distribution (even coverage? clustered?)

## Transformation
- Method: affine / polynomial / thin-plate spline / projective
- Resampling: nearest / bilinear / cubic
- RMS / residuals:
  - Overall RMS:
  - Worst point residual:
- Known distortions / areas to distrust:

## Outputs
- Output type: COG GeoTIFF
- Output path: data/processed/historical/<source_slug>/<sheet>__cog.tif
- Overviews built: yes/no
- Masking / nodata strategy:

## Review
- Who reviewed:
- Date:
- Open issues:
```

> [!WARNING]
> If georeferencing accuracy is poor, donâ€™t hide it.  
> Record it. Surface it. Bound the claims. ğŸ§¯

---

## ğŸ“¦ Output expectations (KFM-friendly defaults)

To keep downstream usage smooth:
- ğŸ–¼ï¸ **Rasters** â†’ prefer **COGs** (`.tif`) for web/tiling performance.
- ğŸ§¬ **Vectors** â†’ prefer **GeoJSON** for portability; optionally keep source shapefiles in raw.
- ğŸŒ **Google Earth / outreach** â†’ optional exports to **KML/KMZ** where helpful.
- ğŸ§± **Tiles** â†’ generated artifacts belong downstream (typically derived from processed COGs/vectors).

> [!NOTE]
> Final delivery often targets **EPSG:4326** for web mapping and cross-tool interoperability.
> Itâ€™s fine (often preferable) to do intermediate work in a local projected CRSâ€”just document it and convert deterministically.

---

## ğŸ§ª Quality gates (pre-PR)

Before submitting changes, confirm:

- [ ] âœ… License is explicit (or restrictions are clearly documented)
- [ ] âœ… Source URL / archive reference is present
- [ ] âœ… Checksums exist for downloaded artifacts (or a reason why not)
- [ ] âœ… Output CRS is stated (and correct)
- [ ] âœ… Geometry validity checks pass (for vector)
- [ ] âœ… Temporal coverage is represented (even if approximate)
- [ ] âœ… STAC/DCAT/PROV records exist for anything in `data/processed/â€¦`
- [ ] âœ… Pipeline is non-interactive + reproducible
- [ ] âœ… Any uncertainty is documented in `notes.md` (not buried in chat logs ğŸ˜…)

---

## ğŸ§‘â€âš–ï¸ Licensing, ethics, sovereignty

KFM is **provenance-first** and **community-accountable**:
- ğŸ§¾ Never ingest â€œmystery data.â€ If you canâ€™t trace it, it doesnâ€™t ship.
- ğŸ¤ For culturally sensitive materials (especially Indigenous/tribal data), document access constraints and community governance expectations.
- ğŸ§  Prefer transparency over completeness.

---

## ğŸ”— Related project docs (start here)

- ğŸ“š Repo overview: `../../../../README.md`
- ğŸ§­ Canonical pipeline + structure: `../../../../docs/MASTER_GUIDE_v13.md`
- ğŸ§¾ Metadata profiles:
  - `../../../../docs/standards/KFM_STAC_PROFILE.md`
  - `../../../../docs/standards/KFM_DCAT_PROFILE.md`
  - `../../../../docs/standards/KFM_PROV_PROFILE.md`
- ğŸ§± Pipelines: `../../../../src/pipelines/`
- ğŸ§ª Experiments / methods: `../../../../mcp/`

---

## â“FAQ

**Q: Can I just drop a GeoTIFF somewhere and point the UI at it?**  
A: ğŸš« Not in KFM. Data must move through the pipeline and publish catalogs + provenance first.

**Q: Where do I put big scans?**  
A: Prefer external storage + checksums + a deterministic fetch step. If using LFS/pointers, document it in `manifest.yml` and `notes.md`.

**Q: What if the map has no date?**  
A: Use best-available approximation, record your reasoning, and mark uncertainty explicitly in metadata + notes.

---

ğŸ§­ *If this README feels â€œstrictâ€â€¦ good. Thatâ€™s the point. A historical atlas is only as trustworthy as its evidence trail.* âœ…

