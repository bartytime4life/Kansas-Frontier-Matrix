# ðŸ—ºï¸ Historical Mapping Source â€” `{{source_name}}` (`{{source_slug}}`)

![Domain](https://img.shields.io/badge/domain-historical-2b6cb0?style=flat-square)
![Layer](https://img.shields.io/badge/layer-external%20mapping-6b46c1?style=flat-square)
![Contracts](https://img.shields.io/badge/contracts-STAC%20%7C%20DCAT%20%7C%20PROV-2f855a?style=flat-square)
![Pipeline](https://img.shields.io/badge/pipeline-ETL%E2%86%92Catalogs%E2%86%92Graph%E2%86%92API%E2%86%92UI-1f2937?style=flat-square)

> ðŸŽ¯ **Purpose:** This folder documents + configures **how** the external historical map source `{{source_slug}}` is *normalized into KFMâ€™s governed pipeline* (ETL â†’ STAC/DCAT/PROV â†’ Graph â†’ API â†’ UI â†’ Story Nodes).  
> ðŸ§  Think of this as the **â€œmapping contractâ€** for a source â€” *not the source data itself*.

---

## âœ… Source Card (fill this in)

| Field | Value |
|---|---|
| **Source Name** | `{{source_name}}` |
| **Source Slug** | `{{source_slug}}` |
| **Provider / Publisher** | `{{publisher}}` |
| **Upstream URL / Catalog** | `{{source_url}}` |
| **Access Method** | `download` \| `WMS/WMTS` \| `tile service` \| `API` \| `manual scan` |
| **License** | `{{license}}` (include link + constraints) |
| **Attribution String** | `{{required_attribution}}` |
| **Spatial Coverage** | `{{bbox_or_region}}` |
| **Temporal Coverage** | `{{time_start}}` â†’ `{{time_end}}` *(depicted time; not ingest time)* |
| **Data Types** | `raster` (scans/imagery) \| `vector` (boundaries/routes) \| `mixed` |
| **Native CRS** | `{{native_crs}}` |
| **KFM Target CRS** | `{{kfm_target_crs}}` (e.g., EPSG:4326 / EPSG:3857 / local) |
| **Update Cadence** | `static` \| `rare` \| `periodic` (how often upstream changes) |
| **Sensitivity / Governance** | `public` \| `restricted` \| `sensitive` (+ why) |
| **Domain Steward** | `@{{owner_handle}}` |

---

## ðŸ“¦ Where the bytes go (KFM staging + catalogs)

Even though this is an **external mapping** folder, KFM still stages and publishes artifacts using the canonical layout:

- ðŸ“¥ **Raw** (read-only copies / downloads / original scans)  
  `data/raw/historical/{{source_slug}}/`

- ðŸ§ª **Work** (intermediate georeferencing, OCR, warps, temporary vectors)  
  `data/work/historical/{{source_slug}}/`

- âœ… **Processed** (final deliverables for serving + analysis)  
  `data/processed/historical/{{source_slug}}/`

- ðŸŒ **STAC** (assets index: Collections + Items)  
  `data/stac/collections/` + `data/stac/items/`

- ðŸ§¾ **DCAT** (dataset discovery entry)  
  `data/catalog/dcat/`

- ðŸ§¬ **PROV** (lineage bundle: raw â†’ work â†’ processed)  
  `data/prov/`

- ðŸ§Š **External cache** (optional; for on-demand pulls or adapter results)  
  `data/processed/external_cache/{{source_slug}}/` *(dated + parameterized; provenance included)*

> ðŸ§· **Rule of thumb:**  
> This folder (`data/external/mappings/historical/{{source_slug}}/`) defines **how to interpret + normalize** the source.  
> The other locations contain **the actual files + published metadata artifacts**.

---

## ðŸ§­ Canonical pipeline (nonâ€‘negotiable)

```mermaid
flowchart LR
  subgraph Data
    A["Raw Sources"] --> B["ETL + Normalization"]
    B --> C["STAC Items + Collections"]
    C --> D["DCAT Dataset Views"]
    C --> E["PROV Lineage Bundles"]
  end

  C --> G["Neo4j Graph (references back to catalogs)"]
  G --> H["API Layer (contracts + redaction)"]
  H --> I["Map UI â€” React Â· MapLibre Â· (optional) Cesium"]
  I --> J["Story Nodes (governed narratives)"]
  J --> K["Focus Mode (provenance-linked context bundle)"]
```

ðŸ”’ **Hard gates:**
- âœ… **Catalogs before graph/UI** (publish STAC/DCAT/PROV first)
- âœ… **Deterministic + idempotent ETL**
- âœ… **No output can be less restricted than its inputs**
- âœ… **UI never queries the graph directly** (API boundary rule)

---

## ðŸ§© What lives in this folder

Recommended layout (keep it small, config-driven, reviewable):

```text
ðŸ“ data/external/mappings/historical/{{source_slug}}/
â”œâ”€â”€ ðŸ“„ README.md                          # this runbook
â”œâ”€â”€ ðŸ“„ source.yaml                        # source registry card (machine-readable)
â”œâ”€â”€ ðŸ“„ license_notes.md                   # any license constraints / attribution text
â”œâ”€â”€ ðŸ“„ field_mapping.csv                  # external â†’ KFM canonical fields (if applicable)
â”œâ”€â”€ ðŸ“„ stac.collection.template.json      # collection template (optional)
â”œâ”€â”€ ðŸ“„ stac.item.template.json            # item template(s) (optional)
â”œâ”€â”€ ðŸ“„ dcat.dataset.template.jsonld       # dataset discovery template (optional)
â”œâ”€â”€ ðŸ“„ prov.bundle.template.json          # provenance template (optional)
â”œâ”€â”€ ðŸ“ examples/                          # minimal examples + fixtures
â”‚   â”œâ”€â”€ ðŸ“„ example_input.json
â”‚   â”œâ”€â”€ ðŸ“„ example_stac_item.json
â”‚   â””â”€â”€ ðŸ“„ example_prov.json
â””â”€â”€ ðŸ“ tests/                             # validation fixtures (optional)
    â””â”€â”€ ðŸ“„ expected_hashes.json
```

> ðŸ§  **Why templates?**  
> They prevent â€œfreehand metadata.â€ If a new source needs extra fields, we extend the project profiles (STAC/DCAT/PROV) rather than inventing ad-hoc keys.

---

## ðŸ” Ingestion & normalization plan (source-specific)

### 1) Acquire ðŸ“¥
- **Upstream artifacts:** `{{describe_upstream_assets}}`
- **Selection rules:** `{{which_sheets_tiles_regions_are_in_scope}}`
- **Download strategy:** `{{manual|script|api|wms}}`
- **Integrity checks:** checksum + size + MIME sniffing (if available)

âœ… Output: raw bytes land in `data/raw/historical/{{source_slug}}/`

---

### 2) Georeference + standardize ðŸ§­
For scanned historical maps, the goal is a consistent, web-friendly output:

- ðŸŽ¯ **Georeference method:** `GCPs` \| `rubber-sheet` \| `known control points`  
- ðŸ—ºï¸ **Warp / reproject into:** `{{kfm_target_crs}}`
- ðŸ§Š **Raster final format:** Cloud-Optimized GeoTIFF (COG)
- ðŸ§© **Vector final format:** GeoJSON (or GeoPackage for heavy layers)

âœ… Output: intermediates in `data/work/...`, finals in `data/processed/...`

---

### 3) Derivatives (optional) ðŸ§ª
Pick only what the UI / analysis needs:

- ðŸ§± Tiles (XYZ / PMTiles / MBTiles) for fast map rendering
- ðŸ–¼ï¸ Thumbnails + browse images
- ðŸ§¾ OCR text extraction + place-name candidates (if map has labels)
- ðŸ§  AI-derived evidence artifacts (must be cataloged + provenanced like any dataset)

---

### 4) Publish catalogs ðŸ—‚ï¸
Minimum publish set for this source:

- ðŸ›°ï¸ **STAC Collection** for `{{source_slug}}`
- ðŸ§© **STAC Item(s)** for each asset (sheet, tile set, vector layer)
- ðŸ§¾ **DCAT Dataset** entry (discovery + distribution links)
- ðŸ§¬ **PROV bundle** (raw â†’ work â†’ processed, including parameters + software)

---

### 5) Graph / API / UI wiring ðŸ”Œ
- Graph stores **references** (STAC IDs, stable identifiers), not full payloads.
- API is the governance enforcement point (classification, redaction, contracts).
- UI consumes only API outputs + catalog links.

---

## ðŸ§¾ Metadata requirements (STAC/DCAT/PROV)

### STAC (asset-level truth ðŸ›°ï¸)
Every published asset should have:
- Spatial footprint + bbox
- Temporal coverage (depicted date or interval)
- License + attribution
- Clear `href` to processed deliverables
- Link back to upstream identifiers (scan ID, catalog ID, shelfmark, etc.)

### DCAT (dataset discovery ðŸ§¾)
The DCAT record should:
- Describe the dataset in human terms (who/what/where/when/why)
- Include distributions linking to:
  - STAC collection/catalog
  - download endpoints (if any)
  - documentation/runbook (this README)

### PROV (how it was made ðŸ§¬)
The PROV bundle must capture:
- **Entities:** raw files, work artifacts, processed outputs
- **Activities:** georeference, warp, tile build, OCR, vectorization, etc.
- **Agents:** person(s), pipeline script, CI runner
- **Parameters:** CRS target, resampling, GCP set, thresholds
- **Repro anchors:** commit hash + config checksum (when possible)

---

## ðŸ•°ï¸ Temporal semantics (important for â€œhistoricalâ€)

Historical sources often have *multiple â€œtimesâ€*:

- ðŸ—ºï¸ **Depicted time:** the year/date the map represents (goes into STAC `datetime` or `start/end_datetime`)
- ðŸ–¨ï¸ **Publication/print time:** when the map was produced/printed (store as a property)
- ðŸ§ª **Ingest/run time:** when KFM processed it (store in PROV + pipeline logs)
- â™»ï¸ **Dataset version time:** when we reprocessed or corrected it (DCAT/PROV revisions)

> âœ… Keep these distinct so timelines donâ€™t lie.

---

## ðŸ§ª QA & validation checklist

### Geospatial correctness ðŸ§­
- [ ] CRS is explicit and correct (native + target)
- [ ] Georeference residuals documented (if using GCPs)
- [ ] No â€œmystery shiftsâ€ (spot-check known landmarks)
- [ ] Nodata/alpha behaves as expected (no black boxes)
- [ ] Overviews/pyramids exist for rasters (if COG)

### Catalog integrity ðŸ—‚ï¸
- [ ] STAC validates against KFM profile
- [ ] DCAT validates against KFM profile
- [ ] PROV references raw â†’ work â†’ processed chain completely
- [ ] Cross-links exist (DCAT â†” STAC, PROV â†” outputs)
- [ ] License + attribution present on every published asset

### Governance & safety âš–ï¸
- [ ] No sensitive sites exposed without review (archaeological, sacred, restricted)
- [ ] Classification propagated (outputs never â€œless restrictedâ€ than inputs)
- [ ] Terms-of-use respected (no prohibited redistribution)

---

## âš–ï¸ Licensing, attribution, and governance

### Attribution ðŸ·ï¸
Put the **exact attribution string** required by the upstream provider here:

> `{{required_attribution}}`

Also note any â€œmust displayâ€ logo/credit requirements: `{{logo_or_credit_requirements}}`

### Usage constraints ðŸš§
- Redistribution allowed? `{{yes/no/conditional}}`
- Derivatives allowed? `{{yes/no/conditional}}`
- Commercial use allowed? `{{yes/no/conditional}}`
- Share-alike / copyleft? `{{yes/no}}`

### Governance review triggers ðŸ””
This source **must** be governance-reviewed if it involves:
- restricted access terms,
- culturally sensitive/CARE-related content,
- precise locations of protected sites.

---

## ðŸ” Secrets & access (if this source uses an API)

- âœ… Use `.env` / secret manager for keys (never commit secrets)
- âœ… Document rate limits + caching rules
- âœ… Store cached results (if enabled) in `data/processed/external_cache/{{source_slug}}/` with:
  - date
  - parameters
  - provenance metadata
  - attribution

---

## ðŸ§° Suggested commands (examples â€” adapt to actual tooling)

```bash
# 1) Validate mapping config (schemas, required fields, templates)
make validate-source SOURCE={{source_slug}}

# 2) Fetch/download raw assets (if automated)
make fetch SOURCE={{source_slug}}

# 3) Run normalization (georeference/warp/COG + vectors)
make normalize SOURCE={{source_slug}}

# 4) Publish catalogs (STAC/DCAT/PROV) + run validators
make publish SOURCE={{source_slug}}

# 5) (Optional) Register with graph + rebuild API indexes
make graph-load SOURCE={{source_slug}}
make api-reindex
```

> âœ… If your project uses a single orchestrator script instead of `make`, mirror the stages above and keep the stage names consistent.

---

## ðŸ—’ï¸ Known issues / TODO

- [ ] `{{todo_1}}`
- [ ] `{{todo_2}}`
- [ ] `{{todo_3}}`

---

## ðŸ§¾ Changelog

- **{{YYYY-MM-DD}}** â€” Created source mapping folder; initial templates.  
- **{{YYYY-MM-DD}}** â€” Added first STAC/DCAT/PROV outputs; validated in CI.  
- **{{YYYY-MM-DD}}** â€” {{change_summary}}.

---

## ðŸ¤ Ownership & contact

- **Domain steward:** `@{{owner_handle}}`
- **Data engineering:** `@{{data_engineer}}`
- **Governance reviewer:** `@{{governance_reviewer}}`

> ðŸ§­ For project-wide rules, see: `../../../../../docs/MASTER_GUIDE_v13.md` (repo root relative path).

