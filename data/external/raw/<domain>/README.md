# ğŸ§± External Raw Data â€” `<domain>`  

![Stage](https://img.shields.io/badge/stage-raw-blue)
![Scope](https://img.shields.io/badge/scope-external-purple)
![Governance](https://img.shields.io/badge/provenance-first-success)
![Principles](https://img.shields.io/badge/principles-FAIR%20%2B%20CARE-informational)

ğŸ“ **Folder:** `data/external/raw/<domain>/`  
ğŸ¯ **Mission:** Store **immutable, byte-for-byte snapshots** of externally sourced data for the **`<domain>`** domain â€” the â€œmap behind the mapâ€ ğŸ—ºï¸ğŸ” (provenance-first, evidence-backed).:contentReference[oaicite:0]{index=0}

---

## âš¡ Quick Rules (Read This First)

> [!IMPORTANT]
> **Raw means raw.** This directory holds **original source artifacts** (or pointers to them) with **no transformations**.  
> Any conversion, cleaning, georeferencing, OCR, tiling, reprojection, etc. happens **after** raw â€” in work/processed stages.:contentReference[oaicite:1]{index=1}

âœ… **DO**
- Keep **original downloads** intact (same bytes as source).:contentReference[oaicite:2]{index=2}
- Record **where it came from** (URL/archive ref), **what license applies**, and **when/how it was retrieved**.
- Add **checksums** so we can prove integrity over time.
- Treat each new pull as a **new version** (donâ€™t overwrite history).

ğŸš« **DONâ€™T**
- Donâ€™t â€œfixâ€ files here (even â€œminorâ€ edits).
- Donâ€™t commit secrets/keys.
- Donâ€™t shortcut the pipeline (Raw â†’ Processed â†’ Catalog/Prov â†’ Database â†’ API â†’ UI).:contentReference[oaicite:3]{index=3}

---

## ğŸ§­ Domain Profile (Fill This In)

| Field | Value |
|---|---|
| **Domain slug** | `<domain>` |
| **What it covers** | _e.g., historical topo maps, land parcels, hydrology, census, remote sensing, etc._ |
| **Primary upstream sources** | _e.g., USGS, NOAA, Kansas Historical Society, etc._ |
| **Update cadence** | _one-time / monthly / quarterly / ad-hoc_ |
| **Steward / Maintainer** | `@handle` |
| **Default license expectation** | _Public domain / CC / ODbL / custom TOU_ |
| **Sensitive data?** | _None / restricted / review required_ |

> [!NOTE]
> KFM governance emphasizes **traceability, reproducibility, and auditability** â€” every dataset and derived artifact should link back to original sources.:contentReference[oaicite:4]{index=4}

---

## ğŸ—‚ï¸ Recommended Folder Layout

Hereâ€™s a **domain-friendly** structure that keeps raw immutable while still being organized:

```text
ğŸ“ data/
  ğŸ“ external/
    ğŸ“ raw/
      ğŸ“ <domain>/
        ğŸ“„ README.md                ğŸ‘ˆ you are here
        ğŸ“„ sources.json             ğŸ§¾ source registry (per-domain)
        ğŸ“„ CHECKSUMS.sha256         ğŸ” integrity proofs
        ğŸ“ licenses/                âš–ï¸ downloaded license/ToS texts (when available)
        ğŸ“ provider_a/              ğŸ›°ï¸ group by upstream provider
          ğŸ“ dataset_x/
            ğŸ“ vYYYYMMDD/           ğŸ§Š versioned snapshots (never overwrite)
              ğŸ—„ï¸ original_file.ext
              ğŸ—„ï¸ original_file.ext.aux.xml   (if source includes it)
```

Why this shape:
- A **catalog of inputs** is foundational (URLs, extents, coverage, notes).:contentReference[oaicite:5]{index=5}
- Versioning preserves reproducibility and avoids â€œsilent drift.â€

---

## ğŸ§¾ `sources.json` (Per-Domain Source Registry)

A lightweight â€œsource-of-truthâ€ inventory for **inputs**. Inspired by the projectâ€™s catalog-first approach (STAC-like metadata + source info).:contentReference[oaicite:6]{index=6}:contentReference[oaicite:7]{index=7}

### âœ… Suggested schema (example)

```json
{
  "domain": "<domain>",
  "datasets": [
    {
      "id": "provider_a.dataset_x",
      "title": "Dataset X (Provider A)",
      "upstream_url": "https://example.org/datasets/x",
      "retrieved_at": "YYYY-MM-DD",
      "retrieval_method": "download|api|bulk",
      "license": {
        "name": "Public Domain|CC-BY-4.0|ODbL|Custom",
        "url": "https://example.org/license",
        "notes": "Any attribution / restrictions / redistribution limits"
      },
      "coverage": {
        "spatial": "Kansas (statewide) OR bbox/geometry ref",
        "temporal": "YYYY-YYYY OR YYYY-MM-DD..YYYY-MM-DD"
      },
      "files": [
        {
          "path": "provider_a/dataset_x/vYYYYMMDD/original_file.ext",
          "sha256": "<computed>",
          "size_bytes": 123456789
        }
      ],
      "notes": "Any upstream quirks, missing metadata, known issues"
    }
  ]
}
```

> [!TIP]
> If your downstream processing builds a STAC collection/item later, keep that **out** of raw. Raw only holds the **inputs** and their acquisition metadata.

---

## ğŸ” Checksums & Integrity

Store checksums at the domain root (or per dataset/version):

- `CHECKSUMS.sha256` should be reproducible and stable.
- Compute against the **exact bytes** stored in raw.

Example:

```bash
# from data/external/raw/<domain>/
find . -type f ! -name "CHECKSUMS.sha256" -print0 \
  | sort -z \
  | xargs -0 sha256sum > CHECKSUMS.sha256
```

Why: this supports provable integrity and audit trails, aligning with KFMâ€™s provenance-first design.:contentReference[oaicite:8]{index=8}

---

## âš–ï¸ Licensing, Attribution, and â€œFail Closedâ€

> [!CAUTION]
> If we donâ€™t know the license/terms, we treat it as **blocked** until clarified.

KFM policy prefers **fail-closed** governance: CI should prevent data ingestion when license/metadata requirements arenâ€™t met.:contentReference[oaicite:9]{index=9}

### Minimum requirements for each dataset
- License name + link (or archived license text in `licenses/`)
- Attribution requirements (if any)
- Redistribution constraints (if any)
- Any privacy/community restrictions (CARE)

---

## ğŸ§Š Large Files (Git vs. DVC vs. Object Storage)

Raw datasets can be huge (rasters, imagery, LiDAR, etc.). Prefer:
- **DVC** pointers in Git + data in remote storage
- Or **object storage** (S3-compatible) with stable URIs
- Or Git LFS (if you already standardized on it)

The project explicitly anticipates large/binary data needing external storage + reference pointers rather than stuffing Git with massive files.:contentReference[oaicite:10]{index=10}

> [!NOTE]
> Even if the bytes live elsewhere, **raw still needs the evidence trail**: `sources.json` + checksums (for pointer manifests) + license docs.

---

## ğŸ” Canonical Handoff to Next Stages

Raw is only step 1. The canonical order is:

```text
Raw â†’ Processed â†’ Catalog/Prov â†’ Database â†’ API â†’ UI
```

Any shortcut is considered flawed unless explicitly justified.:contentReference[oaicite:11]{index=11}

### After raw is committed, the next moves typically are:
- `data/external/work/<domain>/` â€” scratch/intermediate (unzips, staging, temporary outputs)
- `data/external/processed/<domain>/` â€” cleaned, standardized outputs (COGs, GeoJSON, etc.)
- `data/catalog/` or `data/stac/` â€” STAC/DCAT metadata for discovery and reproducibility:contentReference[oaicite:12]{index=12}

---

## âœ… â€œAdding a New Raw Sourceâ€ Checklist

Use this checklist to keep contributions consistent:

- [ ] Create provider/dataset/version folder: `provider_x/dataset_y/vYYYYMMDD/`
- [ ] Download **original files** (no edits)
- [ ] Add/update `sources.json`
- [ ] Save license/ToS text under `licenses/` (if applicable)
- [ ] Generate `CHECKSUMS.sha256`
- [ ] Ensure no secrets / tokens / PII are included
- [ ] Commit with message: `data(raw/<domain>): add <dataset> vYYYYMMDD`

---

## ğŸ“š Project Alignment & References

This README follows KFMâ€™s **provenance-first** + **pipeline-driven** architecture and the repoâ€™s staged data lifecycle design (raw/work/processed + catalog/prov).:contentReference[oaicite:13]{index=13}:contentReference[oaicite:14]{index=14}

### Key project documents (for deeper context)
- **Kansas Frontier Matrix (KFM) â€“ Comprehensive Technical Blueprint** :contentReference[oaicite:15]{index=15}  
  - Provenance-first framing + FAIR/CARE:contentReference[oaicite:16]{index=16}  
  - Canonical pipeline order:contentReference[oaicite:17]{index=17}

- **MARKDOWN_GUIDE_v13 (Repo + Data Lifecycle conventions)** :contentReference[oaicite:18]{index=18}  
  - Staged data lifecycle (raw/work/processed):contentReference[oaicite:19]{index=19}  
  - Raw is immutable snapshot storage:contentReference[oaicite:20]{index=20}  
  - Governance â€œfail closedâ€ philosophy:contentReference[oaicite:21]{index=21}  
  - Handling large/binary data via DVC/object storage:contentReference[oaicite:22]{index=22}  
  - Citation metadata patterns (`CITATION.cff`):contentReference[oaicite:23]{index=23}

- **Kansas-Frontier-Matrix: Open-Source Geospatial Historical Mapping Hub Design** :contentReference[oaicite:24]{index=24}  
  - Repository/data catalog layout (sources/raw/processed/stac):contentReference[oaicite:25]{index=25}  
  - STAC-like catalog emphasis for source traceability:contentReference[oaicite:26]{index=26}

---

## ğŸ§© Template Notes (Replace These)

- Replace `<domain>` everywhere with your domain slug.
- Update the **Domain Profile** table.
- Add `sources.json`, `CHECKSUMS.sha256`, and a `licenses/` folder once you ingest your first dataset.

