# ğŸ“¦ `data/` â€” Versioned Datasets, Metadata, & Provenance (KFM)

![data](https://img.shields.io/badge/data-versioned-informational)
![metadata](https://img.shields.io/badge/metadata-STAC%20%2B%20DCAT-purple)
![provenance](https://img.shields.io/badge/provenance-W3C%20PROV-blue)
![governance](https://img.shields.io/badge/governance-FAIR%20%2B%20CARE-brightgreen)

Welcome to the **canonical source-of-truth** for Kansas Frontier Matrix (KFM) datasets ğŸ—ºï¸  
This folder is **not â€œjust a dump of filesâ€** â€” itâ€™s a *provenance-first* data vault where **every processed layer is traceable back to raw sources** and **discoverable via catalogs**.

> âœ… **Core rule:** If itâ€™s used by the system, it must be **(1) processed**, **(2) cataloged**, and **(3) provenance-linked**.  
> â›” Anything missing metadata / lineage is considered **not publishable**.

---

## ğŸ§­ Quick Nav

- [ğŸ“ Folder layout](#-folder-layout)
- [ğŸ” Data lifecycle](#-data-lifecycle)
- [ğŸ§± What goes where](#-what-goes-where)
- [âœ… Publishing checklist](#-publishing-checklist)
- [ğŸ·ï¸ Dataset naming & conventions](#ï¸-dataset-naming--conventions)
- [ğŸ§¾ STAC / DCAT / PROV expectations](#-stac--dcat--prov-expectations)
- [ğŸ“¦ Handling large files](#-handling-large-files)
- [ğŸ§ª Example: dataset â€œbundleâ€](#-example-dataset-bundle)
- [ğŸ“š Further reading](#-further-reading)

---

## ğŸ“ Folder layout

> The goal is to make data **diffable**, **reviewable**, and **rebuildable** â€” like code.

```text
ğŸ“¦ data/
â”œâ”€ ğŸ“ raw/                      # Immutable source snapshots (evidence) ğŸ§¾
â”‚  â””â”€ ğŸ“ <domain>/              # e.g., census_1900/, usgs_water/, historical_maps/
â”‚
â”œâ”€ ğŸ§ª (optional) ğŸ“ work/        # Intermediate artifacts (pipelines may write here)
â”‚  â””â”€ ğŸ“ <domain>/
â”‚
â”œâ”€ âœ… ğŸ“ processed/              # Curated outputs used by DB/API/UI
â”‚  â””â”€ ğŸ“ <domain>/              # e.g., census/, weather/, land_treaties/
â”‚
â”œâ”€ ğŸ—‚ï¸ ğŸ“ catalog/               # Discovery metadata (machine-readable)
â”‚  â”œâ”€ ğŸ“ stac/                   # STAC Collections + Items
â”‚  â”‚  â”œâ”€ ğŸ“ collections/
â”‚  â”‚  â””â”€ ğŸ“ items/
â”‚  â””â”€ ğŸ“ dcat/                   # DCAT dataset records (JSON-LD / TTL)
â”‚
â”œâ”€ ğŸ§¬ ğŸ“ provenance/             # Lineage logs (W3C PROV bundles)
â”‚
â””â”€ ğŸ§± (optional) ğŸ“ external/    # Pointers/manifests for large assets (S3/LFS/etc.)
```

> ğŸ’¡ If your repo doesnâ€™t currently include `work/`, treat it as a **recommended staging area**.  
> Pipelines can also use ephemeral temp directories â€” but provenance should still *reference* intermediate steps when relevant.

---

## ğŸ” Data lifecycle

```mermaid
flowchart LR
  A[ğŸ“¥ Raw Sources<br/>data/raw/] --> B[ğŸ§¼ ETL + Normalization<br/>pipelines/]
  B --> C[âœ… Processed Assets<br/>data/processed/]
  C --> D[ğŸ—‚ï¸ Catalog Metadata<br/>STAC + DCAT]
  C --> E[ğŸ§¬ Provenance<br/>W3C PROV]
  D --> F[(ğŸ—ƒï¸ PostGIS / Neo4j<br/>Derived caches)]
  E --> F
  F --> G[ğŸ§° API Layer]
  G --> H[ğŸ—ºï¸ UI / Story Nodes / Focus Mode]
```

**Key idea:** Databases are **derivative performance caches**, not the authoritative store.  
If a DB is wiped, it should be rebuildable from `data/processed/` + metadata.

---

## ğŸ§± What goes where

### ğŸ§¾ `raw/` (immutable â€œevidenceâ€)
- Exact snapshots from original sources (ZIPs, CSVs, shapefiles, PDFs, etc.)
- **Never edited by pipelines**
- If something is wrong with a raw source:
  - Prefer adding a corrected *new* snapshot (or replacing via Git history) + documenting it in metadata/provenance

âœ… Good:
- `data/raw/census_1900/census_1900.csv`
- `data/raw/historical_maps/1930_county_map.pdf`

â›” Not allowed:
- â€œFixingâ€ typos in a raw CSV by hand without recording a new snapshot + provenance

---

### âœ… `processed/` (curated â€œready to useâ€ outputs)
- Cleaned, standardized, analysis-ready products
- What the platform ultimately serves (directly or via DB loading scripts)
- Prefer **open formats** and **review-friendly diffs** (more below)

âœ… Good:
- `data/processed/census/1900_population.geojson`
- `data/processed/weather/daily_rainfall.parquet`
- `data/processed/imagery/landsat_2010_kansas.tif` (may be pointer-managed if huge)

---

### ğŸ—‚ï¸ `catalog/` (make data *findable*)
KFM uses:
- **STAC** for spatial/temporal asset metadata (collections + items)
- **DCAT** for higher-level dataset discovery entries

The catalog should enable queries like:
- â€œwheat yields in the 1930sâ€
- â€œprecipitation time series for western Kansasâ€

---

### ğŸ§¬ `provenance/` (make data *trustworthy*)
Provenance records should answer:
- **What inputs produced this output?**
- **Which pipeline + parameters were used?**
- **When did it run, and under what version/commit?**
- **Who/what ran it (agent)?**

If a dataset has no provenance record, treat it as a ğŸš© red flag.

---

## âœ… Publishing checklist

When adding or updating a dataset, a PR is only â€œdoneâ€ when **all** items below are complete:

- [ ] ğŸ“¥ Raw snapshot stored under `data/raw/<domain>/...` (or referenced if externally hosted)
- [ ] ğŸ§¼ Pipeline exists/updated under `pipelines/` and is **deterministic**
- [ ] âœ… Outputs written to `data/processed/<domain>/...`
- [ ] ğŸ—‚ï¸ STAC Collection + Item(s) created/updated (links to assets + provenance)
- [ ] ğŸ—‚ï¸ DCAT record created/updated (title, description, license, keywords, distributions)
- [ ] ğŸ§¬ PROV bundle created/updated (raw â†’ work â†’ processed, agents, timestamps, parameters)
- [ ] âš–ï¸ License clearly declared (and compatible with repo policy)
- [ ] ğŸ§ª Validation passes (schemas, geometry validity, required fields, metadata completeness)
- [ ] ğŸ” PR review includes **data diffs + metadata diffs** (not just code)

> ğŸ§¯ Governance note: KFM is designed to **fail closed**. If metadata/license/provenance is missing, CI should block merge.

---

## ğŸ·ï¸ Dataset naming & conventions

### ğŸ“› Domain folders
Use `snake_case` domain names that match the real-world theme/source:
- `census`, `weather`, `land_treaties`, `railroads`, `soil`, `historical_maps`

### ğŸ§© Dataset IDs (recommended)
A stable dataset identifier keeps catalogs + provenance + narratives aligned:
- `dataset_id`: `kfm.<domain>.<topic>.<version_or_year>`

Example:
- `kfm.census.population.1900`
- `kfm.weather.precip.daily.v1`

### ğŸ—ºï¸ Spatial reference & units
- Prefer a **common coordinate system** across processed layers unless thereâ€™s a strong reason not to.
- Explicitly document:
  - CRS / EPSG
  - Units
  - Null conventions
  - Timezone / temporal resolution (for time series)

---

## ğŸ§¾ STAC / DCAT / PROV expectations

### ğŸ—‚ï¸ STAC (spatiotemporal asset metadata)
A STAC Item typically includes:
- bbox / geometry
- datetime or start/end
- keywords + providers
- license
- links to:
  - the processed asset
  - the provenance bundle

Recommended structure:
- `data/catalog/stac/collections/<collection_id>.json`
- `data/catalog/stac/items/<item_id>.json`

---

### ğŸ—‚ï¸ DCAT (dataset discovery metadata)
DCAT record should minimally include:
- title + description
- license
- keywords/tags
- distributions (links to STAC entry and/or direct asset)

Recommended structure:
- `data/catalog/dcat/<dataset_id>.jsonld`

---

### ğŸ§¬ PROV (lineage + reproducibility)
A provenance bundle should capture:
- **Entities:** raw inputs, intermediate artifacts, processed outputs
- **Activities:** pipeline run(s), parameters, transformations
- **Agents:** script + version, and (when relevant) human trigger

Recommended structure:
- `data/provenance/<dataset_id>.prov.json`

> ğŸ’¡ Bonus: include pipeline Git commit SHA in the PROV activity so future reruns are reproducible.

---

## ğŸ“¦ Handling large files

Some geospatial assets get big fast (rasters, point clouds, dense time series). KFMâ€™s stance:

- âœ… Smallâ€“medium files: store directly in Git (diff-friendly when possible)
- ğŸ§± Large binaries: store via one of these patterns:
  1) **Git LFS** pointer files  
  2) **External object storage** (S3/Blob) + a **recorded checksum/hash** in-repo  
  3) Chunked, diffable formats (e.g., line-delimited GeoJSON) when feasible

Recommended:
- Keep a manifest under `data/external/` that records:
  - asset logical name
  - storage location
  - size
  - checksum (sha256)
  - retrieval method

---

## ğŸ§ª Example: dataset bundle

Letâ€™s say weâ€™re adding a historical census extract:

```text
data/
â”œâ”€ raw/
â”‚  â””â”€ census_1900/
â”‚     â””â”€ census_1900.csv
â”‚
â”œâ”€ processed/
â”‚  â””â”€ census/
â”‚     â””â”€ 1900_population.geojson
â”‚
â”œâ”€ catalog/
â”‚  â”œâ”€ stac/
â”‚  â”‚  â”œâ”€ collections/
â”‚  â”‚  â”‚  â””â”€ kfm.census.population.json
â”‚  â”‚  â””â”€ items/
â”‚  â”‚     â””â”€ kfm.census.population.1900.json
â”‚  â””â”€ dcat/
â”‚     â””â”€ kfm.census.population.1900.jsonld
â”‚
â””â”€ provenance/
   â””â”€ kfm.census.population.1900.prov.json
```

âœ… Now the dataset is:
- **Usable** (processed file exists)
- **Findable** (STAC/DCAT records exist)
- **Auditable** (PROV exists)

---

## ğŸ“š Further reading

These project references strongly influenced how `data/` is organized:

- ğŸ“˜ *Kansas Frontier Matrix (KFM) â€“ Comprehensive Technical Blueprint* (data storage, catalogs, provenance, governance)
- ğŸ§­ *MARKDOWN_GUIDE_v13* (STAC/DCAT/PROV alignment policy, CI gating, lifecycle diagrams)
- ğŸ§  *Data Spaces* (why governance + trustworthy data ecosystems matter)
- ğŸ›°ï¸ *Cloud-Based Remote Sensing with Google Earth Engine* (remote sensing workflows & dataset patterns)
- â³ *Visualization of Time-Oriented Data* (spatiotemporal/time-series analysis & visualization ideas)

---