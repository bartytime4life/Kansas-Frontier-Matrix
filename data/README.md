# ğŸ“¦ `data/` â€” Versioned Datasets, Metadata, & Provenance (KFM) ğŸ—ºï¸

![data](https://img.shields.io/badge/data-versioned-informational)
![truth-path](https://img.shields.io/badge/truth%20path-raw%E2%86%92processed%E2%86%92catalog%E2%86%92api-blue)
![catalogs](https://img.shields.io/badge/catalogs-STAC%20%2B%20DCAT-purple)
![provenance](https://img.shields.io/badge/provenance-W3C%20PROV-005a9c)
![governance](https://img.shields.io/badge/governance-FAIR%20%2B%20CARE-brightgreen)
![fail-closed](https://img.shields.io/badge/policy-fail--closed-critical)
![ci-gated](https://img.shields.io/badge/CI-metadata%20%2B%20license%20gates-orange)
![formats](https://img.shields.io/badge/formats-geojson%20%7C%20parquet%20%7C%20cog%20%7C%20csv-lightgrey)

Welcome to the **canonical source-of-truth** for Kansas Frontier Matrix (KFM) datasets ğŸ§¾  
This folder is **not â€œjust a dump of filesâ€** â€” itâ€™s a *provenance-first* data vault where **every processed layer is traceable back to raw sources** and **discoverable via catalogs**.

> âœ… **KFM invariant:** if itâ€™s used by the system, it must be **(1) processed**, **(2) cataloged**, and **(3) provenance-linked**.  
> â›” Anything missing metadata / lineage / license is treated as **not publishable** (fail-closed by design). ğŸ”’

---

## ğŸ§­ Quick Nav

- [ğŸ“ Folder layout (v13 canonical)](#folder-layout-v13-canonical)
- [ğŸ§© Legacy path compatibility map](#legacy-path-compatibility-map)
- [ğŸ” The Truth Path lifecycle](#the-truth-path-lifecycle)
- [ğŸ§± What goes where](#what-goes-where)
- [ğŸ§¾ STAC / DCAT / PROV alignment](#stac--dcat--prov-alignment)
- [ğŸ·ï¸ Dataset naming & conventions](#dataset-naming--conventions)
- [âœ… Publishing checklist (Definition of Done)](#publishing-checklist-definition-of-done)
- [ğŸ§ª Example: dataset â€œbundleâ€](#example-dataset-bundle)
- [ğŸ“¦ Handling large files](#handling-large-files)
- [ğŸ¤– Evidence artifacts (AI + analysis outputs)](#evidence-artifacts-ai--analysis-outputs)
- [ğŸ§ª Validation & CI gates](#validation--ci-gates)
- [ğŸ“š Further reading](#further-reading)

---

<a id="folder-layout-v13-canonical"></a>
## ğŸ“ Folder layout (v13 canonical)

> Goal: make data **diffable**, **reviewable**, **rebuildable** â€” like code. âœ…  
> Databases are **derivative performance caches**, not the authoritative store.

```text
ğŸ“¦ data/
â”œâ”€ ğŸ§¾ raw/                         # Immutable source snapshots (evidence)
â”‚  â””â”€ ğŸ“ <domain>/                 # e.g., census_1900/, usgs_water/, historical_maps/
â”‚
â”œâ”€ ğŸ§ª work/                        # Intermediate artifacts (recommended staging)
â”‚  â””â”€ ğŸ“ <domain>/
â”‚
â”œâ”€ âœ… processed/                   # Curated outputs used by DB/API/UI
â”‚  â””â”€ ğŸ“ <domain>/
â”‚
â”œâ”€ ğŸ—ºï¸ stac/                        # Spatial/temporal discovery metadata
â”‚  â”œâ”€ ğŸ“ collections/              # STAC Collections
â”‚  â””â”€ ğŸ“ items/                    # STAC Items
â”‚
â”œâ”€ ğŸ—‚ï¸ catalog/
â”‚  â””â”€ ğŸ“ dcat/                     # DCAT dataset records (JSON-LD / TTL)
â”‚
â”œâ”€ ğŸ§¬ prov/                        # Lineage logs (W3C PROV bundles)
â”‚
â””â”€ ğŸ§± external/                    # Manifests/pointers for large assets (LFS/S3/etc.)
   â””â”€ ğŸ“„ manifest.*                # JSON/YAML w/ sha256, size, retrieval method
```

### ğŸ”— Nearby (not inside `data/`, but tightly coupled)
```text
ğŸ§¾ schemas/                        # JSON Schemas for STAC/DCAT/PROV + contracts
âš™ï¸ src/pipelines/                  # Deterministic ETL that writes rawâ†’workâ†’processed
ğŸ“š docs/data/<domain>/README.md     # Domain runbooks & source notes (required for new domains)
```

---

<a id="legacy-path-compatibility-map"></a>
## ğŸ§© Legacy path compatibility map

KFM evolved over time. Some repos may still contain older folder names. âœ…  
**Preferred rule:** write *new* work to the **v13 canonical** layout above, and migrate/alias legacy paths as needed.

| Concept | v13 canonical | Legacy patterns you may still see |
|---|---|---|
| STAC metadata | `data/stac/...` | `data/catalog/stac/...` |
| DCAT metadata | `data/catalog/dcat/...` | `data/catalog/dcat/...` (usually same) |
| Provenance | `data/prov/...` | `data/provenance/...` |
| Intermediate artifacts | `data/work/...` | *(missing; pipelines wrote temp files elsewhere)* |

> ğŸ’¡ If you canâ€™t migrate immediately: consider **symlinks** or a **thin redirect** (README + pointers) so tooling can find canonical outputs.

---

<a id="the-truth-path-lifecycle"></a>
## ğŸ” The Truth Path lifecycle (non-negotiable ordering)

```mermaid
flowchart LR
  subgraph Data["ğŸ“¦ Data"]
    A[ğŸ§¾ Raw Sources<br/>data/raw/] --> B[ğŸ§ª Work Artifacts<br/>data/work/]
    B --> C[âœ… Processed Assets<br/>data/processed/]
    C --> D[ğŸ—ºï¸ STAC<br/>data/stac/]
    C --> E[ğŸ—‚ï¸ DCAT<br/>data/catalog/dcat/]
    C --> F[ğŸ§¬ PROV<br/>data/prov/]
  end

  subgraph Knowledge["ğŸ§  Storage & Knowledge"]
    D --> G[(ğŸ—ƒï¸ PostGIS<br/>spatial cache)]
    D --> H[(ğŸ•¸ï¸ Neo4j<br/>semantic graph)]
    F --> G
    F --> H
  end

  subgraph Delivery["ğŸš€ Delivery"]
    G --> I[ğŸ§° API Layer<br/>(contracts + redaction)]
    H --> I
    I --> J[ğŸ—ºï¸ Map UI<br/>React Â· MapLibre Â· (optional) Cesium]
    J --> K[ğŸ“– Story Nodes<br/>governed narratives]
    K --> L[ğŸ¯ Focus Mode<br/>provenance-linked context bundle]
  end
```

**Key idea:** wipe the DB? No problem. Rebuild from `data/processed/` + STAC/DCAT/PROV + pipeline code. â™»ï¸

---

<a id="what-goes-where"></a>
## ğŸ§± What goes where

### ğŸ§¾ `raw/` â€” immutable â€œevidenceâ€
- Exact snapshots from original sources (ZIPs, CSVs, Shapefiles, PDFs, imagery, etc.)
- **Never edited by pipelines**
- If a source is wrong: add a corrected **new snapshot** and document the change in metadata + provenance

âœ… Good:
- `data/raw/census_1900/census_1900.csv`
- `data/raw/historical_maps/1930_county_map.pdf`

â›” Not allowed:
- Hand-editing raw CSV rows â€œjust to fix a typoâ€ without a new snapshot + provenance

---

### ğŸ§ª `work/` â€” intermediate artifacts (recommended)
- Temporary outputs that matter for auditability or reproducibility
- Examples: cloud masks, cleaned-but-not-final tables, intermediate joins, QA summaries

âœ… Good:
- `data/work/weather/noaa_cleaned_1900_1950.parquet`
- `data/work/imagery/landsat_cloudmask_2010.tif`

> ğŸ’¡ Pipelines may also use ephemeral temp dirs, but if an intermediate step affects interpretability, capture it here and link it in PROV.

---

### âœ… `processed/` â€” curated â€œready-to-serveâ€
- Cleaned, standardized, analysis-ready products
- Primary inputs to DB loaders and API serving
- Prefer **open formats** + **review-friendly diffs** (when possible)

Recommended format defaults:
- Vector: GeoJSON / GeoParquet
- Tabular/time-series: Parquet (partitioned by time/region if large)
- Raster: Cloud Optimized GeoTIFF (COG) or Zarr (when appropriate)

âœ… Good:
- `data/processed/census/1900_population.geojson`
- `data/processed/weather/daily_rainfall.parquet`
- `data/processed/imagery/landsat_2010_kansas.cog.tif`

---

### ğŸ—ºï¸ `stac/` â€” spatial/temporal asset metadata (Findable âœ…)
STAC should answer:  
- â€œWhat is this layer?â€  
- â€œWhere/when does it apply?â€  
- â€œHow do I access the actual asset?â€  
- â€œWhat provenance explains this output?â€

Recommended structure:
- `data/stac/collections/<collection_id>.json`
- `data/stac/items/<item_id>.json`

---

### ğŸ—‚ï¸ `catalog/dcat/` â€” dataset discovery metadata (Discoverable ğŸ”)
DCAT provides higher-level dataset records:
- title, description, publisher/creator, keywords
- license and access notes
- distributions (links to STAC, direct files, or external manifests)

Recommended structure:
- `data/catalog/dcat/<dataset_id>.jsonld`

---

### ğŸ§¬ `prov/` â€” provenance bundles (Trustworthy ğŸ§ )
Provenance should answer:
- **What inputs produced this output?**
- **Which pipeline + parameters were used?**
- **When did it run, and under what repo version/commit?**
- **Who/what ran it (agent)?**

> ğŸš© If a dataset has no provenance bundle, treat it as *suspect* until proven otherwise.

Recommended structure:
- `data/prov/<dataset_id>.prov.json`

---

### ğŸ§± `external/` â€” pointers/manifests for huge assets
Used when assets are too large for normal Git storage:
- Git LFS pointer files
- S3/Blob object storage references
- chunked stores (Zarr) with checksums

Recommended: keep **a manifest** that records:
- logical name
- storage location
- size
- checksum (sha256)
- retrieval method & credentials assumptions (if any)

---

<a id="stac--dcat--prov-alignment"></a>
## ğŸ§¾ STAC / DCAT / PROV alignment

KFM expects **cross-linking** between boundary artifacts so downstream stages can navigate evidence.

### ğŸ”— Cross-linking expectations (minimum)
- **STAC Item â†’ PROV** (link: â€œprovenanceâ€)
- **STAC Item â†’ asset(s)** (local file or external pointer)
- **DCAT Dataset â†’ STAC** (distribution link)
- **PROV â†’ raw + work + processed entities** (entities with hashes where possible)

> âœ… Think of STAC/DCAT/PROV as the **API contract** for the data layer.

### ğŸ§© Recommended â€œdataset contractâ€
A dataset is considered complete when these exist (at minimum):

- `data/processed/<domain>/<something>.<ext>`
- `data/stac/items/<dataset_id>.json`
- `data/catalog/dcat/<dataset_id>.jsonld`
- `data/prov/<dataset_id>.prov.json`

Optional but encouraged:
- `data/stac/collections/<collection_id>.json` (when introducing a new collection)
- `data/external/manifest.json` (when any asset is external/LFS-managed)
- `docs/data/<domain>/README.md` (domain runbook)

---

<a id="dataset-naming--conventions"></a>
## ğŸ·ï¸ Dataset naming & conventions

### ğŸ“› Domain folders
Use `snake_case` domain names that match the real-world theme/source:
- `census`, `weather`, `land_treaties`, `railroads`, `soil`, `historical_maps`, `imagery`

### ğŸ§© Dataset IDs (recommended)
A stable dataset identifier keeps catalogs + provenance + narratives aligned:

**Format**
- `kfm.<domain>.<topic>.<version_or_year>`

**Examples**
- `kfm.census.population.1900`
- `kfm.weather.precip.daily.v1`
- `kfm.historical_maps.county_boundaries.1930`

### ğŸ—‚ï¸ Processed file naming (recommended)
Make file names â€œscan readableâ€:

`<topic>__<coverage>__<time>__<vX>.<ext>`

Example:
- `population__kansas__1900__v1.geojson`
- `precip__kansas__daily__1850-2020__v2.parquet`

### ğŸ—ºï¸ Spatial reference & units
Every processed dataset must document:
- CRS / EPSG
- units
- null conventions
- temporal resolution & timezone assumptions (for time series)

> ğŸ’¡ Put the human-friendly notes in DCAT + domain README, and the machine-critical fields in STAC + PROV.

---

<a id="publishing-checklist-definition-of-done"></a>
## âœ… Publishing checklist (Definition of Done)

A dataset PR is only â€œdoneâ€ when **all** required artifacts exist and pass validation.

### âœ… Required
- [ ] ğŸ“¥ Raw snapshot stored under `data/raw/<domain>/...` (or referenced via `data/external/`)
- [ ] ğŸ§¼ Deterministic pipeline exists/updated (writes `raw â†’ work â†’ processed`)
- [ ] âœ… Outputs written to `data/processed/<domain>/...`
- [ ] ğŸ—ºï¸ STAC Item created/updated (assets + bbox/time + links)
- [ ] ğŸ—‚ï¸ DCAT record created/updated (title/desc/license/keywords/distributions)
- [ ] ğŸ§¬ PROV bundle created/updated (entities, activities, agents, parameters)
- [ ] âš–ï¸ License clearly declared (and compatible with repo policy)
- [ ] ğŸ§ª Validation passes (schemas, geometry validity, required fields)
- [ ] ğŸ” Review includes **data diffs + metadata diffs** (not just code)

### ğŸŒŸ Strongly recommended
- [ ] ğŸ“š Domain runbook updated: `docs/data/<domain>/README.md`
- [ ] ğŸ” Sensitivity classification recorded (CARE-aware handling)
- [ ] ğŸ§¾ External manifest includes sha256 for any off-repo asset

---

<a id="example-dataset-bundle"></a>
## ğŸ§ª Example: dataset bundle

Letâ€™s say we add a historical census extract:

```text
data/
â”œâ”€ raw/
â”‚  â””â”€ census_1900/
â”‚     â””â”€ census_1900.csv
â”‚
â”œâ”€ work/
â”‚  â””â”€ census/
â”‚     â””â”€ census_1900_cleaned.parquet
â”‚
â”œâ”€ processed/
â”‚  â””â”€ census/
â”‚     â””â”€ population__kansas__1900__v1.geojson
â”‚
â”œâ”€ stac/
â”‚  â”œâ”€ collections/
â”‚  â”‚  â””â”€ kfm.census.population.json
â”‚  â””â”€ items/
â”‚     â””â”€ kfm.census.population.1900.json
â”‚
â”œâ”€ catalog/
â”‚  â””â”€ dcat/
â”‚     â””â”€ kfm.census.population.1900.jsonld
â”‚
â””â”€ prov/
   â””â”€ kfm.census.population.1900.prov.json
```

âœ… Now the dataset is:
- **Usable** (processed file exists)
- **Findable** (STAC + DCAT exist)
- **Auditable** (PROV exists)
- **Rebuildable** (pipeline + raw evidence exist)

---

<a id="handling-large-files"></a>
## ğŸ“¦ Handling large files

Geospatial assets get big fast (rasters, point clouds, dense time-series). KFMâ€™s stance:

- âœ… Smallâ€“medium: store directly in Git (prefer diff-friendly formats)
- ğŸ§± Large binaries: store via one of these patterns:
  1) **Git LFS** pointer files  
  2) **External object storage** (S3/Blob) + **checksum/hash recorded in-repo**  
  3) **Chunked, diffable** structures (e.g., partitioned Parquet; Zarr for rasters)

### ğŸ§¾ External manifest (recommended)
Store a manifest under `data/external/` for anything that isnâ€™t fully in Git:

```json
{
  "assets": [
    {
      "logical_name": "landsat_2010_kansas.cog.tif",
      "storage": "s3",
      "uri": "s3://kfm-data/imagery/landsat_2010_kansas.cog.tif",
      "sha256": "REPLACE_ME",
      "size_bytes": 1234567890,
      "retrieval": "aws s3 cp ..."
    }
  ]
}
```

---

<a id="evidence-artifacts-ai--analysis-outputs"></a>
## ğŸ¤– Evidence artifacts (AI + analysis outputs)

KFM treats AI outputs and analysis artifacts as **first-class datasets** ğŸ§   
Examples:
- OCR-derived corpora
- model-predicted map layers
- simulation outputs
- QA-derived â€œconfidence layersâ€

**Rule:** if an artifact can influence a narrative, map, or query result, it must be:
- âœ… stored in `data/processed/...`
- ğŸ—ºï¸ cataloged (STAC/DCAT)
- ğŸ§¬ provenance-linked (PROV)
- ğŸ” governed (license + sensitivity + validation)

> ğŸ§¯ No â€œblack boxâ€ evidence: derived artifacts must be explainable, traceable, and reviewable.

---

<a id="validation--ci-gates"></a>
## ğŸ§ª Validation & CI gates

KFM is designed to **fail closed** ğŸ”’  
CI should block merges when:
- metadata is missing
- provenance is missing
- license is missing/unclear
- schemas donâ€™t validate
- geometries are invalid
- external assets lack checksums

### âœ… Minimum checks (suggested)
- STAC JSON schema validation
- DCAT JSON-LD validation (or shape constraints)
- PROV schema validation
- â€œbundle completenessâ€ check (processed â†” STAC â†” DCAT â†” PROV)
- basic geometry validity + bounding-box sanity
- required fields present (domain-specific)
- checksum verification for external manifests

---

## ğŸ“š Further reading

These project references influenced how `data/` is organized:

- ğŸ“˜ *Kansas Frontier Matrix (KFM) â€“ Comprehensive Technical Blueprint* ğŸ§©
- ğŸ§­ *MARKDOWN_GUIDE_v13* (pipeline ordering, governance, STAC/DCAT/PROV alignment) ğŸ—ï¸
- ğŸ§  *Data Spaces* (data ecosystems, access control patterns, governance) ğŸ”
- ğŸ›°ï¸ *Cloud-Based Remote Sensing with Google Earth Engine* (remote sensing workflows & dataset patterns) â˜ï¸
- â³ *Visualization of Time-Oriented Data* (spatiotemporal/time-series analysis & visualization ideas) ğŸ•°ï¸

---