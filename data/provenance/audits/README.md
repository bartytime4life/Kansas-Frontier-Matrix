<div align="center">

# ğŸ§¾ Kansas-Frontier-Matrix â€” Provenance Audits  
`data/provenance/audits/`

[![Build & Deploy](https://github.com/bartytime4life/Kansas-Frontier-Matrix/actions/workflows/site.yml/badge.svg)](https://github.com/bartytime4life/Kansas-Frontier-Matrix/actions/workflows/site.yml)  
[![STAC Validate](https://github.com/bartytime4life/Kansas-Frontier-Matrix/actions/workflows/stac-validate.yml/badge.svg)](https://github.com/bartytime4life/Kansas-Frontier-Matrix/actions/workflows/stac-validate.yml)  
[![CodeQL](https://github.com/bartytime4life/Kansas-Frontier-Matrix/actions/workflows/codeql.yml/badge.svg)](https://github.com/bartytime4life/Kansas-Frontier-Matrix/actions/workflows/codeql.yml)  
[![Trivy](https://github.com/bartytime4life/Kansas-Frontier-Matrix/actions/workflows/trivy.yml/badge.svg)](https://github.com/bartytime4life/Kansas-Frontier-Matrix/actions/workflows/trivy.yml)

**Mission:** Provide **dataset-level audit logs** that capture transformations, control points, parameters,  
and quality reports for every dataset in the **Kansas Frontier Matrix**.  

Audits extend the **provenance registry** by storing **detailed reproducibility records**:  
GCP residuals, ETL configs, model parameters, QA/QC checks, and lineage notes.  

</div>

---

## ğŸ“ˆ Lifecycle

```mermaid
flowchart TD
  A["Raw data\n(data/raw/**)"] --> B["Process / Transformation\n(scripts, Makefile targets)"]
  B --> C["Audit logs\n(data/provenance/audits/**)"]
  C --> D["Registry entries\n(data/provenance/registry.json)"]
  C --> E["STAC Items\n(data/stac/items/**)"]
  E --> F["Validate\n(stac-validate)"]
  F --> G["Viewer + Experiments\n(web/config/** Â· experiments/**)"]

<!-- END OF MERMAID -->



â¸»

ğŸ¯ Purpose
	â€¢	Document how each dataset was transformed (tools, parameters, configs)
	â€¢	Provide QA/QC reports (RMS error, validation stats, missing values)
	â€¢	Record GCP alignment results for historic maps and aerials
	â€¢	Store ETL configs for tabular/vector cleanup (field mappings, dissolves)
	â€¢	Guarantee lineage transparency for MCP reproducibility

â¸»

ğŸ“‚ Directory Layout

data/provenance/audits/
â”œâ”€â”€ topo_1894_larned_gcp.json    # GCP residuals for USGS Larned topo sheet
â”œâ”€â”€ dem_merge_2018.log           # GDAL/COG conversion + merge parameters
â”œâ”€â”€ floodplain_fema2020_qc.json  # Validation report for FEMA floodplain dataset
â”œâ”€â”€ treaty_overlay_etl.yaml      # ETL config for treaty boundary normalization
â””â”€â”€ README.md


â¸»

ğŸ§­ File Conventions
	â€¢	JSON â†’ QA/QC reports, structured lineage data (residuals, validation stats)
	â€¢	YAML â†’ ETL configs, processing pipelines
	â€¢	LOG / TXT â†’ direct script or GDAL/CLI logs
	â€¢	CSV â†’ tabular QA outputs (point errors, mismatch tables)

Each audit file should include:
	â€¢	Dataset ID (matches registry.json + STAC Item)
	â€¢	Processing date & tool versions
	â€¢	Parameters used (warp order, thresholds, CRS, etc.)
	â€¢	QA/QC results (residuals, completeness, validation checks)
	â€¢	Author / pipeline / commit reference

â¸»

ğŸ“‘ Example Audit (GCP residuals for historic topo)

{
  "dataset_id": "usgs_topo_1894_larned",
  "date": "2025-09-22",
  "tool": "gdalwarp",
  "parameters": {
    "order": 2,
    "target_epsg": 4326,
    "points_used": 10
  },
  "qa": {
    "rms_error": 3.1,
    "max_error": 6.4,
    "units": "meters"
  },
  "lineage_commit": "abc123def456",
  "notes": "Residuals acceptable; control points concentrated along Arkansas River confluence."
}


â¸»

ğŸ”— Integration
	â€¢	Provenance registry â†’ audit files linked in registry.json under each dataset
	â€¢	STAC Items â†’ kfm:lineage or custom audit:href points to audit file
	â€¢	Makefile workflows â†’ audits generated alongside processing outputs
	â€¢	Experiments â†’ MCP experiment logs cite audits for reproducibility

â¸»

ğŸ“ Notes
	â€¢	Every non-trivial dataset (warped, merged, modeled, digitized) must have an audit file
	â€¢	Keep audit logs small & structured (JSON/YAML preferred)
	â€¢	Logs are append-only; never overwrite, only version with new commit reference
	â€¢	Store raw GCPs in data/gcp/ â†’ results (residuals, QA) live here in audits

â¸»

ğŸ“š See Also
	â€¢	../ â†’ provenance registry and licenses
	â€¢	../../gcp/ â†’ Ground Control Points (inputs for georeferencing)
	â€¢	../../stac/items/ â†’ STAC metadata for datasets
	â€¢	../../experiments/ â†’ MCP notebooks + experiment logs

â¸»

âœ… Mission Principle

Audits are the forensic backbone of Kansas Frontier Matrix.
Every dataset must have a traceable, validated, and reproducible audit trail.