---
title: "ğŸ’§ Kansas Frontier Matrix â€” Hydrology Processed Data Index (v11.2.6)"
path: "data/hydrology/processed/README.md"
version: "v11.2.6"
last_updated: "2025-12-11"

release_stage: "Stable / Governed"
lifecycle: "Long-Term Support (LTS)"
lifecycle_stage: "stable"
review_cycle: "Annual Â· Hydrology & Hazards Council"
content_stability: "stable"

status: "Active / Enforced"
doc_kind: "Dataset Index"
intent: "hydrology-processed-dataset-index"
role: "archive-registry"
category: "Data Â· Hydrology Â· Processed Index"

license: "CC-BY 4.0"
mcp_version: "MCP-DL v6.3"
markdown_protocol_version: "KFM-MDP v11.2.6"
ontology_protocol_version: "KFM-OP v11"
pipeline_contract_version: "KFM-PDC v11"
stac_profile: "KFM-STAC v11"
dcat_profile: "KFM-DCAT v11"
prov_profile: "KFM-PROV v11"
fencing_profile: "outer-backticks-inner-tildes-v1"
header_profile: "standard"
footer_profile: "standard"

scope:
  domain: "data-hydrology-processed"
  applies_to:
    - "data/hydrology/processed/**"

fair_category: "F1-A1-I1-R1"
care_label: "Public Â· Mixed-Risk"
sensitivity: "General hydrology; localized masking for sensitive sites"
sensitivity_level: "Mixed"
public_exposure_risk: "Dataset-level"
classification: "Public"
jurisdiction: "Kansas / United States"
indigenous_rights_flag: true
data_steward: "KFM FAIR+CARE Council"
risk_category: "Mixed"
redaction_required: false

ai_transform_permissions:
  - "summarize"
  - "extract-schema-and-contracts"
  - "generate-story-node-suggestions"
  - "suggest-visualizations"
ai_transform_prohibited:
  - "invent-or-alter-source-data"
  - "override-governance-or-licenses"
  - "expose-exact-sensitive-site-coordinates"
  - "reinterpret-Indigenous-knowledge-without-sources"

commit_sha: "<latest-commit-hash>"
previous_version_hash: "<previous-sha256>"
doc_integrity_checksum: "<sha256>"
semantic_document_id: "kfm-data-hydrology-processed-domain-index"
doc_uuid: "urn:kfm:data:hydrology:processed:index:v11.2.6"
event_source_id: "ledger:data/hydrology/processed/README.md"
immutability_status: "version-pinned"

sbom_ref: "../../../releases/v11.2.6/sbom.spdx.json"
manifest_ref: "../../../releases/v11.2.6/manifest.zip"
telemetry_ref: "../../../releases/v11.2.6/focus-telemetry.json"
telemetry_schema: "../../../schemas/telemetry/data-hydrology-index-v1.json"

governance_ref: "../../../docs/standards/governance/ROOT-GOVERNANCE.md"
ethics_ref: "../../../docs/standards/faircare/FAIRCARE-GUIDE.md"
sovereignty_policy: "../../../docs/standards/sovereignty/INDIGENOUS-DATA-PROTECTION.md"

machine_extractable: true
accessibility_compliance: "WCAG 2.1 AA+"
ttl_policy: "Annual review"
sunset_policy: "Superseded upon next hydrology-processed-domain update"
---

<div align="center">

# ğŸ’§ **Kansas Frontier Matrix â€” Hydrology Processed Data Index (v11.2.6)**  
`data/hydrology/processed/README.md`

**Purpose**  
Define the **authoritative v11.2.6 index** for all **processed hydrology datasets** in KFM,  
bridging ETL outputs to STAC/DCAT catalogs, Neo4j graph ingestion, API contracts,  
MapLibre/Cesium layers, Story Nodes, and Focus Mode v3.

This file scopes and governs **everything under** `data/hydrology/processed/**`.

</div>

---

## ğŸ“˜ Overview

### 1. Scope

This index covers **processed hydrology products** generated by the deterministic KFM ETL pipelines from:

- Raw hydrology inputs (`data/hydrology/raw/**`)
- External services (USGS, USACE, KDHE, NOAA, Kansas Mesonet, CMIP, etc.)
- Field campaigns (e.g., WID operations, ecological surveys)

It **does not** describe raw inputs or general hydrology domain taxonomy (see `data/hydrology/README.md` for that); instead, it defines:

- Processed dataset classes and **subdirectory responsibilities**
- File naming conventions and **schema contracts**
- STAC/DCAT alignment for processed outputs
- Knowledge graph mapping (nodes/edges/properties)
- CI/CD checks and data-contract validation
- How processed hydrology data flows into **APIs, maps, Story Nodes, Focus Mode**

By design, this index aligns with **KFM-MDP v11.2.6** heading, directory layout, and governance rules.

---

## ğŸ—‚ï¸ Directory Layout

Processed hydrology outputs live here:

~~~text
ğŸ“ data/hydrology/processed/
â”œâ”€â”€ ğŸ“„ README.md
â”œâ”€â”€ ğŸ“ hydrology-timeseries/          # Canonical time series (flow, stage, storage, climate)
â”‚   â”œâ”€â”€ ğŸ§¾ index.json                 # Logical dataset index (IDs, contracts, STAC links)
â”‚   â”œâ”€â”€ ğŸ§¾ schema-hydrology-timeseries.csvw.json
â”‚   â””â”€â”€ ğŸ§¾ *.csv                      # Contract-validated CSVW-compliant time series
â”‚
â”œâ”€â”€ ğŸ“ turbidity-do/                  # Harmonized turbidity & dissolved oxygen series
â”‚   â”œâ”€â”€ ğŸ§¾ index.json
â”‚   â”œâ”€â”€ ğŸ§¾ schema-turbidity-do.csvw.json
â”‚   â””â”€â”€ ğŸ§¾ *.csv
â”‚
â”œâ”€â”€ ğŸ“ bathymetry/                    # Bathymetric rasters & derived products
â”‚   â”œâ”€â”€ ğŸ§¾ index.json
â”‚   â”œâ”€â”€ ğŸ§¾ schema-bathymetry.json
â”‚   â”œâ”€â”€ ğŸ§¾ *.tif                      # COG bathymetry / DoD rasters
â”‚   â””â”€â”€ ğŸ§¾ *.geojson                  # Sounding points, tiles, or footprints
â”‚
â”œâ”€â”€ ğŸ“ sediment-volumes/             # Sediment mass/volume estimates & grids
â”‚   â”œâ”€â”€ ğŸ§¾ index.json
â”‚   â”œâ”€â”€ ğŸ§¾ schema-sediment-volumes.csvw.json
â”‚   â”œâ”€â”€ ğŸ§¾ *.csv
â”‚   â””â”€â”€ ğŸ§¾ *.tif
â”‚
â”œâ”€â”€ ğŸ“ ecological-surveys/           # Hydrology-linked ecological indices & locations
â”‚   â”œâ”€â”€ ğŸ§¾ index.json
â”‚   â”œâ”€â”€ ğŸ§¾ schema-ecology.geojson.json
â”‚   â””â”€â”€ ğŸ§¾ *.geojson
â”‚
â”œâ”€â”€ ğŸ“ wid/                          # Water Injection Dredging campaigns (e.g., WID 2025)
â”‚   â”œâ”€â”€ ğŸ§¾ index.json
â”‚   â”œâ”€â”€ ğŸ§¾ schema-wid-timeseries.csvw.json
â”‚   â”œâ”€â”€ ğŸ§¾ *.csv                     # Sensors & operations time series
â”‚   â””â”€â”€ ğŸ§¾ *.geojson                 # Transects, plumes, zones
â”‚
â””â”€â”€ ğŸ“ hydroclimate/                 # Climate/hydroclimate grids & summaries
    â”œâ”€â”€ ğŸ§¾ index.json
    â”œâ”€â”€ ğŸ§¾ schema-hydroclimate-netcdf.json
    â”œâ”€â”€ ğŸ§¾ *.nc                      # CF-compliant NetCDF
    â””â”€â”€ ğŸ§¾ *.tif                     # Derived gridded products
~~~

**Conventions**

- Each subdirectory:
  - **MUST** include an `index.json` describing logical datasets, contracts, and STAC/DCAT IDs.
  - **MUST** reference at least one schema file (`schema-*.csvw.json`, `schema-*.geojson.json`, etc.).
  - **MUST NOT** contain raw, unvalidated inputs (those live in `data/hydrology/raw/**`).
- File formats:
  - Time series â†’ CSVW.
  - Vectors â†’ GeoJSON (EPSG:4326).
  - Rasters â†’ COG GeoTIFF.
  - Grids/modeled fields â†’ CF-compliant NetCDF where appropriate.

---

## ğŸ“¦ Data & Metadata

### 1. Processed Dataset Classes

| Subdirectory          | Class                             | Typical Formats          | STAC Collection                  | Notes |
|-----------------------|-----------------------------------|--------------------------|----------------------------------|-------|
| `hydrology-timeseries`| Flows, stages, storage, climate   | CSVW                     | `hydrology`                      | Canonical hydrology time series used across KFM. |
| `turbidity-do`        | Turbidity & DO time series        | CSVW                     | `downstream`, `wid-2025`         | Downstream impact monitoring incl. WID. |
| `bathymetry`          | DEMs, DoD, soundings              | COG, GeoJSON             | `bathymetry`, `sediment`         | Supports sediment budget and change detection. |
| `sediment-volumes`    | Volumes, masses, grain stats      | CSVW, COG                | `sediment`                       | Links bathymetry with sediment-core evidence. |
| `ecological-surveys`  | Fish, mussels, macroinvertebrates | GeoJSON                  | `ecology`, `downstream`          | Hydrology-linked ecology Story Nodes. |
| `wid`                 | WID sensor/log outputs            | CSVW, GeoJSON            | `wid-2025`, `downstream`         | Campaign-specific plume and ops data. |
| `hydroclimate`        | Climate & hydroclimate drivers    | NetCDF, COG              | `hydrology`, `hydroclimate`      | PRISM, station-derived grids, scenarios. |

Each logical dataset entry in `index.json` **MUST** include at minimum:

- `id` â€” stable dataset identifier (`hydro_ts_<waterbody>_<parameter>_<resolution>`).
- `title`, `description` â€” human-readable metadata.
- `data_contract_ref` â€” path to contract/schema file.
- `stac_item` â€” corresponding STAC Item ID.
- `dcat_dataset` â€” DCAT Dataset ID.
- `provenance_chain` â€” ordered list of ETL activities and source Entities.
- `care_profile` â€” CARE/sovereignty notes (even if "none" or "general").

### 2. File Naming Pattern (Non-normative but recommended)

For time series (`*.csv`):

- `<collection>_<site-or-area>_<parameter>_<resolution>_<start>-<end>.csv`  
  e.g., `hydro_tuttle_inflow_cfs_1h_1990-01-01_2025-01-01.csv`

For rasters (`*.tif`):

- `<collection>_<area>_<product>_<crs>_<resolution>_<yyyymmdd>.tif`  
  e.g., `bathymetry_tuttle_dem_epsg4326_10m_20250301.tif`

For NetCDF (`*.nc`):

- `<collection>_<region>_<variable-group>_<scenario>_<frequency>_<start>-<end>.nc`

---

## ğŸ§± Architecture

This directory sits squarely in the **ETL â†’ STAC/DCAT/PROV â†’ Neo4j â†’ API â†’ React/MapLibre/Cesium â†’ Story Nodes â†’ Focus Mode** pipeline.

### 1. Pipeline Placement

For each dataset class:

1. **Deterministic ETL (src/pipelines/hydrology/â€¦)**
   - Reads `data/hydrology/raw/**` and external APIs.
   - Applies QA/QC, gap-filling, unit normalization, resampling.
   - Writes contract-validated outputs into `data/hydrology/processed/**`.

2. **Cataloging (data/hydrology/stac/** + docs/data catalogs)**
   - ETL steps emit or update STAC Items referencing processed assets.
   - DCAT Dataset/Distribution entries are updated/created.

3. **Graph Ingestion (src/graph/hydrology_ingest/â€¦)**
   - Ingest scripts crawl `index.json` files and STAC Items.
   - Materialize:
     - `HydrologyDataset`, `HydroTimeSeries`, `HydroRaster`, `HydroObservation` nodes.
     - Relationships to `Place`, `Event`, `Agent`, `ModelRun`.

4. **API Exposure (src/api/hydrology/**)**
   - REST/GraphQL endpoints expose:
     - Time series slices.
     - Raster metadata and tile URLs.
     - Ecological indicators keyed by hydrologic event.

5. **Frontend & Story Nodes (src/web/**)**
   - MapLibre/Cesium layers driven by catalog metadata.
   - Story Nodes reference doc IDs, STAC IDs, and graph nodesâ€”not raw files.

### 2. Directory-level Responsibilities

- `data/hydrology/processed/` **MUST NOT** implement business logic.
- All transformations are defined in code/config under `src/` and `configs/`.
- This directory is **append- and update-only**; destructive changes must:
  - Be accompanied by updated provenance.
  - Pass provenance & schema CI checks.
  - Update STAC/DCAT and graph ingestion where relevant.

---

## ğŸ›°ï¸ STAC & DCAT Integration

Each logical dataset in `data/hydrology/processed/**` is mirrored by at least one **STAC Item** in `data/hydrology/stac/**`.

### 1. STAC Item Expectations

For a time series asset (example):

~~~json
{
  "stac_version": "1.0.0",
  "type": "Feature",
  "id": "hydro_ts_tuttle_inflow_cfs_1h_1990-2025",
  "collection": "hydrology",
  "geometry": { "type": "Point", "coordinates": [-96.6005, 39.2758] },
  "properties": {
    "datetime": "1990-01-01T00:00:00Z",
    "start_datetime": "1990-01-01T00:00:00Z",
    "end_datetime": "2025-01-01T00:00:00Z",
    "kfm:parameter": "inflow",
    "kfm:units": "cfs",
    "kfm:dataset_class": "hydrology-timeseries",
    "kfm:data_contract_ref": "data/hydrology/processed/hydrology-timeseries/schema-hydrology-timeseries.csvw.json"
  },
  "assets": {
    "timeseries": {
      "href": "../../processed/hydrology-timeseries/hydro_tuttle_inflow_cfs_1h_1990-01-01_2025-01-01.csv",
      "type": "text/csv",
      "roles": ["data"]
    },
    "metadata": {
      "href": "../../processed/hydrology-timeseries/index.json",
      "type": "application/json",
      "roles": ["metadata"]
    }
  }
}
~~~

For rasters and NetCDF, assets **MUST** be typed appropriately (e.g., `image/tiff; application=geotiff; profile=cloud-optimized`, `application/x-netcdf`) and use STAC raster/eo extensions where appropriate.

### 2. DCAT Alignment

DCAT `dcat:Dataset` entries for each processed dataset should:

- Reuse `id` / `title` / `description` from `index.json`.
- Specify:
  - `dct:spatial` and `dct:temporal` coverage.
  - `dct:license`, `dct:rights`.
  - `dcat:distribution` entries pointing at processed files and STAC Items.
- Treat this README itself as a `dcat:CatalogRecord` and `prov:Plan`.

---

## ğŸ•¸ï¸ Knowledge Graph Integration

Processed hydrology data is represented in Neo4j using KFM-OP v11 alignment to **CIDOC-CRM**, **PROV-O**, **GeoSPARQL**, and **OWL-Time**.

### 1. Core Node Types (draft ontology mapping)

- `HydrologyDataset` (subclass of `E73_Information_Object`)
- `HydroTimeSeries` (time series logical entity)
- `HydroObservation` (individual value or small bucket)
- `HydroRaster` / `HydroGrid` (gridded products)
- `Place` (e.g., `E53_Place:Tuttle_Creek_Reservoir`)
- `HydroEvent` (e.g., floods, droughts, WID campaigns)
- `ETLActivity` (ETL steps as `prov:Activity`)
- `Agent` (data providers, KFM ETL systems)

### 2. Relationships (examples)

- `(:HydrologyDataset)-[:PROV_WAS_DERIVED_FROM]->(:HydrologyDataset|:SourceEntity)`
- `(:HydroTimeSeries)-[:DESCRIBES_PARAMETER]->(:HydroParameter)`
- `(:HydroTimeSeries)-[:HAS_OBSERVATION]->(:HydroObservation)`
- `(:HydroObservation)-[:AT_PLACE]->(:Place)`
- `(:HydroObservation)-[:AT_TIME]->(:TimeInstant)`
- `(:HydroRaster)-[:GEO_HAS_GEOMETRY]->(:geo_Geometry)`
- `(:HydrologyDataset)-[:PROV_WAS_GENERATED_BY]->(:ETLActivity)`
- `(:ETLActivity)-[:PROV_WAS_ASSOCIATED_WITH]->(:Agent)`

Graph ingestion scripts **SHOULD** take their dataset inventory from the `index.json` files in `data/hydrology/processed/**` and the corresponding STAC Collections/Items, not by scanning raw filenames.

---

## ğŸ§¬ FAIR+CARE & Sovereignty

Hydrology data is largely public-domain, but **careful handling is still required**:

- **FAIR**
  - All processed datasets must be:
    - Discoverable via STAC/DCAT.
    - Accessible via documented API endpoints (subject to CI and security).
    - Interoperable through stable schemas and units (e.g., cfs, mg/L, Â°C).
    - Reusable with clear licenses and provenance.
- **CARE & Indigenous sovereignty**
  - For any datasets intersecting Indigenous lands, sacred sites, or sensitive ecological areas:
    - Use generalized geometries (e.g., reach-level, HUC-level, or buffered centroids).
    - Record constraints explicitly in:
      - `index.json` (`care_profile`).
      - STAC `kfm:care_profile` properties.
      - Graph nodes as explicit `CARE`-related properties.
  - Do not infer or expose traditional knowledge from hydrology patterns without proper sources and governance.

This README is classified as **Public**, but some referenced datasets may be **masked or generalized**.

---

## ğŸ§ª Validation & CI/CD

Changes under `data/hydrology/processed/**` **MUST** pass relevant CI profiles described in the Markdown protocol and security policy.

### 1. Expected CI Profiles (draft mapping)

When files in `data/hydrology/processed/**` change, CI **SHOULD** run at least:

- `markdown-lint` â€” on this README and related docs.
- `schema-lint` â€” validating:
  - CSVW schemas.
  - GeoJSON schemas.
  - NetCDF metadata templates.
- `metadata-check` â€” ensuring:
  - `index.json` entries have required fields.
  - STAC/DCAT references are consistent.
- `provenance-check` â€” verifying:
  - `provenance_chain` entries.
  - `prov:used` and `prov:wasGeneratedBy` relationships align with ETL logs.
- `secret-scan` / `pii-scan` â€” confirming:
  - No credentials or PII in data or docs.
- `stac-validate-hydrology` (project-specific) â€” STAC Collection/Item validation for hydrology Collections.

### 2. Local Developer Workflow (non-normative example)

- Add or update processed datasets under `data/hydrology/processed/**`.
- Update:
  - `index.json` in the relevant subdirectory.
  - STAC Items under `data/hydrology/stac/**` if needed.
- Run (example):

  ~~~bash
  make lint-docs
  make validate-hydrology-processed
  make validate-stac-hydrology
  ~~~

- Open a PR; verify CI checks for:
  - Docs, schemas, STAC metadata, and security scans.

---

## ğŸ§  Story Nodes & Focus Mode

Processed hydrology datasets are **primary evidence** for hydrologic narratives in KFM.

### 1. Story Node Integration

Story Nodes that rely on hydrology data (e.g., â€œThe 1951 Flood and Aftermathâ€, â€œThe 2025 WID Demonstrationâ€) **SHOULD**:

- Reference:
  - This documentâ€™s `doc_uuid`.
  - Concrete dataset IDs from `index.json`.
  - STAC Item IDs and graph node IDs (`HydroTimeSeries`, `HydroRaster`, `HydroEvent`).
- Distinguish:
  - **Facts** â€” direct from hydrology datasets and metadata.
  - **Interpretation** â€” scientific reading of trends.
  - **Speculation** â€” â€œwhat might beâ€ scenarios (clearly labeled).

### 2. Focus Mode Behaviour

Given the `ai_transform_permissions` above, Focus Mode:

- **MAY**:
  - Summarize hydrologic behavior over time.
  - Highlight anomalies (e.g., extreme flows, turbidity spikes).
  - Suggest visualizations and Story Node segments.
- **MUST NOT**:
  - Alter underlying numeric values.
  - Rewrite governance or licenses.
  - Reveal sensitive coordinates masked by CARE/sovereignty policy.

---

## ğŸ•°ï¸ Version History

| Version | Date       | Notes                                                                                          |
|--------:|-----------:|------------------------------------------------------------------------------------------------|
| v11.2.6 | 2025-12-11 | Initial hydrology processed data index, aligned with KFM-MDP v11.2.6 and hydrology domain v11. |

---

<div align="center">

Â© 2025 Kansas Frontier Matrix â€” CC-BY 4.0  

[â¬…ï¸ Back to Hydrology Domain](../README.md) Â·  
[ğŸ“š Docs Root](../../../docs/README.md) Â· [ğŸ“ Standards Index](../../../docs/standards/README.md) Â· [ğŸ›¡ï¸ Governance Charter](../../../docs/standards/governance/ROOT-GOVERNANCE.md)

</div>

<!-- AI drafting citations for maintainers (remove before commit):
:contentReference[oaicite:0]{index=0}
:contentReference[oaicite:1]{index=1}
-->

