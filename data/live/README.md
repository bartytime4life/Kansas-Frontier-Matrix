# ğŸ“¡ `data/live/` â€” Live & Streaming Data

<p align="center">
  <b>â±ï¸ nearâ€‘realâ€‘time â€¢ ğŸ§¾ contractâ€‘first â€¢ ğŸ§¬ provenanceâ€‘first â€¢ ğŸ›¡ï¸ policyâ€‘gated</b>
</p>

<p align="center">
  <img alt="purpose" src="https://img.shields.io/badge/purpose-live%20data%20window-blue" />
  <img alt="governance" src="https://img.shields.io/badge/governance-FAIR%20%2B%20CARE-7fdbca" />
  <img alt="policy" src="https://img.shields.io/badge/policy-fail--closed-critical" />
  <img alt="metadata" src="https://img.shields.io/badge/metadata-STAC%20%7C%20DCAT%20%7C%20PROV-informational" />
</p>

> A **hot-window landing zone** for continuously updating feeds (sensor streams, polling APIs, near-real-time dashboards, rapid simulations).  
> Designed to power **live map layers**, **dashboards**, and **Pulse-style narrative updates** â€” *without breaking KFMâ€™s evidence-first governance*.

> [!IMPORTANT]
> **Keep `data/live/` operational.** Commit **configs + schemas + tiny â€œgolden samplesâ€** only.  
> Store high-frequency payloads in a runtime volume/object store, and **promote** curated snapshots into `data/raw/` + `data/catalog/`.

ğŸ”— Quick links: [`data/raw`](../raw) â€¢ [`data/processed`](../processed) â€¢ [`data/catalog`](../catalog) â€¢ [`docs`](../../docs)

---

## âœ… TL;DR rules (print this on your forehead ğŸ˜„)

1. **Append-only**: Live facts are time-stamped events. Donâ€™t rewrite history â€” publish new observations.
2. **Contract-first**: Every live source has a **data contract** (schema + metadata + license + sensitivity).
3. **Provenance-first**: Every ingest run writes **who/what/when/how** (checksums + manifests + run context).
4. **Fail closed**: If validation/policy fails â†’ **quarantine + alert**, never â€œbest-effort publish.â€
5. **Sensitive by design**: Classification + redaction/generalization is part of the pipeline (not a UI patch).
6. **UI/AI never bypass**: Nothing appears in UI/Focus Mode without at least stub **STAC/DCAT/PROV**.

---

## ğŸ¯ Why this folder exists

KFM needs two truths at once:

- **Live systems change fast** (minutes/seconds): sensors update, indicators shift, simulations rerun.
- **Knowledge systems must be auditable**: every map pixel and narrative sentence must trace back to evidence.

`data/live/` is the bridge: it holds the *hot window* + checkpoints that make real-time features possible, while staying compatible with downstream reproducible publishing.

Typical use cases ğŸ§©

- ğŸ“¡ Real-time sensor layers (e.g., gauges, weather stations, environmental monitors)
- ğŸ”„ Incremental polling APIs (delta updates, â€œlatest reading,â€ â€œsince last seenâ€)
- ğŸ§ª Rapid simulations / â€œnowcastsâ€ feeding dashboards
- ğŸ§  Automated anomaly â†’ **Pulse Thread** drafting (with evidence manifests)

---

## ğŸ§­ Where `data/live/` sits in the KFM data lifecycle

```mermaid
flowchart LR
  EXT[External feeds<br/>(APIs â€¢ sensors â€¢ sims)] --> W[Watcher<br/>(observe + record)]
  W --> G[Ingestion Gate<br/>(basic validation + policy)]
  G -->|pass| LIVE[data/live<br/>(hot window + checkpoints)]
  G -->|fail| Q[Quarantine + Alert]

  LIVE --> X[Deterministic Transform<br/>(config-driven)]
  X --> DB[(PostGIS / time-series store)]
  X --> KG[(Knowledge Graph / Neo4j)]
  DB --> CAT[Catalog Updates<br/>(STAC/DCAT + PROV)]
  KG --> CAT

  CAT --> UI[UI live layers<br/>+ dashboards]
  CAT --> FM[Focus Mode<br/>(RAG + citations)]

  LIVE -->|scheduled snapshots| RAW[data/raw<br/>(immutable evidence)]
  X -->|stable derivatives| PROC[data/processed<br/>(standardized)]
```

**Mental model:** streaming data is â€œmany small datasets over time.â€  
We keep it windowed + fast in `data/live/`, and we publish durable, curated slices via normal KFM pathways.

---

## ğŸ“ Directory layout (recommended)

> [!NOTE]
> This is a **recommended** structure. If your deployment uses object storage + DB only, keep the *shape* (contracts, telemetry, checkpoints) even if the payloads live elsewhere.

```text
data/live/
  README.md
  .gitignore                 # ğŸ‘ˆ recommended: keep payloads out of git
  _schemas/                  # JSON Schema / Avro / Proto / validators
  _telemetry/                # append-only ingest logs (NDJSON)
  sources/
    <source_id>/
      README.md              # source-specific notes & SLA
      source.json            # contract: license + sensitivity + cadence + upstream
      checkpoints/           # cursors, offsets, watermarks (idempotency helpers)
      raw/                   # hot-window payloads (windowed, not forever)
      derived/               # quick-turn derivatives (UI-friendly, small)
      manifests/             # checksums + fetch manifests + evidence lists
      prov/                  # optional: run-level PROV bundles
      stac/                  # optional: rolling STAC Items/Collection stubs
```

---

## ğŸ§± Windowing, retention & naming (recommended)

Because live streams are high-frequency, `data/live/` should be **windowed** and **partitioned**.

### Suggested defaults

- **Hot window**: keep the most recent `P7D` (7 days) of raw payloads *(tune per source & storage)*
- **Derived window**: keep `P30D` (30 days) of lightweight derivatives (for UI trends & QA)
- **Promotion cadence**: snapshot hourly/daily into `data/raw/` for durable evidence
- **Never delete without a manifest**: deletions/rollups should be logged (telemetry + PROV)

### Naming conventions ğŸ·ï¸

Prefer **time-partitioned, append-only** paths:

```text
raw/YYYY/MM/DD/HH/<source_id>__<observed_at>__<cursor_or_etag>.json
derived/YYYY/MM/DD/<source_id>__latest.geojson
manifests/YYYY/MM/DD/HH/run__<run_id>.manifest.json
```

Rules of thumb:

- Use **UTC ISO-8601** timestamps (`2026-01-24T06:15:00Z`)
- Include a **cursor/etag** (or hash) when available (idempotency + dedupe)
- Never rely on a single mutable `latest.json` *without also writing time-stamped history*

---

## ğŸ§¾ Live source contract (`source.json`)

Every live source MUST ship with a contract that answers:

- **What is it?** (`id`, `title`, `description`, `owner/contact`)
- **Can we use it?** (`license`, `attribution`, `terms`, `rate_limit`)
- **How often should it update?** (`cadence`, `sla_minutes`)
- **Whatâ€™s the schema?** (`schema_ref`, `fields`, `units`, `CRS`)
- **How do we fetch it reproducibly?** (`upstream`, `auth_ref`, `idempotency`)
- **Is it sensitive?** (`sensitivity_class`, `geo_obfuscation`, `access_rules`)
- **How does it publish?** (`catalog_ids`, `output_assets`, `promotion_rules`)

Example (trim as needed):

```json
{
  "id": "river_gauges_live",
  "title": "River Gauge Readings (Live)",
  "description": "Near-real-time point observations for gauge stations.",
  "license": "TBD",
  "attribution": "TBD",
  "cadence_minutes": 15,
  "sla_minutes": 180,

  "sensitivity": {
    "class": "public",
    "geo_obfuscation": null,
    "access": "public"
  },

  "upstream": {
    "type": "api",
    "base_url": "https://example.org/api",
    "docs": "TBD",
    "auth": { "mode": "none", "secret_ref": null }
  },

  "idempotency": {
    "strategy": "etag_or_last_modified",
    "cursor_path": "checkpoints/cursor.json"
  },

  "schema": {
    "format": "json",
    "schema_ref": "../../_schemas/obs_point_v1.schema.json"
  },

  "catalog": {
    "dcat_dataset_id": "dcat:dataset:river_gauges_live",
    "stac_collection_id": "stac:collection:river_gauges_live"
  }
}
```

---

## âš™ï¸ Formats & performance (choose the smallest thing that works)

Live data wants **low-latency** *and* **repeatability**. Use formats that match the job:

- ğŸ§¾ **Telemetry / logs**: `*.ndjson` (append-only, streamable)
- ğŸ“ **Point observations** (small/medium): `GeoJSON` (easy for UI)
- ğŸ§± **Large vectors**: vector tiles (`MVT`) packaged as `PMTiles` (fast map rendering)
- ğŸ—ƒï¸ **Analytics snapshots**: `Parquet / GeoParquet` (columnar, compressible)
- ğŸ›°ï¸ **Rasters**: `COG` (Cloud-Optimized GeoTIFF) for efficient tile/overview reads
- ğŸŒ **3D / volumetrics**: `3D Tiles` (Cesium-friendly streaming)

> [!TIP]
> If a â€œlive layerâ€ is getting heavy, **donâ€™t ship GeoJSON** â€” ship vector tiles and keep GeoJSON for debug/QA.

---

## ğŸ”„ Ingestion loop (the happy path)

> [!NOTE]
> KFM ingestion commonly follows a **Watcher â†’ Planner â†’ Executor (Wâ€‘Pâ€‘E)** pattern:
> - **Watcher**: observes change / SLA breaches / anomalies and opens a run
> - **Planner**: builds an explicit run plan (what to fetch, validate, transform, publish)
> - **Executor**: executes deterministically, writing manifests + PROV + catalog updates

A live source typically follows this loop:

1. ğŸ•µï¸ **Watcher** observes: â€œnew data availableâ€ OR â€œSLA breachâ€ OR â€œanomaly detectedâ€
2. ğŸ“¥ **Fetch** raw payload (prefer idempotent fetch using ETag/Lastâ€‘Modified/cursors)
3. ğŸ›¡ï¸ **Ingestion gate** (lightweight but strict):
   - integrity (hashes/checksums)
   - schema sanity (parseable, required fields exist)
   - governance checks (license present, sensitivity known, size limits, etc.)
   - policy-as-code checks (OPA/Conftest style), **fail closed**
4. ğŸ§¾ Write **manifest + telemetry** (append-only)
5. ğŸ”§ **Deterministic transform** to:
   - UI-friendly derivative (small GeoJSON, pre-joined attributes, etc.)
   - DB ingest (PostGIS/time-series store)
   - graph update (entities + PROV links)
6. ğŸ—‚ï¸ Update **STAC/DCAT/PROV** (at least stubs) so UI/AI can cite & trace
7. ğŸ§  Optional: trigger **Pulse Thread** drafting when patterns/anomalies fire

---

## ğŸ“Š Telemetry & observability (donâ€™t fly blind)

### Recommended telemetry: append-only NDJSON

Put ingest events in:

- `data/live/_telemetry/<source_id>.ndjson` (or per-run files)

Example event:

```json
{
  "ts": "2026-01-24T06:15:00Z",
  "source_id": "river_gauges_live",
  "event": "fetch_success",
  "bytes": 582134,
  "http": { "status": 200, "etag": "\"abc123\"" },
  "checksums": { "sha256": "..." },
  "cursor": { "since": "2026-01-24T06:00:00Z" },
  "policy": { "passed": true, "rules": ["license_present", "sensitivity_labeled"] }
}
```

### Health signals to compute âœ…

- â±ï¸ **Minutes since last seen** (per source) vs. expected SLA  
- ğŸ§µ **Orphan detection** in the knowledge graph (STAC/PROV nodes missing required links)
- ğŸš¦ **Gate failure rate** (spikes = upstream format changes or policy regressions)
- ğŸ§  **Anomaly triggers** (count + severity + review status)

---

## ğŸ§  Live data â†’ UI & Focus Mode

### Live layers in the map ğŸ—ºï¸

A â€œReal-timeâ€ layer should behave like:

- UI requests **latest** reading per station/feature (plus small recent history on demand)
- API serves **GeoJSON** points/lines/polys (or vector tiles) with `value`, `observed_at`
- UI layer panel shows:
  - **Source attribution** (from DCAT/STAC metadata)
  - **Sensitivity warnings** (lock icon / generalized geometry / gated access)
  - **â€œMap behind the mapâ€**: click through to evidence + provenance

### Focus Mode expectations ğŸ¤–

When users ask questions like:

- â€œWhatâ€™s the current water level at X?â€
- â€œAre any gauges trending unusually low in the last 7 days?â€

Focus Mode should:

- detect itâ€™s a **live query**
- retrieve relevant entities (graph) + latest readings (DB)
- return an answer with **citations** (dataset + station + timestamp)
- optionally surface an **Explainable AI** panel (retrieved context + why these sources)
- log a PROV record for the AI answer (derived entity) â€” so the answer is auditable

---

## ğŸ§µ Pulse Threads (optional but ğŸ”¥)

Pulse Threads are **timely, geotagged narratives** tied to live patterns.

Pattern â†’ narrative example:

- Watcher notices: â€œcluster of gauges in lowest 10th percentileâ€
- Planner prepares: evidence bundle + draft narrative
- Human curator reviews â†’ publish

Minimum requirements for a Pulse Thread:

- short narrative (Markdown/JSON)
- geotags (region/place entities)
- **evidence manifest** (exact datasets/queries/timestamps backing the claims)
- versioning + provenance links

---

## ğŸ” Governance, privacy, and ethics (FAIR + CARE)

Live data can be the riskiest data. Apply these defaults:

- **Sensitivity classification is mandatory** (public / restricted / confidential)
- **Geo-obfuscation** for protected locations when required (rounding/generalization)
- **Access control** for restricted layers (UI should hide or gate them)
- **No secrets in repo**: use secret refs and deployment secrets managers
- **Cultural protocols**: support â€œcommunity rulesâ€ where contributors specify how data can be accessed/shared

> [!TIP]
> If you donâ€™t know sensitivity, treat it as **restricted** until reviewed.

---

## ğŸ“¦ Promotion & publishing (making live data durable)

### Snapshot promotion (recommended)

Because live streams are high-frequency, promote *curated slices* on a schedule:

- hourly/daily snapshots â†’ `data/raw/<source_id>/<YYYY-MM-DD>/...`
- stable derivatives â†’ `data/processed/`
- published metadata â†’ `data/catalog/` (STAC/DCAT) + graph PROV

### Artifact distribution (optional, for big deliverables)

For large map artifacts (PMTiles, GeoParquet, COGs), consider content-addressed publishing:

- push artifacts to an **OCI registry** (ORAS)
- sign with **Cosign**
- (optional) attach **SLSA-style provenance attestations** for build/publish steps
- reference immutable digests from catalog metadata

This keeps artifacts reproducible, verifiable, and easy to â€œpull by digest.â€

---

## ğŸ§ª Add a new live source (checklist)

- [ ] Create `data/live/sources/<source_id>/source.json` (license + sensitivity included)
- [ ] Add/verify schema in `data/live/_schemas/`
- [ ] Implement fetch method (idempotent; cursors/ETag supported)
- [ ] Implement ingestion gate checks (parseable + hash + policy)
- [ ] Write telemetry (NDJSON) and manifests (checksums, evidence list)
- [ ] Map to STAC/DCAT/PROV IDs (even if stubs at first)
- [ ] Add UI layer config + attribution and sensitivity UX
- [ ] Add monitoring: â€œminutes since last seenâ€ + failure alerts
- [ ] (Optional) Add Pulse Thread trigger + review workflow

---

## ğŸš« Anti-patterns (please donâ€™t ğŸ™ƒ)

- âŒ â€œJust drop it in PostGISâ€ (no catalog, no provenance, no sensitivity labels)
- âŒ Manually editing live payloads (breaks determinism + auditability)
- âŒ Storing API keys or credentials in `source.json`
- âŒ Publishing sensitive coordinates â€œtemporarilyâ€
- âŒ Overwriting â€œlatest.jsonâ€ without keeping time-stamped history

---

## ğŸ“š Project reference docs (recommended reading)

> If youâ€™re implementing or redesigning live ingestion, skim these first:

- ğŸ“š **KFM Data Intake â€“ Technical & Design Guide** (provenance-first ingestion patterns)
- ğŸ§­ **KFM AI System Overview** (Focus Mode, citations, governance checks)
- ğŸ—ºï¸ **KFM Comprehensive UI System Overview** (live layers, dashboards, sensitivity UX)
- ğŸ—ï¸ **KFM Architecture / Technical Documentation** (policy gates, contracts, trust model)
- ğŸš€ **Latest Ideas & Future Proposals** (Wâ€‘Pâ€‘E agents, FAIR/CARE automation, supply chain attestation)
- ğŸ’¡ **Innovative Concepts** (4D digital twin thinking, cultural protocols, sensitivity-aware access)
- ğŸ§µ **Additional Project Ideas** (Pulse Threads, graph health checks, OCI artifact distribution)
- ğŸ§± **Openâ€‘Source Geospatial Mapping Hub Design** (MapLibre/Leaflet + time slider patterns)
- ğŸ“Š **Data Mining + Geospatial Cookbooks** (data cleansing + PostGIS/GeoJSON implementation recipes)
- ğŸ“¦ **PDF Portfolios** (AI, data management, mapping/WebGL, programming resources) â€” open in Acrobat/Reader for embedded books

<details>
<summary>ğŸ“¦ Full project file bundle (PDFs & portfolios)</summary>

- ğŸ§  `AI Concepts & more.pdf` (PDF portfolio of AI/ML readings)
- ğŸ“Š `Data Managment-Theories-Architures-Data Science-Baysian Methods-Some Programming Ideas.pdf` (PDF portfolio of data engineering / performance / architecture readings)
- ğŸ§° `Various programming langurages & resources 1.pdf` (PDF portfolio of language + tooling references)
- ğŸ—ºï¸ `Maps-GoogleMaps-VirtualWorlds-Archaeological-Computer Graphics-Geospatial-webgl.pdf` (PDF portfolio of mapping/WebGL/virtual-world references)

- ğŸ“š `ğŸ“š Kansas Frontier Matrix (KFM) Data Intake â€“ Technical & Design Guide.pdf`
- ğŸ§­ `Kansas Frontier Matrix (KFM) â€“ AI System Overview ğŸ§­ğŸ¤–.pdf`
- ğŸ—ï¸ `Kansas Frontier Matrix (KFM) â€“ Comprehensive Architecture, Features, and Design.pdf`
- ğŸ§¾ `Kansas Frontier Matrix (KFM) â€“ Comprehensive Technical Documentation.pdf`
- ğŸ›ï¸ `Kansas Frontier Matrix â€“ Comprehensive UI System Overview.pdf`
- ğŸŒŸ `ğŸŒŸ Kansas Frontier Matrix â€“ Latest Ideas & Future Proposals.docx.pdf`
- ğŸ’¡ `Innovative Concepts to Evolve the Kansas Frontier Matrix (KFM).pdf`
- ğŸ§µ `Additional Project Ideas.pdf`

- ğŸ§± `Kansas-Frontier-Matrix_ Open-Source Geospatial Historical Mapping Hub Design.pdf`
- ğŸ `KFM- python-geospatial-analysis-cookbook-... .pdf` (implementation recipes)
- ğŸ§¼ `Data Mining Concepts & applictions.pdf` (data prep + ethics notes)
- ğŸ§ª `Scientific Method _ Research _ Master Coder Protocol Documentation.pdf` (reproducibility + coding standards)

</details>

