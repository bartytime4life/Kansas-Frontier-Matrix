# ğŸ§¾ `data/prov/` â€” Provenance Bundles (W3C PROVâ€‘O) â›“ï¸

![W3C PROV-O](https://img.shields.io/badge/standard-W3C%20PROV--O-blue)
![Format: JSON-LD](https://img.shields.io/badge/format-JSON--LD-lightgrey)
![Catalog: STAC + DCAT](https://img.shields.io/badge/catalog-STAC%20%2B%20DCAT-orange)
![Policy-as-code](https://img.shields.io/badge/policy-OPA%20%2B%20Conftest-informational)
![Philosophy](https://img.shields.io/badge/philosophy-provenance--first-success)

> ğŸ§  **KFM rule of thumb:** if it ships to the **Graph / API / UI**, it ships with **provenance**.  
> No mystery layers. No unsourced outputs. No black boxes. âœ…

This folder contains **W3C PROVâ€‘O lineage records** (typically **JSONâ€‘LD**) that capture **how** datasets, map layers, simulations, and evidence artifacts were produced: **inputs â†’ processing activities â†’ outputs**, plus **who/what ran the process** (agents), **when**, and **with what parameters**.

---

## ğŸ§­ Quick navigation

- [What lives here](#-what-lives-here)
- [The â€œEvidence Tripletâ€](#-the-evidence-triplet)
- [Recommended folder layout](#-recommended-folder-layout)
- [Minimum requirements](#-minimum-requirements)
- [KFMâ€‘PROV profile](#-kfm-prov-profile)
- [Patterns & recipes](#-patterns--recipes)
- [Governance, privacy, and cultural protocols](#-governance-privacy-and-cultural-protocols)
- [Validation & policy gates](#-validation--policy-gates)
- [FAQ](#-faq)

---

## ğŸ“¦ What lives here

Youâ€™ll typically find PROV bundles for:

- **ğŸ—ºï¸ Dataset publications**  
  Ingest/ETL activities that turn `data/raw/` evidence into `data/processed/` products.
- **ğŸ¤– AI / analysis â€œevidence artifactsâ€**  
  OCR corpora, AIâ€‘predicted layers, derived analytics, generated summariesâ€”**treated like firstâ€‘class datasets**.
- **ğŸ® Simulations & scenarios**  
  Sandbox runs and **promoted** simulation outputs (when they become official data products).
- **â±ï¸ Streaming / realâ€‘time snapshots**  
  The provenance backbone for â€œliveâ€ readings: what was used, when it was read, and by which workflow.
- **ğŸ§ª CI / DevOps lineage (optional but encouraged)**  
  Capturing PR/merge/build events as provenance when you want endâ€‘toâ€‘end auditability.

---

## ğŸ§© The â€œEvidence Tripletâ€

KFM uses a **threeâ€‘part publication boundary** for anything that becomes â€œrealâ€ in the platform:

1. **ğŸ“ STAC** â€” spatial/temporal footprint + assets (`data/stac/â€¦`)
2. **ğŸ—‚ï¸ DCAT** â€” dataset catalog record + license/publisher (`data/catalog/dcat/â€¦`)
3. **ğŸ§¾ PROV** â€” lineage + who/what/how (`data/prov/â€¦`)

> âœ… These boundary artifacts are required before data is considered **fully published** and allowed into downstream stages (graph/API/UI).

### ğŸ” Mental model (endâ€‘toâ€‘end)

```mermaid
flowchart LR
  Raw[data/raw ğŸ“¦] -->|prov:used| A[Activity: ingest/transform ğŸ› ï¸]
  A -->|prov:generated| Proc[data/processed âœ…]

  Proc --> STAC[data/stac ğŸ“]
  Proc --> DCAT[data/catalog/dcat ğŸ—‚ï¸]
  A --> PROV[data/prov ğŸ§¾]

  STAC <--> DCAT
  STAC <--> PROV
  DCAT <--> PROV

  Proc --> DB[(PostGIS ğŸ›°ï¸)]
  Proc --> Graph[(Neo4j ğŸ•¸ï¸)]
  DB --> API[API ğŸ”Œ]
  Graph --> API
  API --> UI[UI / Focus Mode ğŸ§­]
```

---

## ğŸ—‚ï¸ Recommended folder layout

This repo may evolve, but the canonical publishing locations follow the KFM v13 structure:

```text
data/
  raw/                 # immutable evidence inputs ğŸ“¦
  work/                # intermediate / sandbox outputs ğŸ§ª
  processed/           # vetted data products âœ…
  stac/                # STAC collections & items ğŸ“
    collections/
    items/
  catalog/
    dcat/              # DCAT dataset records ğŸ—‚ï¸
  prov/                # âœ… YOU ARE HERE: PROV lineage bundles ğŸ§¾
    README.md
```

### Suggested internal organization (recommended, not required)

If `data/prov/` becomes large, use subfolders to keep it clean:

```text
data/prov/
  datasets/            # dataset-level lineage bundles
  runs/                # pipeline run provenance (batch/streaming)
  ai/                  # AI/analysis â€œevidence artifactâ€ provenance
  sims/                # simulation run provenance + promotions
  devops/              # PR/build provenance (optional)
  _schemas/            # optional: local copies of profile/schema references
  README.md
```

---

## âœ… Minimum requirements

A PROV bundle must be **useful for audits** and **reproducibility**, not just â€œtechnically validâ€.

### Must capture (minimum)

- **Entities (ğŸ“¦ things):**
  - at least **one input entity** (raw or upstream dataset)
  - at least **one output entity** (processed asset, derived artifact, published product)
- **Activities (ğŸ› ï¸ transformations):**
  - the pipeline run / transform / simulation / inference step that produced the output
- **Agents (ğŸ‘¤ğŸ¤– actors):**
  - who/what executed the activity (person, org, bot, service)
- **Core relations (ğŸ”— lineage edges):**
  - input(s) **used by** activity
  - output **generated by** activity
  - activity **associated with** agent(s)

### Strongly recommended (KFM-style â€œno black boxesâ€)

- **â±ï¸ timestamps** (start/end, or at least end)
- **ğŸ”’ checksums/hashes** for major inputs/outputs (especially promoted artifacts)
- **ğŸ§¬ pipeline identity** (name + version + config reference)
- **ğŸ§¾ code provenance** (commit hash / tag, container digest, lockfile version, etc.)
- **ğŸ§­ spatial & temporal context** (link out to STAC for footprint; keep minimal duplication)
- **âš–ï¸ governance metadata** (license, sensitivity/classification, access rules)
- **ğŸ“Š uncertainty / confidence** for ML/sim outputs (even a basic metric is better than none)

---

## ğŸ§¬ KFMâ€‘PROV profile

KFM version-controls its metadata profiles (example: **KFMâ€‘PROV v11.0.0**).  
Treat the profile like a **contract**: prefer extending profiles over inventing adâ€‘hoc fields.

**Rule:** If you need new provenance fields for a domain, coordinate a profile update rather than â€œoneâ€‘offâ€ keys.

---

## ğŸ§± Patterns & recipes

### 1) ğŸ—ºï¸ Standard dataset ingest (raw â†’ processed â†’ published)

**When you add/change anything in `data/processed/â€¦`, you must also update:**
- âœ… STAC (item/collection)
- âœ… DCAT dataset record
- âœ… PROV lineage bundle

**Golden rule:** raw inputs remain immutable; transformations happen downstream.

<details>
<summary><strong>ğŸ“„ PROV JSONâ€‘LD starter template (illustrative)</strong> ğŸ”½</summary>

```json
{
  "@context": "https://www.w3.org/ns/prov.jsonld",
  "@id": "urn:kfm:prov:bundle:dataset:<dataset_id>:<dataset_version>",
  "entity": {
    "urn:kfm:entity:raw:<source_id>": {
      "prov:label": "Raw source (as received)",
      "prov:type": "kfm:RawEvidence",
      "kfm:checksum": "sha256:<...>"
    },
    "urn:kfm:entity:processed:<asset_id>": {
      "prov:label": "Processed data product",
      "prov:type": "kfm:ProcessedProduct",
      "kfm:checksum": "sha256:<...>",
      "kfm:stac_ref": "data/stac/items/<...>.json",
      "kfm:dcat_ref": "data/catalog/dcat/<...>.json"
    }
  },
  "activity": {
    "urn:kfm:activity:ingest:<run_id>": {
      "prov:label": "ETL / ingest pipeline run",
      "prov:type": "kfm:PipelineRun",
      "prov:startedAtTime": "2026-01-19T00:00:00Z",
      "prov:endedAtTime": "2026-01-19T00:10:00Z",
      "kfm:pipeline": "pipelines/<domain>/<name>",
      "kfm:code_commit": "git:<commit_sha>",
      "kfm:parameters": { "example_param": "value" }
    }
  },
  "agent": {
    "urn:kfm:agent:person:<handle>": {
      "prov:type": "prov:Person",
      "prov:label": "<Name or handle>"
    },
    "urn:kfm:agent:bot:ci": {
      "prov:type": "prov:SoftwareAgent",
      "prov:label": "CI / Validation Bot"
    }
  },

  "used": {
    "urn:kfm:activity:ingest:<run_id>": ["urn:kfm:entity:raw:<source_id>"]
  },
  "wasGeneratedBy": {
    "urn:kfm:entity:processed:<asset_id>": "urn:kfm:activity:ingest:<run_id>"
  },
  "wasAssociatedWith": {
    "urn:kfm:activity:ingest:<run_id>": [
      "urn:kfm:agent:person:<handle>",
      "urn:kfm:agent:bot:ci"
    ]
  }
}
```

> â„¹ï¸ Exact shape depends on the active KFMâ€‘PROV profile. Use this as a **starting point**, not a rigid schema.

</details>

---

### 2) ğŸ¤– AI / analysis â€œevidence artifactsâ€

KFM treats analysis outputs (including AI-generated outputs) as **firstâ€‘class datasets**:

- âœ… stored under `data/processed/â€¦`
- âœ… cataloged in STAC/DCAT (clearly marked as derived/AI-generated)
- âœ… traced in PROV (inputs, model/method, parameters, confidence)
- âœ… loaded into graph cautiously (explicit provenance links)
- âœ… exposed only via governed APIs (no hard-coding artifacts directly in UI)

**Minimum extras to record for AI artifacts:**
- model name/version (or service identity)
- prompt/config (or a hashed reference)
- confidence/uncertainty measures
- human review status (if applicable)

---

### 3) â±ï¸ Realâ€‘time / streaming provenance

Treat streaming as **many small datasets over time**:

- keep a provenance â€œtrailâ€ that can answer:  
  **â€œWhich reading (timestamped) was used for this view / answer?â€**

If Focus Mode answers using a dynamic query, the system should still log PROV indicating the specific reading (with timestamp) used as an input entity.

---

### 4) ğŸ® Simulations (`data/work/sims/` â†’ promote to official)

Simulation outputs are **sandbox-first**:

- Run in: `data/work/sims/` âœ… (experimental)
- **Never** point Graph/API/UI directly at sandbox outputs âŒ
- If the result becomes â€œofficialâ€, **promote** it to `data/processed/` and publish the triplet:
  - STAC
  - DCAT
  - PROV

**Reproducibility checklist (minimum):**
- pin input dataset hashes
- capture all parameters/configs
- pin environment (container digest / lockfiles)
- record random seeds (if applicable)
- basic verification tests / regression checks
- validation (compare to real data when possible)
- uncertainty quantification (at least a minimal sensitivity/ensemble summary)

---

### 5) ğŸ§ª DevOps â†’ PROV (optional, but powerful)

For full auditability, you can model development events as provenance:

- PR / merge = **prov:Activity**
- commits = **prov:Entity**
- authors/reviewers/CI bot = **prov:Agent**
- linkages via `prov:used`, `prov:wasAssociatedWith`, `prov:wasGeneratedBy`

This enables queries like:
- â€œWhich code version produced this dataset?â€
- â€œWho reviewed the change that modified this pipeline?â€

---

## ğŸ›¡ï¸ Governance, privacy, and cultural protocols

KFM provenance isnâ€™t only â€œhow it was builtâ€â€”itâ€™s also **how it is allowed to be used**.

Include (and enforce) governance metadata such as:

- **ğŸ“œ license presence** (no license â†’ no publish)
- **ğŸ” sensitivity/classification** (output cannot be less restricted than inputs)
- **ğŸ§‘â€ğŸ¤â€ğŸ§‘ cultural protocols / community constraints** when applicable  
  (e.g., restricted access, location obfuscation, context labels)
- **ğŸ·ï¸ contributor credit** (people/communities who contributed data or knowledge)

> âš ï¸ If a dataset is sensitive, build your PROV + catalog metadata so downstream layers (Graph/API/UI) can enforce redaction, filtering, masking, or tiered access.

---

## ğŸ§° Validation & policy gates

KFM enforces automated gates at ingestion, inference, and publication:

- schema validation âœ…
- STAC/DCAT/PROV completeness âœ…
- license presence âœ…
- sensitivity labeling + handling âœ…
- provenance completeness âœ…
- â€œfail closedâ€ behavior âœ…

**Expectation:** CI/policy checks should block merges when provenance is missing or inconsistent (example class of error: processed data changed but PROV didnâ€™t).

---

## â“ FAQ

### â€œDo I need PROV for *every* dataset?â€
If it becomes available through Graph/API/UI: **yes** âœ…. Provenance-first is a platform invariant.

### â€œCan I manually edit processed data to fix something?â€
Prefer updating the pipeline/config and regenerating. Manual tweaks are strongly discouraged because they break reproducibility unless captured as a formal, documented activity.

### â€œWhere should I store PROV for AI outputs?â€
Treat them like datasets:
- put artifacts in `data/processed/â€¦`
- publish STAC/DCAT
- store PROV here (`data/prov/â€¦`) with model/method + confidence

### â€œHow does UI benefit from PROV?â€
UI components (Layer Info, exports, Focus Mode citations, provenance panels) can pull structured lineage to show users â€œthe map behind the mapâ€ and automatically generate proper credits.

---

## ğŸ§· Contributor checklist (copy/paste)

- [ ] raw inputs are **immutable** (`data/raw/` unchanged)
- [ ] processed outputs written to `data/processed/â€¦`
- [ ] STAC record created/updated (`data/stac/â€¦`)
- [ ] DCAT record created/updated (`data/catalog/dcat/â€¦`)
- [ ] PROV bundle created/updated (`data/prov/â€¦`)
- [ ] license + sensitivity fields present
- [ ] hashes recorded for key inputs/outputs
- [ ] CI/policy gates pass âœ…

---