---
title: "ğŸ—„ï¸ Kansas Frontier Matrix â€” Data System Architecture (Diamondâ¹ Î© / CrownâˆÎ© Ultimate Certified)"
path: "data/ARCHITECTURE.md"

version: "v11.2.3"
last_updated: "2025-12-09"
release_stage: "Stable / Governed"
lifecycle: "Long-Term Support (LTS)"
review_cycle: "Quarterly Â· FAIR+CARE Council Oversight"
content_stability: "stable"

commit_sha: "<latest-commit-hash>"
previous_version_hash: "<previous-sha256>"
doc_integrity_checksum: "<sha256>"
doc_uuid: "urn:kfm:doc:data-architecture:v11.2.3"
semantic_document_id: "kfm-doc-data-architecture"
event_source_id: "ledger:data/ARCHITECTURE.md"
immutability_status: "version-pinned"

sbom_ref: "../releases/v11.2.3/sbom.spdx.json"
manifest_ref: "../releases/v11.2.3/manifest.zip"
telemetry_ref: "../releases/v11.2.3/focus-telemetry.json"
telemetry_schema: "../schemas/telemetry/data-architecture-v11.2.3.json"
energy_schema: "../schemas/telemetry/energy-v2.json"
carbon_schema: "../schemas/telemetry/carbon-v2.json"
data_contract_ref: "../docs/contracts/data-contract-v3.json"

governance_ref: "../docs/standards/governance/ROOT-GOVERNANCE.md"
ethics_ref: "../docs/standards/faircare/FAIRCARE-GUIDE.md"
sovereignty_policy: "../docs/standards/sovereignty/INDIGENOUS-DATA-PROTECTION.md"

license: "MIT"
mcp_version: "MCP-DL v6.3"
markdown_protocol_version: "KFM-MDP v11.2.5"
ontology_protocol_version: "KFM-OP v11.0"
pipeline_contract_version: "KFM-PDC v11.0"

status: "Active / Enforced"
doc_kind: "Architecture"
intent: "data-system-architecture"
role: "data-platform-architecture"
category: "Data Â· ETL Â· Governance Â· FAIR+CARE"

fair_category: "F1-A1-I1-R1"
care_label: "Variable â€” Dataset Dependent"
sensitivity_level: "Mixed"
public_exposure_risk: "Dataset-level"
indigenous_rights_flag: "Dataset-level"
data_steward: "KFM FAIR+CARE Council"
risk_category: "Mixed"
redaction_required: false

ontology_alignment:
  cidoc: "E29 Design or Procedure"
  schema_org: "TechArticle"
  owl_time: "TemporalEntity"
  prov_o: "prov:Plan"
  geosparql: "geo:FeatureCollection"

json_schema_ref: "../schemas/json/data-architecture.schema.json"
shape_schema_ref: "../schemas/shacl/data-architecture-shape.ttl"

ai_training_inclusion: false
ai_focusmode_usage: "Allowed with restrictions"
ai_transform_permissions:
  - "summaries"
  - "semantic-highlighting"
  - "a11y-adaptations"
  - "metadata-extraction"
ai_transform_prohibited:
  - "speculative additions"
  - "unverified historical claims"
  - "governance-override"
  - "hallucinated-datasets"

machine_extractable: true
accessibility_compliance: "WCAG 2.1 AA"
jurisdiction: "Kansas / United States"
classification: "Public Document"
lifecycle_stage: "stable"
ttl_policy: "Annual review"
sunset_policy: "Superseded upon next data-platform architecture update"

layout_profiles:
  - "immediate-one-branch-with-descriptions-and-emojis"
badge_profiles:
  - "root-centered-badge-row"
requires_purpose_block: true
requires_directory_layout_section: true
requires_version_history: true
requires_governance_links_in_footer: true
---

<div align="center">

# ğŸ—„ï¸ **Kansas Frontier Matrix â€” Data System Architecture**  
`data/ARCHITECTURE.md`

**Purpose**  
Describe the **endâ€‘toâ€‘end architecture** of the KFM data platform: ingestion, storage, ETL, validation, governance, lineage, STAC/DCAT cataloging, graph loading, and AIâ€‘assisted enrichment â€” all wired into CI/CD, FAIR+CARE, and sovereignty governance.

This is the **canonical reference** for anyone touching `data/**`:

- ETL & pipeline engineers  
- GIS & spatial analysts  
- AI/ML practitioners  
- Governance & FAIR+CARE reviewers  
- Focus Mode / Story Node architects  

Designed to be **machineâ€‘readable**, **governanceâ€‘enforced**, and **GitHubâ€‘safe**.

[ğŸ“¦ Data Directory Overview](README.md) Â· [ğŸ”„ CI/CD Workflows](../.github/workflows/README.md)

</div>

---

## ğŸ“˜ Overview

At v11.2.3, the KFM data system:

- Integrates **historical, environmental, cultural, and geologic data** into a unified platform.  
- Normalizes everything into a **shared spatial, temporal, and semantic frame**.  
- Represents datasets using:
  - **STAC 1.x** catalogs for spatioâ€‘temporal assets,  
  - **DCAT 3.0** catalogs for datasets and distributions,  
  - **JSONâ€‘LD + PROVâ€‘O** for semantic metadata and lineage.  
- Loads curated products into the **Neo4j knowledge graph**, aligned with:
  - CIDOCâ€‘CRM, GeoSPARQL, OWLâ€‘Time, PROVâ€‘O, and KFMâ€‘OP v11.  
- Tracks:
  - **Lineage** (what came from where and how),  
  - **Governance** (FAIR+CARE, sovereignty, risk),  
  - **Sustainability** (energy/carbon telemetry).

Data artifacts ultimately serve:

- **Focus Mode v3** (narrative generation and evidence linking),  
- **Story Nodes v3** (spatiotemporal narrative units),  
- **Public STAC/DCAT catalogs**,  
- **APIs and map UIs** (MapLibre/Cesium web client).

The architecture is designed for:

- **Reproducible ETL**,  
- **Transparent, auditable governance**,  
- **Ethical & sovereign data use**,  
- **Strong sustainability accounting**,  
- **Tight integration with AI & visualization layers**.

---

## ğŸ—‚ï¸ Data System Directory Architecture

This architecture builds on the canonical layout documented in `data/README.md` and treats `data/` as the **physical backbone** of the KFM data plane.

~~~text
ğŸ“ data/
â”œâ”€â”€ ğŸ“„ ARCHITECTURE.md               # This document (data system architecture)
â”œâ”€â”€ ğŸ“„ README.md                     # Data directory overview & governance
â”‚
â”œâ”€â”€ ğŸ“ sources/                      # External dataset manifests & source metadata
â”‚   â”œâ”€â”€ ğŸ“ providers/                # Provider profiles (NOAA, USGS, KGS, tribal partners, etc.)
â”‚   â”œâ”€â”€ ğŸ“ catalogs/                 # Upstream STAC/DCAT links & harvested descriptors
â”‚   â””â”€â”€ ğŸ“ agreements/               # Licensing / MOU summaries (non-sensitive)
â”‚
â”œâ”€â”€ ğŸ“ raw/                          # Original source datasets (immutable, append-only; Git+DVC/LFS)
â”‚   â”œâ”€â”€ ğŸ“ hydrology/
â”‚   â”œâ”€â”€ ğŸ“ geology/
â”‚   â”œâ”€â”€ ğŸ“ history/
â”‚   â”œâ”€â”€ ğŸ“ remote-sensing/
â”‚   â”œâ”€â”€ ğŸ“ environmental/
â”‚   â””â”€â”€ ğŸ“ cultural-sovereignty/     # Culturally sensitive data (governed, often generalized)
â”‚
â”œâ”€â”€ ğŸ“ work/                         # Cleaned, normalized, enriched intermediates
â”‚   â”œâ”€â”€ ğŸ“ tables/                   # Normalized tabular data (CSV, Parquet)
â”‚   â”œâ”€â”€ ğŸ“ spatial/                  # Intermediate GeoJSON/GPKG/rasters/COGs
â”‚   â””â”€â”€ ğŸ“ metadata/                 # Pre-STAC/DCAT JSON/JSON-LD, schema snapshots
â”‚
â”œâ”€â”€ ğŸ“ processed/                    # Deterministic, analysis-ready ETL outputs
â”‚   â”œâ”€â”€ ğŸ“ hydrology/
â”‚   â”œâ”€â”€ ğŸ“ climate/
â”‚   â”œâ”€â”€ ğŸ“ ecology/
â”‚   â”œâ”€â”€ ğŸ“ historical/
â”‚   â”œâ”€â”€ ğŸ“ hazards/
â”‚   â””â”€â”€ ğŸ“ storynodes/               # Precomputed Story Node-ready aggregates (optional)
â”‚
â”œâ”€â”€ ğŸ“ stac/                         # STAC Items & Collections (KFM-STAC v11 profile)
â”‚   â”œâ”€â”€ ğŸ“„ README.md                 # STAC catalog conventions
â”‚   â”œâ”€â”€ ğŸ§¾ catalog.json              # STAC root catalog
â”‚   â”œâ”€â”€ ğŸ“ missions/                 # EO missions (Landsat, Sentinel, NAIP, etc.)
â”‚   â”œâ”€â”€ ğŸ“ hydrology/                # Hydrology STAC domain
â”‚   â”œâ”€â”€ ğŸ“ climate/                  # Climate STAC domain
â”‚   â”œâ”€â”€ ğŸ“ hazards/                  # Hazards STAC domain
â”‚   â”œâ”€â”€ ğŸ“ landcover/                # Landcover / land use domain
â”‚   â””â”€â”€ ğŸ“ tabular/                  # Tabular/non-spatial STAC items
â”‚
â”œâ”€â”€ ğŸ“ dcat/                         # DCAT 3.0 catalogs (JSON-LD)
â”‚   â”œâ”€â”€ ğŸ§¾ catalog.jsonld            # Root DCAT catalog
â”‚   â””â”€â”€ ğŸ“ datasets/                 # DCAT dataset descriptions
â”‚
â”œâ”€â”€ ğŸ“ checksums/                    # SHA-256 lineage & integrity tracking
â”‚   â”œâ”€â”€ ğŸ§¾ raw/
â”‚   â”œâ”€â”€ ğŸ§¾ processed/
â”‚   â””â”€â”€ ğŸ§¾ stac/
â”‚
â”œâ”€â”€ ğŸ“ reports/                      # Validation, FAIR+CARE, audit, telemetry
â”‚   â”œâ”€â”€ ğŸ§¾ self-validation/          # Schema, STAC/DCAT, CARE, checksum reports
â”‚   â”œâ”€â”€ ğŸ§¾ telemetry/                # Sustainability & performance telemetry
â”‚   â””â”€â”€ ğŸ§¾ audit/                    # Governance & compliance audits
â”‚
â”œâ”€â”€ ğŸ“ archive/                      # Versioned/superseded datasets (cold storage)
â”‚   â””â”€â”€ ğŸ“ <year>/                   # Archived by year / major release
â”‚
â””â”€â”€ ğŸ“ tmp/                          # Scratch; NEVER used as an input to production pipelines
~~~

Architectural guarantees:

- `raw/` is **immutable and appendâ€‘only**; changes must create new versions.  
- `processed/` is **deterministic** (raw + config + code â†’ same result).  
- `stac/` and `dcat/` are **schemaâ€‘valid, governanceâ€‘approved catalogs**.  
- `checksums/` + `reports/` provide **verifiable integrity and governance evidence**.

---

## ğŸ”„ Data Lifecycle & CI/CD Integration

KFMâ€™s data lifecycle is a **governed pipeline** that aligns with MCP, the ETL architecture guides, and CI/CD workflows in `.github/workflows/`.

~~~mermaid
flowchart TD
  A["ğŸ“ sources/\nProvider manifests & upstream catalogs"]
    --> B["ğŸ“ raw/\nImmutable ingests (Git+DVC/LFS)"]

  B --> C["ğŸ“ work/\nCleaning Â· normalization Â· enrichment"]
  C --> D["ğŸ“ processed/\nDeterministic ETL outputs"]

  D --> E["ğŸ“ stac/\nSTAC Items & Collections"]
  D --> F["ğŸ“ dcat/\nDCAT Datasets & Distributions"]

  E --> G["ğŸ“ checksums/\nSHA-256 digests (raw/processed/stac)"]
  F --> G

  G --> H["ğŸ“ reports/self-validation/\nSchema Â· FAIR+CARE Â· provenance"]
  H --> I["Neo4j graph ingest\n(CIDOC Â· GeoSPARQL Â· OWL-Time Â· PROV-O)"]
  I --> J["Focus Mode Â· Story Nodes Â· Public Catalogs"]
~~~

**Workflow tieâ€‘ins:**

- `.github/workflows/data_pipeline.yml`  
  - Validates ETL data contracts (KFMâ€‘PDC v11).  
  - Confirms raw â†’ work â†’ processed transitions are reproducible and logged.
- `.github/workflows/stac_validate.yml` / `dcat_validate.yml` / `jsonld_validate.yml`  
  - Enforce STAC/DCAT/JSONâ€‘LD & ontology correctness before merge.  
- `.github/workflows/faircare_validate.yml` / `h3_generalization.yml`  
  - Enforce FAIR+CARE, sovereignty flags, and spatial generalization.  
- `.github/workflows/sbom_verify.yml` / `telemetry_export.yml`  
  - Connect data artifacts to SBOMs and sustainability telemetry.

---

## ğŸŒ Spatial Architecture (CRS, Geometry, H3)

### CRS Policy

Canonical public CRS:

- **EPSG:4326 (WGS84)** for all published STAC and DCAT spatial extents and for most webâ€‘facing geometry.

Permitted working CRSs (inside `work/` or intermediate steps):

- Equalâ€‘area projections for areal statistics (e.g., EPSG:6933).  
- UTM for localâ€‘scale precision tasks.  
- Historical CRSs for georeferenced legacy maps (recorded explicitly in metadata).

Rules:

- Every spatial dataset MUST:
  - Declare its CRS explicitly in metadata,  
  - Record any reprojection steps (source CRS â†’ target CRS),  
  - Pass geometry validity checks (no selfâ€‘intersections, valid polygons, consistent `bbox`).

### H3 Generalization & Indexing

H3 is integral to:

- **Generalization** of sensitive locations (heritage, sacred sites, protected habitats).  
- **Aggregation and indexing** across multiple scales for dashboards and analytics.

Patterns:

- Public datasets that could expose sensitive sites MUST store only **H3 aggregated** coordinates (e.g., at a configured resolution), with raw coordinates retained only in governed, restricted contexts.  
- STAC/DCAT metadata indicate:
  - H3 resolution,  
  - Whether generalization has been applied,  
  - Any residual risk or masking notes.

Compliance is enforced by:

- `h3_masking_check.py` invoked in `h3_generalization.yml`.  

---

## â± Temporal Architecture (OWLâ€‘Time & Event Modeling)

Temporal representation uses **OWLâ€‘Time** and KFMâ€‘OP v11 patterns:

- Each dataset must define:
  - `temporal_start`, `temporal_end`,  
  - Optional uncertainty (e.g., `temporal_uncertainty`, `approximate` flags).

Mappings:

- **STAC**: `extent.temporal.interval` (Instant or Interval).  
- **DCAT**: `dct:temporal` (TimePeriod).  
- **Graph**: `time:Instant` and `time:Interval` nodes attached to datasets, features, and Story Nodes.

Implications:

- Focus Mode timelines are driven directly from these temporal objects.  
- Historical datasets can represent vague/approximate periods while remaining queryable (e.g., â€œlate 19th centuryâ€ modeled as a TimePeriod with uncertainty).

---

## ğŸ“Š Metadata Systems â€” STAC, DCAT, JSONâ€‘LD

### STAC (KFMâ€‘STAC v11 Profile)

- STAC is the **primary schema** for spatial & spatiotemporal assets.  
- `data/stac/` organizes:
  - Root `catalog.json`,  
  - Domain collections (hydrology, climate, hazards, landcover, missions, tabular),  
  - Items referencing assets in `processed/`.

KFMâ€‘STAC v11 profile extends STAC with:

- `kfm_id` (stable internal identifier).  
- FAIR+CARE and sovereignty flags.  
- Lineage references (links to PROVâ€‘O/OpenLineage documents and checksums).  
- Domainâ€‘specific fields (e.g., hydrologic unit, climate variable codes).

Validation:

- Implemented via `.github/actions/stac-validate/` and `stac_validate.yml`.  
- Failures block merges to main/release branches.

### DCAT 3.0

- DCAT provides webâ€‘native dataset descriptions for interoperability.  
- `data/dcat/catalog.jsonld` is the root; each dataset is described under `data/dcat/datasets/`.

Mappings:

- STAC `collection.id` â†’ `dcat:Dataset` `dct:identifier`.  
- STAC Item assets â†’ DCAT `dcat:Distribution`.  
- Licenses â†’ `dct:license`.  
- Access constraints, CARE flags, and sovereignty notes recorded in DCAT as annotations and additional properties.

### JSONâ€‘LD & Ontologies

Core ontologies:

- CIDOCâ€‘CRM â€” cultural/historical entities.  
- GeoSPARQL â€” spatial relationships.  
- OWLâ€‘Time â€” temporal entities/intervals.  
- PROVâ€‘O â€” provenance (Entities, Activities, Agents).  
- KFMâ€‘OP v11 â€” KFMâ€‘specific domain types and roles.

JSONâ€‘LD contexts:

- Ensure that STAC/DCAT metadata can be ingested directly into the graph.  
- Encode FAIR+CARE, sovereignty policies, and risk categories as machineâ€‘readable properties.

---

## ğŸ§® ETL Architecture & Data Contracts

Pipelines live primarily under `src/pipelines/` and are governed by **KFMâ€‘PDC v11** and `data_contract_ref`:

- **Extract**  
  - Ingest from external sources into `data/raw/` and/or `data/sources/`.  
  - Capture source manifests and initial checksums.

- **Transform**  
  - Normalize schemas into `data/work/tables/` and `data/work/spatial/`.  
  - Apply cleaning, harmonization, enrichment, and derived metrics.  
  - Enforce columnâ€‘level contracts (types, ranges, nullability).

- **Load**  
  - Emit deterministic products to `data/processed/**`.  
  - Generate STAC Items/Collections and DCAT datasets.  
  - Trigger lineage & telemetry emission.

Data contracts:

- Stored in `../docs/contracts/` and/or `config/data_contracts/`.  
- Define allowed field sets, encodings, and quality thresholds.  
- Enforced by `validate_pipelines.py` and `data_pipeline.yml`.

Reproducibility:

- Every ETL run must be reâ€‘runnable given:
  - Raw data versions (DVC + checksums),  
  - ETL config,  
  - Containerized environment (e.g., Docker image tags),  
  - Pipeline commit hash.

---

## ğŸ§¬ Lineage Architecture (PROVâ€‘O & OpenLineage)

Lineage is modeled at two layers:

1. **Logical provenance (PROVâ€‘O + JSONâ€‘LD)**  
   - Datasets, tables, layers â†’ `prov:Entity`.  
   - Pipeline runs, AI transforms, manual curation â†’ `prov:Activity`.  
   - Automation, services, human maintainers â†’ `prov:Agent`.  

2. **Executionâ€‘level lineage (OpenLineage)**  
   - Each ETL or model run emits OpenLineage events (jobs, runs, inputs, outputs).  
   - These events can be replayed to understand how a given artifact was produced.

Storage:

- JSON/JSONâ€‘LD lineage documents under `data/reports/audit/` and `data/reports/self-validation/lineage/`.  
- Summaries and timeâ€‘series metrics in telemetry outputs (`focus-telemetry.json`).  
- Graph representation in Neo4j for queryable lineage (e.g., â€œwhat raw sources contributed to this Story Node?â€).

Guarantees:

- Every production dataset in `processed/` that feeds UI/graphs must have:
  - A discoverable PROVâ€‘O trace, and  
  - At least one OpenLineage run record for its generating pipelines.

---

## ğŸ§  AI & Enrichment Architecture

AI/ML is used to **augment** â€” not silently replace â€” data:

Use cases:

- OCR and layout analysis for historical documents and maps.  
- NLP entity extraction (places, people, events) from textual sources.  
- Derived indices (e.g., drought indices from multiâ€‘source climate data).  
- Preâ€‘aggregation and summarization for Focus Mode & Story Nodes.

Constraints:

- All AIâ€‘derived products must:
  - Be materialized in `data/work/` or `data/processed/` with AIâ€‘specific metadata,  
  - Record model details (name, version, provider, seed where relevant),  
  - Capture training data provenance at a sensible level of abstraction,  
  - Be clearly labeled as AIâ€‘derived in STAC/DCAT and graph metadata.

Governance:

- AI behavior and model deployment are gated by `ai_behavior_check.yml` and `focusmode_mlops.yml`.  
- Highâ€‘impact AI transforms (e.g., classification of heritage sites) may require explicit FAIR+CARE and sovereignty council review.

---

## âš– FAIR+CARE & Sovereignty in the Data System

FAIR+CARE and sovereignty are **firstâ€‘class architecture concerns**, not postâ€‘hoc labels.

Enforcement points:

- **Ingestion**  
  - Data in `sources/` and `raw/` includes license and sovereignty notes.  
  - Early tagging of sensitive or sovereign datasets.

- **Transformation**  
  - Aggregation/generalization for sensitive locations.  
  - Removal or obfuscation of PII/PHI.

- **Publication**  
  - Public datasets appear only in catalogs once FAIR+CARE and sovereignty checks pass.  
  - H3 or other masks applied for any highâ€‘risk coordinates.

- **Exposure**  
  - APIs and UI layers respect flags indicating:
    - Usage constraints,  
    - Attribution requirements,  
    - Contact points for data stewards.

Violations:

- CI pipelines block merges when:
  - Required CARE labels or sovereignty flags are missing,  
  - Sensitive spatial precision is detected in public domains.

---

## ğŸŒ± Sustainability & Telemetry Architecture

KFM treats **sustainability telemetry** as part of the data system:

Metrics captured:

- `energy_wh` â€” energy consumed by ETL/model runs,  
- `carbon_gco2e` â€” estimated emissions,  
- Workload metrics (`records_processed`, `bytes_processed`, `compute_time_s`).

Where stored:

- `data/reports/telemetry/` (dataâ€‘plane view),  
- Versioned release telemetry (e.g., `../releases/v11.2.3/focus-telemetry.json`).

Governance:

- Telemetry informs:
  - ETL scheduling decisions (e.g., offâ€‘peak windows),  
  - Optimization priorities (e.g., materializing expensive joins vs. caching),  
  - FAIR+CARE stewardship discussions where data processing may impact energy budgets.

---

## ğŸ“ˆ Data Quality & Fitnessâ€‘forâ€‘Use

Quality controls draw from ISO 19157â€‘style dimensions:

- **Completeness** â€” coverage & missingness.  
- **Logical consistency** â€” referential integrity, valid code lists.  
- **Positional accuracy** â€” spatial precision and error models.  
- **Temporal accuracy** â€” correct timestamps, intervals, and chronology.  
- **Thematic accuracy** â€” correctness of classifications and labels.

Outputs:

- Stored under `data/reports/self-validation/quality/`.  
- Summaries can be attached to STAC Collections and DCAT Datasets as quality notes.  
- Focus Mode and Story Nodes use this information to:
  - Qualify narratives with confidence levels,  
  - Avoid overâ€‘claiming precision where quality is low.

---

## ğŸ§© Ontology & Entity Classes

Core dataâ€‘layer entity classes and mappings:

| KFM Entity    | Description                              | CIDOC       | Schema.org     | DCAT              | PROVâ€‘O     |
|---------------|------------------------------------------|-------------|----------------|-------------------|-----------|
| Dataset       | Logical grouped data product             | E73         | Dataset        | dcat:Dataset      | Entity    |
| Distribution  | Particular file/asset of a dataset       | E73         | DataDownload   | dcat:Distribution | Entity    |
| Feature       | Spatial feature (vector)                 | E53 Place   | Place          | n/a               | Entity    |
| RasterLayer   | Spatial raster layer                     | E36/E73     | Dataset        | Distribution      | Entity    |
| Table         | Tabular dataset                          | E73         | Dataset        | Distribution      | Entity    |
| SensorStream  | Timeâ€‘series sensor stream                | E16/E73     | Dataset        | Dataset           | Entity    |

These are reflected in:

- STAC & DCAT metadata,  
- JSONâ€‘LD context mappings,  
- Graph labels and relationship types.

---

## ğŸ”— STAC/DCAT â†’ Graph Mapping

Canonical mappings into Neo4j:

- STAC `collection.id` â†’ `(:Dataset {kfm_id, stac_id})`.  
- STAC `item.id` â†’ `(:DatasetInstance {kfm_item_id})` or `(:Distribution)` depending on design.  
- STAC `geometry` â†’ `(:Geometry)` with GeoSPARQL WKT/GeoJSON and relationships like `:HAS_GEOMETRY`.  
- DCAT `dct:identifier` â†’ `Dataset.kfm_id`.  
- DCAT `dcat:distribution` â†’ `(:Distribution)` nodes pointing at STAC assets.  
- OWLâ€‘Time intervals â†’ `(:TimeInterval)` connected by `:HAS_TEMPORAL_EXTENT`.  
- PROVâ€‘O lineage â†’ `(:Entity)-[:wasGeneratedBy]->(:Activity)-[:used]->(:Entity)`.

This ensures a **continuous semantic path** from onâ€‘disk files in `data/**` to knowledge graph nodes used by Focus Mode and Story Nodes.

---

## ğŸ§° Validation Toolchain & CI Hooks

Validation is not optional â€” it is embedded in architecture:

Components:

- Schema validation (JSON Schema, SHACL, table contracts).  
- STAC/DCAT/JSONâ€‘LD validators (`stac-validate`, `dcat-validate`, `schema-validate`).  
- FAIR+CARE & sovereignty checks (`run_faircare_checks.py`, `h3_masking_check.py`).  
- Checksum verifiers (matching `checksums/` vs. actual files and manifests).  
- Quality assessment tools (e.g., Great Expectationsâ€‘style tests).  
- Telemetry summarizers.

CI pipelines:

- Merges to `main`/`release/**` branches are blocked when critical validation fails.  
- Dataâ€‘touching PRs should expect:
  - `data_pipeline.yml`,  
  - `stac_validate.yml`,  
  - `dcat_validate.yml`,  
  - `jsonld_validate.yml`,  
  - `faircare_validate.yml`,  
  - `h3_generalization.yml`  
  to run as appropriate.

---

## ğŸ§­ Contributor Workflow (Data Architecture)

When you:

- **Add a new dataset**, or  
- **Extend the data architecture**, or  
- **Introduce a new domain folder under `processed/` or `stac/`**,

you should:

1. Place files in the **correct subdirectory** under `data/`.  
2. Update or add:
   - STAC Collection/Items in `data/stac/**`,  
   - DCAT Dataset record(s) in `data/dcat/datasets/`.  
3. Define or update **data contracts** in `../docs/contracts/` or config.  
4. Ensure FAIR+CARE & sovereignty metadata are set correctly.  
5. Run local validations where possible (schema, STAC/DCAT, FAIR+CARE, H3).  
6. Submit a PR and respond to:
   - CI failures,  
   - Governance/FAIR+CARE comments,  
   - Architecture review requests.

---

## ğŸ•°ï¸ Version History

| Version | Date       | Summary                                                                                                                                            |
|--------:|------------|----------------------------------------------------------------------------------------------------------------------------------------------------|
| v11.2.3 | 2025-12-09 | Aligned with KFM-MDP v11.2.5; synced layout with `data/README.md`; tightened ontology alignment; clarified CI hooks, DVC/lineage semantics, and AI/FAIR+CARE wiring. |
| v11.2.2 | 2025-11-27 | Canonicalized directory layout; added telemetry/schema references; integrated STAC/DCAT/JSONâ€‘LD and checksums into architecture narrative.        |
| v11.0.0 | 2025-11-19 | Initial v11 data system architecture; defined rawâ†’workâ†’processed lifecycle, baseline governance, and ETL patterns.                               |

---

<div align="center">

ğŸ—„ï¸ **Kansas Frontier Matrix â€” Data System Architecture (v11.2.3)**  
Dataâ€‘First Â· FAIR+CAREâ€‘Governed Â· Provenanceâ€‘Aware  

Â© 2025 Kansas Frontier Matrix â€” MIT License  
MCPâ€‘DL v6.3 Â· KFMâ€‘MDP v11.2.5 Â· KFMâ€‘OP v11.0  

[â¬… Back to Data Overview](README.md) Â·  
[â¬… Back to Repository Root](../README.md) Â·  
[âš– Governance Charter](../docs/standards/governance/ROOT-GOVERNANCE.md)

</div>