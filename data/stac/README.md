<div align="center">

# ğŸ“œ Kansas Frontier Matrix â€” Text Source Manifests

`data/sources/text/`

**Mission:** Curate, document, and validate all **external text-based datasets**â€”including digitized documents, OCR archives, oral histories, and treaty transcriptsâ€”
that serve as the linguistic and narrative foundation for the Kansas Frontier Matrix (KFM).

[![Build & Deploy](https://github.com/bartytime4life/Kansas-Frontier-Matrix/actions/workflows/site.yml/badge.svg)](../../../.github/workflows/site.yml)
[![STAC Validate](https://github.com/bartytime4life/Kansas-Frontier-Matrix/actions/workflows/stac-validate.yml/badge.svg)](../../../.github/workflows/stac-validate.yml)
[![Schema Validate](https://img.shields.io/badge/JSON%20Schema-validated-success?logo=json)](../schema/source.schema.json)
[![CodeQL](https://github.com/bartytime4life/Kansas-Frontier-Matrix/actions/workflows/codeql.yml/badge.svg)](../../../.github/workflows/codeql.yml)
[![Docs Â· MCP](https://img.shields.io/badge/Docs-MCP-blue)](../../../docs/)
[![License: Data (CC-BY 4.0)](https://img.shields.io/badge/License-CC--BY%204.0-green)](../../../LICENSE)

</div>

---

## ğŸ“š Overview

The `data/sources/text/` directory houses **JSON manifests** describing every external text-based source integrated into KFMâ€”
ranging from historical treaties and scanned documents to oral history transcripts and OCR corpora.

These sources provide the **linguistic, historical, and cultural backbone** of the Kansas knowledge system.

They support:

* Historical document digitization and transcription
* Named Entity Recognition (NER) and NLP enrichment
* Treaty and land-cession tracking
* Oral history integration within the knowledge graph
* STAC-linked provenance for textual archives

Each manifest adheres to `data/sources/schema/source.schema.json`, enabling transparent provenance, licensing,
and automated validation through CI/CD workflows.

---

## ğŸ—‚ï¸ Directory Layout

```bash
data/sources/text/
â”œâ”€â”€ README.md
â”œâ”€â”€ loc_chronicling_america.json      # Library of Congress historical newspaper corpus
â”œâ”€â”€ kshs_oral_histories.json          # Kansas Historical Society oral history transcripts
â””â”€â”€ yale_avalon_treaties.json         # Yale Avalon Project â€” historical treaties & legal texts
```

> **Note:**
> Every manifest records dataset identifiers, licensing, provenance, and verification timestamps
> to ensure archival integrity and scholarly reproducibility.

---

## ğŸ—ï¸ Example: `loc_chronicling_america.json`

```json
{
  "id": "loc_chronicling_america",
  "title": "Library of Congress â€” Chronicling America Historical Newspaper Corpus",
  "provider": "Library of Congress (LOC)",
  "description": "Digitized and OCR-processed newspaper archives spanning 1789â€“1963.",
  "endpoint": "https://chroniclingamerica.loc.gov/",
  "access_method": "HTTP API",
  "license": "Public Domain (US Government)",
  "data_type": "text",
  "format": "JSONL",
  "spatial_coverage": "Kansas, USA",
  "temporal_coverage": "1854â€“1963",
  "update_frequency": "Monthly",
  "last_verified": "2025-10-12",
  "linked_pipeline": "text_pipeline.py",
  "notes": "Used for NLP entity extraction, OCR correction, and timeline construction."
}
```

---

## ğŸ§­ System Context (GitHub-safe Mermaid)

```mermaid
flowchart TD
  A["External Text Archives\nLOC Â· KSHS Â· Yale Avalon"] --> B["Source Manifests\n`data/sources/text/*.json`"]
  B --> C["ETL Pipeline\n`src/pipelines/text_pipeline.py`"]
  C --> D["Processed Text Corpora\n`data/processed/text/`"]
  D --> E["Derivatives\nTokenized Â· Parsed Â· Linked Text"]
  D --> F["STAC Collections\n`data/stac/collections/text.json`"]
  F --> G["Knowledge Graph\nPeople â†” Places â†” Events â†” Documents"]
  E --> H["Web UI\nSearch Â· Timeline Â· Document Viewer"]
%%END OF MERMAID%%
```

---

## ğŸ§¾ Text Source Summary

| Manifest File                  | Provider    | Description                                   | Coverage          | Format   | Verified     |
| :----------------------------- | :---------- | :-------------------------------------------- | :---------------- | :------- | :----------- |
| `loc_chronicling_america.json` | LOC         | OCR-based historical newspapers               | Kansas            | JSONL    | âœ… 2025-10-12 |
| `kshs_oral_histories.json`     | KSHS        | Transcribed oral histories and interviews     | Kansas            | TXT      | âœ… 2025-10-12 |
| `yale_avalon_treaties.json`    | Yale Avalon | Historical treaty and legal document archives | National / Global | HTML/TXT | âœ… 2025-10-12 |

---

## ğŸ§¾ ETL Integration

**Pipeline:** `src/pipelines/text_pipeline.py`
**Target Directory:** `data/processed/text/`

### Workflow

1. **Validate** manifests against schema (`make sources-validate`)
2. **Ingest** text sources via HTTP/API or download
3. **Normalize** (UTF-8 encoding, OCR cleanup, metadata tagging)
4. **Tokenize** and extract entities via NLP pipeline
5. **Link** documents to STAC items and knowledge graph entities
6. **Publish** checksums and metadata to GitHub

---

## ğŸ§ª Validation Commands

**Manual Validation**

```bash
python src/utils/validate_sources.py data/sources/text/ --schema data/sources/schema/source.schema.json
```

**Make Targets**

```bash
make text-sources
make text-validate
```

**CI/CD Checks**

* Schema structure validation
* Endpoint and access check
* License and attribution validation
* Encoding consistency verification
* Changelog generation on manifest update

---

## ğŸ§© Provenance Integration

| Component                         | Function                                              |
| :-------------------------------- | :---------------------------------------------------- |
| `data/raw/text/`                  | Original OCR or transcript data                       |
| `data/processed/text/`            | Cleaned and NLP-enriched textual datasets             |
| `data/stac/collections/text.json` | STAC metadata linking back to manifests               |
| `data/checksums/text/`            | SHA-256 integrity verification for processed corpora  |
| `src/pipelines/text_pipeline.py`  | ETL process linking text sources to downstream assets |

---

## ğŸ§  MCP Compliance Summary

| MCP Principle           | Implementation                                                  |
| :---------------------- | :-------------------------------------------------------------- |
| **Documentation-first** | Each text dataset captured in a JSON manifest before ingestion. |
| **Reproducibility**     | ETL steps controlled via manifest-driven pipelines.             |
| **Open Standards**      | JSON Schema Â· UTF-8 Â· STAC 1.0 Â· NLP Metadata JSON.             |
| **Provenance**          | Traceable lineage: manifest â†’ processed â†’ knowledge graph.      |
| **Auditability**        | CI-validated manifests and checksum enforcement.                |

---

## ğŸ§¾ Changelog

| Version  | Date       | Summary                                                                   |
| :------- | :--------- | :------------------------------------------------------------------------ |
| **v1.1** | 2025-10-12 | Added workflow diagram, validation workflow, and LOC/KSHS/Yale manifests. |
| v1.0     | 2025-10-04 | Initial creation of text source manifest documentation.                   |

---

## ğŸ·ï¸ Version Block

```text
Component: data/sources/text/README.md
SemVer: 1.1.0
Spec Dependencies: MCP v1.0 Â· STAC 1.0
Last Updated: 2025-10-12
Maintainer: @bartytime4life
```

---

<div align="center">

**Kansas Frontier Matrix** â€” *â€œVoices of the past become data for the future.â€*
ğŸ“ [`data/sources/text/`](.) Â· Canonical registry of historical and linguistic sources powering KFMâ€™s narrative and document intelligence.

</div>


<div align="center">

# ğŸ“œ Kansas Frontier Matrix â€” Text Source Manifests

`data/sources/text/`

**Mission:** Curate, document, and validate all **external text-based data sources** that provide the historical and linguistic backbone for the Kansas Frontier Matrix (KFM).
These include digitized newspapers, OCR archives, oral histories, and treaty collections that together form the stateâ€™s narrative record.

[![Build & Deploy](https://github.com/bartytime4life/Kansas-Frontier-Matrix/actions/workflows/site.yml/badge.svg)](../../../.github/workflows/site.yml)
[![STAC Validate](https://github.com/bartytime4life/Kansas-Frontier-Matrix/actions/workflows/stac-validate.yml/badge.svg)](../../../.github/workflows/stac-validate.yml)
[![Schema Validate](https://img.shields.io/badge/JSON%20Schema-validated-success?logo=json)](../schema/source.schema.json)
[![Docs Â· MCP](https://img.shields.io/badge/Docs-MCP-blue)](../../../docs/)
[![License: Data](https://img.shields.io/badge/License-CC--BY%204.0-green)](../../../LICENSE)

</div>

---

## ğŸ“š Overview

`data/sources/text/` contains **JSON manifests** for every text source used in KFM.
Each manifest captures provenance, licensing, endpoints, and temporal coverage so that
narrative materialsâ€”newspapers, oral histories, treatiesâ€”can be ingested, parsed, and linked to the Knowledge Graph.

---

## ğŸ—‚ï¸ Directory Layout

```bash
data/sources/text/
â”œâ”€â”€ README.md
â”œâ”€â”€ loc_chronicling_america.json      # Library of Congress newspaper corpus
â”œâ”€â”€ kshs_oral_histories.json          # Kansas Historical Society transcripts
â””â”€â”€ yale_avalon_treaties.json         # Yale Avalon Project treaties & legal docs
```

---

## ğŸ—ï¸ Example Manifest

```json
{
  "id": "loc_chronicling_america",
  "title": "Library of Congress â€“ Chronicling America",
  "provider": "Library of Congress",
  "description": "OCR-processed historical newspaper pages (1789â€“1963).",
  "endpoint": "https://chroniclingamerica.loc.gov/",
  "access_method": "HTTP API",
  "license": "Public Domain (US Government)",
  "data_type": "text",
  "format": "JSONL",
  "spatial_coverage": "Kansas, USA",
  "temporal_coverage": "1854â€“1963",
  "update_frequency": "Monthly",
  "last_verified": "2025-10-12",
  "linked_pipeline": "text_pipeline.py",
  "notes": "Used for OCR cleanup, entity extraction, and temporal indexing."
}
```

---

## ğŸ§­ System Context (GitHub-safe Mermaid)

```mermaid
flowchart TD
  A["External Text Archives\nLOC Â· KSHS Â· Yale Avalon"] --> B["Source Manifests\n`data/sources/text/*.json`"]
  B --> C["ETL Pipeline\n`src/pipelines/text_pipeline.py`"]
  C --> D["Processed Text Corpora\n`data/processed/text/`"]
  D --> E["Derivatives\nTokenized Â· Parsed Â· Linked Text"]
  D --> F["STAC Collections\n`data/stac/collections/text.json`"]
  F --> G["Knowledge Graph\nPeople â†” Places â†” Events â†” Documents"]
%%END OF MERMAID%%
```

---

## âš™ï¸ ETL Integration

**Pipeline:** `src/pipelines/text_pipeline.py`
**Output:** `data/processed/text/`

### Steps

1. Validate manifests (`make sources-validate`)
2. Fetch text data via API or HTTP
3. Normalize encodings and metadata
4. Tokenize & extract named entities (NER)
5. Link results to STAC and Knowledge Graph
6. Publish checksums & provenance logs

---

## ğŸ§© Provenance Integration

| Component                         | Function                            |
| --------------------------------- | ----------------------------------- |
| `data/raw/text/`                  | Original OCR or transcript files    |
| `data/processed/text/`            | Cleaned & NLP-ready corpora         |
| `data/stac/collections/text.json` | STAC metadata links                 |
| `data/checksums/text/`            | SHA-256 verification                |
| `src/pipelines/text_pipeline.py`  | Orchestrates ingestion & enrichment |

---

## ğŸ§  MCP Compliance Summary

| MCP Principle           | Implementation                               |
| ----------------------- | -------------------------------------------- |
| **Documentation-first** | Every corpus defined by a JSON manifest.     |
| **Reproducibility**     | Deterministic ETL using manifest parameters. |
| **Open Standards**      | JSON Schema Â· UTF-8 Â· STAC 1.0.              |
| **Provenance**          | Manifest â†’ Processed â†’ STAC â†’ Graph.         |
| **Auditability**        | CI validation and checksum enforcement.      |

---

## ğŸ§¾ Changelog

| Version  | Date       | Summary                                                    |
| -------- | ---------- | ---------------------------------------------------------- |
| **v1.1** | 2025-10-12 | Added diagram, validation workflow, and manifest examples. |
| v1.0     | 2025-10-04 | Initial creation of text source documentation.             |

---

## ğŸ·ï¸ Version Block

```text
Component: data/sources/text/README.md
SemVer: 1.1.0
Spec Dependencies: MCP v1.0 Â· STAC 1.0
Last Updated: 2025-10-12
Maintainer: @bartytime4life
```

---

<div align="center">

**Kansas Frontier Matrix** â€” *â€œVoices of the past become data for the future.â€*
ğŸ“ [`data/sources/text/`](.) Â· Canonical registry of historical and linguistic sources powering KFMâ€™s narrative intelligence.

</div>
