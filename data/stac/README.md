<div align="center">


ğŸ“œ Kansas Frontier Matrix â€” Text Source Manifests

data/sources/text/

Mission: Curate, document, and validate all external text-based datasets â€” digitized newspapers, OCR archives, oral histories, and treaty transcripts â€” that form the linguistic and narrative foundation of the Kansas Frontier Matrix (KFM).

</div>



â¸»

ğŸ§­ Overview

The data/sources/text/ directory contains JSON manifests describing each text dataset integrated into KFM.
These manifests define provenance, licensing, access endpoints, and temporal coverage â€” forming a declarative registry for reproducible ingestion and validation.

Text sources empower:
	â€¢	ğŸ“° Historical newspaper OCR & cleanup
	â€¢	ğŸ—£ï¸ Oral history transcript alignment
	â€¢	ğŸ“œ Treaty & legal text curation
	â€¢	ğŸ§  NLP enrichment (NER, temporal tagging, entity linking)
	â€¢	ğŸ•°ï¸ Knowledge Graph integration & time-aware discovery

â¸»

ğŸ—‚ï¸ Directory Layout

data/sources/text/
â”œâ”€â”€ README.md
â”œâ”€â”€ loc_chronicling_america.json      # Library of Congress â€“ historical newspapers
â”œâ”€â”€ kshs_oral_histories.json          # Kansas Historical Society â€“ oral histories
â””â”€â”€ yale_avalon_treaties.json         # Yale Avalon Project â€“ treaties & legal texts

ğŸ§© Each manifest includes licensing, provenance, access details, and verification timestamps â€” ensuring long-term scholarly reproducibility.

â¸»

ğŸ—ï¸ Example Manifest

{
  "id": "loc_chronicling_america",
  "title": "Library of Congress â€” Chronicling America",
  "provider": "Library of Congress",
  "description": "Digitized, OCR-processed newspaper pages (1789â€“1963).",
  "endpoint": "https://chroniclingamerica.loc.gov/",
  "access_method": "HTTP API",
  "license": "Public Domain (US Government)",
  "data_type": "text",
  "format": "JSONL",
  "spatial_coverage": "Kansas, USA",
  "temporal_coverage": "1854â€“1963",
  "update_frequency": "Monthly",
  "last_verified": "2025-10-13",
  "linked_pipeline": "src/pipelines/text_pipeline.py",
  "notes": "Used for OCR cleanup, NER extraction, and timeline construction."
}


â¸»

ğŸ§® Schema Essentials

ğŸ”– Field	ğŸ“˜ Description
id	Unique short identifier (snake_case)
title	Full descriptive dataset name
provider	Institutional or organizational source
endpoint	Download/API URL or landing page
license	Explicit reuse rights (SPDX or text)
format	Primary data encoding (TXT, JSONL, HTML, CSV)
temporal_coverage	ISO or natural-language range
last_verified	YYYY-MM-DD provenance timestamp


â¸»

ğŸ§­ System Context (GitHub-Safe Mermaid)

flowchart TD
  A["External Text Archives\nLOC Â· KSHS Â· Yale Avalon"] --> B["Source Manifests\n`data/sources/text/*.json`"]
  B --> C["ETL Pipeline\n`src/pipelines/text_pipeline.py`"]
  C --> D["Processed Text Corpora\n`data/processed/text/`"]
  D --> E["Derivatives\nTokenized Â· NER Â· Linked Entities"]
  D --> F["STAC Collections\n`data/stac/collections/text.json`"]
  F --> G["Knowledge Graph\nPeople â†” Places â†” Events â†” Documents"]
  E --> H["Web UI\nSearch Â· Timeline Â· Document Viewer"]
%%END OF MERMAID%%

ğŸ’¡ Tip: Always terminate with %%END OF MERMAID%% to guarantee correct GitHub rendering.

â¸»

âš™ï¸ ETL Integration

Pipeline: src/pipelines/text_pipeline.py
Output: data/processed/text/

Workflow
	1.	ğŸ” Validate manifests (make sources-validate)
	2.	â¬‡ï¸ Ingest via API or HTTP
	3.	ğŸ§¹ Normalize (UTF-8, metadata, structure)
	4.	ğŸ§  Enrich with NLP (NER, temporal parsing)
	5.	ğŸª¢ Link to STAC + Knowledge Graph
	6.	âœ… Verify checksums & publish logs

â¸»

ğŸ§© Provenance Integration

Path	Purpose
data/raw/text/	Immutable source text files
data/processed/text/	Cleaned & NLP-ready corpora
data/stac/collections/text.json	STAC metadata registry
data/checksums/text/	SHA-256 verification
src/pipelines/text_pipeline.py	ETL orchestration layer


â¸»

ğŸ§  MCP Compliance Summary

MCP Principle	Implementation
Documentation-first	JSON manifest precedes ingestion
Reproducibility	Deterministic ETL parameters
Open Standards	JSON Schema Â· UTF-8 Â· STAC 1.0
Provenance	Manifest â†’ Processed â†’ STAC â†’ Graph
Auditability	CI-enforced schema + checksum validation


â¸»

ğŸ§¾ Text Source Summary

Manifest	Provider	Description	Coverage	Format	Verified
loc_chronicling_america.json	LOC	OCR newspaper corpus	Kansas	JSONL	âœ… 2025-10-13
kshs_oral_histories.json	KSHS	Oral history transcripts	Kansas	TXT	âœ… 2025-10-13
yale_avalon_treaties.json	Yale Avalon	Historical treaties & legal texts	US / Global	HTML/TXT	âœ… 2025-10-13


â¸»

ğŸ§ª Validation & CI Commands

python src/utils/validate_sources.py data/sources/text/ \
  --schema data/sources/schema/source.schema.json

make text-sources      # fetch & stage
make text-validate     # schema + endpoint check
make text-stac         # STAC build/validate
make text-checksums    # hash verification

CI Hooks
	â€¢	JSON Schema enforcement
	â€¢	Endpoint availability
	â€¢	License completeness
	â€¢	Encoding normalization
	â€¢	Changelog delta audit

â¸»

ğŸ§¾ Changelog

Version	Date	Highlights
v1.2	2025-10-13	Polished layout, added badges, semantic spacing, and CI hooks.
v1.1	2025-10-12	Added system diagram, validation workflow, and manifest examples.
v1.0	2025-10-04	Initial documentation release.


â¸»

ğŸ·ï¸ Version Block

Component: data/sources/text/README.md
SemVer: 1.2.0
Spec Dependencies: MCP v1.0 Â· STAC 1.0
Last Updated: 2025-10-13
Maintainer: @bartytime4life


â¸»


<div align="center">


âœ´ï¸ â€œVoices of the past become data for the future.â€

Kansas Frontier Matrix Â· Canonical registry of textual archives & narratives
ğŸ“ data/sources/text/

</div>