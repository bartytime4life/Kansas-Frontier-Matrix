<div align="center">


ğŸ“œ Kansas Frontier Matrix â€” Text Source Manifests

data/sources/text/

Mission: Curate, document, and validate all external text-based datasets â€” digitized newspapers, OCR archives, oral histories, and treaty transcripts â€” that form the linguistic and narrative foundation of the Kansas Frontier Matrix (KFM).

</div>



â¸»

ğŸ§­ Overview

The data/sources/text/ directory stores JSON manifests describing each external text dataset used within KFM.
Each manifest provides provenance, licensing, endpoints, and temporal coverage â€” forming a reproducible registry for ingestion and validation.

These datasets enable:
	â€¢	ğŸ“° Historical newspaper OCR & cleanup
	â€¢	ğŸ—£ï¸ Oral-history transcript alignment
	â€¢	ğŸ“œ Treaty & legal-text curation
	â€¢	ğŸ§  NLP enrichment (NER, temporal tagging, entity linking)
	â€¢	ğŸ•°ï¸ Knowledge-Graph integration & timeline discovery

â¸»

ğŸ—‚ï¸ Directory Layout

data/sources/text/
â”œâ”€â”€ README.md
â”œâ”€â”€ loc_chronicling_america.json      # Library of Congress â€“ newspapers
â”œâ”€â”€ kshs_oral_histories.json          # Kansas Historical Society â€“ transcripts
â””â”€â”€ yale_avalon_treaties.json         # Yale Avalon Project â€“ treaties & legal texts

ğŸ§© Each manifest includes licensing, provenance, access details, and verification timestamps to ensure archival reproducibility.

â¸»

ğŸ—ï¸ Example Manifest

{
  "id": "loc_chronicling_america",
  "title": "Library of Congress â€” Chronicling America",
  "provider": "Library of Congress",
  "description": "Digitized, OCR-processed newspaper pages (1789â€“1963).",
  "endpoint": "https://chroniclingamerica.loc.gov/",
  "access_method": "HTTP API",
  "license": "Public Domain (US Government)",
  "data_type": "text",
  "format": "JSONL",
  "spatial_coverage": "Kansas, USA",
  "temporal_coverage": "1854â€“1963",
  "update_frequency": "Monthly",
  "last_verified": "2025-10-13",
  "linked_pipeline": "src/pipelines/text_pipeline.py",
  "notes": "Used for OCR cleanup, NER extraction, and timeline construction."
}


â¸»

ğŸ§® Schema Essentials

Field	Description
id	Unique short identifier (snake_case)
title	Human-readable dataset name
provider	Source institution or organization
endpoint	Download/API URL
license	Explicit reuse rights
format	Data format (TXT, JSONL, HTML, CSV)
temporal_coverage	Date range (ISO 8601)
last_verified	Provenance timestamp (YYYY-MM-DD)


â¸»

ğŸ§­ System Context (GitHub-Safe Mermaid)

flowchart TD
  A["External Text Archives\nLOC Â· KSHS Â· Yale Avalon"] --> B["Source Manifests\n`data/sources/text/*.json`"]
  B --> C["ETL Pipeline\n`src/pipelines/text_pipeline.py`"]
  C --> D["Processed Text Corpora\n`data/processed/text/`"]
  D --> E["Derivatives\nTokenized Â· NER Â· Linked Entities"]
  D --> F["STAC Collections\n`data/stac/collections/text.json`"]
  F --> G["Knowledge Graph\nPeople â†” Places â†” Events â†” Documents"]
  E --> H["Web UI\nSearch Â· Timeline Â· Document Viewer"]
%%END OF MERMAID%%

ğŸ’¡ Always end with %%END OF MERMAID%% for consistent GitHub rendering.

â¸»

âš™ï¸ ETL Integration

Pipeline: src/pipelines/text_pipeline.py
Output: data/processed/text/

Workflow
	1.	ğŸ” Validate manifests (make sources-validate)
	2.	â¬‡ï¸ Ingest via API or HTTP
	3.	ğŸ§¹ Normalize encodings & metadata
	4.	ğŸ§  Enrich with NLP (NER, temporal parsing)
	5.	ğŸª¢ Link to STAC & Knowledge Graph
	6.	âœ… Verify checksums and publish provenance logs

â¸»

ğŸ§© Provenance Integration

Path	Purpose
data/raw/text/	Immutable source text files
data/processed/text/	Cleaned, NLP-ready corpora
data/stac/collections/text.json	STAC metadata registry
data/checksums/text/	SHA-256 integrity verification
src/pipelines/text_pipeline.py	ETL orchestration


â¸»

ğŸ§  MCP Compliance Summary

MCP Principle	Implementation
Documentation-first	JSON manifest precedes ingestion
Reproducibility	Deterministic ETL parameters
Open Standards	JSON Schema Â· UTF-8 Â· STAC 1.0
Provenance	Manifest â†’ Processed â†’ STAC â†’ Graph
Auditability	CI-enforced schema + checksums


â¸»

ğŸ§¾ Text Source Summary

Manifest	Provider	Description	Coverage	Format	Verified
loc_chronicling_america.json	LOC	OCR newspaper corpus	Kansas	JSONL	âœ… 2025-10-13
kshs_oral_histories.json	KSHS	Oral-history transcripts	Kansas	TXT	âœ… 2025-10-13
yale_avalon_treaties.json	Yale Avalon	Historical treaties & legal texts	U.S./Global	HTML/TXT	âœ… 2025-10-13


â¸»

ğŸ§ª Validation & CI Commands

python src/utils/validate_sources.py data/sources/text/ \
  --schema data/sources/schema/source.schema.json

make text-sources
make text-validate
make text-stac
make text-checksums

CI Hooks
	â€¢	JSON Schema enforcement
	â€¢	Endpoint availability
	â€¢	License completeness
	â€¢	Encoding normalization
	â€¢	Changelog delta audit

â¸»

ğŸ§¾ Changelog

Version	Date	Highlights
v1.2	2025-10-13	Reformatted layout, enlarged titles, consistent spacing, full badge header.
v1.1	2025-10-12	Added system diagram, validation workflow, manifest examples.
v1.0	2025-10-04	Initial documentation release.


â¸»

ğŸ·ï¸ Version Block

Component: data/sources/text/README.md
SemVer: 1.2.0
Spec Dependencies: MCP v1.0 Â· STAC 1.0
Last Updated: 2025-10-13
Maintainer: @bartytime4life


â¸»


<div align="center">


âœ´ï¸ â€œVoices of the past become data for the future.â€

Kansas Frontier Matrix â€” Canonical registry of textual archives & narratives
ğŸ“ data/sources/text/

</div>