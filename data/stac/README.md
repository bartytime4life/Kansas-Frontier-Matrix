<div align="center">


üìú Kansas Frontier Matrix ‚Äî Text Source Manifests

data/sources/text/

Mission: Curate, document, and validate all external text-based datasets‚Äîdigitized newspapers, OCR archives, oral histories, treaty transcripts‚Äîthat form the linguistic and narrative backbone of the Kansas Frontier Matrix (KFM).

</div>



‚∏ª

üìö Overview

data/sources/text/ contains JSON manifests that declare provenance, licensing, access endpoints, temporal coverage, and validation metadata for every text source integrated into KFM.
These manifests drive the ETL, enforce reproducibility, and link outputs to STAC and the Knowledge Graph.

They power:
	‚Ä¢	OCR cleanup + normalization (UTF-8, diacritics, Unicode NFC)
	‚Ä¢	NLP enrichment (tokenization, NER, coreference, date normalization)
	‚Ä¢	Treaty & land-cession document tracking
	‚Ä¢	Oral history alignment with people/places/events
	‚Ä¢	STAC-linked provenance for time-aware discovery

‚∏ª

üóÇÔ∏è Directory Layout

data/sources/text/
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ loc_chronicling_america.json      # Library of Congress historical newspapers
‚îú‚îÄ‚îÄ kshs_oral_histories.json          # Kansas Historical Society transcripts
‚îî‚îÄ‚îÄ yale_avalon_treaties.json         # Yale Avalon Project ‚Äî treaties & legal texts

Note: Every manifest includes dataset identifiers, licensing, provenance, verification timestamps, and expected formats‚Äîenabling deterministic ingestion and audit-ready lineage.

‚∏ª

üóûÔ∏è Example Manifest (authoritative pattern)

{
  "id": "loc_chronicling_america",
  "title": "Library of Congress ‚Äî Chronicling America Historical Newspaper Corpus",
  "provider": "Library of Congress (LOC)",
  "description": "Digitized and OCR-processed newspaper archives spanning 1789‚Äì1963.",
  "endpoint": "https://chroniclingamerica.loc.gov/",
  "access_method": "HTTP API",
  "license": "Public Domain (US Government)",
  "license_url": "https://www.loc.gov/legal/",
  "data_type": "text",
  "format": "JSONL",
  "spatial_coverage": "Kansas, USA",
  "temporal_coverage": "1854‚Äì1963",
  "update_frequency": "Monthly",
  "last_verified": "2025-10-12",
  "stac_collection": "data/stac/collections/text.json",
  "linked_pipeline": "src/pipelines/text_pipeline.py",
  "contact": "digital@loc.gov",
  "checksum_policy": "sha256 for downloaded bundles",
  "notes": "NLP: NER, sentence segmentation, and temporal tagging for timeline construction."
}

üîé Schema essentials (../schema/source.schema.json)
	‚Ä¢	Required: id, title, provider, endpoint, license, data_type, format, temporal_coverage, last_verified
	‚Ä¢	Recommended: spatial_coverage, update_frequency, stac_collection, linked_pipeline, checksum_policy, license_url, contact

‚∏ª

üß≠ System Context (GitHub-safe Mermaid)

flowchart TD
  A["External Text Archives\nLOC ¬∑ KSHS ¬∑ Yale Avalon"] --> B["Source Manifests\n`data/sources/text/*.json`"]
  B --> C["ETL Pipeline\n`src/pipelines/text_pipeline.py`"]
  C --> D["Processed Text Corpora\n`data/processed/text/`"]
  D --> E["Derivatives\nTokenized ¬∑ NER ¬∑ Linked Entities"]
  D --> F["STAC Collections\n`data/stac/collections/text.json`"]
  F --> G["Knowledge Graph\nPeople ‚Üî Places ‚Üî Events ‚Üî Documents"]
  E --> H["Web UI\nSearch ¬∑ Timeline ¬∑ Document Viewer"]
%%END OF MERMAID%%

Mermaid render-lock: The final line must be exactly %%END OF MERMAID%% on its own line (with the percent signs on both sides).

‚∏ª

‚öôÔ∏è ETL Integration

Pipeline: src/pipelines/text_pipeline.py
Output: data/processed/text/

Workflow
	1.	Validate manifests against schema ‚Üí make sources-validate
	2.	Ingest via HTTP/API, persist to data/raw/text/ (immutable)
	3.	Normalize encodings/line endings/metadata ‚Üí UTF-8 + NFC
	4.	Enrich with NLP (tokenize, NER, temporal parsing, entity linking)
	5.	Register STAC Items/Collections and Graph edges (doc‚Üíentity)
	6.	Verify checksums, write provenance logs, publish to CI artifacts

‚∏ª

üß™ Validation & CI

Manual validation

python src/utils/validate_sources.py data/sources/text/ \
  --schema data/sources/schema/source.schema.json

Make targets

make text-sources         # fetch + stage
make text-validate        # schema + endpoint checks
make text-stac            # build/validate STAC items
make text-checksums       # sha256 for processed corpora

CI gates (summary)
	‚Ä¢	JSON Schema validation (fail on required-missing/type mismatch)
	‚Ä¢	Endpoint liveness + HTTP 2xx/3xx checks
	‚Ä¢	License presence & attribution checks
	‚Ä¢	Encoding/normalization scan (UTF-8, NFC)
	‚Ä¢	Changelog enforcement on manifest edits

GitHub Actions (snippet)

name: text-sources-validate
on:
  pull_request:
    paths: [ "data/sources/text/**" ]
jobs:
  validate:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - run: pip install -r requirements.txt
      - run: python src/utils/validate_sources.py data/sources/text/ \
               --schema data/sources/schema/source.schema.json
      - run: make text-stac


‚∏ª

üß© Provenance Integration

Component	Purpose
data/raw/text/	Immutable source files (as-fetched)
data/processed/text/	Cleaned corpora ready for NLP/analysis
data/stac/collections/text.json	Discovery + asset metadata (time/space/provenance)
data/checksums/text/	SHA-256 manifests for reproducibility
src/pipelines/text_pipeline.py	Orchestrates ingestion ‚Üí enrich ‚Üí publish ‚Üí validate


‚∏ª

üß† AI / NLP Enrichment Profile (reference)
	‚Ä¢	Tokenization: sentence + word (Punkt or spaCy default)
	‚Ä¢	NER: person/org/place/treaty/legal refs; custom Gazetteer boost for Kansas entities
	‚Ä¢	Temporal: ISO-8601 normalization, fuzzy date ranges, document date vs. event date disambiguation
	‚Ä¢	Linking: entity IDs resolve to Knowledge Graph nodes; STAC relations mirror doc‚Üîasset ties
	‚Ä¢	Confidence: store per-span confidence; surface uncertainty in UI tooltips

‚∏ª

üß† MCP Compliance Checklist (pre-merge)
	‚Ä¢	Documentation before data/code (docs-first)
	‚Ä¢	Schema-valid JSON manifests present and passing CI
	‚Ä¢	Repro steps (make targets) verified locally
	‚Ä¢	STAC items/collections updated and validated
	‚Ä¢	Checksums written for processed corpora
	‚Ä¢	CHANGELOG updated; version bumped (semver)
	‚Ä¢	Mermaid ends with %%END OF MERMAID%%

‚∏ª

üßæ Text Source Summary

Manifest File	Provider	Description	Coverage	Format	Verified
loc_chronicling_america.json	LOC	OCR-based historical newspapers	Kansas	JSONL	‚úÖ 2025-10-12
kshs_oral_histories.json	KSHS	Transcribed oral histories and interviews	Kansas	TXT	‚úÖ 2025-10-12
yale_avalon_treaties.json	Yale Avalon	Treaty and historical legal documents	National/Global	HTML/TXT	‚úÖ 2025-10-12


‚∏ª

‚ùì FAQ (quick hits)
	‚Ä¢	Why JSON manifests? Deterministic, reviewable, CI-enforced ingestion configs.
	‚Ä¢	How do we handle broken endpoints? CI fails with actionable message; fall back to archived mirrors if specified.
	‚Ä¢	Mixed encodings? All inputs normalized to UTF-8/NFC; non-conforming inputs rejected with reason.

‚∏ª

üßæ Changelog

Version	Date	Summary
v1.2	2025-10-13	Added MCP checklist, AI/NLP profile, CI snippet, schema essentials; tightened provenance.
v1.1	2025-10-12	Added system diagram, validation workflow, and LOC/KSHS/Yale manifest examples.
v1.0	2025-10-04	Initial creation of text source manifest documentation.


‚∏ª

üè∑Ô∏è Version Block

Component: data/sources/text/README.md
SemVer: 1.2.0
Spec Dependencies: MCP v1.0 ¬∑ STAC 1.0
Last Updated: 2025-10-13
Maintainer: @bartytime4life


‚∏ª


<div align="center">


Kansas Frontier Matrix ‚Äî ‚ÄúVoices of the past become data for the future.‚Äù
üìç data/sources/text/ ¬∑ Canonical registry of historical & linguistic sources powering KFM‚Äôs narrative intelligence.

</div>
