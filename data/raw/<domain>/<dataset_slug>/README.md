---
title: "Raw Dataset Intake â€” README"
path: "data/raw/<domain>/<dataset_slug>/README.md"
version: "v1.0.0"
last_updated: "2026-01-12"
status: "draft"
doc_kind: "Guide"
license: "CC-BY-4.0"

markdown_protocol_version: "KFM-MDP v11.2.6"
mcp_version: "MCP-DL v6.3"
ontology_protocol_version: "KFM-ONTO v4.1.0"
pipeline_contract_version: "KFM-PPC v11.0.0"
stac_profile: "KFM-STAC v11.0.0"
dcat_profile: "KFM-DCAT v11.0.0"
prov_profile: "KFM-PROV v11.0.0"

governance_ref: "docs/governance/ROOT_GOVERNANCE.md"
review_gates_ref: "docs/governance/REVIEW_GATES.md"
ethics_ref: "docs/governance/ETHICS.md"
sovereignty_policy: "docs/governance/SOVEREIGNTY.md"
fair_category: "FAIR+CARE"
care_label: "TBD"
sensitivity: "mixed"
classification: "mixed"
jurisdiction: "US-KS"

doc_uuid: "urn:kfm:doc:data:raw:dataset:readme:v1.0.0"
semantic_document_id: "kfm-data-raw-dataset-readme-v1.0.0"
event_source_id: "ledger:kfm:doc:data:raw:dataset-readme:v1.0.0"
commit_sha: "<latest-commit-hash>"

ai_transform_permissions:
  - "summarize"
  - "structure_extract"
  - "translate"
  - "keyword_index"
ai_transform_prohibited:
  - "generate_policy"
  - "infer_sensitive_locations"

doc_integrity_checksum: "sha256:<calculate-and-fill>"
---

<a id="top"></a>

<div align="center">

# ğŸ“¥ Raw Dataset Intake â€” `<dataset_slug>` (`data/raw/<domain>/<dataset_slug>/`)

![stage](https://img.shields.io/badge/stage-raw-2563EB)
![domain](https://img.shields.io/badge/domain-<domain>-0EA5E9)
![dataset](https://img.shields.io/badge/dataset-<dataset_slug>-111827)
![policy](https://img.shields.io/badge/policy-append--only-16A34A)
![integrity](https://img.shields.io/badge/integrity-checksums%20%2B%20receipts-7C3AED)
![governance](https://img.shields.io/badge/governance-FAIR%20%2B%20CARE-8B5CF6)
![security](https://img.shields.io/badge/security-no%20secrets%20in%20git-DC2626)

**This folder is the immutable â€œasâ€‘receivedâ€ intake boundary for one upstream dataset.**  
It exists to make provenance boring: **receipts + checksums + appendâ€‘only drops** âœ…ğŸ§¾ğŸ”‘

</div>

> [!IMPORTANT]
> **If you changed bytes, itâ€™s not raw anymore.**  
> OCR, reprojection, resampling, cleanup, schema edits, tiling, â€œmake it Parquet/COGâ€ â†’ `data/work/` (intermediate) or `data/processed/` (publishable).

---

## âš¡ Quick links

- â¬†ï¸ Back to domain intake â†’ [`../README.md`](../README.md)
- â¬†ï¸ Back to raw root rules â†’ [`../../README.md`](../../README.md)
- ğŸ§¾ Source manifests (pointer-first receipts) â†’ [`../../../sources/`](../../../sources/) *(if present)*
- ğŸ§ª Workbench / ETL sandbox â†’ [`../../../work/`](../../../work/)
- ğŸ“¦ Processed outputs (publishable) â†’ [`../../../processed/`](../../../processed/)
- ğŸ›°ï¸ STAC (assets) â†’ [`../../../stac/`](../../../stac/)
- ğŸ—‚ï¸ DCAT (discovery) â†’ [`../../../catalog/dcat/`](../../../catalog/dcat/)
- ğŸ§¬ PROV (lineage) â†’ [`../../../prov/`](../../../prov/)
- ğŸ“‘ Reports (reviewable outputs) â†’ [`../../../reports/`](../../../reports/)
- ğŸ” Security policy â†’ [`../../../../SECURITY.md`](../../../../SECURITY.md) *(or `../../../../.github/SECURITY.md`)*

---

<details>
<summary><strong>ğŸ“Œ Table of contents</strong></summary>

- [ğŸ¯ What this dataset folder is](#-what-this-dataset-folder-is)
- [ğŸ§¾ Dataset identity card](#-dataset-identity-card)
- [ğŸ—‚ï¸ Folder layout (with emojis)](#ï¸-folder-layout-with-emojis)
- [ğŸ“¦ Drop registry (append-only)](#-drop-registry-append-only)
- [ğŸ§¾ Receipts & provenance (source.json)](#-receipts--provenance-sourcejson)
- [ğŸ”‘ Integrity (checksums)](#-integrity-checksums)
- [ğŸ§° Intake SOP (add a new drop)](#-intake-sop-add-a-new-drop)
- [ğŸ§­ Downstream mapping (processed â†’ STAC/DCAT/PROV)](#-downstream-mapping-processed--stacdcatprov)
- [ğŸ” Governance, classification, sovereignty](#-governance-classification-sovereignty)
- [âœ… QA & CI expectations](#-qa--ci-expectations)
- [ğŸ™ƒ Common pitfalls](#-common-pitfalls)
- [ğŸ“š Reference shelf](#-reference-shelf)

</details>

---

## ğŸ¯ What this dataset folder is

### âœ… This folder **IS**
- a **single-dataset intake boundary** (one upstream dataset â†’ many immutable drops)
- the home for **asâ€‘received bytes** and **receipts** that describe them
- the upstream anchor for later **PROV lineage** (raw â†’ work â†’ processed)

### âŒ This folder is **NOT**
- a workspace (`data/work/`)
- a publishing area (`data/processed/`)
- a catalog (`data/stac/`, `data/catalog/dcat/`)
- a place to â€œfixâ€ or â€œnormalizeâ€ inputs

> [!TIP]
> This folder should make it easy to answer in <10 seconds:
> **â€œWhere did these bytes come from, under what terms, and can we verify them?â€** ğŸ§¾ğŸ”

---

## ğŸ§¾ Dataset identity card

Fill this once and keep it current. This becomes the â€œfront of the binderâ€ for the dataset. ğŸ“˜

| Field | Value |
|---|---|
| **Domain** | `<domain>` |
| **Dataset slug** | `<dataset_slug>` |
| **Human title** | `<fill>` |
| **Upstream publisher** | `<fill>` |
| **Upstream landing page** | `<fill>` |
| **License / terms** | `<SPDX or URL or statement>` |
| **Redistribution allowed?** | `yes/no/partial` |
| **Default classification** | `public/internal/confidential/restricted` |
| **CARE label** | `TBD` |
| **Jurisdiction** | `US-KS` *(or more specific)* |
| **Update cadence** | `one-time | annual | monthly | daily | irregular` |
| **Primary formats** | `GeoTIFF / SHP / CSV / PDF / ...` |
| **Expected coverage** | `Kansas statewide | counties | AOI | ...` |
| **Source manifest ref** | `data/sources/<domain>/<dataset_slug>.json` *(if used)* |

> [!IMPORTANT]
> If the dataset is restricted or culturally sensitive:
> - keep receipts **minimal** (no precise sensitive coordinates)
> - store bytes in governed storage
> - link access rules instead of leaking detail here

---

## ğŸ—‚ï¸ Folder layout (with emojis)

```text
ğŸ“ data/
â””â”€â”€ ğŸ“ raw/
    â””â”€â”€ ğŸ“ <domain>/
        â””â”€â”€ ğŸ“ <dataset_slug>/                       ğŸ‘ˆ you are here
            â”œâ”€â”€ ğŸ“„ README.md                          ğŸ§¾ dataset identity + drop registry
            â”œâ”€â”€ ğŸ“„ drops_index.md                     â­ optional: human list of drops
            â”œâ”€â”€ ğŸ“„ drops_registry.csv                 ğŸ¤– optional: machine registry of drops
            â””â”€â”€ ğŸ“ <drop_id>/                         ğŸ§± immutable intake boundary (append-only)
                â”œâ”€â”€ ğŸ“„ README.md                      ğŸ§¾ drop summary (what/why/notes)
                â”œâ”€â”€ ğŸ“„ source.json                    ğŸ§¾ machine receipt (license/retrieval/classification)
                â”œâ”€â”€ ğŸ”‘ checksums.sha256               ğŸ”’ integrity (sha256)
                â”œâ”€â”€ ğŸ“ original/                      ğŸ“¦ as received (preferred)
                â”œâ”€â”€ ğŸ“ extracted/                     ğŸ§© lossless unpack only (optional)
                â”œâ”€â”€ ğŸ“ receipts/                      ğŸ§¾ request/response proofs (redacted; optional)
                â””â”€â”€ ğŸ“ notes/                         ğŸ“ human notes (NO secrets)
```

---

## ğŸ“¦ Drop registry (append-only)

This dataset is **append-only**. New pull/delivery = new `<drop_id>/` folder. Never overwrite.

### âœ… Recommended: maintain a registry

If this dataset will have multiple drops, keep a quick registry right here:

- `drops_index.md` â­ (human-friendly)
- `drops_registry.csv` ğŸ¤– (machine-friendly)

Suggested `drops_registry.csv` columns:
```csv
drop_id,retrieved_at_utc,upstream_version,license,classification,in_git,external_location,sha256_verified,notes
```

### ğŸ—“ï¸ Drop table (starter)

| Drop ID | Retrieved (UTC) | Upstream version | Bytes location | Integrity | Notes |
|---|---:|---|---|---|---|
| `<YYYY-MM-DD>` | `<fill>` | `<fill>` | `ğŸ“¦ in-git` / `ğŸ”’ external` | `âœ… verified` / `âš ï¸ pending` | `<fill>` |
| `<vX>` | `<fill>` | `<fill>` | `ğŸ“¦ in-git` / `ğŸ”’ external` | `âœ… verified` / `âš ï¸ pending` | `<fill>` |

> [!TIP]
> â€œRegistry driftâ€ is how teams lose track of whatâ€™s real. Keep it boring and current. âœ…

---

## ğŸ§¾ Receipts & provenance (`source.json`)

Each drop must include `source.json` as the **machine receipt** for provenance.

### What `source.json` must make obvious
- who the upstream publisher is
- what the license/terms are
- when/how retrieval occurred (and by whom)
- what classification/sensitivity applies
- where bytes live (in repo vs governed external storage)
- where request receipts are stored *(if applicable)*

> [!CAUTION]
> Never store secrets in receipts (headers, bearer tokens, cookies). Redact before commit. ğŸ”

<details>
<summary><strong>ğŸ§¾ Drop README template (copy/paste)</strong></summary>

```markdown
# ğŸ“¦ Raw Drop â€” <dataset_slug> / <drop_id>

## What this is
- **Upstream name:**  
- **Publisher:**  
- **Retrieved:**  
- **Files:** (whatâ€™s inside)

## Why we pulled it
- What question or pipeline needs this?

## Terms / license
- SPDX/URL + any redistribution constraints

## Integrity
- `checksums.sha256` generated: âœ…/âš ï¸
- `sha256sum -c` verified: âœ…/âš ï¸

## Notes / caveats
- Known schema quirks, missing fields, weird encoding, etc.

## Next steps
- Expected ETL work package:
  - `data/work/...`
- Expected processed destination:
  - `data/processed/<domain>/...`
```

</details>

---

## ğŸ”‘ Integrity (checksums)

Checksums are required for every drop.

### Generate (macOS/Linux)
```bash
# from inside the drop directory: .../<drop_id>/
find . -type f \
  ! -name 'checksums.sha256' \
  -print0 | sort -z | xargs -0 sha256sum > checksums.sha256
```

### Verify
```bash
sha256sum -c checksums.sha256
```

> [!NOTE]
> Checksums provide tamper-evidence and a clean way to detect â€œsilent driftâ€ across pulls. ğŸ”’

---

## ğŸ§° Intake SOP (add a new drop)

### 1) Create a new drop folder ğŸ§±
```text
data/raw/<domain>/<dataset_slug>/<drop_id>/
```
Examples:
- `2026-01-12/`
- `v3/`
- `run-20260112-153012Z/`

### 2) Place upstream bytes (or pointers) ğŸ“¥
- Preferred: put upstream delivery in `ğŸ“ original/`
- Optional: losslessly unpack into `ğŸ“ extracted/`
- If bytes cannot be stored in Git:
  - store in approved governed storage
  - record `external_location` in `source.json`
  - keep receipts in-repo

### 3) Write receipts ğŸ§¾
- `ğŸ“„ README.md` (human)
- `ğŸ“„ source.json` (machine)

### 4) Generate + verify checksums ğŸ”‘
- create `ğŸ”‘ checksums.sha256`
- run verification locally

### 5) Update dataset-level registry ğŸ“Œ
- update the Drop table above
- if you use `drops_registry.csv`, update it too

### 6) Open a PR âœ…
Include:
- why the new drop exists
- license/classification notes
- how to reproduce retrieval (if applicable)

---

## ğŸ§­ Downstream mapping (processed â†’ STAC/DCAT/PROV)

Raw is the start; â€œpublishedâ€ begins only after boundary artifacts exist.

### Canonical order
```mermaid
flowchart LR
  RAW["Raw drop\n(data/raw/<domain>/<dataset_slug>/<drop_id>/)"] --> WORK["ETL / work\n(data/work/...)"]
  WORK --> PROC["Processed\n(data/processed/<domain>/...)"]
  PROC --> BOUND["Boundary artifacts\nSTAC + DCAT + PROV"]
```

### Promotion intent (fill when known)

| Target artifact | Expected ID / path |
|---|---|
| ğŸ“¦ Processed dataset root | `data/processed/<domain>/<product_slug>/...` |
| ğŸ›°ï¸ STAC Collection ID | `<fill>` |
| ğŸ›°ï¸ STAC Item pattern | `<collection>__<time_or_tile>...` |
| ğŸ—‚ï¸ DCAT dataset record | `data/catalog/dcat/<dataset_id>.jsonld` |
| ğŸ§¬ PROV bundle pattern | `data/prov/<run_id>/prov.jsonld` |
| ğŸ“‘ Report bundle (if needed) | `data/reports/<domain>/<date>__<slug>__vX/` |

> [!TIP]
> When this dataset becomes decision-significant, add a short link to its **first** processed release + STAC/DCAT/PROV IDs here. Thatâ€™s how we keep provenance navigable. ğŸ§­

---

## ğŸ” Governance, classification, sovereignty

### Minimum rule set
- classification is declared **at ingest time** in each dropâ€™s `source.json`
- no classification downgrade across the pipeline
- sensitive locations: do not leak precise coordinates in public receipts
- culturally sensitive sources: follow sovereignty policy and review gates

> [!IMPORTANT]
> If thereâ€™s any chance this dataset touches protected resources, communities, or person-adjacent inference:
> escalate early, keep receipts minimal, and enforce access via the API boundary. ğŸ›¡ï¸

---

## âœ… QA & CI expectations

**For PRs touching this dataset folder:**
- [ ] append-only (no editing old drops)
- [ ] each new drop contains `README.md`, `source.json`, `checksums.sha256`
- [ ] checksums verify
- [ ] license + classification are present
- [ ] secrets scan passes (no tokens in receipts)

**Recommended additional gates (fast, high value):**
- [ ] validate `source.json` against a schema (if you add one under `schemas/sources/`)
- [ ] naming lint (`dataset_slug`, `drop_id`)
- [ ] restricted redistribution respected (receipt-only pattern)

---

## ğŸ™ƒ Common pitfalls

- â€œI fixed something in the raw dropâ€ â†’ âŒ create a new drop; fix in `data/work/`
- â€œI only kept the extracted filesâ€ â†’ âŒ also keep the original archive in `original/`
- â€œI stored a token in receiptsâ€ â†’ âŒ redact + rotate + follow `SECURITY.md`
- â€œI canâ€™t remember where this came fromâ€ â†’ âŒ update receipts immediately; provenance is not optional

---

## ğŸ“š Reference shelf

<details>
<summary><strong>ğŸ“– Project library (influence map)</strong></summary>

> âš ï¸ These references inform rigor (provenance, validation, governance) and may have licenses different from repo code/data.

### ğŸ§­ Core KFM system + governance framing
- `Kansas Frontier Matrix (KFM) â€“ Comprehensive Technical Documentation.docx`
- `ğŸŒŸ Kansas Frontier Matrix â€“ Latest Ideas & Future Proposals.docx`

### ğŸ—ºï¸ GIS + mapping + file formats
- `python-geospatial-analysis-cookbook.pdf`
- `PostgreSQL Notes for Professionals - PostgreSQLNotesForProfessionals.pdf`
- `making-maps-a-visual-guide-to-map-design-for-gis.pdf`
- `Mobile Mapping_ Space, Cartography and the Digital - 9789048535217.pdf`
- `compressed-image-file-formats-jpeg-png-gif-xbm-bmp.pdf`

### ğŸ›°ï¸ Remote sensing
- `Cloud-Based Remote Sensing with Google Earth Engine-Fundamentals and Applications.pdf`

### ğŸ“ˆ Statistical integrity + experiments
- `Understanding Statistics & Experimental Design.pdf`
- `regression-analysis-with-python.pdf`
- `Regression analysis using Python - slides-linear-regression.pdf`
- `graphical-data-analysis-with-r.pdf`
- `think-bayes-bayesian-statistics-in-python.pdf`

### ğŸ§ª Simulation & modeling discipline
- `Scientific Modeling and Simulation_ A Comprehensive NASA-Grade Guide.pdf`

### âš™ï¸ Systems, scaling, interoperability
- `Scalable Data Management for Future Hardware.pdf`
- `Data Spaces.pdf`
- `concurrent-real-time-and-distributed-programming-in-java-threads-rtsj-and-rmi.pdf`

### ğŸŒ Web + visualization (downstream consumers)
- `responsive-web-design-with-html5-and-css3.pdf`
- `webgl-programming-guide-interactive-3d-graphics-programming-with-webgl.pdf`

### â¤ï¸ Ethics + autonomy + AI law
- `Introduction to Digital Humanism.pdf`
- `Principles of Biological Autonomy - book_9780262381833.pdf`
- `On the path to AI Lawâ€™s prophecies and the conceptual foundations of the machine learning age.pdf`

### ğŸ›¡ï¸ Security (defensive mindset only)
- `ethical-hacking-and-countermeasures-secure-network-infrastructures.pdf`
- `Gray Hat Python - Python Programming for Hackers and Reverse Engineers (2009).pdf`

### ğŸ§° General programming shelf (bundles)
- `A programming Books.pdf`
- `B-C programming Books.pdf`
- `D-E programming Books.pdf`
- `F-H programming Books.pdf`
- `I-L programming Books.pdf`
- `M-N programming Books.pdf`
- `O-R programming Books.pdf`
- `S-T programming Books.pdf`
- `U-X programming Books.pdf`
- `Deep Learning for Coders with fastai and PyTorch - Deep.Learning.for.Coders.with.fastai.and.PyTorchpdf` *(filename as provided)*

</details>

---

<p align="right"><a href="#top">â¬†ï¸ Back to top</a></p>

