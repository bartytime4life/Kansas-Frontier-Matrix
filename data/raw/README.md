---
title: "Raw Data â€” README"
path: "data/raw/README.md"
version: "v1.1.0"
last_updated: "2026-01-12"
status: "draft"
doc_kind: "Guide"
license: "CC-BY-4.0"

markdown_protocol_version: "KFM-MDP v11.2.6"
mcp_version: "MCP-DL v6.3"
ontology_protocol_version: "KFM-ONTO v4.1.0"
pipeline_contract_version: "KFM-PPC v11.0.0"
stac_profile: "KFM-STAC v11.0.0"
dcat_profile: "KFM-DCAT v11.0.0"
prov_profile: "KFM-PROV v11.0.0"

governance_ref: "docs/governance/ROOT_GOVERNANCE.md"
review_gates_ref: "docs/governance/REVIEW_GATES.md"
ethics_ref: "docs/governance/ETHICS.md"
sovereignty_policy: "docs/governance/SOVEREIGNTY.md"
fair_category: "FAIR+CARE"
care_label: "TBD"
sensitivity: "public"
classification: "open"
jurisdiction: "US-KS"

doc_uuid: "urn:kfm:doc:data:raw:readme:v1.1.0"
semantic_document_id: "kfm-data-raw-readme-v1.1.0"
event_source_id: "ledger:kfm:doc:data:raw:readme:v1.1.0"
commit_sha: "<latest-commit-hash>"

ai_transform_permissions:
  - "summarize"
  - "structure_extract"
  - "translate"
  - "keyword_index"
ai_transform_prohibited:
  - "generate_policy"
  - "infer_sensitive_locations"

doc_integrity_checksum: "sha256:<calculate-and-fill>"
---

<div align="center">

# ğŸ“¥ `data/raw/` â€” Raw Data (Immutable Inputs)

![stage](https://img.shields.io/badge/data%20stage-raw-2563EB)
![policy](https://img.shields.io/badge/policy-append--only-16A34A)
![integrity](https://img.shields.io/badge/integrity-checksums%20%2B%20receipts-7C3AED)
![provenance](https://img.shields.io/badge/provenance-source.json%20%2B%20PROV-0EA5E9)
![governance](https://img.shields.io/badge/governance-FAIR%20%2B%20CARE-8B5CF6)
![security](https://img.shields.io/badge/security-no%20secrets%20in%20git-DC2626)

**Raw data is KFMâ€™s first trust boundary.**  
We ingest external sources here **asâ€‘received**, preserve them **immutably**, then perform deterministic ETL in `data/work/` and publish stable products in `data/processed/`. ğŸ§¾â¡ï¸ğŸ› ï¸â¡ï¸ğŸ“¦

</div>

> [!IMPORTANT]
> **If you changed bytes, itâ€™s not raw anymore.**  
> Reprojection, cleanup, OCR, tiling, resampling, column edits, format conversion â†’ belongs in `data/work/` (intermediate) or `data/processed/` (publishable).

---

## ğŸ”— Quick links

- ğŸ§­ Repo overview â†’ `../../README.md`
- ğŸ§¾ Source manifests (pointer-first intake) â†’ [`../sources/`](../sources/) *(if present)*
- ğŸ§ª Intermediate artifacts â†’ [`../work/`](../work/)
- ğŸ“¦ Final products â†’ [`../processed/`](../processed/)
- ğŸ§ª Catalogs & lineage:
  - ğŸ›°ï¸ STAC â†’ [`../stac/`](../stac/)
  - ğŸ—‚ï¸ DCAT â†’ [`../catalog/`](../catalog/) Â· [`../catalog/dcat/`](../catalog/dcat/)
  - ğŸ§¬ PROV â†’ [`../prov/`](../prov/)
- ğŸ§ª QA runbooks & validators â†’ [`../../tools/validation/`](../../tools/validation/) *(recommended home)*
- ğŸ§° Catalog QA tool (metadata gate) â†’ `../../tools/validation/catalog_qa/` *(if present)*
- ğŸ›¡ï¸ Vulnerability reporting â†’ `../../SECURITY.md` *(or `../../.github/SECURITY.md`)*
- âš–ï¸ Governance & review gates â†’ `../../docs/governance/ROOT_GOVERNANCE.md` Â· `../../docs/governance/REVIEW_GATES.md` *(if present)*

---

<details>
<summary><strong>ğŸ“Œ Table of contents</strong></summary>

- [ğŸ§­ Where raw fits in the KFM pipeline](#pipeline)
- [ğŸ§¾ Source manifests (`data/sources/`) â€” when bytes canâ€™t live in Git](#sources)
- [âœ… What belongs here](#allowed)
- [ğŸš« What does NOT belong here](#not-allowed)
- [â­ Raw-stage non-negotiables](#non-negotiables)
- [ğŸ—‚ï¸ Directory layout](#layout)
- [ğŸ§¾ Raw drop contract](#drop-contract)
- [ğŸ“„ `source.json` template](#source-json)
- [ğŸ”‘ Checksums](#checksums)
- [ğŸ“¦ Large files & restricted redistribution](#large-files)
- [ğŸ—ºï¸ Geospatial + document specifics](#geo-specifics)
- [ğŸ” Security, privacy, sovereignty](#security)
- [ğŸ§ª QA & CI expectations](#qa)
- [ğŸ§° Intake SOP: add a new raw drop](#sop)
- [ğŸ™ƒ Common anti-patterns](#anti-patterns)
- [ğŸ“š Project reference shelf](#reference-shelf)

</details>

---

<a id="pipeline"></a>

## ğŸ§­ Where raw fits in the KFM pipeline

**Canonical ordering (nonâ€‘negotiable):**  
**Raw â†’ Work/ETL â†’ Processed â†’ STAC/DCAT/PROV â†’ Graph â†’ API â†’ UI â†’ Story Nodes â†’ Focus Mode**

```mermaid
flowchart LR
  SOURCES[ğŸ§¾ Source manifests<br/>data/sources/] --> RAW
  RAW[ğŸ“¥ Raw inputs<br/>data/raw/] --> WORK[ğŸ§ª Work / ETL<br/>data/work/]
  WORK --> PROC[ğŸ“¦ Processed outputs<br/>data/processed/]
  PROC --> STAC[ğŸ›°ï¸ STAC catalogs<br/>data/stac/]
  PROC --> DCAT[ğŸ—‚ï¸ DCAT datasets<br/>data/catalog/dcat/]
  PROC --> PROV[ğŸ§¬ PROV lineage<br/>data/prov/]
  STAC --> GRAPH[ğŸ•¸ï¸ Graph (Neo4j)]
  DCAT --> GRAPH
  PROV --> GRAPH
  GRAPH --> API[ğŸ”Œ Governed API]
  API --> UI[ğŸ—ºï¸ Web UI]
  UI --> STORY[ğŸ¬ Story Nodes]
  STORY --> FOCUS[ğŸ§  Focus Mode]
```

> [!NOTE]
> Raw is not â€œless important.â€ Itâ€™s the foundation for **reproducibility**, **auditability**, and **tamper-evidence** across catalogs, models, and narratives.

---

<a id="sources"></a>

## ğŸ§¾ Source manifests (`data/sources/`) â€” when bytes canâ€™t live in Git

KFM supports a **pointer-first intake** pattern for cases where the raw bytes are:
- too large for Git,
- restricted for redistribution,
- stored in an external object store / partner system.

In that case:
- `data/sources/` holds **machine-readable manifests** (URLs, licensing, retrieval method, access constraints, expected extents).
- `data/raw/` holds either:
  - the actual **as-received bytes** *(preferred when possible)*, **or**
  - a **receipt-only drop** (README + `source.json` + checksums for the receipts/pointers) describing where the bytes live and how to retrieve them.

> [!TIP]
> Treat `data/sources/` as the â€œcatalog of inputsâ€ and `data/raw/` as the â€œevidence snapshots / receiptsâ€ that pipelines can anchor to.

---

<a id="allowed"></a>

## âœ… What belongs here

**Allowed raw inputs (as-received):**
- ğŸ“¦ Vendor/agency deliveries (ZIP/TAR bundles, exports, archives)
- ğŸ—ºï¸ Original GIS deliveries (GeoTIFF, SHP, GPKG, CSV, JSON, KML, etc.)
- ğŸ§¾ Documents & scans (PDFs, TIFF/JPEG/PNG masters)
- ğŸ›°ï¸ Remote sensing exports / pulls where you can persist **the exported files** and/or **the original response payload**
- ğŸ§ª Sensor dumps / logs (when permitted) â€” stored â€œas recordedâ€

**Also allowed (with strict rules):**
- ğŸ“ **Lossless extraction** into `extracted/` **only if** you also keep the original archive in `original/`
  - unzip/untar is allowed; *editing content is not*
- ğŸ§¾ Retrieval receipts (headers, request params, query JSON) stored as sidecars in the drop (no secrets)

---

<a id="not-allowed"></a>

## ğŸš« What does NOT belong here

**Not allowed in `data/raw/`:**
- ğŸ§¼ Cleaned tables, renamed columns, changed encodings
- ğŸ§­ Reprojection, resampling, tiling, simplification, topology repair
- ğŸ§Š â€œMake it a COGâ€, â€œmake it Parquetâ€, â€œmake it GeoJSONâ€
- ğŸ§  Analysis outputs / model outputs / simulation outputs / reports  
  â†’ these are first-class artifacts in `data/processed/` (and/or `data/reports/`) and must ship with **STAC/DCAT/PROV** if they are used downstream

> [!WARNING]
> If the only explanation for a file is â€œtrust me,â€ it will fail review (and often CI).

---

<a id="non-negotiables"></a>

## â­ Raw-stage non-negotiables

These rules keep the pipeline deterministic and governance-safe:

- ğŸ§± **Append-only**: never mutate an existing drop; new pull â†’ new folder
- ğŸ§Š **Bytes preserved**: keep originals + sidecars; donâ€™t â€œhelpfully convertâ€
- ğŸ§¾ **Receipts required**: every drop has `README.md`, `source.json`, `checksums.sha256`
- ğŸ·ï¸ **Stable identity**: `dataset_id` + `drop_id` become PROV keys later
- ğŸ›¡ï¸ **Governance up front**: license, classification, sensitivity declared at ingest time
- ğŸ” **No secrets in Git**: use `.env` + secret stores; rotate if exposed
- ğŸš« **No classification downgrade**: outputs must not become less restricted than inputs

---

<a id="layout"></a>

## ğŸ—‚ï¸ Directory layout

Organize raw data by **domain â†’ dataset â†’ immutable drop**:

```text
ğŸ“ data/
â””â”€â”€ ğŸ“ raw/
    â””â”€â”€ ğŸ“ <domain>/                         # imagery, hydro, census, docs, etc.
        â””â”€â”€ ğŸ“ <dataset_slug>/               # kebab-case, stable (no dates inside)
            â””â”€â”€ ğŸ“ <drop_id>/                # YYYY-MM-DD | vX | YYYY-MM-DDa | run-YYYYMMDD-HHMMSSZ
                â”œâ”€â”€ ğŸ“„ README.md
                â”œâ”€â”€ ğŸ“„ source.json
                â”œâ”€â”€ ğŸ”‘ checksums.sha256
                â”œâ”€â”€ ğŸ“ original/             # upstream bundle(s) exactly as received
                â”œâ”€â”€ ğŸ“ extracted/            # optional: lossless unpack output (no transforms)
                â”œâ”€â”€ ğŸ“ receipts/             # optional: request params, headers, landing-page HTML (NO secrets)
                â””â”€â”€ ğŸ“ notes/                # optional: human notes (NO secrets)
```

### ğŸ·ï¸ Naming guidance

- `<domain>`: broad, stable bucket (donâ€™t overfit)
- `<dataset_slug>`: stable handle (`kebab-case`)
- `<drop_id>`:
  - `YYYY-MM-DD` for dated pulls/deliveries
  - `vX` for upstream versioned releases
  - if re-pulling â€œthe sameâ€ drop: `YYYY-MM-DDa`, `YYYY-MM-DDb` (never overwrite)
  - for pipeline-fetched drops: `run-YYYYMMDD-HHMMSSZ` *(guarantees uniqueness)*

> [!TIP]
> â€œBoring namingâ€ is a feature: it makes automation, QA, and provenance simpler.

---

<a id="drop-contract"></a>

## ğŸ§¾ Raw drop contract

Every raw drop is a **reviewable, machine-validatable boundary**.

| Artifact | Required | Why it exists | Minimum contents |
|---|---:|---|---|
| ğŸ“„ `README.md` | âœ… | Human context | what it is, where it came from, whatâ€™s inside, caveats |
| ğŸ“„ `source.json` | âœ… | Machine provenance | source URLs, license, retrieval time/method, classification, extents |
| ğŸ”‘ `checksums.sha256` | âœ… | Integrity + tamper evidence | sha256 of all files in the drop (except itself) |
| ğŸ“ `original/` | â—»ï¸ | â€œAs receivedâ€ archive(s) | ZIP/TAR/PDF bundles, vendor deliveries |
| ğŸ“ `extracted/` | â—»ï¸ | Lossless unpack only | unzip/untar output (no semantic changes) |
| ğŸ“ `receipts/` | â—»ï¸ | Deterministic retrieval proof | request JSON, query params, response headers (redacted) |

> [!CAUTION]
> If redistribution is restricted: keep **only receipts** (README + `source.json` + checksums) in Git, store bytes in restricted storage, and document access clearly.

---

<a id="source-json"></a>

## ğŸ“„ `source.json` template

`source.json` is the raw-stage **receipt** ğŸ§¾ â€” it should let a reviewer (or future-you) re-acquire and re-verify the same inputs.

```json
{
  "receipt_version": "v1",

  "dataset_id": "<domain>/<dataset_slug>",
  "domain": "<domain>",
  "dataset_slug": "<dataset_slug>",
  "drop_id": "<YYYY-MM-DD_or_vX_or_run-*>",
  "source_manifest_ref": "data/sources/<domain>/<dataset_slug>.json",

  "title": "Human-friendly dataset name",
  "description": "What this drop contains (1â€“3 sentences).",

  "upstream": {
    "publisher": "Agency / org / vendor",
    "source_urls": ["https://â€¦"],
    "retrieved_from": "https://â€¦",
    "license": "SPDX id or URL or text statement",
    "citation": "Preferred citation string (if provided)",
    "terms_notes": "Redistribution limits / constraints.",
    "attribution_required": true
  },

  "retrieval": {
    "retrieved_at": "YYYY-MM-DDTHH:MM:SSZ",
    "method": "manual|script|api|mirror",
    "performed_by": "name_or_handle",
    "tooling": {
      "script_path": "tools/fetch/<something>.sh",
      "container": "docker image tag (if used)",
      "commit": "git commit hash (if applicable)"
    },
    "request_receipts": [
      "receipts/request.json",
      "receipts/response_headers.txt"
    ]
  },

  "coverage": {
    "spatial": {
      "crs": "EPSG:4326 | unknown",
      "bbox_wgs84": [-102.05, 36.99, -94.59, 40.00]
    },
    "temporal": {
      "start": "YYYY-MM-DD",
      "end": "YYYY-MM-DD"
    }
  },

  "sensitivity": {
    "classification": "public|internal|confidential|restricted",
    "care_label": "TBD",
    "notes": "Sovereignty, sensitive sites, PII risk, redaction expectations."
  },

  "storage": {
    "in_git": true,
    "dvc_tracked": false,
    "external_location": "s3://â€¦ or gs://â€¦ or partner system ref (if bytes not in repo)"
  },

  "files": [
    {
      "path": "original/source_bundle.zip",
      "media_type": "application/zip",
      "size_bytes": 0,
      "sha256": "<optional duplicate of checksums.sha256>"
    }
  ]
}
```

> [!TIP]
> Keep `README.md` **human**, keep `source.json` **machine**. Donâ€™t hide licensing or sensitivity only in prose.

---

<a id="checksums"></a>

## ğŸ”‘ Checksums

### Generate (macOS/Linux)

```bash
# from inside the drop directory: .../<drop_id>/
find . -type f \
  ! -name 'checksums.sha256' \
  -print0 | sort -z | xargs -0 sha256sum > checksums.sha256
```

> [!NOTE]
> macOS may not ship `sha256sum` by default. If needed:
> - use `shasum -a 256` (different output format), or
> - install coreutils and use `gsha256sum`.

### Verify (macOS/Linux)

```bash
sha256sum -c checksums.sha256
```

### Windows (PowerShell)

```powershell
Get-ChildItem -Recurse -File |
  Where-Object { $_.Name -ne "checksums.sha256" } |
  ForEach-Object {
    $h = (Get-FileHash $_.FullName -Algorithm SHA256).Hash.ToLower()
    "$h  $($_.FullName.Replace((Get-Location).Path + '\','').Replace('\','/'))"
  } | Set-Content checksums.sha256
```

> [!NOTE]
> Checksums are practical tamper-evidence **and** a fast way to debug data drift.

---

<a id="large-files"></a>

## ğŸ“¦ Large files & restricted redistribution

Raw often includes huge rasters and long time-series.

### Recommended patterns

- ğŸ§³ **Small/medium files**: store directly in Git (still include checksums)
- ğŸ§± **Large binaries**: use DVC or Git LFS (repo policy-dependent) with receipts in `data/raw/`
- ğŸ”’ **Redistribution restricted**: keep only receipts in Git; store bytes in restricted storage; reference via `data/sources/` manifests

> [!IMPORTANT]
> The **drop folder** remains the contract boundary even if bytes live elsewhere.

---

<a id="geo-specifics"></a>

## ğŸ—ºï¸ Geospatial + document specifics

### ğŸ›°ï¸ Raster deliveries (GeoTIFF/IMG/etc.)
âœ… Raw: keep â€œas delivered,â€ including `.aux.xml`, `.tfw`, metadata sidecars  
âŒ Not raw: COG conversion, resampling, overviews, tiling (do this in `data/work/`)

### ğŸ§­ Vector deliveries (SHP/GPKG/GeoJSON/CSV)
âœ… Raw: keep as delivered, preserve encoding + schema  
âŒ Not raw: reprojection, geometry fixes, attribute normalization

> [!TIP]
> For Shapefiles: make sure you keep the *whole set* (`.shp`, `.shx`, `.dbf`, `.prj`, and any `.cpg`, `.sbn/.sbx`, etc.). Checksums should cover all of them.

### ğŸ§¾ Documents & scans (PDF/JPEG/PNG/TIFF)
âœ… Raw: keep original masters (donâ€™t OCR in place)  
âŒ Not raw: OCR text outputs, rotated/cleaned images, compressed previews  
â¡ï¸ Put OCR + derivatives in `data/work/` (publish in `data/processed/` if they ship)

### ğŸ§Š 3D / graphics assets (glTF / 3D Tiles / meshes)
Treat as **untrusted inputs**:
- store raw assets unchanged
- validate parsers and conversion steps in `data/work/`
- never execute embedded scripts/macros; strip or sandbox during ETL

### ğŸ›°ï¸ API pulls (remote sensing, web services)
If you pull via API:
- store the **raw payload** (or exported files) if possible
- store the **exact request parameters** (query, filters, time window) in `receipts/`
- store the script path + commit hash in `source.json`
- redact headers that could contain tokens

> [!TIP]
> â€œReproducible retrievalâ€ is part of provenance. If a pull canâ€™t be repeated, document why (rate limits, paid access, ephemeral tokens, etc.).

---

<a id="security"></a>

## ğŸ” Security, privacy, sovereignty

Geospatial raw data can carry real-world risk.

### Hard rules
- ğŸ” **No secrets in Git**: tokens/keys go in `.env` + secret stores
- ğŸ§ **No PII in public repos** unless explicitly governed and approved
- ğŸ§­ **No restricted coordinates** in public drops when locations are sensitive
- ğŸ·ï¸ **Declare classification** in `source.json` (and donâ€™t â€œdowngradeâ€ later)

### Review gates & policy enforcement (recommended)
If your repo includes review gates/policy packs:
- follow `docs/governance/REVIEW_GATES.md` *(if present)*
- ensure policy checks run in CI (secrets scan, sensitive-location linting, classification consistency)

> [!WARNING]
> â€œProcessed outputs can still leak.â€ Even aggregated or derived data can reveal sensitive patterns. Raw discipline is the first step; API governance is the last.

---

<a id="qa"></a>

## ğŸ§ª QA & CI expectations

Raw changes should be easy to validate automatically.

**Minimum checks for PRs touching `data/raw/**`:**
- [ ] Drop is append-only (no edits to existing drops)
- [ ] `README.md`, `source.json`, `checksums.sha256` exist
- [ ] `checksums.sha256` verifies locally
- [ ] `source.json` includes license + classification
- [ ] No secrets/credentials committed (scan)
- [ ] Sensitive data is flagged and handled per governance

**Recommended additional CI gates (fast, high-value):**
- [ ] â€œNo classification downgradeâ€ checks across raw â†’ processed â†’ catalogs
- [ ] Sensitive-location safeguards (no precise restricted coordinates in public metadata)
- [ ] Link/receipt linting: `source_manifest_ref` resolves *(if used)*
- [ ] If the PR also updates STAC/DCAT: run `tools/validation/catalog_qa/` *(if present)*

> [!NOTE]
> Deeper geospatial QA (CRS checks, geometry validity, bounds) usually happens in `data/work/` and `data/processed/`â€”but raw must still declare what it *claims* to be.

---

<a id="sop"></a>

## ğŸ§° Intake SOP: add a new raw drop

### 1) Create the drop boundary ğŸ§±
- choose `<domain>/<dataset_slug>/<drop_id>/`
- never overwrite an existing drop

### 2) Acquire upstream bytes ğŸ“¥
- place the upstream bundle in `original/`
- optional: losslessly extract into `extracted/`

> If bytes canâ€™t be stored in Git:
> - store them in approved restricted storage
> - include receipts and pointers in `source.json`
> - (recommended) add/update a `data/sources/` manifest

### 3) Write the receipts ğŸ§¾
- `README.md` (human: what/where/why/caveats)
- `source.json` (machine: license, retrieval, classification, extents, pointers)

### 4) Lock integrity ğŸ”’
- generate `checksums.sha256`
- verify it locally

### 5) Open a PR âœ…
Include:
- what changed (new dataset vs new drop)
- any licensing/sensitivity concerns
- how to reproduce retrieval (if applicable)

---

<a id="anti-patterns"></a>

## ğŸ™ƒ Common anti-patterns

- â€œI fixed the CSV in placeâ€ â†’ new drop; do cleanup in `data/work/`
- â€œI reprojected it so it lines upâ€ â†’ `data/work/` / `data/processed/`
- â€œI renamed files for convenienceâ€ â†’ keep originals; map names in docs
- â€œI added a token to a download scriptâ€ â†’ use `.env`; rotate exposed tokens
- â€œI posted sensitive coordinates in a public dropâ€ â†’ stop, remove, report privately

---

<a id="reference-shelf"></a>

## ğŸ“š Project reference shelf

<details>
<summary><strong>ğŸ“– Reference library (project files)</strong></summary>

> âš ï¸ Reference PDFs may have licenses different from repository code.  
> Prefer `docs/library/` for references (or keep them outside the repo) and respect upstream terms.

### ğŸ§­ Core KFM system + protocols
- `docs/specs/Kansas Frontier Matrix (KFM) â€“ Comprehensive Technical Documentation.docx`
- `docs/specs/MARKDOWN_GUIDE_v13.md` *(or equivalent; may be `.gdoc` in some repos)*
- `docs/specs/Comprehensive Markdown Guide_ Syntax, Extensions, and Best Practices.docx`
- `docs/specs/Kansas-Frontier-Matrix Design Audit â€“ Gaps and Enhancement Opportunities.pdf`
- `docs/specs/ğŸŒŸ Kansas Frontier Matrix â€“ Latest Ideas & Future Proposals.docx`

### ğŸ—ºï¸ GIS, mapping, cartography, geospatial tooling
- `docs/library/making-maps-a-visual-guide-to-map-design-for-gis.pdf`
- `docs/library/Mobile Mapping_ Space, Cartography and the Digital - 9789048535217.pdf`
- `docs/library/python-geospatial-analysis-cookbook.pdf`
- `docs/library/KFM- python-geospatial-analysis-cookbook-over-60-recipes-to-work-with-topology-overlays-indoor-routing-and-web-application-analysis-with-python.pdf` *(if present)*
- `docs/library/PostgreSQL Notes for Professionals - PostgreSQLNotesForProfessionals.pdf`

### ğŸ›°ï¸ Remote sensing workflows
- `docs/library/Cloud-Based Remote Sensing with Google Earth Engine-Fundamentals and Applications.pdf`

### ğŸ–¼ï¸ Documents, scans & file formats
- `docs/library/compressed-image-file-formats-jpeg-png-gif-xbm-bmp.pdf`

### ğŸ“Š Statistics, experiments, inference & modeling integrity
- `docs/library/Understanding Statistics & Experimental Design.pdf`
- `docs/library/regression-analysis-with-python.pdf`
- `docs/library/Regression analysis using Python - slides-linear-regression.pdf`
- `docs/library/graphical-data-analysis-with-r.pdf`
- `docs/library/think-bayes-bayesian-statistics-in-python.pdf`

### ğŸ¤– ML theory & practice
- `docs/library/Understanding Machine Learning - From Theory to Algorithms.pdf`
- `docs/library/Deep Learning for Coders with fastai and PyTorch - Deep.Learning.for.Coders.with.fastai.and.PyTorchpdf` *(if present; filename as provided)*

### ğŸ§ª Simulation + optimization + graph math
- `docs/library/Scientific Modeling and Simulation_ A Comprehensive NASA-Grade Guide.pdf`
- `docs/library/Spectral Geometry of Graphs.pdf`
- `docs/library/Generalized Topology Optimization for Structural Design.pdf`

### âš™ï¸ Systems, scale, interoperability, software design
- `docs/library/Scalable Data Management for Future Hardware.pdf`
- `docs/library/Data Spaces.pdf`
- `docs/library/concurrent-real-time-and-distributed-programming-in-java-threads-rtsj-and-rmi.pdf`
- `docs/library/Flexible Software Design.pdf`

### ğŸŒ Web UI & 3D graphics
- `docs/library/responsive-web-design-with-html5-and-css3.pdf`
- `docs/library/webgl-programming-guide-interactive-3d-graphics-programming-with-webgl.pdf`

### â¤ï¸ Ethics, autonomy, AI law
- `docs/library/Introduction to Digital Humanism.pdf`
- `docs/library/Principles of Biological Autonomy - book_9780262381833.pdf`
- `docs/library/On the path to AI Lawâ€™s prophecies and the conceptual foundations of the machine learning age.pdf`

### ğŸ›¡ï¸ Security (defensive references only)
- `docs/library/ethical-hacking-and-countermeasures-secure-network-infrastructures.pdf`
- `docs/library/Gray Hat Python - Python Programming for Hackers and Reverse Engineers (2009).pdf`

> These are used to inform **defensive controls** (threat modeling, incident response, secure coding).  
> They are **not** a request for offensive tooling contributions.

### ğŸ§° General programming shelf (bundles)
- `docs/library/A programming Books.pdf`
- `docs/library/B-C programming Books.pdf`
- `docs/library/D-E programming Books.pdf`
- `docs/library/F-H programming Books.pdf`
- `docs/library/I-L programming Books.pdf`
- `docs/library/M-N programming Books.pdf`
- `docs/library/O-R programming Books.pdf`
- `docs/library/S-T programming Books.pdf`
- `docs/library/U-X programming Books.pdf`

</details>

---

## âœ… Definition of Done (for this README)

- [x] â€œRaw means bytes preservedâ€ rule is explicit
- [x] Append-only + checksums + receipts contract defined
- [x] `data/sources/` manifest pattern documented (pointer-first intake)
- [x] Layout + naming guidance included
- [x] Security/privacy/sovereignty guardrails included
- [ ] Linked from `data/README.md` (recommended)
- [ ] Reviewed by maintainers / data stewards (recommended)

---

## ğŸ•°ï¸ Version history

| Version | Date | Change |
|---|---|---|
| v1.0.0 | 2025-12-26 | Initial `data/raw/` README scaffold |
| v1.1.0 | 2026-01-12 | Align raw intake with `data/sources/` manifests + CI/review-gate expectations; expand receipts/pointer patterns |

<p align="right"><a href="#pipeline">â¬†ï¸ Back to top</a></p>
