# ğŸ“¦ `data/raw/` â€” Immutable Source Data (Readâ€‘Only) ğŸ§Š

![Data Stage](https://img.shields.io/badge/data_stage-raw-informational)
![Truth Path](https://img.shields.io/badge/policy-truth_path-critical)
![Provenance](https://img.shields.io/badge/provenance-required-success)
![No Edits](https://img.shields.io/badge/rule-never_edit_in_place-red)

> ğŸ§­ **â€œThe map behind the mapâ€ starts here.**  
> `data/raw/` holds **unaltered source snapshots** that feed KFM pipelines. Treat it like a museum archive: label it, checksum it, donâ€™t â€œfixâ€ it.

---

## ğŸ¯ Purpose

This folder is the **landing zone for original inputs** (downloads, exports, scans, vendor drops, agency releases) **before** they are cleaned, standardized, or transformed.

**Raw âœ Processed âœ Catalog/Provenance âœ Databases âœ API âœ UI/AI**  
Nothing skips the line. âœ…

---

## ğŸ§± Golden Rules (Nonâ€‘Negotiable)

1. **ğŸš« Never edit raw files in place**
   - If something is â€œwrong,â€ thatâ€™s part of the historical record.
   - Fixes happen in `pipelines/` and land in `data/processed/`.

2. **ğŸ§¾ Every dataset gets â€œsource contextâ€**
   - Record where it came from, when it was fetched, license, and whatâ€™s inside.

3. **ğŸ§¼ No derived outputs in `raw/`**
   - No cleaned CSVs, reprojected GeoJSON, clipped rasters, simplified shapes, etc.

4. **ğŸ§© Preserve original packaging**
   - Keep vendor/agency folder structure when possible.
   - Prefer storing the original `.zip` plus (optional) extracted contents *only if required*.

5. **ğŸ” No secrets or sensitive personal data**
   - Never store API keys, tokens, passwords, or private datasets here.

---

## ğŸ—‚ Recommended Folder Structure

You can organize by **domain** then **dataset** (preferred), or by dataset onlyâ€”pick one approach and be consistent.

```text
data/raw/
  README.md  âœ… (this file)
  <domain>/                        # e.g., hydrology/, historical/, remote_sensing/
    <dataset_id>/                  # stable slug (snake_case)
      source.yaml                  # required ğŸ§¾
      checksums.sha256             # strongly recommended ğŸ”’
      snapshots/                   # optional but great for immutability ğŸ§Š
        2026-02-03/                # ISO date of acquisition
          original.zip
          extracted/               # only if needed by pipeline
            ...
```

### âœ… Dataset IDs (`<dataset_id>`)
Use **lowercase `snake_case`** and keep it stable over time:
- `census_1900_county`
- `usgs_nwis_daily_discharge`
- `kdot_roads_centerlines`
- `landsat_scenes_kansas`

---

## ğŸ·ï¸ Naming Conventions

- **Dates:** ISO format `YYYY-MM-DD` (sortable!)
- **Versioned snapshots:** `snapshots/YYYY-MM-DD/`
- **Avoid spaces:** use `_` not spaces
- **Keep originals recognizable:** donâ€™t rename beyond necessity

Examples:
- `2026-02-03__kdot_roads.zip`
- `2025-11-01__nwis_daily.csv`

---

## ğŸ§¾ Required Sidecar Metadata: `source.yaml`

Each dataset folder must include a `source.yaml` describing provenance and constraints.

<details>
<summary><strong>ğŸ“„ Minimal <code>source.yaml</code> template (copy/paste)</strong></summary>

```yaml
id: "<dataset_id>"
title: ""
description: ""

origin:
  publisher: ""
  source_urls:
    - ""
  retrieved_at: "YYYY-MM-DD"
  retrieved_by: ""
  license: ""             # SPDX if possible (e.g., CC-BY-4.0), otherwise plain text
  citation: ""            # preferred formal citation if provided

scope:
  geography: "Kansas"
  spatial_extent:
    bbox_wgs84: [minLon, minLat, maxLon, maxLat]   # optional but helpful
  temporal_extent:
    start: "YYYY-MM-DD"     # optional
    end: "YYYY-MM-DD"       # optional

files:
  packaging: "zip|folder|single_file|api_export"
  contents:
    - path: "snapshots/YYYY-MM-DD/original.zip"
      description: ""
      sha256: ""            # optional here if using checksums.sha256

notes:
  known_issues: []
  pii: "none|possible|present"
  restrictions: ""
```
</details>

---

## ğŸ”’ Checksums: `checksums.sha256` (Strongly Recommended)

Why: helps detect accidental edits, corrupted transfers, and supports deterministic re-runs.

Example file:
```text
<sha256>  snapshots/2026-02-03/original.zip
<sha256>  snapshots/2026-02-03/extracted/roads.shp
```

---

## ğŸ§° Large Files (COGs, LiDAR, Big Rasters) ğŸ˜

Raw artifacts can be huge. Recommended options:
- **DVC** (preferred for big data artifacts)
- **Git LFS** (acceptable when appropriate)
- **External object storage pointers** (S3/Azure/GCS), tracked with metadata

Rule of thumb:
- If it makes Git painful, donâ€™t force it into Git. Track it cleanly.

---

## âš™ï¸ How Pipelines Should Use `data/raw/`

Pipelines should:
- âœ… **Read from** `data/raw/<domain>/<dataset_id>/...`
- ğŸš« **Never modify** anything in `data/raw/`
- âœ… Write outputs to:
  - `data/processed/` (cleaned/standardized outputs)
  - `data/catalog/` (STAC/DCAT metadata)
  - `data/provenance/` (W3C PROV lineage logs)

> If you canâ€™t regenerate processed outputs from raw + pipeline code, itâ€™s not reproducible. ğŸ§ª

---

## ğŸ” Updating or Reâ€‘Fetching Data (Donâ€™t Overwrite!)

If a source updates:
- Add a **new snapshot**: `snapshots/YYYY-MM-DD/`
- Update `source.yaml` if scope or license changed
- Run the pipeline to create updated processed outputs + provenance

âœ… This keeps an audit trail and supports time-based comparisons.

---

## âœ… Intake Checklist (Before a PR)

- [ ] Created `data/raw/<domain>/<dataset_id>/`
- [ ] Added `source.yaml` with **URL + retrieval date + license**
- [ ] Stored raw artifact(s) in `snapshots/YYYY-MM-DD/`
- [ ] Added `checksums.sha256` (or equivalent)
- [ ] Confirmed no derived/cleaned outputs are placed in `raw/`
- [ ] Confirmed no secrets / sensitive personal data present
- [ ] Ran pipeline and produced:
  - [ ] `data/processed/...`
  - [ ] `data/catalog/...`
  - [ ] `data/provenance/...`

---

## ğŸ†˜ Common Gotchas

- **â€œI reprojected the shapefile to EPSG:4326 and replaced it.â€**  
  âŒ Donâ€™t. Put the reprojected result in `data/processed/`.

- **â€œThe agency ZIP has nested folders and weird names.â€**  
  âœ… Keep it. Normalize in the pipeline.

- **â€œThis dataset contains addresses / individuals.â€**  
  ğŸš« Stop. Move to restricted storage and document the handling plan.

---

## ğŸ”— Related (Repoâ€‘Local)

- `pipelines/` â€” ETL scripts and notebooks
- `data/processed/` â€” cleaned outputs
- `data/catalog/` â€” STAC/DCAT metadata
- `data/provenance/` â€” lineage logs (W3C PROV)
- `docs/standards/` â€” project governance + profiles (STAC/DCAT/PROV)

---

**âœ¨ Reminder:** Raw is sacred. Processing is where the magic happens. ğŸ§™â€â™‚ï¸