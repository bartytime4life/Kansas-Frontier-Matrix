<div align="center">

# ğŸ—ƒï¸ Kansas Frontier Matrix â€” Raw Data Directory

`data/raw/`

**Mission:** Preserve and document all **immutable, original datasets** downloaded or collected
from verified external sources â€” providing the foundational inputs for every ETL,
validation, and scientific workflow in the **Kansas Frontier Matrix (KFM)**.

[![Build & Deploy](https://github.com/bartytime4life/Kansas-Frontier-Matrix/actions/workflows/site.yml/badge.svg)](../../.github/workflows/site.yml)
[![Docs Â· MCP](https://img.shields.io/badge/Docs-MCP-blue)](../../docs/)
[![License: Data](https://img.shields.io/badge/License-CC--BY%204.0-green)](../../LICENSE)
[![License: Code](https://img.shields.io/badge/License-MIT-yellow)](../../LICENSE)

</div>

---

## ğŸ§© Versioning

| Field            | Value                                                                            |
| :--------------- | :------------------------------------------------------------------------------- |
| **Version**      | `v1.0.1`                                                                         |
| **Status**       | Stable                                                                           |
| **Maintainer**   | KFM Data Acquisition & Provenance Team                                           |
| **Last Updated** | 2025-10-12                                                                       |
| **Scope**        | Immutable source datasets (`terrain`, `hydrology`, `landcover`, `climate`, etc.) |
| **Compliance**   | MCP v1.0 Â· STAC 1.0.0 Â· Provenance Verified                                      |

---

## ğŸ“š Overview

The `data/raw/` directory contains **original, unmodified datasets** acquired from
registered external sources listed in `data/sources/`.
These represent the **ground truth inputs** for all downstream processing, validation,
and derived data products within KFM.

**Key Characteristics**

* ğŸ§© **Immutable** â€” no manual editing, reformatting, or manipulation
* ğŸ” **Reproducible** â€” each file corresponds to a manifest in `data/sources/`
* ğŸ”— **Traceable** â€” origin, license, retrieval method, and checksum recorded
* âœ… **Validated** â€” verified SHA-256 checksum and metadata logged on acquisition

> âš ï¸ **Do not modify files inside `data/raw/`.**
> To update a dataset, fetch a new copy via its source manifest and re-ingest it through ETL.

---

## ğŸ—‚ï¸ Directory Layout

```bash
data/raw/
â”œâ”€â”€ README.md
â”œâ”€â”€ terrain/             # LiDAR DEMs, USGS 3DEP data
â”‚   â”œâ”€â”€ ks_3dep_tiles.zip
â”‚   â””â”€â”€ lidar_tile_index.geojson
â”œâ”€â”€ hydrology/           # NHD, WBD, NFHL datasets
â”‚   â”œâ”€â”€ nhd_flowlines.zip
â”‚   â”œâ”€â”€ wbd_huc12.zip
â”‚   â””â”€â”€ fema_nfhl.zip
â”œâ”€â”€ landcover/           # NLCD, CDL, vegetation maps
â”‚   â”œâ”€â”€ nlcd_1992_2021.zip
â”‚   â””â”€â”€ usda_cdl_2020.tif
â”œâ”€â”€ climate/             # Daymet, NOAA, Drought Monitor
â”‚   â”œâ”€â”€ daymet_1980_2024.nc
â”‚   â”œâ”€â”€ noaa_normals_1991_2020.csv
â”‚   â””â”€â”€ drought_monitor_2000_2025.zip
â”œâ”€â”€ hazards/             # Tornado, flood, wildfire, drought sources
â”‚   â”œâ”€â”€ noaa_storm_events.csv
â”‚   â”œâ”€â”€ usgs_wildfire_perimeters.zip
â”‚   â””â”€â”€ fema_flood_events.geojson
â”œâ”€â”€ tabular/             # Census, USDA, BEA, BLS datasets
â”‚   â”œâ”€â”€ census_population_1860_2020.csv
â”‚   â”œâ”€â”€ usda_agriculture_production.csv
â”‚   â””â”€â”€ bea_economic_indicators.csv
â””â”€â”€ text/                # OCR documents, transcripts, treaties, newspapers
    â”œâ”€â”€ loc_chronicling_america.zip
    â”œâ”€â”€ kshs_oral_histories.json
    â””â”€â”€ yale_avalon_treaties.txt
```

Each domain mirrors the **KFM data taxonomy**, directly tied to manifests in `data/sources/`.

---

## âš™ï¸ Acquisition Workflow

All raw data are acquired automatically using **source manifests** that define URLs, metadata, and retrieval protocols.

### Makefile Target

```bash
make fetch-raw
```

### Python Command

```bash
python src/utils/fetch_data.py --manifest data/sources/hydrology/usgs_nhd_flowlines.json
```

### Workflow Steps

1. Validate manifest (`data/sources/*.json`).
2. Fetch via HTTPS, API, or FTP.
3. Save to `data/raw/<domain>/` with standardized filenames.
4. Generate `.sha256` checksum + metadata JSON.
5. Log acquisition event in `data/checksums/` and `data/sources/`.

---

## ğŸ§­ Mermaid Acquisition Flow

```mermaid
flowchart TD
  A["Source Manifests<br/>data/sources/*.json"] --> B["Acquisition<br/>fetch Â· validate Â· log"]
  B --> C["Raw Data Storage<br/>data/raw/<domain>/"]
  C --> D["Checksums & Metadata<br/>sha256 Â· license Â· manifest"]
  D --> E["ETL Pipelines<br/>data/processed/"]
  E --> F["STAC Catalog & Provenance<br/>data/stac/"]
%% END OF MERMAID %%
```

---

## ğŸ§¾ Raw Data Metadata Schema

| Field              | Description                     | Example                      |
| :----------------- | :------------------------------ | :--------------------------- |
| `source_id`        | Unique ID from `data/sources/`. | `usgs_3dep_dem`              |
| `retrieved_on`     | ISO 8601 timestamp.             | `2025-10-04T12:30:00Z`       |
| `checksum`         | SHA-256 digest of file.         | `b8494a...3b61cb6ac8d`       |
| `file_size`        | File size in MB.                | `1420.5`                     |
| `license`          | Data usage license.             | `Public Domain (USGS)`       |
| `retrieval_method` | Transfer method.                | `REST API`                   |
| `linked_pipeline`  | Downstream ETL script.          | `terrain_pipeline.py`        |
| `notes`            | Additional comments.            | `Fetched via USGS 3DEP API.` |

---

## ğŸ§© Integration with KFM Pipelines

| Linked Component              | Purpose                                    |
| :---------------------------- | :----------------------------------------- |
| `data/sources/`               | Defines provenance + acquisition metadata. |
| `src/pipelines/*`             | ETL scripts transforming raw â†’ processed.  |
| `data/processed/`             | Repository of standardized outputs.        |
| `data/checksums/`             | Tracks download integrity.                 |
| `.github/workflows/fetch.yml` | CI/CD automation for fetch validation.     |

---

## ğŸ§¹ Cleanup & Validation

Although raw datasets are immutable, they may be **revalidated or refreshed** when a source is updated.

### Revalidate

```bash
make validate-raw
```

### Refresh from Source

```bash
make fetch-raw-refresh
```

### Manual Cleanup *(rarely used)*

```bash
rm -rf data/raw/<domain>/*
```

---

## ğŸ§  MCP Compliance Summary

| MCP Principle       | Implementation                                     |
| :------------------ | :------------------------------------------------- |
| Documentation-first | README defines structure and workflow.             |
| Reproducibility     | Deterministic acquisition from manifests.          |
| Open Standards      | All datasets use open formats/APIs.                |
| Provenance          | Full lineage traceable to source manifest.         |
| Auditability        | CI/CD logs each acquisition + checksum validation. |

---

## ğŸ“ Related Directories

| Path                       | Description                                   |
| :------------------------- | :-------------------------------------------- |
| `data/sources/`            | Source manifests defining dataset origins.    |
| `data/checksums/`          | Integrity validation for raw/processed files. |
| `data/processed/`          | Cleaned, validated, and derived datasets.     |
| `data/processed/metadata/` | STAC metadata linking raw â†’ processed.        |

---

## ğŸ“… Version History

| Version   | Date       | Summary                                                |
| :-------- | :--------- | :----------------------------------------------------- |
| **1.0.1** | 2025-10-12 | Added version metadata + Mermaid flow + MCP alignment. |
| **1.0.0** | 2025-10-04 | Initial raw data documentation + acquisition workflow. |

---

<div align="center">

> **Kansas Frontier Matrix â€” â€œEvery Dataset Begins Here: Untouched, Immutable, and Proven.â€**
> ğŸ“ [`data/raw/`](.) Â· Archive of all original, verifiable datasets powering the Kansas Frontier Matrix.

</div>
