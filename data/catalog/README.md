# ğŸ“š `data/catalog/` â€” Dataset Discovery Catalog (DCAT) ğŸ§­

![Metadata](https://img.shields.io/badge/metadata-DCAT%20%7C%20STAC%20%7C%20PROV-blue)
![Pipeline](https://img.shields.io/badge/pipeline-Raw%E2%86%92Processed%E2%86%92Catalog%E2%86%92DB%E2%86%92API%E2%86%92UI-5b8cff)
![Reproducible](https://img.shields.io/badge/ETL-deterministic%20%26%20reproducible-brightgreen)

> **This folder is the â€œdiscovery layerâ€ for KFM datasets.**  
> âœ… Put **catalog metadata** here (DCAT).  
> ğŸš« Do **NOT** put raw or processed data files here.

---

## ğŸ¯ What this folder is for

`data/catalog/` contains **dataset discovery records** that answer:

- **What** is this dataset? (title, description, keywords)
- **Who** made it? (publisher/maintainer)
- **Where/when** does it apply? (spatial/temporal coverage)
- **How** do I access it? (distributions: downloads, STAC links, API endpoints)

In practice, this is where we keep our **DCAT dataset entries** so the platform (and humans) can *discover* what exists and how to use it.

---

## ğŸ§± â€œPublishedâ€ data requires boundary artifacts

A dataset isnâ€™t considered â€œpublishedâ€ until it has:

- ğŸ—ºï¸ **STAC Collection + Item(s)** (spatial/temporal indexing of geospatial assets)
- ğŸ§¾ **DCAT Dataset entry** (this folder â†’ discovery catalog record)
- ğŸ§¬ **PROV lineage bundle** (full provenance: inputs â†’ activities â†’ outputs)

> Think of these as the **contract** between the data pipeline and the downstream layers (graph, API, UI).  
> If any one is missing, the dataset is *not complete*.

---

## ğŸ—‚ï¸ Folder layout

```text
ğŸ“ data/
â”œâ”€ ğŸ“ raw/                      ğŸ§¾ source snapshots (immutable / read-only)
â”œâ”€ ğŸ“ work/                     ğŸ§ª scratch + intermediate artifacts (ephemeral)
â”œâ”€ ğŸ“ processed/                âœ… final outputs consumed by DB/API/UI
â”œâ”€ ğŸ“ stac/                     ğŸ›°ï¸ STAC Collections + Items (geospatial catalog)
â”‚  â”œâ”€ ğŸ“ collections/           ğŸ§© collection JSON (grouping + semantics)
â”‚  â””â”€ ğŸ“ items/                 ğŸ“¦ item JSON (per asset/scene/tile/chunk)
â”œâ”€ ğŸ“ prov/                     ğŸ§¬ provenance bundles (W3C PROV-style receipts)
â””â”€ ğŸ“ catalog/                  ğŸ—‚ï¸ discovery catalog (DCAT)
   â””â”€ ğŸ“ dcat/
      â”œâ”€ ğŸ“„ README.md                      ğŸ‘ˆ you are here
      â”œâ”€ ğŸ“„ <dataset_id>.jsonld            âœ… canonical dataset record (JSON-LD)
      â”œâ”€ ğŸ“„ <dataset_id>.ttl               â—»ï¸ optional (only if adopted + validated)
      â””â”€ ğŸ“„ catalog.jsonld                 â—»ï¸ optional aggregate dcat:Catalog export
```

---

## ğŸ§© DCAT entries (what goes in `data/catalog/dcat/`)

Each dataset should have **one** DCAT record (JSON-LD) that includes, at minimum:

- `dct:title` (human title)
- `dct:description` (what it is / why it exists)
- `dct:license` (SPDX-style if possible)
- `dcat:keyword` (searchability)
- `dct:spatial` / `dct:temporal` (coverage)
- `dcat:distribution` (download(s), STAC link(s), API link(s))

### âœ… Practical rule of thumb
If someone asks: â€œWhat is this dataset and how do I get it?â€  
â€¦the DCAT record should answer that without needing tribal knowledge.

---

## ğŸ”— Cross-linking rules (DCAT â‡„ STAC â‡„ PROV)

To keep the system coherent, we rely on **stable IDs** and **explicit links**:

### DCAT â†’ STAC
- Add a `dcat:distribution` entry that points to:
  - the **STAC Collection** (for the dataset)
  - and/or the relevant **STAC Item(s)** (for individual assets)

### DCAT â†’ Processed outputs
- Add at least one `dcat:distribution` that points to the processed artifact(s), e.g.:
  - `data/processed/<domain>/<dataset>.parquet`
  - `data/processed/<domain>/<dataset>.geojson`

### DCAT â†’ PROV
- Include a distribution or relation that points to the provenance file in `data/prov/`
- The PROV record should name:
  - input raw entities
  - processing activity (pipeline run)
  - output entity (processed file)

---

## âœ… New dataset checklist (PR-ready)

Use this checklist before opening a PR:

- [ ] ğŸ“¥ Raw inputs placed under `data/raw/<domain>/...`
- [ ] ğŸ§ª Pipeline script/notebook exists under `pipelines/` and runs end-to-end without prompts
- [ ] ğŸ“¦ Outputs written to `data/processed/<domain>/...`
- [ ] ğŸ—ºï¸ STAC written/updated under `data/stac/collections/` and `data/stac/items/`
- [ ] ğŸ§¾ DCAT JSON-LD added to `data/catalog/dcat/<dataset-id>.jsonld`
- [ ] ğŸ§¬ PROV bundle added to `data/prov/<dataset-id>.prov.json` (or project naming convention)
- [ ] ğŸ” All links resolve: DCAT â†” STAC â†” PROV â†” processed outputs
- [ ] ğŸ§¹ Naming is stable and slug-safe (avoid renames unless absolutely necessary)

---

## ğŸ§ª Validation & CI expectations

CI is expected to verify that:

- new/changed `data/processed/**` artifacts have matching:
  - DCAT entry
  - STAC entry
  - PROV record
- basic format validity checks pass (GeoJSON/JSON/Parquet integrity, etc.)

If CI fails with â€œmissing catalog/prov,â€ it usually means:
- the pipeline generated outputs but didnâ€™t publish the metadata artifacts, **or**
- IDs donâ€™t match across layers (common when a dataset slug changed).

---

## ğŸ§¾ DCAT template (starter)

<details>
<summary><strong>ğŸ“„ Minimal DCAT JSON-LD skeleton (copy/paste)</strong></summary>

```json
{
  "@context": {
    "dcat": "http://www.w3.org/ns/dcat#",
    "dct": "http://purl.org/dc/terms/",
    "foaf": "http://xmlns.com/foaf/0.1/",
    "xsd": "http://www.w3.org/2001/XMLSchema#"
  },
  "@type": "dcat:Dataset",
  "@id": "kfm:dataset/<dataset-id>",
  "dct:title": "<Human Title>",
  "dct:description": "<What it is, why it exists, key caveats>",
  "dcat:keyword": ["kansas", "frontier", "railroad"],
  "dct:license": "<License URL or SPDX-like string>",
  "dcat:distribution": [
    {
      "@type": "dcat:Distribution",
      "dct:title": "Processed data (Parquet)",
      "dcat:downloadURL": "data/processed/<domain>/<file>.parquet",
      "dcat:mediaType": "application/parquet"
    },
    {
      "@type": "dcat:Distribution",
      "dct:title": "STAC Collection",
      "dcat:accessURL": "data/stac/collections/<collection-id>.json",
      "dcat:mediaType": "application/json"
    },
    {
      "@type": "dcat:Distribution",
      "dct:title": "Provenance (PROV)",
      "dcat:accessURL": "data/prov/<dataset-id>.prov.json",
      "dcat:mediaType": "application/json"
    }
  ],
  "dct:publisher": {
    "@type": "foaf:Organization",
    "foaf:name": "Kansas Frontier Matrix"
  }
}
```

</details>

---

## ğŸ§¬ Provenance mindset (quick reminder)

PROV records should make it easy to answer:

> â€œHow was this produced?â€  
> â€œFrom which sources?â€  
> â€œUsing which pipeline + parameters?â€  
> â€œAt what time, and by whom/what agent?â€

If the provenance canâ€™t reconstruct the story of the dataset, itâ€™s not done yet.

---

## ğŸ§  Architecture snapshot (why the linking matters)

```mermaid
flowchart LR
  subgraph Data_Lifecycle["ğŸ“¦ Data Lifecycle"]
    A["Raw Sources"] --> B["ETL + Normalization"]
    B --> C["STAC Items + Collections"]
    C --> D["DCAT Dataset Views (this folder)"]
    C --> E["PROV Lineage Bundles"]
  end

  C --> G["Neo4j Graph (references back to catalogs)"]
  G --> H["API Layer (contracts + redaction)"]
  H --> I["Map UI â€” React Â· MapLibre Â· (optional) Cesium"]
  I --> J["Story Nodes (governed narratives)"]
```

---

## ğŸ·ï¸ IDs, naming, and versioning

- Prefer **stable dataset IDs** (`<domain>.<topic>.<time-range>` style is a good pattern)
- Changing a dataset ID is expensive:
  - it breaks links across DCAT/STAC/PROV/graph/UI
- Use Git history/tags/releases for â€œwhat version did you use?â€ tracking

---

## ğŸ§° Tips & gotchas

- âœ… Keep distributions explicit: â€œdownloadURLâ€ for files, â€œaccessURLâ€ for services/catalogs.
- âœ… If a dataset is huge (rasters, tilesets), store a **reference + checksum** in metadata and keep the *catalog-of-record* here.
- ğŸš« Avoid â€œmystery fields.â€ If you need extra metadata, extend the project profiles instead of inventing one-off keys.

---

## ğŸ“ Related reading

- `data/stac/` â€” STAC Collections + Items (geospatial indexing)
- `data/prov/` â€” PROV lineage bundles (traceability)
- `pipelines/` â€” ETL that must publish *both* data + metadata artifacts
- `docs/` â€” narrative + architecture + standards

---
