# ğŸ—„ï¸ tools/db â€” Database Tooling (KFM)

![DB](https://img.shields.io/badge/DB-PostgreSQL%20%2B%20PostGIS-336791?logo=postgresql&logoColor=white)
![Graph](https://img.shields.io/badge/Graph-Neo4j-008CC1?logo=neo4j&logoColor=white)
![Ops](https://img.shields.io/badge/Ops-Migrations%20%2B%20Backups%20%2B%20Validation-orange)
![Governance](https://img.shields.io/badge/Governance-Contract%20%2B%20Provenance%20First-6f42c1)
![Style](https://img.shields.io/badge/Style-Idempotent%20%26%20Repeatable-brightgreen)

This directory is the **operational runbook + tooling home** for databases in Kansas Frontier Matrix (KFM):  
**bring up local DBs**, **apply migrations**, **seed test data**, **validate geodata**, and **backup/restore** reliably. ğŸ§°

> [!NOTE]
> This folder is intentionally â€œboring.â€ It should contain **repeatable DB operations**, not business logic.

---

## ğŸ¯ What this folder owns (and what it doesnâ€™t)

âœ… Owns:
- ğŸ§± **Schema & migrations** (Postgres/PostGIS + graph migrations if applicable)
- ğŸ§ª **Validation** (geometry validity, CRS checks, metadata gates)
- ğŸ’¾ **Backups / restores** (dev + ops playbooks)
- ğŸ“¦ **Local dev orchestration** (Docker Compose, init scripts, seed fixtures)
- ğŸ“ˆ **Performance hygiene** (indexes, analyze/vacuum helpers, benchmark harness)

ğŸš« Does **not** own:
- ğŸ§  Domain/business rules (belongs in your domain layer / services)
- ğŸ–¥ï¸ UI-to-DB access paths (UI must go through the governed API boundary)
- ğŸ—ºï¸ â€œMystery layersâ€ (anything not traceable to catalogs + provenance)

---

## ğŸ§± Datastores in KFM (mental model)

KFM treats **data + metadata + provenance** as first-class citizens.

- ğŸ˜ **Postgres/PostGIS**: spatial indexing + queries; vector footprints; query-time joins; routing/network analysis when needed.
- ğŸ•¸ï¸ **Neo4j (graph)**: relationships + semantic linking (**references back to catalogs**, not bulky payload storage).
- ğŸ§Š **Object storage / filesystem**: large assets (e.g., COGs, documents, imagery) with **catalog references**.

```mermaid
flowchart LR
  A[ğŸ“¦ Raw Sources] --> B[ğŸ§ª ETL + Normalization]
  B --> C[ğŸ§¾ Catalogs<br/>STAC + DCAT + PROV]
  C --> P[ğŸ˜ Postgres/PostGIS<br/>indexes + spatial query]
  C --> G[ğŸ•¸ï¸ Neo4j Graph<br/>relationships + refs]
  P --> API[ğŸ”Œ Governed API<br/>contracts + redaction]
  G --> API
  API --> UI[ğŸ—ºï¸ Map UI<br/>React Â· MapLibre Â· (optional) Cesium]
  UI --> N[ğŸ§  Story Nodes / Focus Mode<br/>provenance-linked narrative]
```

---

## ğŸ“ Suggested layout

If a folder doesnâ€™t exist yet, create it when you add the capability (keep it tidy âœ¨):

```text
tools/db/
  ğŸ“ bin/                 # tiny CLIs: migrate, backup, restore, validate
  ğŸ“ docker/              # compose files + init scripts for local dev
  ğŸ“ migrations/
  â”‚   â”œâ”€ ğŸ“ postgres/      # SQL/Alembic migrations for Postgres/PostGIS
  â”‚   â””â”€ ğŸ“ graph/         # Neo4j/graph migrations (if you version them)
  ğŸ“ sql/                 # idempotent admin scripts (indexes, extensions)
  ğŸ“ seed/                # seed fixtures for dev/tests (small + deterministic)
  ğŸ“ bench/               # micro-benchmarks / load test harness
  ğŸ“ README.md
```

---

## âš¡ Quickstart (local dev)

> [!TIP]
> Prefer **Docker Compose** for onboarding: one command to get a clean, reproducible environment.

### 1) Start the databases ğŸ³
Example (adjust filenames to match your repo):
```bash
docker compose -f tools/db/docker/docker-compose.yml up -d
```

### 2) Create extensions + schemas (PostGIS-first) ğŸ§©
Run once per database (migration-friendly approach):

```sql
CREATE EXTENSION IF NOT EXISTS postgis;
CREATE EXTENSION IF NOT EXISTS pgcrypto; -- optional, but handy for UUID/crypto utilities
CREATE SCHEMA IF NOT EXISTS geodata;
```

> [!NOTE]
> Keeping spatial tables in a separate schema like `geodata` makes organization cleaner and can simplify backup/restore boundaries.

### 3) Apply migrations ğŸ§±
Recommended CLI shape (implement however you like):
```bash
./tools/db/bin/migrate up
```

### 4) Run validation gates âœ…
```bash
./tools/db/bin/validate all
```

---

## ğŸ” Environment variables

These names are intentionally conventionalâ€”use what your stack prefers, but keep it consistent.

| Variable | Example | Used for |
|---|---:|---|
| `DATABASE_URL` | `postgresql://user:pass@localhost:5432/kfm` | App + tooling connection string |
| `PGHOST` / `PGPORT` | `localhost` / `5432` | CLI-friendly overrides |
| `PGUSER` / `PGPASSWORD` | `kfm` / `***` | Local dev auth |
| `PGDATABASE` | `kfm` | Target DB name |
| `NEO4J_URI` | `bolt://localhost:7687` | Graph tooling |
| `NEO4J_USER` / `NEO4J_PASSWORD` | `neo4j` / `***` | Graph auth |

---

## ğŸ§¾ Common operations

### Connect (psql) ğŸ˜
```bash
psql "$DATABASE_URL"
```

### Backup (binary format) ğŸ’¾
Keep backups timestamped and **test restores** periodically.
```bash
mkdir -p backups
pg_dump -Fc -f "backups/kfm_$(date +%F).dump" "$DATABASE_URL"
```

### Restore (into an empty DB) â™»ï¸
```bash
pg_restore -d "$DATABASE_URL" "backups/kfm_2026-01-14.dump"
```

> [!WARNING]
> Restores overwrite reality fast. For safety: restore into a fresh DB name first, then swap.

---

## ğŸ§ª Validation patterns (geospatial)

### Geometry validity checks (PostGIS) ğŸ§­
Use PostGIS validity checks before â€œpublishingâ€ a dataset into the pipeline:

```sql
-- Find invalid geometries
SELECT id, ST_IsValid(geom) AS is_valid
FROM geodata.my_layer
WHERE NOT ST_IsValid(geom);

-- Get details on *why* it's invalid (helpful for fixes)
SELECT id, (ST_IsValidDetail(geom)).*
FROM geodata.my_layer
WHERE NOT ST_IsValid(geom);
```

### CRS sanity (rule-of-thumb) ğŸŒ
- Pick a canonical CRS for storage (commonly **EPSG:4326**), and transform on ingest.
- Validate bounds if you store lon/lat (lon âˆˆ [-180, 180], lat âˆˆ [-90, 90]).

---

## ğŸ§© Migrations: conventions that scale

### Postgres/PostGIS migrations
- âœ… **Forward-only** is simplest (rollbacks optional, but donâ€™t rely on them).
- âœ… Prefer **small, composable migrations**.
- âœ… Add indexes intentionally (GiST for geometry, B-tree for lookups).
- âœ… Migrations should be reproducible on clean DB and safe on existing DBs.

Suggested naming:
- `YYYYMMDDHHMMSS__short_description.sql`  
  Example: `20260114093000__add_layer_bounds_index.sql`

### Graph migrations (if used)
Keep graph changes versioned too:
- constraints / indexes
- ontology label/property conventions
- relationship type changes

> [!TIP]
> Treat â€œschemaâ€ broadly: Postgres schema + graph constraints + metadata profiles all evolve together.

---

## ğŸ“¦ Metadata + provenance are the â€œreal interfaceâ€

KFMâ€™s pipeline is governed by **boundary artifacts**:
- ğŸ§¾ STAC (assets + spatial/temporal indexing)
- ğŸ—‚ï¸ DCAT (dataset discovery + distributions)
- ğŸ§¬ PROV (lineage: inputs â†’ activities â†’ outputs)

The DB is an implementation detail **behind** those contracts.  
DB tools should therefore:
- enforce validators locally (pre-commit / CI),
- reject â€œpublishâ€ operations without complete catalogs,
- keep DB rows linkable to stable catalog identifiers.

---

## ğŸ“ˆ Performance & reliability playbook

Even â€œroutineâ€ operations can affect end-user experience:
- backups
- vacuum/analyze
- index creation
- compaction or large deletes
- replication catch-up (if applicable)

Practical habits:
- ğŸ§ª benchmark representative workloads (read/write mix matters)
- ğŸ“Š track tail latency (p95/p99), not only averages
- ğŸ•°ï¸ schedule heavy operations, or run them in lower-traffic windows
- ğŸ§° keep a â€œrestore drillâ€ script so disaster recovery stays real

---

## ğŸ§· Federation & â€œdata spacesâ€ mindset

If KFM needs to **connect to external datasets** (without copying everything), use â€œdata spaceâ€ patterns:
- federated access
- strong governance + trust rules
- metadata-first interoperability

Postgres tip (optional): FDWs can help with **read-only federation** (treat remote sources as connectors, not â€œmagic tablesâ€).

---

## ğŸ§  Analytics & modeling support

KFM is not only a map viewerâ€”itâ€™s a research platform. DB tooling should make it easy to:
- export clean analysis datasets (CSV/Parquet)
- snapshot training datasets for regression/Bayesian modeling
- support reproducible simulation/optimization runs with traceable inputs/outputs

Example export:
```sql
COPY (
  SELECT * FROM analytics.v_dataset_for_model
) TO STDOUT WITH CSV HEADER;
```

---

## ğŸ§¯ Troubleshooting (fast hits)

| Symptom | Likely cause | Fix |
|---|---|---|
| `psql: connection refused` | DB container not running / wrong port | `docker ps`, check mapped ports |
| `permission denied for schema` | role lacks privileges | grant schema/table privileges; avoid using `postgres` everywhere |
| `extension "postgis" does not exist` | PostGIS not installed in image | use a PostGIS-enabled image or install packages |
| slow spatial queries | missing GiST index | add GiST index on geometry column |
| migrations fail in CI only | hidden state / non-idempotent scripts | ensure migrations run on clean DB in CI |
| invalid geometries | bad source data / wrong CRS | run `ST_IsValidDetail`, fix at ingest stage |

---

## ğŸ¤ Contributing

When you add/change DB behavior:
1. ğŸ§± Add a migration (`tools/db/migrations/...`)
2. âœ… Add/extend validators (geometry + metadata + contract checks)
3. ğŸ§¾ Ensure STAC/DCAT/PROV references remain valid
4. ğŸ“ˆ Add an index or benchmark if it impacts query patterns
5. ğŸ“ Update this README if the operator workflow changed

---

## ğŸ“š Project Reference Library (the docs powering our conventions)

<details>
<summary><strong>Click to expand ğŸ“–</strong> (kept here so DB conventions stay aligned with the projectâ€™s â€œsource libraryâ€)</summary>

### Core KFM docs (architecture + governance)
- ğŸ§­ **Kansas Frontier Matrix (KFM) â€“ Comprehensive Technical Documentation**
- ğŸ§± **MARKDOWN_GUIDE_v13** (contract-first + pipeline ordering + governance invariants)
- ğŸ—ºï¸ **Kansas-Frontier-Matrix â€” Open-Source Geospatial Historical Mapping Hub Design**

### Databases, data management, and performance
- ğŸ˜ **PostgreSQL Notes for Professionals**
- ğŸ“ˆ **Database Performance at Scale**
- ğŸ§Š **Scalable Data Management for Future Hardware**
- ğŸ§© **Data Spaces** (federation, governance, trust)

### Geospatial + mapping stack
- ğŸ§­ **python-geospatial-analysis-cookbook**
- ğŸ—ºï¸ **making-maps-a-visual-guide-to-map-design-for-gis**
- ğŸ“± **Mobile Mapping: Space, Cartography and the Digital**
- ğŸ›ï¸ **Archaeological 3D GIS**

### Analytics, stats, and modeling (what DB exports should enable)
- ğŸ“‰ **regression-analysis-with-python**
- ğŸ“Š **Understanding Statistics & Experimental Design**
- ğŸ“ˆ **graphical-data-analysis-with-r**
- ğŸ§  **think-bayes-bayesian-statistics-in-python**
- ğŸ›°ï¸ **Cloud-Based Remote Sensing with Google Earth Engine**
- ğŸ§ª **Scientific Modeling and Simulation (NASA-grade guide)**
- ğŸ§® **Generalized Topology Optimization for Structural Design**
- ğŸ•¸ï¸ **Spectral Geometry of Graphs**
- ğŸ“š **Regression analysis using Python (slides)**

### Engineering & platform concerns
- ğŸ§µ **concurrent-real-time-and-distributed-programming-in-java** (systems thinking: concurrency + reliability)
- ğŸ–¼ï¸ **compressed-image-file-formats-jpeg-png-gif-xbm-bmp** (assets, storage, distribution)
- ğŸ§‘â€ğŸ’» **responsive-web-design-with-html5-and-css3** (UI constraints influence API/DB shaping)
- ğŸŒ **webgl-programming-guide** (3D visualization influences tiling + query patterns)

### Security + ethics + governance (defensive posture)
- ğŸ” **ethical-hacking-and-countermeasures-secure-network-infrastructures**
- ğŸ§¯ **Gray Hat Python** (treat as defensive learning / threat modeling)
- âš–ï¸ **On the path to AI Lawâ€™s propheciesâ€¦** (data governance, ML-era considerations)
- ğŸ§­ **Introduction to Digital Humanism**
- ğŸ§¬ **Principles of Biological Autonomy**

### â€œProgramming Booksâ€ collections (grab-bag references)
- ğŸ“¦ **A programming Books**
- ğŸ“¦ **B-C programming Books**
- ğŸ“¦ **D-E programming Books**
- ğŸ“¦ **F-H programming Books**
- ğŸ“¦ **I-L programming Books**
- ğŸ“¦ **M-N programming Books**
- ğŸ“¦ **O-R programming Books**
- ğŸ“¦ **S-T programming Books**
- ğŸ“¦ **U-X programming Books**

</details>

