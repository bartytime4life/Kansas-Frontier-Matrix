# ğŸ¦€ tools/rs â€” Rust Tooling for Kansas Frontier Matrix (KFM)

<p align="center">
  <img alt="Rust" src="https://img.shields.io/badge/Rust-stable-000000?logo=rust&logoColor=white">
  <img alt="Cargo" src="https://img.shields.io/badge/Built%20with-Cargo-3b2f2f?logo=rust&logoColor=white">
  <img alt="Deterministic" src="https://img.shields.io/badge/Deterministic-outputs-success">
  <img alt="Provenance First" src="https://img.shields.io/badge/Provenance-first-blue">
  <img alt="Geo" src="https://img.shields.io/badge/Geo-GIS%20%2B%20Remote%20Sensing-2b8a3e">
  <img alt="Safe Defaults" src="https://img.shields.io/badge/Security-safe%20defaults-orange">
  <img alt="Ports & Adapters" src="https://img.shields.io/badge/Architecture-ports%20%26%20adapters-6f42c1">
</p>

<p align="center">
  <b>High-performance, reproducible Rust utilities</b> for geospatial ingest, transforms, tiling, streaming, and analysis â€” designed to plug cleanly into KFMâ€™s <b>provenance-first</b> + <b>clean architecture</b> pipeline. ğŸŒ¾ğŸ—ºï¸
</p>

---

## ğŸ§­ Quick nav

- [âœ¨ What lives in `tools/rs/`](#-what-lives-in-toolsrs)
- [ğŸ§± Non-negotiables](#-non-negotiables)
- [ğŸ§¬ KFM pipeline placement](#-kfm-pipeline-placement)
- [ğŸ“‘ Data + metadata contracts](#-data--metadata-contracts)
- [ğŸ§° CLI contract](#-cli-contract)
- [ğŸš€ Quick start](#-quick-start)
- [ğŸ§ª Testing strategy](#-testing-strategy)
- [ğŸ” Reproducible builds](#-reproducible-builds)
- [âš¡ Performance at scale](#-performance-at-scale)
- [ğŸ”’ Security defaults](#-security-defaults)
- [ğŸ§‘â€âš–ï¸ Ethics & governance](#-ethics--governance)
- [ğŸ§‘â€ğŸ’» Adding a new Rust tool](#-adding-a-new-rust-tool)
- [ğŸ“¦ Workspace layout](#-workspace-layout)
- [âœ… Definition of Done](#-definition-of-done-for-rust-tools)
- [ğŸ“š Project library](#-project-library-why-these-tools-are-designed-this-way)
- [ğŸ”— Related](#-related)

---

## âœ¨ What lives in `tools/rs/`

This folder is the **Rust sidecar** for KFM: small, sharp, composable tools that are easiest to do in Rust because we want:

- âš¡ **Speed + memory safety** (big rasters, big vectors, big graphs)
- ğŸ” **Determinism + reproducibility** (stable outputs, stable ordering, stable provenance)
- ğŸ§© **Portability** (CLI binaries, containers, and optionally WASM modules)
- ğŸ§¾ **Provenance-first outputs** (metadata + citations are first-class artifacts)
- ğŸ§± **Clean architecture boundaries** (Rust as adapters/accelerators, not domain â€œtruthâ€)

> [!NOTE]
> The â€œsource of truthâ€ for platform architecture remains KFMâ€™s governed docs. Rust tools should **not** re-implement business rules already defined elsewhere â€” they should accelerate well-defined tasks behind clean interfaces. ğŸ§¼

---

## ğŸ§± Non-negotiables

### 1) Provenance-first by default ğŸ§¾
Every Rust tool MUST be able to explain:

- âœ… **what went in** (inputs + versions + hashes)
- âœ… **what happened** (steps + parameters + environment)
- âœ… **what came out** (artifacts + checksums + schema)
- âœ… **how to reproduce** (exact command + config + seeds)

Practical rules:

- Always emit a **machine-readable run record** (JSON/JSONL) alongside outputs.
- Always emit an **artifact manifest** (file list + hashes).
- Prefer **content addressing** (hashes) and **stable identifiers** for generated artifacts.
- Keep **metadata & citations next to the data** (donâ€™t bury truth in logs).

> [!TIP]
> Treat provenance like a **data product**, not a debug feature.

---

### 2) Pipeline ordering is absolute ğŸ§±â¡ï¸ğŸ—‚ï¸â¡ï¸ğŸ•¸ï¸â¡ï¸ğŸ§ â¡ï¸ğŸ–¥ï¸
KFM has a strict pipeline invariant:

**ETL â†’ Catalogs (STAC/DCAT/PROV) â†’ Graph â†’ API â†’ UI â†’ Story Nodes â†’ Focus Mode**

Rust tools live primarily in:
- ğŸ§± **ETL/processing**
- ğŸ—‚ï¸ **Catalog/metadata generation + validation**
- ğŸŒŠ **Ingest/stream normalization**

Rust tools MUST NOT:
- âŒ publish anything to the graph/UI without catalog + provenance
- âŒ embed â€œstoryâ€ narrative as if it were data truth
- âŒ bypass the governed API boundary (UI never hits the graph directly)

> [!IMPORTANT]
> If it isnâ€™t cataloged + provenanced, it doesnâ€™t exist (for KFM). âœ…

---

### 3) Deterministic + idempotent transformations ğŸ”
KFM tools must behave like â€œrerunnable scienceâ€:

- Same inputs + same config + same seed â‡’ **bitwise-stable outputs** (where feasible)
- Re-running a pipeline should be **safe** (no surprise mutations; avoid side effects)

**Do this:**
- Stable sorting of features/rows
- Canonical JSON output (sorted keys, consistent formatting)
- Explicit RNG seeding (`--seed`, recorded in provenance)
- Explicit CRS/units/timezones (never â€œguessâ€)

---

### 4) Stable identifiers (meaning-free) ğŸ†”
Identifiers are **not a place to store meaning**. Avoid IDs that embed â€œKansas-2026-County-05â€.

Recommended patterns:
- **Dataset IDs**: stable, human-friendly slug + version (in catalog)
- **Artifact IDs**: hash-based (sha256/blake3) for content addressing
- **Run IDs**: UUIDv7 (time-sortable) or hash of (inputs+params+tool version)

> [!TIP]
> A name can change. A stable ID shouldnâ€™t.

---

### 5) Clean boundaries (ports & adapters) ğŸ§©
Rust tools should behave like **well-bounded components**:

- Minimal assumptions about infrastructure
- Explicit inputs/outputs
- Replaceable backends (filesystem vs object storage, PostGIS vs files, etc.)
- Data contracts > â€œmagic behaviorâ€

If youâ€™re adding a new feature, ask:

**Is this domain logic, or a tool adapter/accelerator?**  
If itâ€™s domain logic, it likely belongs outside `tools/rs`.

---

### 6) Human-centered + accountable outputs ğŸ¤
KFMâ€™s ethos: tools should **augment human understanding**, not obscure it.

- Make outputs inspectable (small summaries, optional `.report.md`)
- Make failure modes clear (structured errors)
- Prefer explainable diagnostics over â€œit failedâ€ logs

---

## ğŸ§¬ KFM pipeline placement

Rust tools are â€œenginesâ€ that produce **governed artifacts** for the rest of the system.

### ğŸ§± ETL + Processing
- Convert formats (GeoJSON â‡„ GeoParquet, Shapefile/GPKG â†’ Parquet)
- Validate geometry + CRS
- Produce derived layers (buffers, intersects, filters)
- Compute dataset stats for cataloging (bbox, counts, min/max, histograms)

### ğŸ—‚ï¸ Catalog + provenance
- Generate/update STAC/DCAT metadata
- Emit PROV lineage records (or a KFM run record that can be mapped to PROV)
- Validate license + schema compliance before publish

### ğŸ•¸ï¸ Graph pre-ingest helpers
- Produce normalized node/edge tables (CSV/Parquet) for graph ingestion jobs
- Never â€œwrite to graph directlyâ€ from Rust (keep the boundary clean)

### ğŸ§  Analysis accelerators (bounded)
- Feature engineering primitives
- Graph computations (centrality, clustering, spectral-ish routines)
- Statistical primitives (only when scoped + reproducible)

### ğŸŒŠ Streaming & sensor workloads
- Parse sensor/event feeds
- Windowed aggregations (time buckets, rolling stats)
- Emit append-only results + provenance logs

---

## ğŸ“‘ Data + metadata contracts

### Preferred data formats (KFM-friendly) ğŸ“¦
Pick formats that scale, stream, and validate:

- ğŸ§Š **Vectors (large):** GeoParquet / Parquet (+ Geo metadata)
- ğŸ—ºï¸ **Vectors (small):** GeoJSON (debug/samples only)
- ğŸ§± **Rasters:** Cloud-Optimized GeoTIFF (COG)
- ğŸ§© **Tiles:** PMTiles / MBTiles (vector tiles) + TileJSON-like metadata
- ğŸ§¾ **Tabular/time series:** Parquet (preferred), CSV (ingest-only), JSONL (events)
- ğŸ§  **Graph interchange:** edge/node tables in Parquet/CSV with stable IDs

> [!NOTE]
> â€œSmall and friendlyâ€ formats (GeoJSON, CSV) are great for fixtures/tests â€” not for canonical big data outputs.

---

### Standard artifacts (every tool output) ğŸ§¾
Alongside primary outputs, generate:

| Artifact | Purpose |
|---|---|
| `*.run.json` or `*.prov.json` | machine-readable run record (inputs â†’ steps â†’ outputs) |
| `*.manifest.json` | artifact list + hashes + sizes |
| `*.report.md` (optional) | human-readable summary + sanity checks |
| `*.stac.json` / catalog entry (when applicable) | dataset discoverability + governance fields |

---

### Provenance run record (recommended schema) ğŸ§¾
Keep it boring and predictable. Example:

```json
{
  "run_id": "01J2ZK0YH9GQ8K2K3Z3E6J2C9A",
  "tool": {
    "name": "kfm-geo-convert",
    "version": "0.3.0",
    "git_commit": "abc1234",
    "rustc": "stable",
    "target": "x86_64-unknown-linux-gnu"
  },
  "started_at": "2026-01-14T18:22:10Z",
  "ended_at": "2026-01-14T18:22:14Z",
  "command": ["kfm-geo-convert", "--input", "raw/roads.gpkg", "--output", "processed/roads.parquet"],
  "config": { "path": "pipelines/roads/config.toml", "sha256": "..." },
  "inputs": [
    { "path": "raw/roads.gpkg", "sha256": "...", "media_type": "application/geopackage+sqlite3" }
  ],
  "parameters": {
    "crs_out": "EPSG:4326",
    "stable_sort": true
  },
  "outputs": [
    { "path": "processed/roads.parquet", "sha256": "...", "role": "primary" },
    { "path": "processed/roads.manifest.json", "sha256": "...", "role": "manifest" }
  ],
  "stats": { "features": 124812, "bbox": [-101.2, 36.9, -94.6, 40.0] },
  "warnings": [],
  "errors": []
}
```

> [!TIP]
> If your run record canâ€™t recreate the run, itâ€™s a log â€” not provenance. ğŸ§¾

---

### Naming + versioning conventions ğŸ·ï¸
- Prefer `dataset_id` + `version` in filenames and directories
- Record all versions in metadata:
  - tool version
  - schema version
  - dataset version
  - source version (URL + retrieval date + hash when possible)

Examples:
- `data/processed/<dataset_id>/<version>/...`
- `data/catalog/<dataset_id>/<version>.json`
- `data/prov/<dataset_id>/<version>/<run_id>.run.json`

---

## ğŸ§° CLI contract

Even if we have multiple binaries, they should feel consistent.

### Standard flags
- `--input <path|url>`
- `--output <path>`
- `--config <path>` (âœ… default: **TOML**)
- `--format <...>` (geojson / parquet / gpkg / pmtiles / mbtiles / cog / â€¦)
- `--dry-run` (prints plan + expected artifacts)
- `--emit-provenance <path|dir>` (or emit by default next to output)
- `--seed <u64>` (if randomness exists at all)
- `--threads <n>` (only if parallelism affects determinism; otherwise auto)

### Standard exit behavior
- `0` success
- Non-zero for validation failures, missing inputs, or runtime errors
- Errors MUST be structured and actionable (print â€œwhat + how to fixâ€)

### Output expectations
- `--help` contains:
  - one-line purpose
  - at least **2 examples**
  - artifact list (â€œwhat files will be writtenâ€)

---

## ğŸš€ Quick start

### âœ… Prereqs
- Rust toolchain (stable): `rustup` + `cargo`
- (Optional) native deps depending on chosen crates:
  - GDAL/PROJ (if linking)
  - GEOS (if using GEOS-backed ops)
  - Postgres client libs (if connecting to PostGIS)

### ğŸ§± Build
From repo root:

```bash
cd tools/rs
cargo build --release
```

### ğŸ§ª Test + lint
```bash
cargo test
cargo fmt --all
cargo clippy --all-targets --all-features -- -D warnings
```

### ğŸ” Supply chain + license checks (recommended)
```bash
cargo audit
# Optional (if adopted):
# cargo deny check
# cargo vet
```

### ğŸ“¦ Install a local tool (example)
```bash
cargo install --path tools/rs/crates/<crate-name>
```

---

## ğŸ§ª Testing strategy

Rust tools touch real-world messy data. Test like it.

### âœ… Required test layers
- **Unit tests**: pure logic, parsing, normalization
- **Integration tests**: run CLI against `fixtures/`
- **Contract/golden tests**: compare outputs to checked-in â€œgoldensâ€
- **Property tests** (optional but strong): geometry invariants, round-trips
- **Fuzzing** (recommended for parsers): `cargo fuzz` for untrusted inputs
- **Benchmarks** (when performance matters): `cargo bench` + dataset notes

> [!IMPORTANT]
> Golden tests must be deterministic. If floating point or parallelism causes drift, fix it or record it explicitly.

---

## ğŸ” Reproducible builds

### Build reproducibility
- Commit `Cargo.lock` for binaries (workspace policy)
- Prefer `cargo build --locked` in CI
- Record `git_commit`, tool version, target triple in `--version` output
- Consider `--remap-path-prefix` for path-stable builds (advanced)

### Runtime reproducibility
- Stable ordering for any collection output
- Canonical serialization for JSON
- Explicit CRS and axis order (never â€œguessâ€)
- Explicit seed handling
- Capture environment details in provenance

---

## âš¡ Performance at scale

Rust is here because â€œfast enoughâ€ matters â€” but only if it stays correct.

### Practical performance rules
- Stream I/O (donâ€™t load entire datasets unless required)
- Chunked processing for rasters and very large vectors
- Use columnar formats (Parquet/Arrow) for scan-heavy work
- Avoid row-by-row DB patterns:
  - batch inserts
  - COPY when appropriate
  - use indexes intentionally (and measure)

### Parallelism without nondeterminism
- Parallelize **pure** workloads where ordering doesnâ€™t matter
- If you must reduce floats in parallel:
  - use stable reduction strategies
  - document acceptable tolerance
  - record it in run record

### â€œFuture hardwareâ€ mindset ğŸ§ âš™ï¸
Design tools so we can adopt:
- more cores / SIMD
- persistent memory / new storage tiers
- JIT compilation for hot query paths (where justified)
- GPU acceleration (via separate adapter/service if needed)

---

## ğŸ”’ Security defaults

These tools will process untrusted files. Default posture: **assume hostile input**.

- Avoid `unsafe` unless benchmarked + reviewed
- Validate sizes/limits:
  - decompression bombs (ZIP, TIFF, PNG)
  - JSON depth/size
  - geometry complexity (self-intersections, pathological rings)
- Safe path handling:
  - never overwrite unless `--force`
  - write temp â†’ fsync â†’ atomic rename
- Add CI checks (recommended):
  - `cargo audit`
  - dependency review
  - license allowlist/denylist

> [!TIP]
> Parsers are attack surfaces. Fuzz them.

---

## ğŸ§‘â€âš–ï¸ Ethics & governance

KFMâ€™s â€œdigital humanismâ€ stance means:
- We favor transparency over opacity
- We avoid black-box automation in high-stakes contexts
- We preserve accountability (who ran what, when, on which evidence)

Rust tooling implications:
- Never emit â€œauthoritative interpretationsâ€ without evidence references
- Prefer **evidence-first outputs** (data + provenance), not narrative claims
- When tools compute model outputs:
  - document limitations
  - include uncertainty where feasible
  - make assumptions explicit in metadata/report

---

## ğŸ§‘â€ğŸ’» Adding a new Rust tool

1) Create a crate (binary or library):

```bash
cd tools/rs
cargo new crates/<your-crate-name> --bin
```

2) Implement the **KFM contract**:
- `--help` with examples
- `--dry-run`
- deterministic output
- run record + manifest

3) Add tests:
- fixtures (small)
- golden outputs
- CI-friendly runtime (fast)

4) Document:
- one-line purpose
- examples
- input/output formats
- artifacts emitted

> [!NOTE]
> Default to **Python** unless performance, portability, safety, or determinism strongly argue for Rust. ğŸ§ 

---

## ğŸ“¦ Workspace layout

> If the repo already has a Rust layout, follow it. If not, this is the recommended default.

```text
tools/rs/ ğŸ¦€
â”œâ”€ ğŸ§° Cargo.toml                    # workspace manifest
â”œâ”€ ğŸ§· rust-toolchain.toml           # pinned toolchain (recommended)
â”œâ”€ ğŸ“¦ crates/                       # Rust crates live here
â”‚  â”œâ”€ ğŸ§­ kfm-cli/                   # unified CLI (optional)
â”‚  â”œâ”€ ğŸ§¾ kfm-prov/                  # run records + manifests + hashing + IDs
â”‚  â”œâ”€ ğŸ—‚ï¸ kfm-catalog/               # STAC/DCAT helpers + metadata validation
â”‚  â”œâ”€ ğŸŒ kfm-geo/                   # vector/raster utilities + conversions
â”‚  â”œâ”€ ğŸ§± kfm-tiles/                 # PMTiles/MBTiles/COG/tiling helpers
â”‚  â”œâ”€ ğŸŒŠ kfm-stream/                # sensor/event ingestion + windowing
â”‚  â”œâ”€ ğŸ•¸ï¸ kfm-graph/                 # graph analytics helpers (optional)
â”‚  â””â”€ ğŸ§ª kfm-sim/                   # simulation + VVUQ helpers (optional)
â”œâ”€ ğŸ§ª fixtures/                     # small test datasets (never huge)
â”œâ”€ ğŸ“ schemas/                      # JSON schemas / contracts (optional, recommended)
â””â”€ ğŸ“˜ README.md                     # you are here
```

---

## ğŸ§ª Reproducibility & scientific rigor checklist

When your Rust tool produces **analytical** or **simulation** outputs, include:

- âœ… **Verification** (did we build the thing right?)
- âœ… **Validation** (does output match reality / reference data where possible?)
- âœ… **Uncertainty quantification** (sensitivity / Monte Carlo / confidence intervals)
- âœ… **Experiment design metadata** (scenarios, seeds, run counts, timestep, etc.)
- âœ… **Graph + catalog compatibility** (results are ingestible and evidence-linked)

> [!IMPORTANT]
> If you canâ€™t reproduce it from the run record + inputs, itâ€™s not done.

---

## âœ… Definition of Done for Rust tools

- [ ] `cargo test` âœ…
- [ ] `cargo fmt` âœ…
- [ ] `cargo clippy` âœ… (warnings treated as errors)
- [ ] Tool has `--help` with at least **2 examples**
- [ ] Deterministic outputs (stable ordering)
- [ ] Tool emits provenance artifacts (run record + manifest)
- [ ] Contract test(s) exist with small fixtures
- [ ] Security checks considered (`cargo audit` minimum)
- [ ] Docs updated (this README + crate README if needed)

---

## ğŸ“š Project library (why these tools are designed this way)

<details>
<summary><b>Click to expand ğŸ“¦</b> (Project reference texts & docs)</summary>

### ğŸ§­ KFM architecture, standards, governance
- **Kansas Frontier Matrix (KFM) â€“ Comprehensive Technical Documentation.pdf**
- **MARKDOWN_GUIDE_v13.md.gdoc** (pipeline invariants, governed docs structure, evidence-first rules)
- **Comprehensive Markdown Guide_ Syntax, Extensions, and Best Practices.docx**
- **Data Spaces.pdf** (ports/adapters, microservices, data ecosystems)

### ğŸŒ GIS, cartography, remote sensing, mapping UX
- **python-geospatial-analysis-cookbook.pdf**
- **PostgreSQL Notes for Professionals - PostgreSQLNotesForProfessionals.pdf** (PostGIS + DB ops)
- **Database Performance at Scale.pdf** (workload patterns + performance strategy)
- **making-maps-a-visual-guide-to-map-design-for-gis.pdf**
- **Mobile Mapping_ Space, Cartography and the Digital - 9789048535217.pdf**
- **Archaeological 3D GIS_26_01_12_17_53_09.pdf**
- **Cloud-Based Remote Sensing with Google Earth Engine-Fundamentals and Applications.pdf**
- **webgl-programming-guide-interactive-3d-graphics-programming-with-webgl.pdf**
- **responsive-web-design-with-html5-and-css3.pdf**
- **compressed-image-file-formats-jpeg-png-gif-xbm-bmp.pdf**

### ğŸ“ˆ Statistics, modeling, simulation, inference
- **Scientific Modeling and Simulation_ A Comprehensive NASA-Grade Guide.pdf**
- **Understanding Statistics & Experimental Design.pdf**
- **regression-analysis-with-python.pdf**
- **Regression analysis using Python - slides-linear-regression.pdf**
- **graphical-data-analysis-with-r.pdf**
- **think-bayes-bayesian-statistics-in-python.pdf**
- **Generalized Topology Optimization for Structural Design.pdf**
- **Spectral Geometry of Graphs.pdf**
- **Understanding Machine Learning: From Theory to Algorithms.pdf**

### ğŸ§± Systems, performance, concurrency
- **Scalable Data Management for Future Hardware.pdf**
- **concurrent-real-time-and-distributed-programming-in-java-threads-rtsj-and-rmi.pdf**

### ğŸ§‘â€âš–ï¸ Ethics, law, safety, security
- **Introduction to Digital Humanism.pdf**
- **On the path to AI Lawâ€™s prophecies and the conceptual foundations of the machine learning age.pdf**
- **ethical-hacking-and-countermeasures-secure-network-infrastructures.pdf**
- **Gray Hat Python - Python Programming for Hackers and Reverse Engineers (2009).pdf**

### ğŸ§° Programming reference library (compilations)
- **A programming Books.pdf**
- **B-C programming Books.pdf**
- **D-E programming Books.pdf**
- **F-H programming Books.pdf**
- **I-L programming Books.pdf**
- **M-N programming Books.pdf**
- **O-R programming Books.pdf**
- **S-T programming Books.pdf**
- **U-X programming Books.pdf**

</details>

---

## ğŸ”— Related

- ğŸ“ `api/` (Python backend services)
- ğŸŒ `web/` (frontend + map UI)
- ğŸ—‚ï¸ `data/` (raw/processed/catalog artifacts)
- ğŸ§¾ `docs/standards/` (metadata profiles & governance, if present)

> [!TIP]
> When in doubt: keep Rust tools small, explicit, testable, and provenance-complete. ğŸŒ¾ğŸ§¾
