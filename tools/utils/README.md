<div align="center">

# ğŸ§° Kansas Frontier Matrix â€” **Tools & Utilities**  
`/tools/utils/`

**Automation Â· Integrity Â· Reproducibility**  
*â€œEvery Tool Leaves a Trace â€” Every Trace Ensures Reproducibility.â€*

[![Docs Â· MCP](https://img.shields.io/badge/Docs-MCP-blue)](../../docs/)
[![Build & Test](https://img.shields.io/github/actions/workflow/status/bartytime4life/Kansas-Frontier-Matrix/ci.yml?label=Build%20%26%20Test)](../../.github/workflows/ci.yml)
[![STAC Validate](https://img.shields.io/badge/STAC-Validated-blueviolet.svg)](../../.github/workflows/stac-validate.yml)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](../../LICENSE)
[![Maturity: Production](https://img.shields.io/badge/Maturity-Production-orange)](../../docs/)

</div>

---

## ğŸ§­ Overview

The `/tools/utils/` directory contains **shared command-line utilities** and **automation scripts** used throughout the **Kansas Frontier Matrix (KFM)** project.  

These tools uphold the **Master Coder Protocol (MCP)** â€” emphasizing **documentation-first**, **reproducibility**, and **traceability** â€” by automating data validation, provenance logging, and pipeline reproducibility.

Key functions include:

- ğŸ” **Checksum validation** (SHA-256 integrity verification)  
- ğŸŒ **Geospatial conversions** (Shapefile â†’ GeoJSON / COG)  
- ğŸ§© **STAC & JSON Schema validation**  
- âš™ï¸ **Data fetching, cleaning, and transformation**  
- ğŸ§® **Provenance logging and trace audits**  
- ğŸ§¹ **Linting & documentation checks for CI/CD pipelines**

Each utility script is self-contained, idempotent, and versioned â€” every action leaves an auditable trace in the projectâ€™s provenance log.

---

## ğŸ—‚ï¸ Directory Layout

```bash
tools/
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ checksum.py           # Compute & verify SHA-256 hashes
â”‚   â”œâ”€â”€ convert_geojson.py    # Convert shapefiles or CSVs to GeoJSON
â”‚   â”œâ”€â”€ generate_stac.py      # Build & validate STAC catalogs
â”‚   â”œâ”€â”€ validate_json.py      # Validate JSON files against schemas
â”‚   â”œâ”€â”€ lint_markdown.sh      # Run markdown linting & link checks
â”‚   â”œâ”€â”€ fetch_remote.py       # Download or fetch from APIs w/ logging
â”‚   â”œâ”€â”€ summarize_logs.py     # Summarize CI / pipeline logs
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ README.md             # (this file)
â””â”€â”€ ...

Each script follows the MCP-DL protocol:
	1.	ğŸ§¾ Docstring header â€” author, version, date, purpose
	2.	âš™ï¸ CLI entrypoint using argparse or click
	3.	ğŸ§± Logging and error handling (logs â†’ /logs/utils/)
	4.	ğŸ“œ Provenance entry (timestamp, commit ID, result)
	5.	ğŸ§ª Unit tests in tests/tools/test_utils.py

â¸»

âš™ï¸ Usage Examples

Run utilities individually or through the Makefile:

# âœ… Validate all source JSON manifests
python tools/utils/validate_json.py data/sources/

# ğŸŒ Generate STAC catalog from processed data
python tools/utils/generate_stac.py --input data/processed/ --output data/stac/

# ğŸ” Verify data integrity
python tools/utils/checksum.py verify --dir data/raw/

# ğŸ§¹ Lint and check Markdown docs
bash tools/utils/lint_markdown.sh

These scripts integrate with CI/CD workflows:
	â€¢	.github/workflows/ci.yml
	â€¢	.github/workflows/stac-validate.yml
	â€¢	Makefile â†’ make validate, make checksums, make stac

â¸»

ğŸ§® Dependencies

Category	Library / Tool	Purpose
Python 3.11+	jsonschema	Validate metadata & STAC files
	requests, urllib3	Fetch APIs / remote datasets
	pystac, pystac-client	STAC indexing and validation
	hashlib, argparse	Core CLI / hashing / logging
System	make, jq, bash	Command automation & parsing
Linting	markdownlint, linkcheck	CI documentation QA

Install dependencies via:

pip install -r requirements.txt


â¸»

ğŸ§± Integration with MCP Workflows

All utilities contribute to MCP verification chains:
	â€¢	ğŸª¶ Provenance Logging â†’ Every operation records [timestamp, user, commit, action]
	â€¢	ğŸ”„ Reproducibility â†’ Deterministic outputs, verifiable hashes
	â€¢	ğŸ§© Validation â†’ STAC 1.0, DCAT 2.0, GeoJSON schema checks
	â€¢	ğŸ”” Automation Hooks â†’ Triggered via pre-commit, CI workflows, or scheduled jobs

This ensures full auditability of all data operations â€” every fetch, hash, or conversion can be retraced through commit history and log archives.

â¸»

ğŸ§  Best Practices
	â€¢	Keep utilities stateless and idempotent.
	â€¢	Include --help and example usage in every CLI.
	â€¢	Never hardcode paths â€” use relative or config variables.
	â€¢	Log all results to /logs/provenance.log.
	â€¢	Follow PEP 8 + Black formatting + type hints.
	â€¢	Document each new utility in this README under Directory Layout.
	â€¢	Add unit tests for every tool added or modified.

â¸»

ğŸ§¾ Contribution Workflow

When adding a new tool:
	1.	Create the script in /tools/utils/ (verb_noun.py convention).
	2.	Add inline header:

# Tool: fetch_remote.py
# Version: 1.3.0
# Author: @kfm-architecture
# License: MIT
# Last Updated: 2025-10-16


	3.	Write unit tests â†’ tests/tools/test_utils.py.
	4.	Update this README (under Layout).
	5.	Add Makefile target or CI trigger if applicable.
	6.	Ensure provenance logging works.
	7.	Commit with Signed-off-by: and include Docs: Updated tools/utils/README.md.

â¸»

ğŸ§¾ Version History

Version	Date	Author	Description
v1.0.0	2025-10-16	@kfm-architecture	Initial release â€” baseline utilities doc
v1.1.0	â€”	â€”	â€”


â¸»

ğŸ“š References
	â€¢	ğŸ§­ Architecture Overview
	â€¢	ğŸ—‚ï¸ Data & File Architecture
	â€¢	ğŸ“˜ MCP Markdown Standards
	â€¢	ğŸ›°ï¸ STAC Specification 1.0
	â€¢	ğŸ“– W3C DCAT 2.0 Recommendation

â¸»


<div align="center">


ğŸ—ï¸ Kansas Frontier Matrix â€” A Living Atlas of Time Â· Terrain Â· History
Maintained under the Master Coder Protocol v6.2

</div>
```