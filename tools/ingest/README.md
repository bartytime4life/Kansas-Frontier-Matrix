# ğŸ§° tools/ingest â€” KFM Ingest Toolkit

![KFM](https://img.shields.io/badge/KFM-living_atlas-2b7a78)
![Provenance](https://img.shields.io/badge/provenance--first-required-1f6feb)
![Contracts](https://img.shields.io/badge/contract--first-schemas_%26_specs-6f42c1)
![Catalogs](https://img.shields.io/badge/STAC%2FDCAT%2FPROV-boundary_artifacts-0b7285)
![Determinism](https://img.shields.io/badge/deterministic_ETL-idempotent_%E2%9C%85-22863a)

> ğŸ§­ **Purpose:** Turn heterogeneous Kansas data (historical + scientific + spatiotemporal) into **governed evidence artifacts** that downstream systems can trust: catalogs â†’ graph â†’ API â†’ UI â†’ narratives.  
> ğŸ”’ **Rule of thumb:** If it isnâ€™t in **STAC/DCAT/PROV**, it doesnâ€™t ship.

---

## ğŸ“Œ What lives in `tools/ingest/`

This directory is the home for **ingestion utilities**: scripts and small libraries that help bring new datasets into KFMâ€™s canonical pipeline (fetch â†’ normalize â†’ publish catalogs â†’ validate â†’ optionally graph-load).

It complements (not replaces) domain pipelines, which are expected to live under `src/pipelines/`.:contentReference[oaicite:0]{index=0}

âœ… Typical responsibilities:
- ğŸ§² **Acquire** external sources (downloaders, API pullers, archive extractors)
- ğŸ§¼ **Normalize** formats (vector/raster/doc/time-series), reproject, clean, dedupe
- ğŸ§¾ **Publish catalogs** (STAC/DCAT/PROV) as first-class outputs
- ğŸ§ª **Validate** metadata + content (CI-friendly)
- ğŸ“¦ **Version** large artifacts (DVC-friendly patterns for rasters/models)

ğŸš« Not the place for:
- One-off analysis notebooks (belongs in `notebooks/` / `mcp/`)
- UI-only transformations (the UI consumes **cataloged** outputs, not ad-hoc files)
- Untracked â€œmystery layersâ€ (anything shown must have lineage)

---

## ğŸ§± Nonâ€‘negotiables (pipeline invariants)

1) **Contractâ€‘first** ğŸ“œ  
Schemas and API contracts are first-class repo artifacts; changes require strict versioning + compatibility checks.:contentReference[oaicite:1]{index=1}

2) **Deterministic / idempotent ETL** ğŸ”  
Transformations are config-driven and repeatable: same inputs + same config â‡’ same outputs, fully logged.:contentReference[oaicite:2]{index=2}

3) **Provenanceâ€‘first publishing** ğŸ§¬  
KFM treats lineage as fundamental; publish **STAC/DCAT/PROV** before anything reaches graph, APIs, UI, or story nodes.:contentReference[oaicite:3]{index=3}

4) **Catalog boundary artifacts are required** ğŸ§¾  
STAC + DCAT + PROV are required â€œinterfacesâ€ to downstream stages (graph/API/UI).:contentReference[oaicite:4]{index=4}

5) **Evidence artifacts are datasets** ğŸ§   
Any derived output (OCR, model predictions, simulations, change maps) must be stored + cataloged + traced like any other dataset.:contentReference[oaicite:5]{index=5}

---

## ğŸ—ºï¸ Canonical flow (data â†’ catalogs â†’ graph â†’ API â†’ UI â†’ narrative)

```mermaid
flowchart LR
  subgraph Data
    A["Raw Sources"] --> B["ETL + Normalization"]
    B --> C["STAC Items + Collections"]
    C --> D["DCAT Dataset Views"]
    C --> E["PROV Lineage Bundles"]
  end

  C --> G["Neo4j Graph (references back to catalogs)"]
  G --> H["API Layer (contracts + redaction)"]
  H --> I["Map UI â€” React Â· MapLibre Â· (optional) Cesium"]
  I --> J["Story Nodes (governed narratives)"]
  J --> K["Focus Mode (provenance-linked context bundle)"]
```

(Flow adapted from the KFM Master Guide v13.):contentReference[oaicite:6]{index=6}

---

## ğŸ—‚ï¸ Data staging + catalog outputs

### ğŸ“¦ Expected staging (per domain)

KFM standardizes where â€œrawâ€, â€œworkâ€, and â€œprocessedâ€ live so itâ€™s always obvious what stage a file is in.:contentReference[oaicite:7]{index=7}

```text
data/
â”œâ”€â”€ ğŸ›°ï¸ stac/                     # STAC metadata layer (Collections + Items pointing to assets/artifacts)
â”‚   â”œâ”€â”€ ğŸ—‚ï¸ collections/          # STAC Collections (dataset-level metadata: extent/license/providers/links)
â”‚   â””â”€â”€ ğŸ§· items/                # STAC Items (time/run snapshots: assets, roles, hrefs, timestamps)
â”œâ”€â”€ ğŸ—‚ï¸ catalog/
â”‚   â””â”€â”€ ğŸ—‚ï¸ dcat/                 # DCAT discovery records (dataset + distributions + access/license metadata)
â”œâ”€â”€ ğŸ§¬ prov/                     # PROV lineage bundles (JSON-LD) linking rawâ†’workâ†’processed + agents/tools/params
â”œâ”€â”€ ğŸ—‚ï¸ <domain>/                 # Domain bucket (keeps related raw/work/processed together for a domain)
â”‚   â”œâ”€â”€ ğŸ“¥ raw/                  # Read-only originals (or DVC pointers); immutable evidence boundary
â”‚   â”œâ”€â”€ ğŸ§ª work/                 # Intermediate artifacts (scratch, checkpoints); rebuildable staging
â”‚   â””â”€â”€ âœ… processed/             # Publishable outputs (versioned artifacts promoted for UI/API/graph use)
â””â”€â”€ âœ…ğŸ“„ README.md                # ğŸ‘ˆ you are here ğŸ“Œ Naming/versioning rules + promotion lanes + validation expectations
```

Catalog outputs locations (required):
- **STAC**: `data/stac/collections/` + `data/stac/items/`:contentReference[oaicite:8]{index=8}
- **DCAT**: `data/catalog/dcat/`:contentReference[oaicite:9]{index=9}
- **PROV**: `data/prov/`:contentReference[oaicite:10]{index=10}

---

## ğŸ§¾ Ingest run contract (what every ingest must produce)

A typical pipeline/ingest job is expected to:
1) **Fetch** source data (often using `data/sources` manifests for URLs)  
2) **Transform** (format conversions, cleaning, joins, simulations)  
3) **Output standardized results** to `data/processed/â€¦` and **create/update metadata + provenance logs**:contentReference[oaicite:11]{index=11}

> Example from KFM design: a historical map pipeline may download an image, georeference it, output COG/tiles, and write STAC metadata for that layer.:contentReference[oaicite:12]{index=12}

---

## ğŸš€ Quick start (recommended CLI shape)

> âš ï¸ This is a **spec** for how ingest tooling should feel. If your repo already has commands, map them to this mental model.

```bash
# 1) Fetch raw inputs
python -m tools.ingest fetch <dataset_id>

# 2) Transform â†’ produce publishable artifacts
python -m tools.ingest transform <dataset_id>

# 3) Publish catalogs (STAC/DCAT/PROV) for the outputs
python -m tools.ingest publish <dataset_id>

# 4) Validate everything (local + CI)
python -m tools.ingest validate <dataset_id|all>

# 5) (Optional) Load/refresh graph exports
python -m tools.ingest graph-export <dataset_id>
```

### ğŸ§· Dataset IDs (naming convention)

Use a stable, grep-friendly ID:
- `kfm.<domain>.<topic>.<source>.<version_or_year>`
- Example: `kfm.historical.maps.usgs_topo.1890`

---

## ğŸ§© Adding a new dataset (the â€œhappy pathâ€)

1) ğŸ“ **Register the source**
   - Add/extend a source manifest (e.g., `data/sources/<domain>/<dataset_id>.json|yaml`)
   - Include license + citation + source URL(s) + known caveats

2) ğŸ§± **Create the ingest adapter**
   - Implement `fetch â†’ normalize â†’ publish` steps
   - Keep it config-driven (no hard-coded paths beyond the canonical layout)

3) ğŸ—ºï¸ **Write outputs**
   - Put publishable files in `data/<domain>/processed/<dataset_id>/â€¦`
   - If large: use DVC patterns to avoid bloating git (version data separately).:contentReference[oaicite:13]{index=13}

4) ğŸ§¾ **Generate catalog artifacts**
   - STAC Collection + Items
   - DCAT dataset entry (JSON-LD)
   - PROV bundle describing inputs, steps, agents, parameters

5) ğŸ§ª **Run validators**
   - Ensure metadata is schema-valid and file references resolve
   - Confirm geometry validity, expected ranges, etc.

6) ğŸ” **Make it repeatable**
   - Re-run from scratch and verify identical outputs for same inputs/config (determinism)

---

## ğŸ›°ï¸ Special handling: large remote sensing + rasters

KFM expects big rasters to be optimized for interactive use:
- **Cloud-Optimized GeoTIFFs (COGs)** for partial HTTP reads (range requests) so clients fetch only what they need.:contentReference[oaicite:14]{index=14}
- Optional **pre-tiling** (XYZ / MBTiles) for heavy base layers to trade storage for speed.:contentReference[oaicite:15]{index=15}
- Optional integration with **Google Earth Engine (GEE)** for at-scale remote sensing access/compute; ingest results back into KFM as cataloged assets.:contentReference[oaicite:16]{index=16}

---

## ğŸ§¬ Evidence artifacts (AI / analysis / simulation outputs)

Anything derived is still a dataset:
- Store as `data/<domain>/processed/â€¦`
- Catalog in STAC/DCAT (mark as derived/AI-generated when appropriate)
- Trace in PROV (inputs, method/model, parameters, confidence):contentReference[oaicite:17]{index=17}

Think: OCR corpora, regression outputs, Bayesian posteriors, land-cover change rasters, simulation outputs, topology optimization results â€” all â€œevidence artifactsâ€.

---

## ğŸ§ª Validation & QA (must be CI-friendly)

KFM tooling includes catalog QA concepts:
- A validator can scan catalog JSON files for required fields (license, spatial extent, etc.)
- Check for broken links/file references
- Run basic data checks (geometry validity, value ranges)
- CI should fail merges on incomplete/inconsistent data:contentReference[oaicite:18]{index=18}

> ğŸ›ï¸ GIS note: validation should support both **query-by-attribute** and **query-by-geography** expectations (SQL + spatial predicates are fundamental to GIS workflows).:contentReference[oaicite:19]{index=19}

---

## âš¡ Performance & scaling notes (ingest at â€œKansas scaleâ€)

KFM ingest needs to handle **mixed spatiotemporal sources** (vector + raster + streams) that often require visualization and cross-source fusion (e.g., join trajectories with remote sensing).:contentReference[oaicite:20]{index=20}

Practical guidance:
- ğŸ§± Prefer chunked processing (tile-based rasters, batch vector ops)
- ğŸ§µ Parallelize safely (bounded workers, deterministic output ordering)
- ğŸ—„ï¸ Index intelligently (PostGIS spatial + temporal indexes; avoid full rescans)
- ğŸ§Š Cache intermediate steps in `data/<domain>/work/` to support restartability

---

## ğŸ›¡ï¸ Security, governance, and ethics

- ğŸ” Never commit secrets (API keys, DB passwords). Use env vars / secret stores.
- ğŸ§¾ Respect licenses: every dataset must declare terms before publication.
- ğŸ§­ Governance: follow FAIR/CARE + sovereignty guardrails (especially for sensitive cultural sites or personal data).:contentReference[oaicite:21]{index=21}
- ğŸ§½ If redaction is required, enforce it **at the API boundary** and record it in provenance.

---

## âœ… Ingest â€œDefinition of Doneâ€ checklist

- [ ] Raw inputs stored (or referenced) under `data/<domain>/raw/â€¦`
- [ ] Processed outputs written under `data/<domain>/processed/â€¦`
- [ ] STAC Collection + Items created
- [ ] DCAT entry created (JSON-LD)
- [ ] PROV bundle created (inputs, activity, agent, params)
- [ ] Validator passes locally and in CI
- [ ] Re-run is deterministic (same inputs/config â‡’ same outputs)
- [ ] License + attribution is complete

---

## ğŸ“š Project library (how each file informs ingest)

> ğŸ§  This repoâ€™s ingest approach is intentionally â€œover-documentedâ€: we keep a living technical library so contributors can implement pipelines with shared assumptions.

### ğŸ§± Core KFM governance & architecture
- **Kansas Frontier Matrix (KFM) â€“ Comprehensive Technical Documentation** â€” architecture + pipelines + contract/provenance rules. :contentReference[oaicite:22]{index=22} :contentReference[oaicite:23]{index=23}
- **MARKDOWN_GUIDE_v13.md.gdoc** â€” canonical pipeline ordering + directory layout + STAC/DCAT/PROV policy. :contentReference[oaicite:24]{index=24}
- **Kansas-Frontier-Matrix_ Open-Source Geospatial Historical Mapping Hub Design** â€” DVC strategy, reproducibility focus, repo structure ideas. :contentReference[oaicite:25]{index=25}

### ğŸ—ºï¸ GIS, mapping, and spatial data engineering
- **python-geospatial-analysis-cookbook.pdf** â€” practical PostGIS/GeoPython patterns for ingest + analysis. :contentReference[oaicite:26]{index=26}
- **making-maps-a-visual-guide-to-map-design-for-gis.pdf** â€” cartographic constraints that inform what â€œprocessedâ€ should look like (symbology-ready layers).
- **Mobile Mapping_ Space, Cartography and the Digital - 9789048535217.pdf** â€” field/mobile data realities (GPS noise, human capture workflows).
- **Archaeological 3D GIS_26_01_12_17_53_09.pdf** â€” 3D assets + spatial archaeology considerations. :contentReference[oaicite:27]{index=27}

### ğŸ›°ï¸ Remote sensing & large rasters
- **Cloud-Based Remote Sensing with Google Earth Engine-Fundamentals and Applications.pdf** â€” GEE-based ingest patterns.
- **compressed-image-file-formats-jpeg-png-gif-xbm-bmp.pdf** â€” scanned maps & imagery format handling.
- **Scalable Data Management for Future Hardware.pdf** â€” spatiotemporal pipelines + performance + robustness ideas. :contentReference[oaicite:28]{index=28}

### ğŸ—„ï¸ Databases, storage, and performance
- **PostgreSQL Notes for Professionals - PostgreSQLNotesForProfessionals.pdf** â€” Postgres/PostGIS operational notes.
- **Database Performance at Scale.pdf** â€” scaling + indexing + workload-aware design. :contentReference[oaicite:29]{index=29}
- **Data Spaces.pdf** â€” federation/interoperability mindset (catalog-first discovery).

### ğŸ§® Statistics, analytics, and â€œevidence artifactâ€ outputs
- **Understanding Statistics & Experimental Design.pdf** â€” QA logic for derived layers (experimental rigor).
- **think-bayes-bayesian-statistics-in-python.pdf** â€” Bayesian evidence artifacts (posteriors as datasets).
- **regression-analysis-with-python.pdf** â€” regression workflows that can produce publishable evidence layers.
- **Regression analysis using Python - slides-linear-regression.pdf** â€” quick reference for regression-based evidence outputs. :contentReference[oaicite:30]{index=30}
- **graphical-data-analysis-with-r.pdf** â€” EDA practices (helps define sanity checks).

### ğŸ§  Machine learning & deep learning
- **Deep Learning for Coders with fastai and PyTorch - â€¦** â€” model training/inference patterns (treat outputs as evidence artifacts; keep model cards). *(Note: file may be restricted in some tooling views.)*
- **Understanding Machine Learning: From Theory to Algorithms** â€” theoretical grounding for model governance. :contentReference[oaicite:31]{index=31}

### ğŸ§¬ Systems, graphs, and simulation
- **Spectral Geometry of Graphs.pdf** â€” graph thinking relevant to Neo4j layer design.
- **Scientific Modeling and Simulation_ A Comprehensive NASA-Grade Guide.pdf** â€” simulation outputs as governed evidence.
- **Generalized Topology Optimization for Structural Design.pdf** â€” example of computational evidence pipelines.
- **Principles of Biological Autonomy - book_9780262381833.pdf** â€” autonomy & systems framing (useful for designing resilient pipelines).

### ğŸŒ Web + visualization constraints (ingest must support UI)
- **webgl-programming-guide-interactive-3d-graphics-programming-with-webgl.pdf** â€” 3D asset formats + performance constraints.
- **responsive-web-design-with-html5-and-css3.pdf** â€” front-end constraints that shape what â€œprocessedâ€ should deliver.

### ğŸ§µ Concurrency & distributed execution references
- **concurrent-real-time-and-distributed-programming-in-java-threads-rtsj-and-rmi.pdf** â€” worker patterns, real-time considerations.
- **B-C programming Books.pdf** â€” concurrency primitives + thread lifecycle patterns.
- **A programming Books.pdf**, **D-E programming Books.pdf**, **F-H programming Books.pdf**, **I-L programming Books.pdf**, **M-N programming Books.pdf**, **O-R programming Books.pdf**, **S-T programming Books.pdf**, **U-X programming Books.pdf** â€” language/tooling references across the stack (ETL, APIs, infra).

### ğŸ›¡ï¸ Security & governance context
- **ethical-hacking-and-countermeasures-secure-network-infrastructures.pdf** â€” threat modeling for data pipelines.
- **Gray Hat Python - Python Programming for Hackers and Reverse Engineers (2009).pdf** â€” secure coding awareness.
- **Introduction to Digital Humanism.pdf** â€” human-centered governance lens.
- **On the path to AI Lawâ€™s prophecies and the conceptual foundations of the machine learning age.pdf** â€” legal/ethical framing for ML evidence artifacts.
- **Flexible Software Design** â€” modularity + change resilience in ingest architecture. :contentReference[oaicite:32]{index=32}

---

## ğŸ§­ Next upgrades (recommended)

- ğŸ§ª Add `tools/ingest/validate_all.py` that runs:
  - metadata schema validation (STAC/DCAT/PROV)
  - file existence checks
  - geometry validity checks
  - raster sanity checks (bounds/CRS)
- ğŸ§¾ Add `tools/ingest/prov/` helpers to generate consistent PROV bundles per run
- ğŸ“¦ Add DVC recipes (`.dvc` patterns) for COGs, tilesets, and model artifacts
- ğŸ§° Add â€œstarter adaptersâ€ (templates) for:
  - ğŸ—ºï¸ historical scanned maps â†’ georeference â†’ COG
  - ğŸ›°ï¸ GEE raster exports â†’ COG/tiles
  - ğŸ§¾ CSV time-series â†’ Parquet + catalog
  - ğŸ§¬ OCR text corpora â†’ indexed evidence artifact

---

