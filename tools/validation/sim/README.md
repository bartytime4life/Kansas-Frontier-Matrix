# ğŸ§ª Simulation Validation Harness (`tools/validation/sim`)

![Status](https://img.shields.io/badge/status-spec%20%2F%20scaffold-yellow)
![Scope](https://img.shields.io/badge/scope-validation%20%2B%20simulation-blue)
![Policy](https://img.shields.io/badge/policy-evidence--first%20%7C%20contract--first-brightgreen)
![Metadata](https://img.shields.io/badge/metadata-STAC%20%7C%20DCAT%20%7C%20PROV-purple)
![Reproducibility](https://img.shields.io/badge/reproducibility-deterministic%20runs-success)
![Security](https://img.shields.io/badge/security-defense--minded-informational)

> **Goal:** run *credible* simulations (hydrology, land-use, economics, archaeology, etc.), validate them rigorously, and publish the outputs as **first-class evidence artifacts** in KFM (with provenance + catalogs) âœ…

---

## ğŸŒ¾ Why this exists

KFMâ€™s north star is **â€œsearchable, mappable, auditable, and modelableâ€** knowledge. Simulation is how we go from *observations* to *scenarios* â€” but only if we can prove:
- where inputs came from ğŸ§¾
- how outputs were produced âš™ï¸
- how well they match reality (or at least expectations) ğŸ“
- how uncertain they are ğŸ²
- how they evolve over time (versioning) ğŸ·ï¸

This directory is the â€œ**VVUQ**â€ corner (Verification, Validation, Uncertainty Quantification) of the project.

---

## âœ… Nonâ€‘negotiables (KFM-aligned)

- **Evidence-first:** sim outputs are treated like any other dataset (published before being interpreted) ğŸ§©  
- **Contract-first:** schemas + API contracts are first-class and versioned (breaking changes require explicit bumps) ğŸ”’  
- **Deterministic pipelines:** config-driven + idempotent transforms (same inputs â†’ same outputs) â™»ï¸  
- **Provenance-first:** every artifact is traceable to sources + processing steps (no black boxes) ğŸ”  
- **Cataloged outputs:** STAC/DCAT/PROV are produced for outputs (not optional) ğŸ—ƒï¸  
- **Governed exposure:** anything shown in UI goes through API gates (classification + redaction rules) ğŸ›¡ï¸  

> If a simulation canâ€™t be explained, reproduced, and cited, it doesnâ€™t ship.

---

## ğŸ—ºï¸ Where this fits in the KFM pipeline

```mermaid
flowchart LR
  A[ğŸ§¾ Scenario config<br/>YAML/JSON] --> B[âš™ï¸ Simulation Runner]
  B --> C[ğŸ“¦ Outputs<br/>raster / vector / tables / reports]
  C --> D[âœ… Validation Suites<br/>schema â€¢ spatial â€¢ stats â€¢ domain]
  D --> E[ğŸ“ˆ Metrics + Findings<br/>CI-friendly summaries]
  C --> F[ğŸ—‚ï¸ Publish as Evidence Artifact<br/>data/processed/...]
  F --> G[ğŸŒ Catalog Boundary Artifacts<br/>STAC â€¢ DCAT â€¢ PROV]
  G --> H[(ğŸ•¸ï¸ Graph refs<br/>optional & governed)]
  G --> I[ğŸ”Œ API]
  I --> J[ğŸ–¥ï¸ UI + ğŸ§  Story Nodes<br/>evidence-linked only]
```

---

## ğŸš€ Quickstart (developer workflow)

> This is written as an **interface contract** for the module. If your repo already has a CLI entrypoint, map these commands to it.

```bash
# 1) From repo root: create an environment (example)
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt

# 2) Run an example scenario
python -m tools.validation.sim.cli run \
  --scenario tools/validation/sim/examples/scenario_demo.yaml \
  --out mcp/runs/demo_run

# 3) Validate outputs
python -m tools.validation.sim.cli validate \
  --run mcp/runs/demo_run \
  --suite tools/validation/sim/suites/core.yaml

# 4) Publish outputs into KFMâ€™s governed data/catalog pipeline
python -m tools.validation.sim.cli publish \
  --run mcp/runs/demo_run \
  --domain hydrology \
  --dataset-slug demo_drought_sim_v0
```

---

## ğŸ§© Core concepts

### 1) Scenario ğŸ§¾
A declarative config (YAML/JSON) that defines:
- model to use
- region of interest (geometry / bbox)
- time range
- parameters + interventions
- input datasets (by stable IDs + versions)
- random seeds (if stochastic)

### 2) Model âš™ï¸
A versioned implementation that can be executed deterministically given:
`(scenario, inputs, seed, environment) â†’ outputs`

### 3) Run ğŸ§ª
A single execution instance that **must** produce a machine-readable manifest.

### 4) Evidence artifact ğŸ“¦
A produced dataset/output that is:
- stored under `data/processed/...` (or staging equivalent)
- cataloged in **STAC/DCAT**
- traced end-to-end in **PROV**
- optionally referenced in the knowledge graph (with provenance links)

---

## ğŸ—‚ï¸ Suggested directory layout

> Keep this folder small, modular, and swappable (clean architecture mindset ğŸ§±).

```text
tools/validation/sim/
â”œâ”€â”€ ğŸ“„ README.md
â”œâ”€â”€ ğŸ cli.py                      # `run | validate | report | publish`
â”œâ”€â”€ ğŸ“ engine/                     # common runtime (seeding, IO, scheduling)
â”œâ”€â”€ ğŸ“ models/                     # model implementations (or adapters)
â”œâ”€â”€ ğŸ“ scenarios/                  # curated scenarios (versioned)
â”œâ”€â”€ ğŸ“ suites/                     # validation suite definitions (YAML)
â”œâ”€â”€ ğŸ“ validators/                 # spatial/statistical/domain validators
â”œâ”€â”€ ğŸ“ metrics/                    # metric computation + aggregations
â”œâ”€â”€ ğŸ“ reports/                    # report templates (HTML/MD) + builders
â””â”€â”€ ğŸ“ examples/                   # minimal runnable demos
```

---

## ğŸ§¾ Run output contract (required)

A run is a folder with **reproducibility boundary artifacts**.

```text
mcp/runs/<run_id>/
â”œâ”€â”€ ğŸ“„ run.manifest.json          # inputs, parameters, seeds, versions, hashes
â”œâ”€â”€ ğŸ“„ metrics.json               # machine-readable metrics summary
â”œâ”€â”€ ğŸ“„ validation.report.json     # validator outputs + pass/fail gates
â”œâ”€â”€ ğŸ“„ logs.txt                   # structured logs preferred
â”œâ”€â”€ ğŸ“ outputs/                   # files produced by the model
â”‚   â”œâ”€â”€ ğŸ—ºï¸ result.tif             # (example raster)
â”‚   â”œâ”€â”€ ğŸ§­ features.geoparquet     # (example vector)
â”‚   â””â”€â”€ ğŸ“Š timeseries.parquet      # (example table)
â””â”€â”€ ğŸ“ catalogs/                  # if publishing from the run directory
    â”œâ”€â”€ ğŸŒ stac_item.json
    â”œâ”€â”€ ğŸŒ dcat_dataset.json
    â””â”€â”€ ğŸŒ prov_bundle.jsonld
```

### Minimal manifest fields (recommendation)

```json
{
  "run_id": "sim-2026-01-14T000000Z-acde123",
  "model": { "id": "hydrology.swat", "version": "0.1.0" },
  "scenario": { "id": "demo_drought", "version": "0.1.0" },
  "inputs": [
    { "dataset_id": "noaa.precip_monthly", "version": "2.0.0", "uri": "data/processed/noaa/precip_v2.parquet", "sha256": "..." }
  ],
  "parameters": { "irrigation_restriction_pct": 25 },
  "random_seed": 12345,
  "time": { "start": "1980-01-01", "end": "2020-12-31" },
  "region": { "crs": "EPSG:4326", "bbox": [-102.0, 36.9, -94.6, 40.0] },
  "code": { "git_sha": "abcdef123456", "dirty": false },
  "environment": { "python": "3.x", "platform": "linux", "deps_lock": "requirements.txt" },
  "outputs": [
    { "path": "outputs/result.tif", "type": "raster", "role": "prediction", "stac_asset_key": "prediction_raster" }
  ]
}
```

---

## âœ… Validation pyramid (what â€œvalidatedâ€ means here)

Think of validation as stacked gates â€” you donâ€™t do fancy stats on broken geometry.

### Gate 0 â€” Smoke / execution
- run completes
- outputs exist
- runtime within budget

### Gate 1 â€” Schema & integrity
- file formats readable
- CRS present + correct
- geometries valid (no self-intersections, etc.)
- nodata/NaN rules respected
- units + ranges plausible

### Gate 2 â€” Spatial correctness (GIS validation)
- overlays and joins behave as expected
- topology constraints hold (if relevant)
- raster alignment / resolution / extent checks
- spatial â€œdiffâ€ against baseline (where applicable)

### Gate 3 â€” Statistical validation
- regression diagnostics: residual bias, error metrics, calibration
- time-series checks: drift, autocorrelation surprises, seasonality mismatch
- classification checks: confusion matrix, PR curves (if applicable)
- uncertainty checks: coverage of prediction intervals (if UQ provided)

### Gate 4 â€” Domain rules (model-specific)
Examples:
- hydrology: water balance constraints, mass conservation checks
- agriculture: crop yield bounds and plausible response curves
- economics: budget constraints, stability checks, sanity of elasticities
- archaeology: viewshed/visibility constraints, terrain-consistent outputs

### Gate 5 â€” Reproducibility & provenance
- manifest complete
- inputs hashed + versioned
- outputs cataloged (STAC/DCAT)
- lineage recorded (PROV)

---

## ğŸ² Uncertainty & Sensitivity (UQ)

If the model is stochastic or parameter-sensitive, we treat UQ as **first-class**:

- Monte Carlo ensembles (multiple seeds) ğŸ¯  
- Bootstrapping & resampling for metrics ğŸ“¦  
- Sensitivity analysis (e.g., OAT, factorial, LHS) ğŸ§ª  
- Bayesian calibration (when warranted) ğŸ§   

**Rule:** every uncertainty artifact must still be **cataloged and traced**, not dumped in notebooks.

---

## ğŸŒ Cataloging & provenance (STAC â€¢ DCAT â€¢ PROV)

Simulation outputs are â€œevidence artifactsâ€ and must be published like any other dataset:

- **Stored** in `data/processed/<domain>/...`  
- **Cataloged** in STAC/DCAT (discovery + spatial/temporal indexing)  
- **Traced** in PROV (inputs â†’ activity â†’ outputs)  
- **Optionally referenced** in graph with explicit provenance pointers  

### Versioning expectations ğŸ·ï¸
- dataset updates produce new versions linked via `prov:wasRevisionOf`
- graph/ontology changes are migrated intentionally (no silent breakage)
- API changes are versioned (OpenAPI is the contract)
- repo releases follow semantic versioning (major/minor/patch)

---

## ğŸ—„ï¸ Storage & performance notes

Simulation outputs can be *large*. Design like itâ€™s going to scale:

- Prefer cloud-optimized / query-friendly formats:
  - **COG** for rasters ğŸ—ºï¸
  - **GeoParquet** for vectors ğŸ§­
  - **Parquet** for tabular/time-series ğŸ“Š
  - **Zarr** where chunked arrays make sense ğŸ§Š
- If storing in Postgres/PostGIS:
  - use spatial indexes (GiST/SP-GiST)
  - partition big tables by time or region
  - precompute aggregates for UI browsing (donâ€™t DDOS yourself)

> Validation should include *performance budgets* (e.g., â€œthis scenario must run under X minutes on CI hardwareâ€).

---

## ğŸ” Governance & security (defense-minded)

Simulation pipelines touch:
- external datasets
- lots of file IO
- databases
- user-authored scenario configs

So we assume adversarial inputs are possible:
- validate & sanitize scenario files âœ…
- avoid dynamic code execution in scenario configs âŒ
- parameterize all database queries âœ…
- restrict published artifacts through API governance âœ…
- treat â€œpredictionsâ€ as potentially sensitive; label them clearly ğŸ·ï¸

---

## ğŸ–¼ï¸ Visualization integration

This tool is not the UI â€” but it should produce UI-friendly artifacts:
- map-ready layers (tiled where needed)
- cartography-aware symbology hints (optional)
- 3D-ready outputs (when relevant) for WebGL/3D GIS workflows
- mobile-friendly summaries (small, legible, responsive)

---

## ğŸ§± Extending this module (the â€œhappy pathâ€)

### Add a new simulation model
1. Create a `models/<model_id>/` folder (or adapter to external engine)
2. Implement the model interface (run + metadata)
3. Add at least:
   - one example scenario
   - one validation suite
   - one baseline/golden dataset comparison (if feasible)
4. Ensure publishing emits STAC/DCAT/PROV

### Add a new validator
1. Add `validators/<name>.py`
2. Define inputs/outputs clearly
3. Add unit tests + at least one failure case fixture
4. Make it composable in suite YAML

### Add a new suite
1. Create `suites/<suite_name>.yaml`
2. Keep suites layered (schema â†’ spatial â†’ stats â†’ domain)
3. Add CI gate semantics: required vs warning-only

---

## ğŸ“š Project library mapping (using *all* project files)

<details>
<summary>ğŸ“– Click to expand (this repoâ€™s PDFs inform both the scientific and systems design)</summary>

### Simulation credibility & VVUQ
- **Scientific Modeling and Simulation_ A Comprehensive NASA-Grade Guide.pdf** â†’ verification/validation/UQ mindset, run discipline
- **Understanding Statistics & Experimental Design.pdf** â†’ DOE planning for simulation campaigns
- **think-bayes-bayesian-statistics-in-python.pdf** â†’ Bayesian calibration + uncertainty reasoning
- **graphical-data-analysis-with-r.pdf** â†’ EDA patterns for output inspection

### Evaluation, regression & ML validation
- **regression-analysis-with-python.pdf** â†’ regression diagnostics + evaluation workflows
- **Regression analysis using Python - slides-linear-regression.pdf** â†’ quick metric/diagnostic patterns
- **I-L programming Books.pdf** â†’ supervised learning validation patterns (for surrogate models)
- **F-H programming Books.pdf** â†’ ML foundations + validation/regularization concepts (for ML components)
- **Understanding Machine Learning (internal reference)** â†’ generalization, bias/variance, learning theory
- **Data Mining - Concepts and Applications (internal reference)** â†’ anomaly detection/pattern mining for outputs

### Geospatial modeling & cartographic quality
- **python-geospatial-analysis-cookbook.pdf** â†’ geospatial transforms, raster/vector ops
- **making-maps-a-visual-guide-to-map-design-for-gis.pdf** â†’ map communication & design quality
- **Cloud-Based Remote Sensing with Google Earth Engine-Fundamentals and Applications.pdf** â†’ remote sensing inputs + validation sources
- **Archaeological 3D GIS_26_01_12_17_53_09.pdf** â†’ 3D GIS domain workflows + evidence handling
- **Mobile Mapping_ Space, Cartography and the Digital - 9789048535217.pdf** â†’ mobile UX implications for map-based evidence
- **webgl-programming-guide-interactive-3d-graphics-programming-with-webgl.pdf** â†’ 3D visualization pipeline
- **responsive-web-design-with-html5-and-css3.pdf** â†’ responsive dashboards for results

### Data management, provenance, and scale
- **PostgreSQL Notes for Professionals - PostgreSQLNotesForProfessionals.pdf** â†’ DB fundamentals
- **Database Performance at Scale.pdf** â†’ indexing, performance budgets, query tuning
- **Scalable Data Management for Future Hardware.pdf** â†’ future-proof performance + heterogeneous execution concepts
- **Data Spaces.pdf** â†’ interoperability + trust/security as cross-cutting concerns

### Domain extensions & systems modeling
- **Principles of Biological Autonomy - book_9780262381833.pdf** â†’ complex systems / agent-based inspiration
- **Spectral Geometry of Graphs.pdf** â†’ network/graph modeling ideas for infrastructure & flows
- **Generalized Topology Optimization for Structural Design.pdf** â†’ structural simulation patterns
- **concurrent-real-time-and-distributed-programming-in-java-threads-rtsj-and-rmi.pdf** â†’ distributed/real-time orchestration ideas

### Governance, ethics, and transparency
- **Introduction to Digital Humanism.pdf** â†’ transparency/accountability expectations around automated systems
- **On the path to AI Lawâ€™s prophecies and the conceptual foundations of the machine learning age.pdf** â†’ governance framing around ML artifacts

### Security & defensive posture
- **ethical-hacking-and-countermeasures-secure-network-infrastructures.pdf** â†’ hardening mindset
- **Gray Hat Python - Python Programming for Hackers and Reverse Engineers (2009).pdf** â†’ â€œknow the attackerâ€ for safer tooling
- **compressed-image-file-formats-jpeg-png-gif-xbm-bmp.pdf** â†’ choose safe + correct media formats for outputs

### Language/tooling breadth (engineering references)
- **A programming Books.pdf**
- **B-C programming Books.pdf**
- **D-E programming Books.pdf**
- **F-H programming Books.pdf**
- **I-L programming Books.pdf**
- **M-N programming Books.pdf**
- **O-R programming Books.pdf**
- **S-T programming Books.pdf**
- **U-X programming Books.pdf**
  - Used as a general toolbox reference for implementations (Python, Bash automation, MATLAB interoperability, mobile/iOS considerations, etc.)

### KFM-specific north star & repo standards
- **Kansas Frontier Matrix (KFM) â€“ Comprehensive Technical Documentation.pdf** â†’ mission, provenance-first, â€œmodelable & auditableâ€
- **MARKDOWN_GUIDE_v13.md.gdoc (Master Guide)** â†’ canonical pipeline ordering + STAC/DCAT/PROV requirements + versioning rules
- **Scientific Method _ Research _ Master Coder Protocol Documentation.pdf** â†’ reproducible research documentation discipline

</details>

---

## ğŸ§¾ What â€œdoneâ€ looks like (Definition of Done)

A simulation contribution is â€œdoneâ€ when:
- âœ… scenario(s) exist and are runnable
- âœ… validation suite(s) exist and pass on CI
- âœ… manifest is complete (hashes, versions, parameters, seeds)
- âœ… outputs are stored in the right place
- âœ… STAC/DCAT/PROV exist and validate against project profiles
- âœ… exposure path is governed (API, labeling, classification if needed)
- âœ… docs are updated (this README + domain runbook if applicable)

---

## ğŸ¤ Contributing

- Keep PRs small and auditable.
- Prefer adding **one scenario + one suite** per PR.
- Never ship uncited outputs into narratives.
- If you need a new schema/contract, treat it as a first-class artifact and version it.

---

## ğŸ§­ Related docs (repo paths)

- `docs/MASTER_GUIDE_v13.md` (canonical pipeline + contracts)
- `docs/standards/KFM_STAC_PROFILE.md`
- `docs/standards/KFM_DCAT_PROFILE.md`
- `docs/standards/KFM_PROV_PROFILE.md`
- `docs/governance/ETHICS.md`
- `docs/governance/SOVEREIGNTY.md`

> If any of these paths differ in your current repo layout, update this section to match the canonical home.
